{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49n2AC6spxQS"
      },
      "source": [
        "# üé® **AEIONic Diffusion Deluxe Edition** üë®‚Äçüé®Ô∏è - Full Featured Fancy Flet/Flutter Framework\n",
        "\n",
        "*...using `üß®diffusers`* and many advanced bonus features...\n",
        "\n",
        "---\n",
        "### Designed by [**Skquark**, Inc.](https://www.Skquark.com) üòã - Project at [AEIONic.com](https://AEIONic.com)\n",
        "<p align=center>\n",
        "<a href=\"https://github.com/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb\"><img src=\"https://badgen.net/badge/icon/github?icon=github&label\" alt=\"Github\"></a> <a href=\"https://github.com/Skquark/AEIONic\"><img src=\"https://badgen.net/github/release/Skquark/AEIONic/stable\" alt=\"Release version\"></a>\n",
        "<a href=\"https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb\"><img src=\"https://img.shields.io/badge/Open-in%20Colab-brightgreen?logo=google-colab&style=flat-square\" alt=\"Open in Google Colab\"/></a>\n",
        "</p>\n",
        "\n",
        "*   Runs in a pretty WebUI using [Flet - Flutter for Python](https://flet.dev) with themes, interactivity & sound\n",
        "*   Saves all settings/parameters in your config file, don't need to Copy to Drive\n",
        "*   Run a batch list of prompts at once, so queue many and walk away\n",
        "*   Option to override any parameter per prompt in queue\n",
        "*   Use Stable Diffusion [XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9), [2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1), [2.0](https://huggingface.co/stabilityai/stable-diffusion-2), [1.5 ](https://huggingface.co/runwayml/stable-diffusion-v1-5), or [1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4) Checkpoint Model File\n",
        "*   Supports many custom community Finetuned Model checkpoints & DreamBooth Libraries\n",
        "*   Supports Stable Diffusion image2image to use an init_image\n",
        "*   Supports Stable Diffusion [Inpaint](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) mask_image layer\n",
        "*   Supports Negative Prompts to specify what you don't want\n",
        "*   Supports Long Prompt Weighting to emphasize (positive) & [negative] word strengths\n",
        "*   Prompt tweening to combine latent space of 2 prompts in a series\n",
        "*   Can use Interpolation to walk steps between latent space of prompt list\n",
        "*   Can use CLIP Guidance with LAION & OpenAI ViT models\n",
        "*   Can use Textual Inversion Conceptualizer with 760+ Community Concepts\n",
        "*   Can Centipede prompts as init images feeding down the list\n",
        "*   Can use Composable weighted | prompt | segments for img2img\n",
        "*   Can use iMagic to edit init image with your prompt \n",
        "*   Can use Kakaobrain unCLIP Generator, which gives similar results to DALL-E 2\n",
        "*   Can save all images to your Google Drive (PyDrive support soon)\n",
        "*   Can Upscale automatically with Real-ESRGAN enlarging\n",
        "*   Option to use Stability-API tokens for more samplers, bigger size & CPU runtime\n",
        "*   Embeds exif metadata directly into png files\n",
        "*   Disabled NSFW filtering and added custom sampler options\n",
        "*   Renames image filenames to the prompt text, with options\n",
        "*   OpenAI Prompt Generator, Remixer, Brainstormer & Noodle Soup Prompt Writer included\n",
        "*   Standalone ESRGAN Upscaler for batch uploads and image splitting\n",
        "*   Train your own models with DreamBooth, LoRA or Textual-Inversion & upload to HuggingFace\n",
        "*   Replicate Material Diffusion to make Seamless Tile Textures\n",
        "*   Experimental HarmonAI Dance Diffusion audio generator\n",
        "*   Experimental DreamFusion 3D model generator with texture & video\n",
        "*   Image2Text CLIP-Interrogator to get prompt from images\n",
        "*   Bonus Image Generators DALL-E 2 and Kandinsky 2 to compare\n",
        "*   Additional features added regularly...\n",
        "\n",
        "If you have a powerful enough GPU with > 8GB VRAM, you can run Desktop app locally with [AEIONic-setup.exe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic-Diffusion-Deluxe/AEIONic-setup.exe) or [aeionic-linux.sh](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic-Diffusion-Deluxe/aeionic-linux.sh). Still in beta...\n",
        "\n",
        "Can also use origional Colab implementation of [Enhanced Stable Diffusion](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/Enhanced_Stable_Diffusion_with_diffusers.ipynb) instead..\n",
        "\n",
        "Try these other useful notebooks [Enhanced DiscoArt](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/DiscoArt_%5B_w_Batch_Prompts_%26_GPT_3_Generator%5D.ipynb) and [Structured Prompt Generator](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/Structured_Prompt_Generator.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jf90tMc3tu9q"
      },
      "outputs": [],
      "source": [
        "#@title üñ•Ô∏è Check GPU Status (A100 > G100 > V100 > P100 > T4 > K8)\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = True #@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    print(subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "else:\n",
        "    print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EH3ClNnvV1S"
      },
      "source": [
        "# üß∞ **AEIONic Diffusion Deluxe** - Easy Web App Launcher\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KTwZrpxEuKR0"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚öôÔ∏è Install WebUI Framework & Initiallize Settings\n",
        "#markdown If you're running on Google Colab, authorize Google Drive with the popup to connect storage. If you're on a local or cloud Jupyter Notebook, you must get a OAuth json using instructions below to save images to GDrive, however the feature is not currently working and in progress. When set, continue to run the Web UI and experiment away..\n",
        "#@markdown We'll connect to your Google Drive and save all of your preferences in realtime there, as well as your created images. This is the folder location and file name we recommend in your mounted gdrive, will be created if you're new, but you can save elsewhere.  Launches webpage with localtunnel, but in case it's down you can use ngrok instead.\n",
        "storage_type = \"Colab Google Drive\" #@param [\"Colab Google Drive\", \"Local Drive\"]\n",
        "#, \"PyDrive Google Drive\"\n",
        "Google_OAuth_client_secret_json = \"/content/client_secrets.json\" #param {'type': 'string'}\n",
        "save_to_GDrive = True #param {'type': 'boolean'}\n",
        "saved_settings_json = '/content/drive/MyDrive/AI/Diffusion_out/aeionic-settings.json' #@param {'type': 'string'}\n",
        "tunnel_type = \"localtunnel\" #@param [\"localtunnel\", \"ngrok\"]\n",
        "#, \"cloudflared\"\n",
        "auto_launch_website = False #@param {'type': 'boolean'}\n",
        "force_updates = True\n",
        "newest_flet = True\n",
        "AEIONic_version = \"v2.0.4\"\n",
        "import os, subprocess, sys, shutil, re\n",
        "import random as rnd\n",
        "from IPython.display import clear_output\n",
        "root_dir = '/content/'\n",
        "dist_dir = root_dir\n",
        "is_Colab = True\n",
        "try:\n",
        "  import google.colab\n",
        "  root_dir = '/content/'\n",
        "except:\n",
        "  root_dir = os.getcwd()\n",
        "  dist_dir = os.path.join(root_dir, 'dist', 'AEIONic-Diffusion-Deluxe')\n",
        "  if not os.path.isdir(dist_dir):\n",
        "    dist_dir = root_dir\n",
        "  print(f'Root: {root_dir} Dist:{dist_dir}')\n",
        "  is_Colab = False\n",
        "  pass\n",
        "stable_dir = root_dir\n",
        "env = os.environ.copy()\n",
        "def run_sp(cmd_str, cwd=None, realtime=False, output_column=None):\n",
        "  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()\n",
        "  if realtime or output_column != None:\n",
        "    if cwd is None:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "    else:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "    while True:\n",
        "      realtime_output = process.stdout.readline()\n",
        "      if realtime_output == '' and process.poll() is not None:\n",
        "        break\n",
        "      if realtime_output:\n",
        "        if not output_column:\n",
        "            print(realtime_output.strip(), flush=False)\n",
        "        else:\n",
        "            output_column.controls.append(ft.Text(realtime_output.strip()))\n",
        "            output_column.update()\n",
        "        sys.stdout.flush()\n",
        "  else:\n",
        "    if cwd is None:\n",
        "      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')\n",
        "    else:\n",
        "      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')\n",
        "save_to_GDrive = storage_type == \"Colab Google Drive\"\n",
        "if save_to_GDrive:\n",
        "  if not os.path.isdir(f'{root_dir}drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "elif storage_type == \"PyDrive Google Drive\":\n",
        "  \"pip install PyDrive2\"\n",
        "stable_dir = os.path.join(root_dir, 'Stable_Diffusion')\n",
        "if not os.path.exists(stable_dir):\n",
        "  os.makedirs(stable_dir)\n",
        "sample_data = '/content/sample_data'\n",
        "if os.path.exists(sample_data):\n",
        "  for f in os.listdir(sample_data):\n",
        "    os.remove(os.path.join(sample_data, f))\n",
        "  os.rmdir(sample_data)\n",
        "os.chdir(stable_dir)\n",
        "#loaded_Stability_api = False\n",
        "#loaded_img2img = False\n",
        "#use_Stability_api = False\n",
        "def version_checker():\n",
        "  response = requests.get(\"https://raw.githubusercontent.com/Skquark/AEIONic/main/aeionic_version.txt\")\n",
        "  current_v = response.text.strip()\n",
        "  if current_v != AEIONic_version:\n",
        "    print(f'A new update is available. You are running {AEIONic_version} and {current_v} is up. We recommended refreshing AEIONic Diffusion Deluxe for the latest cool features or fixes.\\nhttps://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb\\nChangelog if interested: https://github.com/Skquark/AEIONic/commits/main/AEIONic_Diffusion_Deluxe.ipynb')\n",
        "def ng():\n",
        "  response = requests.get(\"https://raw.githubusercontent.com/Skquark/AEIONic/main/_ng\")\n",
        "  ng_list = response.text.strip().split('\\n')\n",
        "  _ng = rnd.choice(ng_list).partition('_')\n",
        "  return _ng[2]+_ng[1]+_ng[0]\n",
        "\n",
        "def download_file(url, to=None, filename=None, raw=True, ext=\"png\", replace=False):\n",
        "    if filename != None:\n",
        "        local_filename = filename\n",
        "    else:\n",
        "        local_filename = url.split('/')[-1]\n",
        "        if '?' in local_filename:\n",
        "            local_filename = local_filename.rpartition('?')[0]\n",
        "    if '.' not in local_filename:\n",
        "        local_filename += f\".{ext}\"\n",
        "    local_filename = os.path.join(to if to != None else root_dir, local_filename)\n",
        "    if to != None:\n",
        "        if not os.path.exists(to):\n",
        "            os.makedirs(to)\n",
        "    else: to = root_dir\n",
        "    if os.path.isfile(local_filename) and not replace:\n",
        "        return local_filename\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            if raw:\n",
        "              shutil.copyfileobj(r.raw, f)\n",
        "            else:\n",
        "              f.write(r.content)\n",
        "    return local_filename\n",
        "def wget(url, to):\n",
        "    res = subprocess.run(['wget', '-q', url, '-O', to], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "try:\n",
        "  import flet\n",
        "except ImportError as e:\n",
        "  if newest_flet:\n",
        "    run_sp(\"pip install --upgrade flet\", realtime=False)\n",
        "    #0.3.0, 0.3.1, 0.3.2, 0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.5.1, 0.5.2, 0.6.0.dev1309, 0.6.0.dev1310, 0.6.0.dev1312, 0.6.0.dev1322, 0.6.0.dev1336, 0.6.0.dev1344, 0.6.0.dev1350, 0.6.0.dev1352, 0.6.0.dev1355, 0.6.0.dev1357, 0.6.0, 0.6.1, 0.6.2, 0.7.0.dev1359, 0.7.0.dev1361, 0.7.0.dev1363, 0.7.0.dev1365, 0.7.0.dev1372, 0.7.0.dev1395, 0.7.0.dev1397, 0.7.0.dev1398, 0.7.0.dev1399, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.8.0.dev1402, 0.8.0.dev1406, 0.8.0.dev1425, 0.8.0.dev1436, 0.8.0.dev1441, 0.8.0.dev1443, 0.8.0.dev1459\n",
        "  else:\n",
        "    run_sp(\"pip install --upgrade flet==0.3.2\", realtime=False)\n",
        "  #run_sp(\"pip install -i https://test.pypi.org/simple/ flet\")\n",
        "  #run_sp(\"pip install --upgrade git+https://github.com/flet-dev/flet.git@controls-s3#egg=flet-dev\")\n",
        "  #run_sp(\"pip install --upgrade flet_ivid\")\n",
        "  pass\n",
        "'''try:\n",
        "  from flet_ivid import VideoContainer\n",
        "except ModuleNotFoundError:\n",
        "  run_sp(\"pip install --upgrade flet_ivid\")\n",
        "  from flet_ivid import VideoContainer\n",
        "  pass'''\n",
        "try:\n",
        "  import requests\n",
        "except ModuleNotFoundError:\n",
        "  run_sp(\"pip install -q requests\", realtime=True)\n",
        "  import requests\n",
        "  pass\n",
        "try:\n",
        "  from emoji import emojize\n",
        "except ImportError as e:\n",
        "  run_sp(\"pip install emoji --quiet\", realtime=False)\n",
        "  from emoji import emojize\n",
        "  pass\n",
        "if 'url' not in locals():\n",
        "  url=\"\"\n",
        "if tunnel_type == \"ngrok\":\n",
        "  try:\n",
        "    import pyngrok\n",
        "  except ImportError as e:\n",
        "    run_sp(\"pip install pyngrok --quiet\", realtime=False)\n",
        "    #run_sp(f\"ngrok authtoken {ng()}\", realtime=False)\n",
        "    run_sp(f\"ngrok config add-authtoken {ng()}\", realtime=False)\n",
        "    run_sp(\"ngrok config upgrade\", realtime=False)\n",
        "    import pyngrok\n",
        "    pass\n",
        "elif tunnel_type == \"localtunnel\":\n",
        "  if not bool(url):\n",
        "    import re\n",
        "    run_sp(\"npm install -g -q localtunnel\", realtime=False)\n",
        "    #localtunnel = subprocess.Popen(['lt', '--port', '80', 'http'], stdout=subprocess.PIPE)\n",
        "    #url = str(localtunnel.stdout.readline())\n",
        "    #url = (re.search(\"(?P<url>https?:\\/\\/[^\\s]+loca.lt)\", url).group(\"url\"))\n",
        "    #print(url)\n",
        "\n",
        "gdrive = None\n",
        "if storage_type == \"PyDrive Google Drive\":\n",
        "  if not os.path.isfile(Google_OAuth_client_secret_json):\n",
        "    raise ValueError(\"Couldn't locate your client_secret.json file to authenticate. Follow instructions below then copy certificate to your root dir.\")\n",
        "  try:\n",
        "    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "    from oauth2client.contrib.gce import AppAssertionCredentials\n",
        "  except ImportError as e:\n",
        "    run_sp(\"pip install PyDrive2 -q\")\n",
        "    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "    from oauth2client.contrib.gce import AppAssertionCredentials\n",
        "    pass\n",
        "  import httplib2\n",
        "\n",
        "  old_local_webserver_auth = GoogleAuth.LocalWebserverAuth\n",
        "  def LocalWebServerAuth(self, *args, **kwargs):\n",
        "      if isinstance(self.credentials, AppAssertionCredentials):\n",
        "          self.credentials.refresh(httplib2.Http())\n",
        "          return\n",
        "      return old_local_webserver_auth(self, *args, **kwargs)\n",
        "  GoogleAuth.LocalWebserverAuth = LocalWebServerAuth\n",
        "\n",
        "  #scope = 'https://www.googleapis.com/auth/drive'\n",
        "  #credentials = ServiceAccountCredentials.from_json_keyfile_name(Google_OAuth_client_secret_json, scope)\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.LoadCredentialsFile(Google_OAuth_client_secret_json)\n",
        "  #gauth.LocalWebserverAuth()\n",
        "  if is_Colab: gauth.CommandLineAuth()\n",
        "  else:\n",
        "    gauth.LocalWebserverAuth()\n",
        "    gauth.SaveCredentialsFile(Google_OAuth_client_secret_json)\n",
        "  gdrive = GoogleDrive(gauth)\n",
        "slash = '/'\n",
        "from pathlib import Path\n",
        "if not is_Colab:\n",
        "    image_output = os.path.join(Path.home(), \"Pictures\", \"Diffusion_out\")\n",
        "    if \"\\\\\" in image_output:\n",
        "        slash = '\\\\'\n",
        "else:\n",
        "    image_output = '/content/drive/MyDrive/AI/Diffusion_out/images_out'\n",
        "\n",
        "favicon = os.path.join(root_dir, \"favicon.png\")\n",
        "loading_animation = os.path.join(root_dir, \"loading_animation.png\")\n",
        "assets = os.path.join(root_dir, \"assets\")\n",
        "if not os.path.isfile(favicon):\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/favicon.png?raw=true\")\n",
        "if not os.path.isfile(loading_animation):\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/loading_animation.png?raw=true\")\n",
        "if not os.path.exists(assets):\n",
        "    os.makedirs(assets)\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-alert.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-delete.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-error.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-done.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-drop.mp3?raw=true\", to=assets)\n",
        "uploads_dir = os.path.join(root_dir, \"uploads\")\n",
        "if not os.path.exists(uploads_dir):\n",
        "    os.makedirs(uploads_dir)\n",
        "#clear_output()\n",
        "\n",
        "import json\n",
        "prefs = {}\n",
        "def load_settings_file():\n",
        "  global prefs\n",
        "  if os.path.isfile(saved_settings_json):\n",
        "    with open(saved_settings_json) as settings:\n",
        "      prefs = json.load(settings)\n",
        "    print(\"Successfully loaded settings json...\")\n",
        "  else:\n",
        "    print(\"Settings file not found, starting with defaults...\")\n",
        "    prefs = {\n",
        "      'save_to_GDrive': True,\n",
        "      'image_output': image_output,\n",
        "      'file_prefix': 'sd-',\n",
        "      'file_suffix_seed': False,\n",
        "      'file_max_length': 220,\n",
        "      'file_allowSpace': False,\n",
        "      'file_datetime': False,\n",
        "      'save_image_metadata': True,\n",
        "      'meta_ArtistName':'',\n",
        "      'meta_Copyright': '',\n",
        "      'save_config_in_metadata': True,\n",
        "      'save_config_json': False,\n",
        "      'theme_mode': 'Dark',\n",
        "      'theme_color': 'Green',\n",
        "      'enable_sounds': True,\n",
        "      'show_stats': False,\n",
        "      'stats_used': True,\n",
        "      'stats_update': 5,\n",
        "      'start_in_installation': False,\n",
        "      'disable_nsfw_filter': True,\n",
        "      'retry_attempts': 3,\n",
        "      'HuggingFace_api_key': \"\",\n",
        "      'Stability_api_key': \"\",\n",
        "      'OpenAI_api_key': \"\",\n",
        "      'PaLM_api_key': \"\",\n",
        "      'TextSynth_api_key': \"\",\n",
        "      'Replicate_api_key': \"\",\n",
        "      'AIHorde_api_key': \"0000000000\",\n",
        "      'luma_api_key': \"\",\n",
        "      'HuggingFace_username': \"\",\n",
        "      'scheduler_mode': \"DDIM\",\n",
        "      'higher_vram_mode': False,\n",
        "      'enable_xformers': False,\n",
        "      'enable_attention_slicing': True,\n",
        "      'enable_bitsandbytes': False,\n",
        "      'memory_optimization': \"None\",\n",
        "      'sequential_cpu_offload': False,\n",
        "      'vae_slicing': True,\n",
        "      'vae_tiling': False,\n",
        "      'enable_torch_compile': False,\n",
        "      'enable_stable_fast': False,\n",
        "      'enable_tome': False,\n",
        "      'tome_ratio': 0.5,\n",
        "      'enable_freeu': False,\n",
        "      'freeu_args': {'b1': 1.2, 'b2':1.4, 's1':0.9, 's2':0.2},\n",
        "      'enable_deepcache': False,\n",
        "      'cache_dir': '',\n",
        "      'install_diffusers': True,\n",
        "      'install_interpolation': False,\n",
        "      'install_text2img': True,\n",
        "      'install_img2img': False,\n",
        "      'install_SDXL': False,\n",
        "      'install_megapipe': True,\n",
        "      'install_CLIP_guided': False,\n",
        "      'install_OpenAI': False,\n",
        "      'install_TextSynth': False,\n",
        "      'install_dreamfusion': False,\n",
        "      'install_repaint': False,\n",
        "      'install_imagic': False,\n",
        "      'install_composable': False,\n",
        "      'install_safe': False,\n",
        "      'install_versatile': False,\n",
        "      'install_depth2img': False,\n",
        "      'install_alt_diffusion': False,\n",
        "      'install_attend_and_excite': False,\n",
        "      'install_SAG': False,\n",
        "      'install_panorama': False,\n",
        "      'install_upscale': False,\n",
        "      'upscale_method': 'Real-ESRGAN',\n",
        "      'upscale_model': 'RealESRGAN_x4plus',\n",
        "      'safety_config': 'Strong',\n",
        "      'use_imagic': False,\n",
        "      'SD_compel': False,\n",
        "      'use_SDXL': False,\n",
        "      'SDXL_high_noise_frac': 0.7,\n",
        "      'SDXL_negative_conditions': False,\n",
        "      'SDXL_compel': False,\n",
        "      'SDXL_watermark': False,\n",
        "      'SDXL_model': 'SDXL-Base v1',\n",
        "      'SDXL_custom_model': '',\n",
        "      'SDXL_custom_models': [],\n",
        "      'use_composable': False,\n",
        "      'use_safe': False,\n",
        "      'use_versatile': False,\n",
        "      'use_alt_diffusion': False,\n",
        "      'use_attend_and_excite': False,\n",
        "      'max_iter_to_alter': 25,\n",
        "      'use_SAG': False,\n",
        "      'sag_scale': 0.75,\n",
        "      'use_panorama': False,\n",
        "      'panorama_width': 2048,\n",
        "      'use_upscale': False,\n",
        "      'upscale_noise_level': 20,\n",
        "      'install_conceptualizer': False,\n",
        "      'use_conceptualizer': False,\n",
        "      'concepts_model': 'cat-toy',\n",
        "      'use_ip_adapter': False,\n",
        "      'ip_adapter_image': '',\n",
        "      'ip_adapter_model': 'SD v1.5',\n",
        "      'ip_adapter_SDXL_model': 'SDXL',\n",
        "      'ip_adapter_strength': 0.8,\n",
        "      'model_ckpt': 'Stable Diffusion v1.5',\n",
        "      'finetuned_model': 'Midjourney v4 style',\n",
        "      'dreambooth_model': 'disco-diffusion-style',\n",
        "      'custom_model': '',\n",
        "      'custom_models': [],\n",
        "      'tortoise_custom_voices': [],\n",
        "      'custom_dance_diffusion_models': [],\n",
        "      'clip_model_id': \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\",\n",
        "      'install_Stability_api': False,\n",
        "      'use_Stability_api': False,\n",
        "      'model_checkpoint': \"stable-diffusion-768-v2-1\",\n",
        "      'generation_sampler': \"K_EULER_ANCESTRAL\",\n",
        "      'clip_guidance_preset': \"FAST_BLUE\",\n",
        "      'install_AIHorde_api': False,\n",
        "      'use_AIHorde_api': False,\n",
        "      'AIHorde_model': 'stable_diffusion',\n",
        "      'AIHorde_sampler': 'k_euler_a',\n",
        "      'AIHorde_post_processing': \"None\",\n",
        "      'install_ESRGAN': True,\n",
        "      'batch_folder_name': \"\",\n",
        "      'batch_size': 1,\n",
        "      'n_iterations': 1,\n",
        "      'steps': 50,\n",
        "      'eta': 0.4,\n",
        "      'seed': 0,\n",
        "      'guidance_scale': 8,\n",
        "      'width': 960,\n",
        "      'height': 512,\n",
        "      'init_image': \"\",\n",
        "      'mask_image': \"\",\n",
        "      'init_image_strength': 0.25,\n",
        "      'alpha_mask': False,\n",
        "      'invert_mask': False,\n",
        "      'negative_prompt': \"\",\n",
        "      'precision': 'autocast',\n",
        "      'use_inpaint_model': False,\n",
        "      'centipede_prompts_as_init_images': False,\n",
        "      'use_depth2img': False,\n",
        "      'use_LoRA_model': False,\n",
        "      'LoRA_model': 'Von Platen LoRA',\n",
        "      'active_LoRA_layers': [],\n",
        "      'active_SDXL_LoRA_layers': [],\n",
        "      'custom_LoRA_models': [],\n",
        "      'custom_LoRA_model': \"\",\n",
        "      'SDXL_LoRA_model': 'Papercut SDXL',\n",
        "      'custom_SDXL_LoRA_models': [],\n",
        "      'custom_SDXL_LoRA_model': \"\",\n",
        "      'use_interpolation': False,\n",
        "      'num_interpolation_steps': 22,\n",
        "      'use_clip_guided_model': False,\n",
        "      'clip_guidance_scale': 571,\n",
        "      'use_cutouts': True,\n",
        "      'num_cutouts': 4,\n",
        "      'unfreeze_unet': True,\n",
        "      'unfreeze_vae': True,\n",
        "      'apply_ESRGAN_upscale': True,\n",
        "      'enlarge_scale': 1.5,\n",
        "      'face_enhance':False,\n",
        "      'display_upscaled_image': False,\n",
        "      'negatives': [\"Blurry\"],\n",
        "      'custom_negatives': \"\",\n",
        "      'prompt_styler': '',\n",
        "      'prompt_style': 'cinematic-default',\n",
        "      'prompt_styles': ['cinematic-default'],\n",
        "      'prompt_styler_multi': False,\n",
        "      'prompt_list': [],\n",
        "      'prompt_generator': {\n",
        "          'phrase': '',\n",
        "          'subject_detail': '',\n",
        "          'phrase_as_subject': False,\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "          'request_mode': 3,\n",
        "          'AI_temperature': 0.9,\n",
        "          'AI_engine': \"ChatGPT-3.5 Turbo\",\n",
        "          'economy_mode': True,\n",
        "      },\n",
        "      'prompt_remixer': {\n",
        "          'seed_prompt': '',\n",
        "          'optional_about_influencer': '',\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "          'request_mode': 3,\n",
        "          'AI_temperature': 0.9,\n",
        "          'AI_engine': \"ChatGPT-3.5 Turbo\",\n",
        "      },\n",
        "      'prompt_brainstormer': {\n",
        "          'AI_engine': 'ChatGPT-3.5 Turbo',\n",
        "          'about_prompt': '',\n",
        "          'request_mode': 'Brainstorm',\n",
        "          'AI_temperature': 0.9,\n",
        "      },\n",
        "      'prompt_writer': {\n",
        "          'art_Subjects': '',\n",
        "          'negative_prompt': '',\n",
        "          'by_Artists': '',\n",
        "          'art_Styles': '',\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "      },\n",
        "    }\n",
        "\n",
        "load_settings_file()\n",
        "version_checker()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SV5d8rB5uYZC"
      },
      "outputs": [],
      "source": [
        "#@title ## **‚ñ∂Ô∏è Run AEIONic Diffusion Deluxe** - Flet/Flutter WebUI App\n",
        "import flet as ft\n",
        "#from flet import *\n",
        "import flet.canvas as cv\n",
        "from flet import Page, View, Column, Row, ResponsiveRow, Container, Text, Stack, TextField, Checkbox, Switch, Image, ElevatedButton, FilledButton, IconButton, Markdown, Tab, Tabs, AppBar, Divider, VerticalDivider, GridView, Tooltip, SnackBar, AnimatedSwitcher, ButtonStyle, FloatingActionButton, Audio, Theme, Dropdown, Slider, ListTile, ListView, TextButton, PopupMenuButton, PopupMenuItem, AlertDialog, Banner, Icon, ProgressBar, ProgressRing, GestureDetector, KeyboardEvent, FilePicker, FilePickerResultEvent, FilePickerUploadFile, FilePickerUploadEvent, UserControl, Ref\n",
        "from flet import icons, dropdown, colors, padding, margin, alignment, border_radius, theme, animation, KeyboardType, TextThemeStyle, AnimationCurve\n",
        "from flet import TextAlign, FontWeight, ClipBehavior, MainAxisAlignment, CrossAxisAlignment, ScrollMode, ImageFit, ThemeMode, BlendMode\n",
        "from flet import Image as Img\n",
        "try:\n",
        "    import PIL\n",
        "except ModuleNotFoundError:\n",
        "    run_sp(\"pip install Pillow\", realtime=False)\n",
        "    run_sp(\"pip install image\", realtime=False)\n",
        "    import PIL\n",
        "    pass\n",
        "from PIL import Image as PILImage # Avoids flet conflict\n",
        "if not hasattr(PILImage, 'Resampling'):  # Allow Pillow<9.0\n",
        "   PILImage.Resampling = PILImage\n",
        "import random as rnd\n",
        "import io, shutil, traceback, string, gc, datetime, time, threading\n",
        "from packaging import version\n",
        "from contextlib import redirect_stdout\n",
        "try:\n",
        "  import numpy as np\n",
        "except ModuleNotFoundError:\n",
        "  run_sp(\"pip install numpy\", realtime=False)\n",
        "  import numpy as np\n",
        "  pass\n",
        "try:\n",
        "    import psutil\n",
        "except ModuleNotFoundError:\n",
        "    run_sp(\"pip install -U psutil\", realtime=False)\n",
        "    import psutil\n",
        "    pass\n",
        "try:\n",
        "    import cv2\n",
        "except ModuleNotFoundError:\n",
        "    run_sp(\"pip install opencv-python\", realtime=False)\n",
        "    import cv2\n",
        "    pass\n",
        "\n",
        "if 'prefs' not in locals():\n",
        "    raise ValueError(\"Setup not initialized. Run the previous code block first and authenticate your Drive storage.\")\n",
        "status = {\n",
        "    'installed_diffusers': False,\n",
        "    'installed_txt2img': False,\n",
        "    'installed_img2img': False,\n",
        "    'installed_SDXL': False,\n",
        "    'installed_stability': False,\n",
        "    'installed_AIHorde': False,\n",
        "    'installed_megapipe': False,\n",
        "    'installed_interpolation': False,\n",
        "    'installed_clip': False,\n",
        "    'installed_ESRGAN': False,\n",
        "    'installed_OpenAI': False,\n",
        "    'installed_TextSynth': False,\n",
        "    'installed_conceptualizer': False,\n",
        "    'installed_dreamfusion': False,\n",
        "    'installed_repaint': False,\n",
        "    'installed_imagic': False,\n",
        "    'installed_composable': False,\n",
        "    'installed_safe': False,\n",
        "    'installed_versatile': False,\n",
        "    'installed_depth2img': False,\n",
        "    'installed_attend_and_excite': False,\n",
        "    'installed_SAG': False,\n",
        "    'installed_panorama': False,\n",
        "    'installed_alt_diffusion': False,\n",
        "    'installed_upscale': False,\n",
        "    'installed_xformers': False,\n",
        "    'finetuned_model': False,\n",
        "    'loaded_scheduler': '',\n",
        "    'loaded_model': '',\n",
        "    'loaded_task': '',\n",
        "    'loaded_controlnet': '',\n",
        "    'loaded_controlnet_type': '',\n",
        "    'loaded_SDXL': '',\n",
        "    'loaded_SDXL_model': '',\n",
        "    'loaded_ip_adapter_mode': '',\n",
        "    'loaded_ip_adapter': '',\n",
        "    'changed_settings': False,\n",
        "    'changed_installers': False,\n",
        "    'changed_parameters': False,\n",
        "    'changed_prompts': False,\n",
        "    'changed_prompt_generator': False,\n",
        "    'changed_prompt_remixer': False,\n",
        "    'changed_prompt_brainstormer': False,\n",
        "    'changed_prompt_writer': False,\n",
        "    'kandinsky_3': 'Kandinsky 3.0',\n",
        "    'kandinsky_version': 'Kandinsky 3.0',\n",
        "    'kandinsky_2_2': True,\n",
        "    'kandinsky_fuse_2_2': True,\n",
        "    'initialized': False,\n",
        "}\n",
        "\n",
        "if 'last_updated' in prefs:\n",
        "    last_time = datetime.datetime.strptime(prefs['last_updated'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    #diff = datetime.datetime.now() - last_time\n",
        "    if last_time <  datetime.datetime.now() - datetime.timedelta(days=4):\n",
        "        force_updates = True\n",
        "else:\n",
        "    force_updates = True\n",
        "\n",
        "aeionic_utils_py = os.path.join(root_dir, \"aeionic_utils.py\")\n",
        "aeionic_components_py = os.path.join(root_dir, \"aeionic_components.py\")\n",
        "if not os.path.exists(aeionic_utils_py) or force_updates:\n",
        "    download_file(\"https://raw.githubusercontent.com/Skquark/AEIONic/main/aeionic_utils.py\", to=root_dir, raw=False, replace=True)\n",
        "if not os.path.exists(aeionic_components_py) or force_updates:\n",
        "    download_file(\"https://raw.githubusercontent.com/Skquark/AEIONic/main/aeionic_components.py\", to=root_dir, raw=False, replace=True)\n",
        "#sys.path.append(aeionic_utils_py)\n",
        "import aeionic_utils\n",
        "from aeionic_utils import LoRA_models, SDXL_models, SDXL_LoRA_models, finetuned_models, dreambooth_models, styles, artists, concepts, Real_ESRGAN_models, SwinIR_models, SD_XL_BASE_RATIOS\n",
        "import aeionic_components\n",
        "from aeionic_components import PanZoom, VideoContainer\n",
        "\n",
        "def save_settings_file(page, change_icon=True):\n",
        "    if change_icon:\n",
        "        page.app_icon_save()\n",
        "    if not os.path.isfile(saved_settings_json):\n",
        "        settings_path = saved_settings_json.rpartition(slash)[0]\n",
        "        os.makedirs(settings_path, exist_ok=True)\n",
        "    with open(saved_settings_json, \"w\") as write_file:\n",
        "        json.dump(prefs, write_file, indent=4)\n",
        "\n",
        "current_tab = 0\n",
        "def tab_on_change (e):\n",
        "    global current_tab, status\n",
        "    t = e.control\n",
        "    #print (f\"tab changed from {current_tab} to: {t.selected_index}\")\n",
        "    #print(str(t.tabs[t.selected_index].text))\n",
        "    if current_tab == 0:\n",
        "      #if not status['initialized']:\n",
        "      #  initState(e.page)\n",
        "      #  status['initialized'] = True\n",
        "      if status['changed_settings']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_settings'] = False\n",
        "        #print(len(e.page.Settings.content.controls))\n",
        "        #save_settings(e.page.Settings.content.controls)\n",
        "    if current_tab == 1:\n",
        "      if status['changed_installers']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_installers'] = False\n",
        "        #print(\"Saving Installers\")\n",
        "      e.page.show_install_fab(False)\n",
        "    if current_tab == 2:\n",
        "      if status['changed_parameters']:\n",
        "        update_args()\n",
        "        e.page.update_prompts()\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_parameters'] = False\n",
        "      e.page.show_apply_fab(False)\n",
        "    if current_tab == 3:\n",
        "      if status['changed_prompts']:\n",
        "        e.page.save_prompts()\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_prompts'] = False\n",
        "      e.page.show_run_diffusion_fab(False, p=e.page)\n",
        "    if current_tab == 5:\n",
        "      if status['changed_prompt_generator']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_prompt_generator'] = False\n",
        "\n",
        "    current_tab = t.selected_index\n",
        "    if current_tab == 1:\n",
        "      refresh_installers(e.page.Installers.controls[0].content.controls)\n",
        "      e.page.show_install_fab(True)\n",
        "      #page.Installers.init_boxes()\n",
        "    if current_tab == 2:\n",
        "      update_parameters(e.page)\n",
        "      #for p in e.page.Parameters.content.controls:\n",
        "      e.page.Parameters.controls[0].content.update()\n",
        "      e.page.Parameters.update()\n",
        "      e.page.show_apply_fab(len(prompts) > 0 and status['changed_parameters'])\n",
        "    if current_tab == 3:\n",
        "      e.page.show_run_diffusion_fab(True, p=e.page) #len(prompts) > 0\n",
        "    e.page.update()\n",
        "\n",
        "def buildTabs(page):\n",
        "    page.Settings = buildSettings(page)\n",
        "    page.Installers = buildInstallers(page)\n",
        "    page.Parameters = buildParameters(page)\n",
        "    page.PromptsList = buildPromptsList(page)\n",
        "    page.PromptHelpers = buildPromptHelpers(page)\n",
        "    page.Images = buildImages(page)\n",
        "    page.ImageAIs = buildImageAIs(page)\n",
        "    page.Trainers = buildTrainers(page)\n",
        "    page.VideoAIs = buildVideoAIs(page)\n",
        "    page.AudioAIs = buildAudioAIs(page)\n",
        "    page.Text3DAIs = build3DAIs(page)\n",
        "    page.Extras = buildExtras(page)\n",
        "\n",
        "    t = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Settings\", content=page.Settings, icon=icons.SETTINGS_OUTLINED),\n",
        "            Tab(text=\"Installation\", content=page.Installers, icon=icons.INSTALL_DESKTOP),\n",
        "            Tab(text=\"Image Parameters\", content=page.Parameters, icon=icons.DISPLAY_SETTINGS),\n",
        "            Tab(text=\"Prompts List\", content=page.PromptsList, icon=icons.FORMAT_LIST_BULLETED),\n",
        "            Tab(text=\"Generate Images\", content=page.Images, icon=icons.IMAGE_OUTLINED),\n",
        "            Tab(text=\"Prompt Helpers\", content=page.PromptHelpers, icon=icons.BUBBLE_CHART_OUTLINED),\n",
        "            Tab(text=\"Image AIs\", content=page.ImageAIs, icon=icons.PALETTE),\n",
        "            Tab(text=\"Video AIs\", content=page.VideoAIs, icon=icons.VIDEO_CAMERA_BACK),\n",
        "            Tab(text=\"3D AIs\", content=page.Text3DAIs, icon=icons.VIEW_IN_AR),\n",
        "            Tab(text=\"Audio AIs\", content=page.AudioAIs, icon=icons.EQUALIZER),\n",
        "            Tab(text=\"AI Trainers\", content=page.Trainers, icon=icons.TSUNAMI),\n",
        "            Tab(text=\"Extras\", content=page.Extras, icon=icons.ALL_INBOX),\n",
        "        ],\n",
        "    )\n",
        "    page.tabs = t\n",
        "    return t\n",
        "\n",
        "def buildPromptHelpers(page):\n",
        "    page.generator = buildPromptGenerator(page)\n",
        "    page.remixer = buildPromptRemixer(page)\n",
        "    page.brainstormer = buildPromptBrainstormer(page)\n",
        "    page.writer = buildPromptWriter(page)\n",
        "    page.negatives = buildNegatives(page)\n",
        "    page.styler = buildPromptStyler(page)\n",
        "    page.Image2Text = buildImage2Text(page)\n",
        "    page.MagicPrompt = buildMagicPrompt(page)\n",
        "    page.DistilGPT2 = buildDistilGPT2(page)\n",
        "    page.RetrievePrompts = buildRetrievePrompts(page)\n",
        "    page.InitFolder = buildInitFolder(page)\n",
        "    page.InitVideo = buildInitVideo(page)\n",
        "    page.BLIP2Image2Text = buildBLIP2Image2Text(page)\n",
        "    promptTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Prompt Writer\", content=page.writer, icon=icons.CLOUD_CIRCLE),\n",
        "            Tab(text=\"Prompt Generator\", content=page.generator, icon=icons.CLOUD),\n",
        "            Tab(text=\"Prompt Remixer\", content=page.remixer, icon=icons.CLOUD_SYNC_ROUNDED),\n",
        "            Tab(text=\"Prompt Brainstormer\", content=page.brainstormer, icon=icons.CLOUDY_SNOWING),\n",
        "            Tab(text=\"Prompt Styler\", content=page.styler, icon=icons.FORMAT_COLOR_FILL),\n",
        "            Tab(text=\"Negatives\", content=page.negatives, icon=icons.REMOVE_CIRCLE),\n",
        "            Tab(text=\"Image2Text\", content=page.Image2Text, icon=icons.WRAP_TEXT),\n",
        "            Tab(text=\"Magic Prompt\", content=page.MagicPrompt, icon=icons.AUTO_FIX_HIGH),\n",
        "            Tab(text=\"Distil GPT-2\", content=page.DistilGPT2, icon=icons.FILTER_ALT),\n",
        "            Tab(text=\"Retrieve Prompt from Image\", content=page.RetrievePrompts, icon=icons.PHOTO_LIBRARY_OUTLINED),\n",
        "            Tab(text=\"Init Images from Folder\", content=page.InitFolder, icon=icons.FOLDER_SPECIAL),\n",
        "            Tab(text=\"Init Images from Video\", content=page.InitVideo, icon=icons.SWITCH_VIDEO),\n",
        "            Tab(text=\"BLIP2 Image2Text\", content=page.BLIP2Image2Text, icon=icons.BATHTUB),\n",
        "        ],\n",
        "    )\n",
        "    return promptTabs\n",
        "\n",
        "def buildImageAIs(page):\n",
        "    page.RePainter = buildRepainter(page)\n",
        "    page.unCLIP = buildUnCLIP(page)\n",
        "    page.unCLIP_Interpolation = buildUnCLIP_Interpolation(page)\n",
        "    page.unCLIP_ImageInterpolation = buildUnCLIP_ImageInterpolation(page)\n",
        "    page.UnCLIP_ImageVariation = buildUnCLIP_ImageVariation(page)\n",
        "    page.BLIPDiffusion = buildBLIPDiffusion(page)\n",
        "    page.AnyText = buildAnyText(page)\n",
        "    page.IP_Adapter = buildIP_Adapter(page)\n",
        "    page.Reference = buildReference(page)\n",
        "    page.ControlNetQR = buildControlNetQR(page)\n",
        "    page.ControlNetSegmentAnything = buildControlNetSegmentAnything(page)\n",
        "    page.Null_Text = buildNull_Text(page)\n",
        "    page.EDICT = buildEDICT(page)\n",
        "    page.DiffEdit = buildDiffEdit(page)\n",
        "    page.ImageVariation = buildImageVariation(page)\n",
        "    page.CLIPstyler = buildCLIPstyler(page)\n",
        "    page.MagicMix = buildMagicMix(page)\n",
        "    page.SemanticGuidance = buildSemanticGuidance(page)\n",
        "    page.DemoFusion = buildDemoFusion(page)\n",
        "    page.PaintByExample = buildPaintByExample(page)\n",
        "    page.InstructPix2Pix = buildInstructPix2Pix(page)\n",
        "    page.ControlNet = buildControlNet(page)\n",
        "    page.ControlNetXL = buildControlNetXL(page)\n",
        "    page.ControlNetXS = buildControlNetXS(page)\n",
        "    page.DeepFloyd = buildDeepFloyd(page)\n",
        "    page.Amused = buildAmused(page)\n",
        "    page.Wuerstchen = buildWuerstchen(page)\n",
        "    page.PixArtAlpha = buildPixArtAlpha(page)\n",
        "    page.LMD_Plus = buildLMD_Plus(page)\n",
        "    page.LCM = buildLCM(page)\n",
        "    page.LCMInterpolation = buildLCMInterpolation(page)\n",
        "    page.MaterialDiffusion = buildMaterialDiffusion(page)\n",
        "    page.DallE2 = buildDallE2(page)\n",
        "    page.DallE3 = buildDallE3(page)\n",
        "    page.Kandinsky = buildKandinsky3(page) if status['kandinsky_version'] == \"Kandinsky 3.0\" else buildKandinsky(page)\n",
        "    page.KandinskyFuse = buildKandinskyFuse(page) if status['kandinsky_fuse_2_2'] else buildKandinsky21Fuse(page)\n",
        "    page.KandinskyControlNet = buildKandinskyControlNet(page)\n",
        "    page.DiT = buildDiT(page)\n",
        "    page.DreamFusion = buildDreamFusion(page)\n",
        "    page.Point_E = buildPoint_E(page)\n",
        "    page.Shap_E = buildShap_E(page)\n",
        "    page.InstantNGP = buildInstantNGP(page)\n",
        "    page.DeepDaze = buildDeepDaze(page)\n",
        "    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Instruct Pix2Pix\", content=page.InstructPix2Pix, icon=icons.SOLAR_POWER),\n",
        "            Tab(text=\"ControlNet\", content=page.ControlNet, icon=icons.HUB),\n",
        "            Tab(text=\"ControlNet SDXL\", content=page.ControlNetXL, icon=icons.PEST_CONTROL),\n",
        "            Tab(text=\"ControlNet-SX\", content=page.ControlNetXS, icon=icons.WEBHOOK),\n",
        "            Tab(text=\"Kandinsky\", content=page.Kandinsky, icon=icons.TOLL),\n",
        "            Tab(text=\"Kandinsky Fuse\", content=page.KandinskyFuse, icon=icons.FIREPLACE),\n",
        "            Tab(text=\"Kandinsky ControlNet\", content=page.KandinskyControlNet, icon=icons.CAMERA_ENHANCE),\n",
        "            Tab(text=\"DeepFloyd-IF\", content=page.DeepFloyd, icon=icons.LOOKS),\n",
        "            Tab(text=\"aMUSEd\", content=page.Amused, icon=icons.ATTRACTIONS),\n",
        "            Tab(text=\"W√ºrstchen\", content=page.Wuerstchen, icon=icons.SAVINGS),\n",
        "            Tab(text=\"PixArt-Œ±\", content=page.PixArtAlpha, icon=icons.PIX),\n",
        "            Tab(text=\"DemoFusion\", content=page.DemoFusion, icon=icons.COTTAGE),\n",
        "            Tab(text=\"LMD+\", content=page.LMD_Plus, icon=icons.HIGHLIGHT_ALT),\n",
        "            Tab(text=\"LCM\", content=page.LCM, icon=icons.MEMORY),\n",
        "            Tab(text=\"LCM Interpolation\", content=page.LCMInterpolation, icon=icons.TRANSFER_WITHIN_A_STATION),\n",
        "            Tab(text=\"QRCode\", content=page.ControlNetQR, icon=icons.QR_CODE_2),\n",
        "            Tab(text=\"unCLIP\", content=page.unCLIP, icon=icons.ATTACHMENT_SHARP),\n",
        "            Tab(text=\"unCLIP Interpolation\", content=page.unCLIP_Interpolation, icon=icons.TRANSFORM),\n",
        "            Tab(text=\"unCLIP Image Interpolation\", content=page.unCLIP_ImageInterpolation, icon=icons.ANIMATION),\n",
        "            Tab(text=\"unCLIP Image Variation\", content=page.UnCLIP_ImageVariation, icon=icons.AIRLINE_STOPS),\n",
        "            Tab(text=\"Image Variation\", content=page.ImageVariation, icon=icons.FORMAT_COLOR_FILL),\n",
        "            Tab(text=\"BLIP-Diffusion\", content=page.BLIPDiffusion, icon=icons.RADAR),\n",
        "            Tab(text=\"AnyText\", content=page.AnyText, icon=icons.TEXT_ROTATE_VERTICAL),\n",
        "            Tab(text=\"IP-Adapter\", content=page.IP_Adapter, icon=icons.ROOM_PREFERENCES),\n",
        "            Tab(text=\"Reference-Only\", content=page.Reference, icon=icons.CRISIS_ALERT),\n",
        "            Tab(text=\"Re-Segment-Anything\", content=page.ControlNetSegmentAnything, icon=icons.SEND_TIME_EXTENSION),\n",
        "            Tab(text=\"Null-Text\", content=page.Null_Text, icon=icons.FORMAT_OVERLINE),\n",
        "            Tab(text=\"EDICT Edit\", content=page.EDICT, icon=icons.AUTO_AWESOME),\n",
        "            Tab(text=\"DiffEdit\", content=page.DiffEdit, icon=icons.AUTO_GRAPH),\n",
        "            Tab(text=\"RePainter\", content=page.RePainter, icon=icons.FORMAT_PAINT),\n",
        "            Tab(text=\"MagicMix\", content=page.MagicMix, icon=icons.BLENDER),\n",
        "            Tab(text=\"Paint-by-Example\", content=page.PaintByExample, icon=icons.FORMAT_SHAPES),\n",
        "            Tab(text=\"CLIP-Styler\", content=page.CLIPstyler, icon=icons.STYLE),\n",
        "            Tab(text=\"Semantic Guidance\", content=page.SemanticGuidance, icon=icons.ROUTE),\n",
        "            Tab(text=\"Material Diffusion\", content=page.MaterialDiffusion, icon=icons.TEXTURE),\n",
        "            Tab(text=\"DALL‚Ä¢E 2\", content=page.DallE2, icon=icons.BLUR_CIRCULAR),\n",
        "            Tab(text=\"DALL‚Ä¢E 3\", content=page.DallE3, icon=icons.BLUR_ON),\n",
        "            Tab(text=\"DiT\", content=page.DiT, icon=icons.ANALYTICS),\n",
        "            Tab(text=\"DeepDaze\", content=page.DeepDaze, icon=icons.FACE),\n",
        "        ],\n",
        "    )\n",
        "    return diffusersTabs\n",
        "\n",
        "def build3DAIs(page):\n",
        "    page.DreamFusion = buildDreamFusion(page)\n",
        "    page.Point_E = buildPoint_E(page)\n",
        "    page.Shap_E = buildShap_E(page)\n",
        "    page.ZoeDepth = buildZoeDepth(page)\n",
        "    page.MarigoldDepth = buildMarigoldDepth(page)\n",
        "    page.LDM3D = buildLDM3D(page)\n",
        "    page.InstantNGP = buildInstantNGP(page)\n",
        "    page.Luma = buildLuma(page)\n",
        "    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"DreamFusion 3D\", content=page.DreamFusion, icon=icons.THREED_ROTATION),\n",
        "            Tab(text=\"Point-E 3D\", content=page.Point_E, icon=icons.SWIPE_UP),\n",
        "            Tab(text=\"Shap-E 3D\", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),\n",
        "            Tab(text=\"ZoeDepth 3D\", content=page.ZoeDepth, icon=icons.GRADIENT),\n",
        "            Tab(text=\"MarigoldDepth\", content=page.MarigoldDepth, icon=icons.FILTER_VINTAGE),\n",
        "            Tab(text=\"LDM3D\", content=page.LDM3D, icon=icons.ROTATE_90_DEGREES_CW),\n",
        "            Tab(text=\"Instant-NGP\", content=page.InstantNGP, icon=icons.STADIUM),\n",
        "            Tab(text=\"Luma Video-to-3D\", content=page.Luma, icon=icons.NIGHTS_STAY),\n",
        "        ],\n",
        "    )\n",
        "    return diffusersTabs\n",
        "\n",
        "def buildTrainers(page):\n",
        "    page.DreamBooth = buildDreamBooth(page)\n",
        "    page.TexualInversion = buildTextualInversion(page)\n",
        "    page.LoRA_Dreambooth = buildLoRA_Dreambooth(page)\n",
        "    page.LoRA = buildLoRA(page)\n",
        "    page.Converter = buildConverter(page)\n",
        "    page.CheckpointMerger = buildCheckpointMerger(page)\n",
        "    trainersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"LoRA DreamBooth\", content=page.LoRA_Dreambooth, icon=icons.SETTINGS_BRIGHTNESS),\n",
        "            Tab(text=\"LoRA\", content=page.LoRA, icon=icons.SETTINGS_SYSTEM_DAYDREAM),\n",
        "            Tab(text=\"DreamBooth\", content=page.DreamBooth, icon=icons.PHOTO),\n",
        "            Tab(text=\"Texual-Inversion\", content=page.TexualInversion, icon=icons.PHOTO_ALBUM),\n",
        "            Tab(text=\"Model Converter\", content=page.Converter, icon=icons.PUBLISHED_WITH_CHANGES),\n",
        "            Tab(text=\"Checkpoint Merger\", content=page.CheckpointMerger, icon=icons.JOIN_FULL),\n",
        "        ],\n",
        "    )\n",
        "    return trainersTabs\n",
        "\n",
        "def buildVideoAIs(page):\n",
        "    page.TextToVideo = buildTextToVideo(page)\n",
        "    page.TextToVideoZero = buildTextToVideoZero(page)\n",
        "    page.VideoToVideo = buildVideoToVideo(page)\n",
        "    page.InfiniteZoom = buildInfiniteZoom(page)\n",
        "    page.Potat1 = buildPotat1(page)\n",
        "    page.StableAnimation = buildStableAnimation(page)\n",
        "    page.SVD = buildSVD(page)\n",
        "    page.ControlNet = buildControlNet(page)\n",
        "    page.ControlNet_Video2Video = buildControlNet_Video2Video(page)\n",
        "    page.TemporalNet_XL = buildTemporalNet_XL(page)\n",
        "    page.Roop = buildROOP(page)\n",
        "    page.Video_ReTalking = buildVideoReTalking(page)\n",
        "    page.StyleCrafter = buildStyleCrafter(page)\n",
        "    page.RAVE = buildRAVE(page)\n",
        "    page.TokenFlow = buildTokenFlow(page)\n",
        "    page.AnimateDiff = buildAnimateDiff(page)\n",
        "    page.HotshotXL = buildHotshotXL(page)\n",
        "    page.Rerender_a_video = buildRerender_a_video(page)\n",
        "\n",
        "    videoAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"AnimateDiff\", content=page.AnimateDiff, icon=icons.AUTO_MODE),\n",
        "            Tab(text=\"Stable Animation\", content=page.StableAnimation, icon=icons.SHUTTER_SPEED),\n",
        "            Tab(text=\"SVD Image-to-Video\", content=page.SVD, icon=icons.SLOW_MOTION_VIDEO),\n",
        "            Tab(text=\"Text-to-Video\", content=page.TextToVideo, icon=icons.MISSED_VIDEO_CALL),\n",
        "            Tab(text=\"Text-to-Video Zero\", content=page.TextToVideoZero, icon=icons.ONDEMAND_VIDEO),\n",
        "            Tab(text=\"Potat1\", content=page.Potat1, icon=icons.FILTER_1),\n",
        "            Tab(text=\"ROOP Face-Swap\", content=page.Roop, icon=icons.FACE_RETOUCHING_NATURAL),\n",
        "            Tab(text=\"Video-ReTalking\", content=page.Video_ReTalking, icon=icons.RECORD_VOICE_OVER),\n",
        "            Tab(text=\"Infinite Zoom\", content=page.InfiniteZoom, icon=icons.ZOOM_IN_MAP),\n",
        "            Tab(text=\"StyleCrafter\", content=page.StyleCrafter, icon=icons.HIGHLIGHT),\n",
        "            Tab(text=\"TokenFlow\", content=page.TokenFlow, icon=icons.TOKEN),\n",
        "            Tab(text=\"RAVE\", content=page.RAVE, icon=icons.FLUTTER_DASH),\n",
        "            Tab(text=\"Rerender-a-Video\", content=page.Rerender_a_video, icon=icons.MEMORY),\n",
        "            Tab(text=\"Hotshot-XL\", content=page.HotshotXL, icon=icons.HOT_TUB),\n",
        "            Tab(text=\"ControlNet Video2Video\", content=page.ControlNet_Video2Video, icon=icons.PSYCHOLOGY),\n",
        "            Tab(text=\"Video-to-Video\", content=page.VideoToVideo, icon=icons.CAMERA_ROLL),\n",
        "            Tab(text=\"TemporalNet-XL\", content=page.TemporalNet_XL, icon=icons.HOURGLASS_BOTTOM),\n",
        "            Tab(text=\"ControlNet Init-Video\", content=page.ControlNet, icon=icons.HUB),\n",
        "        ],\n",
        "    )\n",
        "    return videoAIsTabs\n",
        "\n",
        "def buildAudioAIs(page):\n",
        "    page.TortoiseTTS = buildTortoiseTTS(page)\n",
        "    page.MusicGen = buildMusicGen(page)\n",
        "    page.DanceDiffusion = buildDanceDiffusion(page)\n",
        "    page.AudioDiffusion = buildAudioDiffusion(page)\n",
        "    page.AudioLDM = buildAudioLDM(page)\n",
        "    page.AudioLDM2 = buildAudioLDM2(page)\n",
        "    page.MusicLDM = buildMusicLDM(page)\n",
        "    page.Bark = buildBark(page)\n",
        "    page.Riffusion = buildRiffusion(page)\n",
        "    page.Mubert = buildMubert(page)\n",
        "    page.Whisper = buildWhisper(page)\n",
        "    page.VoiceFixer = buildVoiceFixer(page)\n",
        "    audioAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Tortoise-TTS\", content=page.TortoiseTTS, icon=icons.RECORD_VOICE_OVER),\n",
        "            Tab(text=\"MusicGen\", content=page.MusicGen, icon=icons.MUSIC_NOTE),\n",
        "            Tab(text=\"MusicLDM\", content=page.MusicLDM, icon=icons.EARBUDS),\n",
        "            Tab(text=\"AudioLDM\", content=page.AudioLDM, icon=icons.NOISE_AWARE),\n",
        "            Tab(text=\"AudioLDM-2\", content=page.AudioLDM2, icon=icons.NOISE_CONTROL_OFF),\n",
        "            Tab(text=\"Bark\", content=page.Bark, icon=icons.PETS),\n",
        "            Tab(text=\"Riffusion\", content=page.Riffusion, icon=icons.SPATIAL_AUDIO),\n",
        "            Tab(text=\"Audio Diffusion\", content=page.AudioDiffusion, icon=icons.GRAPHIC_EQ),\n",
        "            Tab(text=\"HarmonAI Dance Diffusion\", content=page.DanceDiffusion, icon=icons.QUEUE_MUSIC),\n",
        "            Tab(text=\"Whisper-STT\", content=page.Whisper, icon=icons.HEARING),\n",
        "            Tab(text=\"Voice Fixer\", content=page.VoiceFixer, icon=icons.VOICE_CHAT),\n",
        "            Tab(text=\"Mubert Music\", content=page.Mubert, icon=icons.MUSIC_VIDEO),\n",
        "        ],\n",
        "    )\n",
        "    return audioAIsTabs\n",
        "\n",
        "def buildExtras(page):\n",
        "    page.ESRGAN_upscaler = buildESRGANupscaler(page)\n",
        "    page.CachedModelManager = buildCachedModelManager(page)\n",
        "    page.CustomModelManager = buildCustomModelManager(page)\n",
        "    page.MaskMaker = buildDreamMask(page)\n",
        "    page.BackgroundRemover = buildBackgroundRemover(page)\n",
        "    #page.Kandinsky21 = buildKandinsky21(page)\n",
        "    #page.Kandinsky21Fuse = buildKandinsky21Fuse(page)\n",
        "    extrasTabs = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Real-ESRGAN Batch Upscaler\", content=page.ESRGAN_upscaler, icon=icons.PHOTO_SIZE_SELECT_LARGE),\n",
        "            Tab(text=\"Model Manager\", content=page.CustomModelManager, icon=icons.DIFFERENCE),\n",
        "            Tab(text=\"Cache Manager\", content=page.CachedModelManager, icon=icons.CACHED),\n",
        "            #Tab(text=\"Dream Mask Maker\", content=page.MaskMaker, icon=icons.GRADIENT),\n",
        "            Tab(text=\"Background Remover\", content=page.BackgroundRemover, icon=icons.WALLPAPER),\n",
        "            #Tab(text=\"Kandinsky 2.1\", content=page.Kandinsky21, icon=icons.AC_UNIT),\n",
        "            #Tab(text=\"Kandinsky Fuse\", content=page.Kandinsky21Fuse, icon=icons.FIREPLACE),\n",
        "        ],\n",
        "    )\n",
        "    return extrasTabs\n",
        "\n",
        "def b_style():\n",
        "    return ButtonStyle(elevation=8)\n",
        "def dict_diff(dict1, dict2):\n",
        "    return {k: v for k, v in dict1.items() if k in dict2 and v != dict2[k]}\n",
        "def arg_diffs(dict1, dict2):\n",
        "    diff = dict_diff(dict1, dict2)\n",
        "    if len(diff) > 0:\n",
        "      dif = []\n",
        "      for k, v in diff.items():\n",
        "        dif.append(f'{to_title(k)}: {v}')\n",
        "      return ', '.join(dif)\n",
        "    else: return None\n",
        "def get_color(color):\n",
        "    if color == \"green\": return colors.GREEN\n",
        "    elif color == \"blue\": return colors.BLUE\n",
        "    elif color == \"indigo\": return colors.INDIGO\n",
        "    elif color == \"red\": return colors.RED\n",
        "    elif color == \"purple\": return colors.DEEP_PURPLE\n",
        "    elif color == \"orange\": return colors.ORANGE\n",
        "    elif color == \"amber\": return colors.AMBER\n",
        "    elif color == \"brown\": return colors.BROWN\n",
        "    elif color == \"teal\": return colors.TEAL\n",
        "    elif color == \"yellow\": return colors.YELLOW\n",
        "\n",
        "\n",
        "# Delete these after everyone's updated\n",
        "if 'negative_prompt' not in prefs: prefs['negative_prompt'] = ''\n",
        "if 'file_datetime' not in prefs: prefs['file_datetime'] = False\n",
        "if 'install_conceptualizer' not in prefs: prefs['install_conceptualizer'] = False\n",
        "if 'use_conceptualizer' not in prefs: prefs['use_conceptualizer'] = False\n",
        "if 'concepts_model' not in prefs: prefs['concepts_model'] = 'cat-toy'\n",
        "if 'memory_optimization' not in prefs: prefs['memory_optimization'] = 'None'\n",
        "if 'enable_xformers' not in prefs: prefs['enable_xformers'] = False\n",
        "if 'enable_attention_slicing' not in prefs: prefs['enable_attention_slicing'] = True\n",
        "if 'enable_bitsandbytes' not in prefs: prefs['enable_bitsandbytes'] = False\n",
        "if 'sequential_cpu_offload' not in prefs: prefs['sequential_cpu_offload'] = False\n",
        "if 'vae_slicing' not in prefs: prefs['vae_slicing'] = True\n",
        "if 'vae_tiling' not in prefs: prefs['vae_tiling'] = False\n",
        "if 'show_stats' not in prefs: prefs['show_stats'] = False\n",
        "if 'stats_used' not in prefs: prefs['stats_used'] = True\n",
        "if 'stats_update' not in prefs: prefs['stats_update'] = 5\n",
        "if 'upscale_method' not in prefs: prefs['upscale_method'] = 'Real-ESRGAN'\n",
        "if 'upscale_model' not in prefs: prefs['upscale_model'] = 'realesr-general-x4v3'\n",
        "if 'use_inpaint_model' not in prefs: prefs['use_inpaint_model'] = False\n",
        "if 'cache_dir' not in prefs: prefs['cache_dir'] = ''\n",
        "if 'Replicate_api_key' not in prefs: prefs['Replicate_api_key'] = ''\n",
        "if 'install_dreamfusion' not in prefs: prefs['install_dreamfusion'] = False\n",
        "if 'install_repaint' not in prefs: prefs['install_repaint'] = False\n",
        "if 'finetuned_model' not in prefs: prefs['finetuned_model'] = 'Midjourney v4 style'\n",
        "if 'dreambooth_model' not in prefs: prefs['dreambooth_model'] = 'disco-diffusion-style'\n",
        "if 'custom_model' not in prefs: prefs['custom_model'] = ''\n",
        "if 'custom_models' not in prefs: prefs['custom_models'] = []\n",
        "if 'start_in_installation' not in prefs: prefs['start_in_installation'] = False\n",
        "if 'install_imagic' not in prefs: prefs['install_imagic'] = False\n",
        "if 'use_imagic' not in prefs: prefs['use_imagic'] = False\n",
        "if 'install_composable' not in prefs: prefs['install_composable'] = False\n",
        "if 'use_composable' not in prefs: prefs['use_composable'] = False\n",
        "if 'install_safe' not in prefs: prefs['install_safe'] = False\n",
        "if 'use_safe' not in prefs: prefs['use_safe'] = False\n",
        "if 'safety_config' not in prefs: prefs['safety_config'] = \"Strong\"\n",
        "if 'install_versatile' not in prefs: prefs['install_versatile'] = False\n",
        "if 'use_versatile' not in prefs: prefs['use_versatile'] = False\n",
        "if 'install_depth2img' not in prefs: prefs['install_depth2img'] = False\n",
        "if 'use_depth2img' not in prefs: prefs['use_depth2img'] = False\n",
        "if 'install_alt_diffusion' not in prefs: prefs['install_alt_diffusion'] = False\n",
        "if 'use_alt_diffusion' not in prefs: prefs['use_alt_diffusion'] = False\n",
        "if 'install_upscale' not in prefs: prefs['install_upscale'] = False\n",
        "if 'use_upscale' not in prefs: prefs['use_upscale'] = False\n",
        "if 'upscale_noise_level' not in prefs: prefs['upscale_noise_level'] = 20\n",
        "if 'alpha_mask' not in prefs: prefs['alpha_mask'] = False\n",
        "if 'invert_mask' not in prefs: prefs['invert_mask'] = False\n",
        "if 'clip_guidance_preset' not in prefs: prefs['clip_guidance_preset'] = \"FAST_BLUE\"\n",
        "if 'tortoise_custom_voices' not in prefs: prefs['tortoise_custom_voices'] = []\n",
        "if 'use_LoRA_model' not in prefs: prefs['use_LoRA_model'] = False\n",
        "if 'LoRA_model' not in prefs: prefs['LoRA_model'] = \"Von Platen LoRA\"\n",
        "if 'active_LoRA_layers' not in prefs: prefs['active_LoRA_layers'] = []\n",
        "if 'active_SDXL_LoRA_layers' not in prefs: prefs['active_SDXL_LoRA_layers'] = []\n",
        "if 'custom_LoRA_models' not in prefs: prefs['custom_LoRA_models'] = []\n",
        "if 'custom_LoRA_model' not in prefs: prefs['custom_LoRA_model'] = ''\n",
        "if 'SDXL_LoRA_model' not in prefs: prefs['SDXL_LoRA_model'] = \"Papercut SDXL\"\n",
        "if 'custom_SDXL_LoRA_models' not in prefs: prefs['custom_SDXL_LoRA_models'] = []\n",
        "if 'custom_SDXL_LoRA_model' not in prefs: prefs['custom_SDXL_LoRA_model'] = ''\n",
        "if 'custom_dance_diffusion_models' not in prefs: prefs['custom_dance_diffusion_models'] = []\n",
        "if 'negative_prompt' not in prefs['prompt_writer']: prefs['prompt_writer']['negative_prompt'] = ''\n",
        "if 'install_attend_and_excite' not in prefs: prefs['install_attend_and_excite'] = False\n",
        "if 'use_attend_and_excite' not in prefs: prefs['use_attend_and_excite'] = False\n",
        "if 'max_iter_to_alter' not in prefs: prefs['max_iter_to_alter'] = 25\n",
        "if 'install_SAG' not in prefs: prefs['install_SAG'] = False\n",
        "if 'use_SAG' not in prefs: prefs['use_SAG'] = False\n",
        "if 'sag_scale' not in prefs: prefs['sag_scale'] = 0.75\n",
        "if 'SD_compel' not in prefs: prefs['SD_compel'] = False\n",
        "if 'install_SDXL' not in prefs: prefs['install_SDXL'] = False\n",
        "if 'use_SDXL' not in prefs: prefs['use_SDXL'] = False\n",
        "if 'SDXL_high_noise_frac' not in prefs: prefs['SDXL_high_noise_frac'] = 0.7\n",
        "if 'SDXL_compel' not in prefs: prefs['SDXL_compel'] = False\n",
        "if 'SDXL_negative_conditions' not in prefs: prefs['SDXL_negative_conditions'] = False\n",
        "if 'SDXL_watermark' not in prefs: prefs['SDXL_watermark'] = False\n",
        "if 'SDXL_model' not in prefs: prefs['SDXL_model'] = 'SDXL-Base v1'\n",
        "if 'SDXL_custom_model' not in prefs: prefs['SDXL_custom_model'] = ''\n",
        "if 'install_panorama' not in prefs: prefs['install_panorama'] = False\n",
        "if 'use_panorama' not in prefs: prefs['use_panorama'] = False\n",
        "if 'panorama_circular_padding' not in prefs: prefs['panorama_circular_padding'] = False\n",
        "if 'panorama_width' not in prefs: prefs['panorama_width'] = 2048\n",
        "if 'AI_engine' not in prefs['prompt_generator']: prefs['prompt_generator']['AI_engine'] = 'ChatGPT-3.5 Turbo'\n",
        "if 'AI_engine' not in prefs['prompt_remixer']: prefs['prompt_remixer']['AI_engine'] = 'ChatGPT-3.5 Turbo'\n",
        "if 'luma_api_key' not in prefs: prefs['luma_api_key'] = ''\n",
        "if 'AIHorde_api_key' not in prefs: prefs['AIHorde_api_key'] = '0000000000'\n",
        "if 'install_AIHorde_api' not in prefs: prefs['install_AIHorde_api'] = False\n",
        "if 'use_AIHorde_api' not in prefs: prefs['use_AIHorde_api'] = False\n",
        "if 'AIHorde_model' not in prefs: prefs['AIHorde_model'] = 'stable_diffusion'\n",
        "if 'AIHorde_sampler' not in prefs: prefs['AIHorde_sampler'] = 'k_euler_a'\n",
        "if 'AIHorde_post_processing' not in prefs: prefs['AIHorde_post_processing'] = \"None\"\n",
        "if 'PaLM_api_key' not in prefs: prefs['PaLM_api_key'] = ''\n",
        "if 'enable_torch_compile' not in prefs: prefs['enable_torch_compile'] = False\n",
        "if 'enable_stable_fast' not in prefs: prefs['enable_stable_fast'] = False\n",
        "if 'enable_tome' not in prefs: prefs['enable_tome'] = False\n",
        "if 'tome_ratio' not in prefs: prefs['tome_ratio'] = 0.5\n",
        "if 'enable_freeu' not in prefs: prefs['enable_freeu'] = False\n",
        "if 'freeu_args' not in prefs: prefs['freeu_args'] = {'b1': 1.2, 'b2':1.4, 's1':0.9, 's2':0.2}\n",
        "if 'enable_deepcache' not in prefs: prefs['enable_deepcache'] = False\n",
        "if 'negatives' not in prefs: prefs['negatives'] = ['Blurry']\n",
        "if 'custom_negatives' not in prefs: prefs['custom_negatives'] = \"\"\n",
        "if 'prompt_style' not in prefs: prefs['prompt_style'] = \"cinematic-default\"\n",
        "if 'prompt_styles' not in prefs: prefs['prompt_styles'] = [\"cinematic-default\"]\n",
        "if 'prompt_styler' not in prefs: prefs['prompt_styler'] = \"\"\n",
        "if 'prompt_styler_multi' not in prefs: prefs['prompt_styler_multi'] = False\n",
        "if 'use_ip_adapter' not in prefs: prefs['use_ip_adapter'] = False\n",
        "if 'ip_adapter_image' not in prefs: prefs['ip_adapter_image'] = \"\"\n",
        "if 'ip_adapter_model' not in prefs: prefs['ip_adapter_model'] = \"SD v1.5\"\n",
        "if 'ip_adapter_SDXL_model' not in prefs: prefs['ip_adapter_SDXL_model'] = \"SDXL\"\n",
        "if 'ip_adapter_strength' not in prefs: prefs['ip_adapter_strength'] = 0.8\n",
        "\n",
        "if bool(prefs['init_image']):\n",
        "    if not os.path.isfile(prefs['init_image']): prefs['init_image'] = \"\"\n",
        "if bool(prefs['mask_image']):\n",
        "    if not os.path.isfile(prefs['mask_image']): prefs['mask_image'] = \"\"\n",
        "    \n",
        "def initState(page):\n",
        "    global status, current_tab\n",
        "    if os.path.isdir(os.path.join(root_dir, 'Real-ESRGAN')):\n",
        "      status['installed_ESRGAN'] = True\n",
        "    page.load_prompts()\n",
        "\n",
        "    # TODO: Try to load from assets folder\n",
        "    page.snd_alert = Audio(src=os.path.join(assets, \"snd-alert.mp3\"), autoplay=False)\n",
        "    page.snd_delete = Audio(src=os.path.join(assets, \"snd-delete.mp3\"), autoplay=False)\n",
        "    page.snd_error = Audio(src=os.path.join(assets, \"snd-error.mp3\"), autoplay=False)\n",
        "    page.snd_done = Audio(src=os.path.join(assets, \"snd-done.mp3\"), autoplay=False)\n",
        "    page.snd_drop = Audio(src=os.path.join(assets, \"snd-drop.mp3\"), autoplay=False)\n",
        "    #page.snd_notification = Audio(src=\"https://github.com/Skquark/AEIONic/blob/main/assets/snd-notification.mp3?raw=true\", autoplay=False)\n",
        "    page.overlay.append(page.snd_alert)\n",
        "    page.overlay.append(page.snd_delete)\n",
        "    page.overlay.append(page.snd_error)\n",
        "    page.overlay.append(page.snd_done)\n",
        "    page.overlay.append(page.snd_drop)\n",
        "    #page.overlay.append(page.snd_notification)\n",
        "    #print(\"Running Init State\")\n",
        "    import importlib\n",
        "    if '_PYIBoot_SPLASH' in os.environ and importlib.util.find_spec(\"pyi_splash\"):\n",
        "      import pyi_splash\n",
        "      pyi_splash.update_text('Ready to get creative...')\n",
        "      pyi_splash.close()\n",
        "      #log.info('Splash screen closed.')\n",
        "    if prefs['scheduler_mode'] != \"DDIM\":\n",
        "      for eta in page.etas:\n",
        "        if isinstance(eta, SliderRow):\n",
        "          eta.show = False\n",
        "        else:\n",
        "          eta.visible = False\n",
        "          eta.update()\n",
        "    if prefs['start_in_installation'] and current_tab == 0:\n",
        "      page.tabs.selected_index = 1\n",
        "      page.tabs.update()\n",
        "      page.show_install_fab(True)\n",
        "      page.update()\n",
        "      current_tab = 1\n",
        "    if prefs['show_stats']:\n",
        "      start_thread(page)\n",
        "      #start_polling(prefs['stats_update'], update_stats(page))\n",
        "\n",
        "def buildSettings(page):\n",
        "  global prefs, status\n",
        "  def open_url(e):\n",
        "    page.launch_url(e.data)\n",
        "  def save_settings(e):\n",
        "    save_settings_file(e.page)\n",
        "    page.snack_bar = SnackBar(content=Text(f\"Saving all settings to {saved_settings_json.rpartition(slash)[2]}\"))\n",
        "    page.snack_bar.open = True\n",
        "    page.tabs.selected_index = 1\n",
        "    page.tabs.update()\n",
        "    page.update()\n",
        "  def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "      has_changed = True\n",
        "      page.update()\n",
        "      status['changed_settings'] = True\n",
        "  def change_theme_mode(e):\n",
        "    prefs['theme_mode'] = e.control.value\n",
        "    if prefs['theme_mode'].lower() == \"dark\":\n",
        "      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    else:\n",
        "      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    page.theme_mode = prefs['theme_mode'].lower()\n",
        "    page.update()\n",
        "    status['changed_settings'] = True\n",
        "  def change_theme_color(e):\n",
        "    prefs['theme_color'] = e.control.value\n",
        "    if prefs['theme_mode'].lower() == \"dark\":\n",
        "      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    else:\n",
        "      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    page.update()\n",
        "    status['changed_settings'] = True\n",
        "  def toggle_nsfw(e):\n",
        "    #TODO: Add Popup alert with disclaimer, age verification and I Accept the Terms\n",
        "    global safety\n",
        "    retry_attempts.width = 0 if e.control.value else None\n",
        "    retry_attempts.update()\n",
        "    changed(e, 'disable_nsfw_filter')\n",
        "    safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}\n",
        "  def default_cache_dir(e):\n",
        "    default_dir = prefs['image_output'].strip()\n",
        "    if default_dir.endswith(slash):\n",
        "      default_dir = default_dir[:-1]\n",
        "    default_dir = default_dir.rpartition(slash)[0]\n",
        "    default_dir = os.path.join(default_dir, 'models')\n",
        "    prefs['cache_dir'] = default_dir\n",
        "    optional_cache_dir.value = default_dir\n",
        "    optional_cache_dir.update()\n",
        "  def folder_picker_result(e):\n",
        "    folder = folder_picker.result\n",
        "    if folder != None and folder.path != None:\n",
        "      image_output.value = folder.path\n",
        "      image_output.update()\n",
        "  folder_picker = FilePicker(on_result=folder_picker_result)\n",
        "  page.overlay.append(folder_picker)\n",
        "  def pick_output_dir(e):\n",
        "    if not is_Colab:\n",
        "      folder_picker.get_directory_path(dialog_title=\"Pick Directory to Save Outputs\")\n",
        "  image_output = TextField(label=\"Image Output Path\", value=prefs['image_output'], on_change=lambda e:changed(e, 'image_output'), col={\"md\":12, \"lg\":6}, suffix=IconButton(icon=icons.FOLDER_OUTLINED, on_click=pick_output_dir))\n",
        "  optional_cache_dir = TextField(label=\"Optional Cache Directory (saves large models to drive)\", hint_text=\"(button on right inserts recommended folder)\", value=prefs['cache_dir'], on_change=lambda e:changed(e, 'cache_dir'), suffix=IconButton(icon=icons.ARCHIVE, tooltip=\"Insert recommended models cache path\", on_click=default_cache_dir), col={\"md\":12, \"lg\":6})\n",
        "  file_prefix = TextField(label=\"Filename Prefix\",  value=prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "  file_suffix_seed = Checkbox(label=\"Filename Suffix Seed   \", tooltip=\"Appends -seed# to the end of the image name\", value=prefs['file_suffix_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_suffix_seed'))\n",
        "  file_allowSpace = Checkbox(label=\"Filename Allow Space\", tooltip=\"Otherwise will replace spaces with _ underscores\", value=prefs['file_allowSpace'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_allowSpace'))\n",
        "  file_max_length = TextField(label=\"Filename Max Length\", tooltip=\"How long can the name taken from prompt text be? Max 250\", value=prefs['file_max_length'], keyboard_type=KeyboardType.NUMBER, width=150, height=60, on_change=lambda e:changed(e, 'file_max_length'))\n",
        "  file_datetime = Checkbox(label=\"Filename DateTime instead of Prompt Text\", tooltip=\"Save File with Date-Time Stamp to protect your prompt.\", value=prefs['file_datetime'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_datetime'))\n",
        "  save_image_metadata = Checkbox(label=\"Save Image Metadata in png\", tooltip=\"Embeds your Artist Name & Copyright in the file's EXIF\", value=prefs['save_image_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_image_metadata'))\n",
        "  meta_ArtistName = TextField(label=\"Artist Name Metadata\", value=prefs['meta_ArtistName'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_ArtistName'))\n",
        "  meta_Copyright = TextField(label=\"Copyright Metadata\", value=prefs['meta_Copyright'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_Copyright'))\n",
        "  save_config_in_metadata = Checkbox(label=\"Save Config in Metadata    \", tooltip=\"Embeds all prompt parameters in the file's EXIF to recreate\", value=prefs['save_config_in_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_in_metadata'))\n",
        "  save_config_json = Checkbox(label=\"Save Config JSON files\", tooltip=\"Creates a json text file with all prompt parameters with each image\", value=prefs['save_config_json'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_json'))\n",
        "  theme_mode = Dropdown(label=\"Theme Mode\", width=200, options=[dropdown.Option(\"Dark\"), dropdown.Option(\"Light\")], value=prefs['theme_mode'], on_change=change_theme_mode)\n",
        "  theme_color = Dropdown(label=\"Accent Color\", width=200, options=[dropdown.Option(\"Green\"), dropdown.Option(\"Blue\"), dropdown.Option(\"Red\"), dropdown.Option(\"Indigo\"), dropdown.Option(\"Purple\"), dropdown.Option(\"Orange\"), dropdown.Option(\"Amber\"), dropdown.Option(\"Brown\"), dropdown.Option(\"Teal\"), dropdown.Option(\"Yellow\")], value=prefs['theme_color'], on_change=change_theme_color)\n",
        "  enable_sounds = Checkbox(label=\"Enable UI Sound Effects    \", tooltip=\"Turn on for audible errors, deletes and generation done notifications\", value=prefs['enable_sounds'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_sounds'))\n",
        "  start_in_installation = Checkbox(label=\"Start in Installation Page\", tooltip=\"When launching app, switch to Installer tab. Saves time..\", value=prefs['start_in_installation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'start_in_installation'))\n",
        "  disable_nsfw_filter = Checkbox(label=\"Disable NSFW Filters for Uncensored Images\", value=prefs['disable_nsfw_filter'], tooltip=\"If you're over 18 & promise not to abuse, allow Not Safe For Work. Otherwise, will filter mature content...\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_nsfw)\n",
        "  retry_attempts = Container(NumberPicker(label=\"Retry Attempts if Not Safe\", min=0, max=8, value=prefs['retry_attempts'], on_change=lambda e:changed(e, 'retry_attempts')), padding=padding.only(left=20), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  retry_attempts.width = 0 if prefs['disable_nsfw_filter'] else None\n",
        "  def toggle_stats(e):\n",
        "      global stop_thread\n",
        "      prefs['show_stats'] = e.control.value\n",
        "      if prefs['show_stats']:\n",
        "          start_thread(page)\n",
        "      else:\n",
        "          stop_thread = True\n",
        "          page.stats.controls[0].value = \"\"\n",
        "          page.stats.controls[1].value = \"\"\n",
        "          page.stats.update()\n",
        "      stats_settings.width = 0 if not prefs['show_stats'] else None\n",
        "      stats_settings.update()\n",
        "  def toggle_used(e):\n",
        "      prefs['stats_used'] = e.control.value\n",
        "      update_stats(page)\n",
        "  show_stats = Checkbox(label=\"Show Memory Stats\", tooltip=\"Gives an updating VRAM and RAM information on top appbar.\", value=prefs['show_stats'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_stats)\n",
        "  stats_used = Checkbox(label=\"Memory Used\", tooltip=\"Otherwise show available free memory\", value=prefs['stats_used'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_used)\n",
        "  page.stats_used = stats_used\n",
        "  stats_settings = Container(Row([stats_used, NumberPicker(label=\" Update Interval (s):\", min=1, max=30, value=prefs['stats_update'], on_change=lambda e:changed(e, 'stats_update'))]), padding=padding.only(left=0), animate_size=animation.Animation(700, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  stats_settings.width = 0 if not prefs['show_stats'] else None\n",
        "  \n",
        "  api_instructions = Container(height=170, content=Markdown(\"Get **HuggingFace API key** from https://huggingface.co/settings/tokens, preferably the WRITE access key.\\n\\nGet **Stability-API key** from https://beta.dreamstudio.ai/membership?tab=apiKeys then API key\\n\\nGet **OpenAI GPT-3 API key** from https://beta.openai.com, user menu, View API Keys\\n\\nGet **Google Gemini API Token** from https://developers.generativeai.google/tutorials/setup\\n\\n\\n\\nGet **TextSynth GPT-J key** from https://TextSynth.com, login, Setup\\n\\nGet **AIHorde API Token** from https://aihorde.net/register, for Stable Horde cloud\", extension_set=\"gitHubWeb\", on_tap_link=open_url))\n",
        "  HuggingFace_api = TextField(label=\"HuggingFace API Key\", value=prefs['HuggingFace_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'HuggingFace_api_key'))\n",
        "  Stability_api = TextField(label=\"Stability.ai API Key (optional)\", value=prefs['Stability_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Stability_api_key'))\n",
        "  OpenAI_api = TextField(label=\"OpenAI API Key (optional)\", value=prefs['OpenAI_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'OpenAI_api_key'))\n",
        "  PaLM_api = TextField(label=\"Google Gemini API Key (optional)\", value=prefs['PaLM_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'PaLM_api_key'))\n",
        "  TextSynth_api = TextField(label=\"TextSynth API Key (optional)\", value=prefs['TextSynth_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'TextSynth_api_key'))\n",
        "  Replicate_api = TextField(label=\"Replicate API Key (optional)\", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Replicate_api_key'))\n",
        "  AIHorde_api = TextField(label=\"AIHorde API Key (optional)\", value=prefs['AIHorde_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'AIHorde_api_key'))\n",
        "  #save_button = ElevatedButton(content=Text(value=\"üíæ  Save Settings\", size=20), on_click=save_settings, style=b_style())\n",
        "\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚öôÔ∏è   AEIONic Diffusion Deluxe Settings & Preferences\"),\n",
        "        ResponsiveRow([image_output, optional_cache_dir], run_spacing=2),\n",
        "        Row([file_prefix, file_suffix_seed]) if (page.width if page.web else page.window_width) > 500 else Column([file_prefix, file_suffix_seed]),\n",
        "        Row([file_max_length, file_allowSpace]),\n",
        "        file_datetime,\n",
        "        Row([disable_nsfw_filter, retry_attempts]),\n",
        "        save_image_metadata,\n",
        "        Row([meta_ArtistName, meta_Copyright]) if (page.width if page.web else page.window_width) > 712 else Column([meta_ArtistName, meta_Copyright]),\n",
        "        Row([save_config_in_metadata, save_config_json]),\n",
        "        Row([theme_mode, theme_color]),\n",
        "        Row([enable_sounds, start_in_installation, show_stats, stats_settings]),\n",
        "        HuggingFace_api,\n",
        "        Stability_api,\n",
        "        OpenAI_api,\n",
        "        PaLM_api,\n",
        "        TextSynth_api,\n",
        "        #Replicate_api,\n",
        "        AIHorde_api,\n",
        "        api_instructions,\n",
        "        #save_button,\n",
        "        Container(content=None, height=32),\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO,)\n",
        "  return c\n",
        "\n",
        "def run_process(cmd_str, cwd=None, realtime=True, page=None, close_at_end=False, show=False, print=False): # show when debugging\n",
        "  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()\n",
        "  if realtime:\n",
        "    if cwd is None:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "    else:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "    while True:\n",
        "      realtime_output = process.stdout.readline()\n",
        "      if realtime_output == '' and process.poll() is not None:\n",
        "        break\n",
        "      if realtime_output and show and page != None:\n",
        "        #print(realtime_output.strip(), flush=False)\n",
        "        page.banner.content.controls.append(Text(realtime_output.strip()))\n",
        "        page.update()\n",
        "        sys.stdout.flush()\n",
        "      if print:\n",
        "        print(realtime_output.strip())\n",
        "    if close_at_end:\n",
        "      page.banner.open = False\n",
        "      page.update()\n",
        "  else:\n",
        "    if cwd is None:\n",
        "      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')\n",
        "    else:\n",
        "      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')\n",
        "    if print:\n",
        "      print(output)\n",
        "    return output\n",
        "\n",
        "def close_alert_dlg(e):\n",
        "      e.page.alert_dlg.open = False\n",
        "      e.page.update()\n",
        "def alert_msg(page, msg, content=None, okay=\"\", sound=True, width=None, wide=False):\n",
        "      try:\n",
        "        if page.alert_dlg.open == True: return\n",
        "      except Exception: pass\n",
        "      try:\n",
        "        if prefs['enable_sounds'] and sound: page.snd_error.play()\n",
        "      except Exception:\n",
        "        msg += \" May have to restart runtime.\"\n",
        "        pass\n",
        "      okay_button = ElevatedButton(content=Text(\"üëå  OKAY \" if okay == \"\" else okay, size=18), on_click=close_alert_dlg)\n",
        "      if content == None: content = Container(content=None)\n",
        "      if not isinstance(content, list):\n",
        "          content = [content]\n",
        "      page.alert_dlg = AlertDialog(title=Text(msg), content=Column(content, scroll=ScrollMode.AUTO), actions=[okay_button], actions_alignment=MainAxisAlignment.END)#, width=None if not wide else (page.width if page.web else page.window_width) - 200)\n",
        "      page.dialog = page.alert_dlg\n",
        "      page.alert_dlg.open = True\n",
        "      try:\n",
        "        page.update()\n",
        "      except Exception: pass\n",
        "\n",
        "def save_installers(controls):\n",
        "  for c in controls:\n",
        "    if isinstance(c, Switch):\n",
        "      #print(f\"elif c.value == '{c.label}': prefs[''] = c.value\")\n",
        "      if c.value == 'Install HuggingFace Diffusers Pipeline': prefs['install_diffusers'] = c.value\n",
        "      elif c.value == 'Install Stability-API DreamStudio Pipeline': prefs['install_Stability_api'] = c.value\n",
        "      elif c.value == 'Install Real-ESRGAN AI Upscaler': prefs['install_ESRGAN'] = c.value\n",
        "      elif c.value == 'Install OpenAI GPT-3 Text Engine': prefs['install_OpenAI'] = c.value\n",
        "      elif c.value == 'Install TextSynth GPT-J Text Engine': prefs['install_TextSynth'] = c.value\n",
        "    '''elif isinstance(c, Container):\n",
        "      try:\n",
        "        for i in c.content.controls:\n",
        "          if isinstance(i, Switch):\n",
        "            print(f\"elif i.value == '{c.label}': prefs[''] = i.value\")\n",
        "      except: continue'''\n",
        "def refresh_installers(controls):\n",
        "  for c in controls:\n",
        "    if isinstance(c, Switch):\n",
        "      c.update()\n",
        "\n",
        "def buildInstallers(page):\n",
        "  global prefs, status, model_path\n",
        "  def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "      #page.update()\n",
        "      status['changed_installers'] = True\n",
        "  def changed_status(e, stat=None):\n",
        "      if stat is not None:\n",
        "        status[stat] = e.control.value\n",
        "  def toggle_diffusers(e):\n",
        "      prefs['install_diffusers'] = e.control.value\n",
        "      diffusers_settings.height=None if prefs['install_diffusers'] else 0\n",
        "      diffusers_settings.update()\n",
        "      status['changed_installers'] = True\n",
        "  install_diffusers = Switcher(label=\"Install HuggingFace Diffusers Pipeline\", value=prefs['install_diffusers'], disabled=status['installed_diffusers'], on_change=toggle_diffusers, tooltip=\"Required Libraries for most Image Generation functionality\")\n",
        "  def change_scheduler(e):\n",
        "      show = e.control.value == \"DDIM\"\n",
        "      update = prefs['scheduler_mode'] == \"DDIM\" or show\n",
        "      changed(e, 'scheduler_mode')\n",
        "      if update:\n",
        "          for eta in page.etas:\n",
        "            if isinstance(eta, SliderRow):\n",
        "              eta.show = show\n",
        "            else:\n",
        "              eta.visible = show\n",
        "              eta.update()\n",
        "  scheduler_mode = Dropdown(label=\"Scheduler/Sampler Mode\", hint_text=\"They're very similar, with minor differences in the generated noise\", width=220,\n",
        "            options=[\n",
        "                dropdown.Option(\"DDIM\"),\n",
        "                dropdown.Option(\"LMS Discrete\"),\n",
        "                dropdown.Option(\"PNDM\"),\n",
        "                dropdown.Option(\"IPNDM\"),\n",
        "                dropdown.Option(\"DPM Solver\"),\n",
        "                dropdown.Option(\"DPM Solver++\"),\n",
        "                dropdown.Option(\"DPM Solver Inverse\"),\n",
        "                dropdown.Option(\"SDE-DPM Solver++\"),\n",
        "                #dropdown.Option(\"DPM Stochastic\"),\n",
        "                dropdown.Option(\"K-Euler Discrete\"),\n",
        "                dropdown.Option(\"K-Euler Ancestral\"),\n",
        "                dropdown.Option(\"DEIS Multistep\"),\n",
        "                dropdown.Option(\"UniPC Multistep\"),\n",
        "                dropdown.Option(\"Heun Discrete\"),\n",
        "                dropdown.Option(\"Karras Heun Discrete\"),\n",
        "                dropdown.Option(\"K-DPM2 Ancestral\"),\n",
        "                dropdown.Option(\"K-DPM2 Discrete\"),\n",
        "                dropdown.Option(\"Karras-LMS\"),\n",
        "                dropdown.Option(\"LCM\"),\n",
        "            ], value=prefs['scheduler_mode'], autofocus=False, on_change=change_scheduler,\n",
        "        )\n",
        "  def changed_model_ckpt(e):\n",
        "      changed(e, 'model_ckpt')\n",
        "      model = get_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      try:\n",
        "          model_card.update()\n",
        "          if e.control.value.startswith(\"Stable\"):\n",
        "            custom_area.content = model_card\n",
        "          elif e.control.value == \"Community Finetuned Model\":\n",
        "            custom_area.content = Row([finetuned_model, model_card])\n",
        "          elif e.control.value == \"DreamBooth Library Model\":\n",
        "            custom_area.content = Row([dreambooth_library, model_card])\n",
        "          elif e.control.value == \"Custom Model Path\":\n",
        "            custom_area.content = Row([custom_model, model_card])\n",
        "          custom_area.update()\n",
        "      except Exception:\n",
        "          pass\n",
        "  def changed_finetuned_model(e):\n",
        "      changed(e, 'finetuned_model')\n",
        "      model = get_finetuned_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def changed_dreambooth_library(e):\n",
        "      changed(e, 'dreambooth_model')\n",
        "      model = get_dreambooth_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def changed_custom_model(e):\n",
        "      changed(e, 'custom_model')\n",
        "      model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def changed_SDXL_model(e):\n",
        "      changed(e, 'SDXL_model')\n",
        "      SDXL_custom_model.visible = prefs['SDXL_model'] == 'Custom Model'\n",
        "      SDXL_custom_model.update()\n",
        "      model_card_SDXL.visible = not prefs['SDXL_model'] == 'Custom Model'\n",
        "      model_card_SDXL.update()\n",
        "      #model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}\n",
        "      #model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      #model_card.update()\n",
        "      model_SDXL = get_SDXL_model(e.control.value)\n",
        "      model_card_SDXL.value = f\"  [**Model Card**](https://huggingface.co/{model_SDXL['path']})\"\n",
        "      try:\n",
        "          model_card_SDXL.update()\n",
        "      except Exception:\n",
        "          pass\n",
        "  def scheduler_help(e):\n",
        "      alert_msg(page, \"üé∞   Sampler/Scheduler Modes Info\", content=[Text(\"It's difficult to explain the visible difference each scheduler will make on your diffusion, and we can't really say which is better for what. The variations are subtle, and there's a lot of complex random math going on, so it depends on your personal taste, style and prompts.\"),\n",
        "        Text(\"All the samplers are different algorithms for numerically approximating solutions to differential equations (DEs). In SD's case this is a high-dimensional differential equation that determines how the initial noise must be diffused (spread around the image) to produce a result image that minimizes a loss function (essentially the distance to a hypothetical 'perfect' match to the initial noise, but with additional push applied by the prompt). This incredibly complex differential equation is basically what's encoded in the billion+ floating-point numbers that make up a Stable Diffusion model.\"),\n",
        "        Text(\"A sampler essentially works by taking the given number of steps, sampling the latent space on each step to compute the local gradient (slope), to figure out which direction the next step should be taken in. Like a ball rolling down a hill, the sampler tries to get as low as possible in terms of minimizing the loss function. But what locally looks like the fastest route may not actually net you an optimal solution ‚Äì you may get stuck in a local optimum (a valley) and sometimes you have to first go up to find a better route down.\"),\n",
        "        Markdown(\"\"\"* **DDIM -** Denoising Diffusion Implicit Models is one of the first samplers for solving diffusion models. The image direction is approximated by the noise estimated by the noise predictor. It adds an _ETA_ parameter to set the amount of starting noise.\n",
        "* **LMS Discrete -** Linear Multistep Scheduler algorithm is a simple and widely used method based on Least Mean Square for estimating the parameters of a linear model. It iteratively updates the model's parameters in order to minimize the error between the model's predictions and the actual outputs of the system.\n",
        "* **PNDM -** Pseudo Numerical methods for Diffusion Models proposes uses advanced Ordinary Differential Equations integration techniques, namely Runge-Kutta method and a linear multi-step method. Generates higher quality synthetic images with fewer steps.\n",
        "* **IPNDM -** A fourth-order Improved Pseudo Linear Multistep scheduler with interesting results.\n",
        "* **DPM Solver -** Diffusion Probabilistic Model solver specifically designed for solving diffusion differential equations.\n",
        "* **DPM Solver++ -** Improved version of DPM that refines results at high guidance scale (CFG) values.\n",
        "* **DPM Solver Inverse -** Inverted scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. For Null-text Inversion for Editing Real Images.\n",
        "* **SDE-DPM Solver++ -** A fast Stochastic Differential Equation solver for the reverse diffusion SDE. Introduces some random drift to the process on each step to possibly find a route to a better solution than a fully deterministic solver.\n",
        "* **K-Euler Discrete -** This is a fast scheduler which can often generate good outputs in 20-40 steps. From the Elucidating the Design Space of Diffusion-Based Generative Models paper.\n",
        "* **K-Euler Ancestral -** Uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 30-40 steps. One of my personal favorites with the subtleties.\n",
        "* **DEIS Multistep -** Diffusion Exponential Integrator Sampler modifies the polynomial fitting formula in log-rho space instead of the original linear `t` space in the DEIS paper. The modification enjoys closed-form coefficients for exponential multistep update instead of replying on the numerical solver. aims to accelerate the sampling process while maintaining high sample quality. \n",
        "* **UniPC Multistep -** Unified Predictor-Corrector inspired by the predictor-corrector method in ODE solvers, it can achieve high-quality image generation in 5-10 steps. Combines UniC and UniP to create a powerful image improvement tool.\n",
        "* **Heun Discrete -** A more accurate improvement to Euler's method, but needs to predict noise twice in each step, so it is twice as slow as Euler. Uses a correction step to reduce error and is thus an example of a predictor‚Äìcorrector algorithm.\n",
        "* **Karras Heun Discrete -** Uses a different noise schedule empirically found by Tero Karras et al. Takes large steps at first and small steps at the end. In the context of DPMs, it's employed to simulate the evolution of data over time. \n",
        "* **K-DPM2 Ancestral -** Karras Diffusion Probabilistic Model Solver is accurate up to the second order. Ancestral sampling traces the data's evolution backward in time, from its final form back to its initial noisy state.\n",
        "* **K-DPM2 Discrete -** The solver discretizes the diffusion process into smaller time steps. This discretization allows for efficient and accurate sampling, balanced between speed and sample quality.\n",
        "* **Karras-LMS -** Linear Multi-Step Method is a standard method for solving ordinary differential equations. It aims at improving accuracy by clever use of the values of the previous time steps.\n",
        "* **LCM -** Multistep and onestep scheduler (Algorithm 3) used alongside Latent Consistency Model Pipeline in 1-8 steps. Use with LCM LoRA too.\n",
        "\n",
        "[Diffusers Scheduler Overview](https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md) | [Stable Diffusion Samplers: A Comprehensive Guide](https://stable-diffusion-art.com/samplers/) | [Sampler Differences Explained](https://www.reddit.com/r/StableDiffusion/comments/zgu6wd/comment/izkhkxc/)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "      ], okay=\"ü•¥  Hard to Pick...\", sound=False, wide=True)\n",
        "  scheduler_help_btn = IconButton(icons.HELP_OUTLINE, tooltip=\"Help with Sampler/Scheduler Modes\", on_click=scheduler_help)\n",
        "  def toggle_safe(e):\n",
        "      changed(e, 'install_safe')\n",
        "      safety_config.visible = e.control.value\n",
        "      safety_config.update()\n",
        "  def toggle_SD(e):\n",
        "      changed(e, 'install_text2img')\n",
        "      SD_params.height = None if e.control.value else 0\n",
        "      SD_params.update()\n",
        "  def toggle_SDXL(e):\n",
        "      changed(e, 'install_SDXL')\n",
        "      #TODO: Reenable when Compel works right\n",
        "      SDXL_params.height = None if e.control.value else 0\n",
        "      SDXL_params.update()\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  model_SDXL = get_SDXL_model(prefs['SDXL_model'])\n",
        "  model_path = model['path']\n",
        "  model_ckpt = Container(Dropdown(label=\"SD Model Checkpoint\", width=262, options=[\n",
        "      dropdown.Option(\"Stable Diffusion v2.1 x768\"), dropdown.Option(\"Stable Diffusion v2.1 x512\"),\n",
        "      dropdown.Option(\"Stable Diffusion v2.0 x768\"), dropdown.Option(\"Stable Diffusion v2.0 x512\"), dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\"),\n",
        "      dropdown.Option(\"Community Finetuned Model\"), dropdown.Option(\"DreamBooth Library Model\"), dropdown.Option(\"Custom Model Path\")], value=prefs['model_ckpt'], tooltip=\"Make sure you accepted the HuggingFace Model Cards first\", autofocus=False, on_change=changed_model_ckpt), col={'xs':9, 'lg':4}, width=262)\n",
        "  finetuned_model = Dropdown(label=\"Finetuned Model\", tooltip=\"Make sure you accepted the HuggingFace Model Cards first\", width=370, options=[], value=prefs['finetuned_model'], autofocus=False, on_change=changed_finetuned_model, col={'xs':11, 'lg':6})\n",
        "  model_card = Markdown(f\"  [**Model Card**](https://huggingface.co/{model['path']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "  model_card_SDXL = Markdown(f\"  [**Model Card**](https://huggingface.co/{model_SDXL['path']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "  for mod in finetuned_models:\n",
        "      finetuned_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "  page.finetuned_model = finetuned_model\n",
        "  dreambooth_library = Dropdown(label=\"DreamBooth Library\", hint_text=\"\", width=370, options=[], value=prefs['dreambooth_model'], autofocus=False, on_change=changed_dreambooth_library, col={'xs':10, 'md':4})\n",
        "  for db in dreambooth_models:\n",
        "      dreambooth_library.options.append(dropdown.Option(db[\"name\"]))\n",
        "  custom_model = TextField(label=\"Custom Model Path\", value=prefs['custom_model'], width=370, on_change=changed_custom_model)\n",
        "  page.custom_model = custom_model\n",
        "  #custom_area = AnimatedSwitcher(model_card, transition=\"scale\", duration=500, reverse_duration=200, switch_in_curve=AnimationCurve.EASE_OUT, switch_out_curve=\"easeIn\")\n",
        "  custom_area = Container(model_card, col={'xs':12, 'lg':6})\n",
        "  if prefs['model_ckpt'].startswith(\"Stable\"):\n",
        "      custom_area.content = model_card\n",
        "  elif prefs['model_ckpt'] == \"Community Finetuned Model\":\n",
        "      custom_area.content = Row([finetuned_model, model_card], col={'xs':9, 'lg':4})\n",
        "  elif prefs['model_ckpt'] == \"DreamBooth Library Model\":\n",
        "      custom_area.content = Row([dreambooth_library, model_card], col={'xs':9, 'lg':4})\n",
        "  elif prefs['model_ckpt'] == \"Custom Model Path\":\n",
        "      custom_area.content = Row([custom_model, model_card], col={'xs':9, 'lg':4})\n",
        "  model_row = ResponsiveRow([model_ckpt, custom_area], run_spacing=8, vertical_alignment=CrossAxisAlignment.CENTER)\n",
        "  SDXL_model = Dropdown(label=\"SDXL Model Checkpoint\", hint_text=\"\", width=370, options=[dropdown.Option(\"Custom Model\")], value=prefs['SDXL_model'], autofocus=False, on_change=changed_SDXL_model, col={'xs':9, 'md':4})\n",
        "  for xl in SDXL_models:\n",
        "      SDXL_model.options.append(dropdown.Option(xl[\"name\"]))\n",
        "  SDXL_custom_model = TextField(label=\"Custom Model Path\", value=prefs['SDXL_custom_model'], width=370, expand=True, visible=prefs['SDXL_model']=='Custom Model', on_change=lambda e:changed(e,'SDXL_custom_model'), col={'xs':3, 'md':8})\n",
        "  SDXL_model_row = Row([SDXL_model, SDXL_custom_model, model_card_SDXL], run_spacing=8, vertical_alignment=CrossAxisAlignment.CENTER)\n",
        "\n",
        "  enable_xformers = Checkbox(label=\"Enable Xformers Mem Efficient Attention\", tooltip=\"If using Torch < 2, speeds up diffusion process.\", value=prefs['enable_xformers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_xformers'))\n",
        "  enable_bitsandbytes = Checkbox(label=\"Enable BitsandBytes\", tooltip=\"For 8-Bit Low-Mem Optimizers and Trainers.\", value=prefs['enable_bitsandbytes'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_bitsandbytes'))\n",
        "  memory_optimization = Dropdown(label=\"Enable Memory Optimization\", width=290, options=[dropdown.Option(\"None\"), dropdown.Option(\"Attention Slicing\")], value=prefs['memory_optimization'], on_change=lambda e:changed(e, 'memory_optimization'))\n",
        "  if version.parse(torch.__version__) < version.parse(\"2.0.0\"):\n",
        "      memory_optimization.options.append(dropdown.Option(\"Xformers Mem Efficient Attention\"))\n",
        "      enable_xformers.visible = True\n",
        "  else:\n",
        "      enable_xformers.visible = False\n",
        "      prefs['enable_xformers'] = False\n",
        "  higher_vram_mode = Checkbox(label=\"Higher VRAM Mode\", tooltip=\"Adds a bit more precision & uses much more GPU memory. Not recommended unless you have >16GB VRAM.\", value=prefs['higher_vram_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'higher_vram_mode'))\n",
        "  sequential_cpu_offload = Checkbox(label=\"Enable Sequential CPU Offload\", tooltip=\"Offloads all models to CPU using accelerate, significantly reducing memory usage.\", value=prefs['sequential_cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'sequential_cpu_offload'))\n",
        "  enable_attention_slicing = Checkbox(label=\"Enable Attention Slicing\", tooltip=\"Saves VRAM while creating images so you can go bigger without running out of mem.\", value=prefs['enable_attention_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_attention_slicing'))\n",
        "  enable_vae_tiling = Checkbox(label=\"Enable VAE Tiling\", tooltip=\"The VAE will split the input tensor into tiles to compute decoding and encoding in several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\", value=prefs['vae_tiling'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_tiling'))\n",
        "  enable_vae_slicing = Checkbox(label=\"Enable VAE Slicing\", tooltip=\"Sliced VAE decode latents for larger batches of images with limited VRAM. Splits the input tensor in slices to compute decoding in several steps\", value=prefs['vae_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_slicing'))\n",
        "  enable_tome = Checkbox(label=\"Enable Token Merging\", tooltip=\"ToMe optimizes the Pipelines to create images faster, at the expense of some quality. Works by merging the redundant tokens / patches progressively in the forward pass of a Transformer-based network.\", value=prefs['enable_tome'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_tome'))\n",
        "  enable_deepcache = Checkbox(label=\"Enable DeepCache\", tooltip=\"Accelerates pipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture. Slightly reduces quality for speed...\", value=prefs['enable_deepcache'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_deepcache'))\n",
        "  enable_torch_compile = Checkbox(label=\"Enable Torch Compiling\", tooltip=\"Speeds up Torch 2.0 Processing, but takes a bit longer to initialize.\", value=prefs['enable_torch_compile'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_torch_compile'))\n",
        "  enable_torch_compile.visible = not sys.platform.startswith('win')\n",
        "  enable_freeu = Checkbox(label=\"Enable FreeU: Free Lunch\", tooltip=\"Technique to improve image quality by rebalancing the contributions from the UNet‚Äôs skip connections and backbone feature maps. Applied during inference, does not require any additional training or mem. Works on most pipeline tasks.\", value=prefs['enable_freeu'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_freeu'))\n",
        "  '''def toggle_freeu(e):\n",
        "      changed(e,'enable_freeu')\n",
        "      freeu_args.height = None if prefs['enable_freeu'] else 0\n",
        "      freeu_args.update()\n",
        "  b1 = SliderRow(label=\"b1\", min=1.0, max=1.2, divisions=4, round=2, pref=prefs['freeu_args'], key='b1', expand=True, col={'md':6}, tooltip=\"Backbone factor of the first stage block of decoder.\")\n",
        "  b2 = SliderRow(label=\"b2\", min=1.2, max=1.6, divisions=8, round=2, pref=prefs['freeu_args'], key='b2', expand=True, col={'md':6}, tooltip=\"Backbone factor of the second stage block of decoder.\")\n",
        "  s1 = SliderRow(label=\"s1\", min=0, max=1, divisions=20, round=1, pref=prefs['freeu_args'], key='s1', expand=True, col={'md':6}, tooltip=\"Skip factor of the first stage block of decoder.\")\n",
        "  s2 = SliderRow(label=\"s2\", min=0, max=1, divisions=20, round=1, pref=prefs['freeu_args'], key='s2', expand=True, col={'md':6}, tooltip=\"Skip factor of the second stage block of decoder.\")\n",
        "  freeu_args = Container(content=Column([ResponsiveRow([b1, b2, s1, s2])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=0, left=13), height = None if prefs['enable_freeu'] else 0)'''\n",
        "\n",
        "  #install_megapipe = Switcher(label=\"Install Stable Diffusion txt2image, img2img & Inpaint Mega Pipeline\", value=prefs['install_megapipe'], disabled=status['installed_megapipe'], on_change=lambda e:changed(e, 'install_megapipe'))\n",
        "  #install_text2img = Switcher(label=\"Install Stable Diffusion text2image, image2image & Inpaint Pipeline (/w Long Prompt Weighting)\", value=prefs['install_text2img'], disabled=status['installed_txt2img'], on_change=lambda e:changed(e, 'install_text2img'), tooltip=\"The best general purpose component. Create images with long prompts, weights & models\")\n",
        "  install_text2img = Switcher(label=\"Install Stable Diffusion text2image, image2image & Inpaint Pipelines\", value=prefs['install_text2img'], disabled=status['installed_txt2img'], on_change=toggle_SD, tooltip=\"Best general-purpose component for most SD models <= 2.1, but don't need if using SDXL.\")\n",
        "  SD_compel = Checkbox(label=\"Use Compel Long Prompt Weighting Embeds with SD\", tooltip=\"Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.\", value=prefs['SD_compel'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SD_compel'))\n",
        "  #SD_compel = Switcher(label=\"Use Compel Long Prompt Weighting Embeds with SD\", tooltip=\"Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.\", value=prefs['SD_compel'], on_change=lambda e:changed(e, 'SD_compel'))\n",
        "  SD_params = Container(Column([SD_compel]), padding=padding.only(top=0, left=32), height=None if prefs['install_text2img'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "  #SDXL_model_card = Markdown(f\"  [**Accept Model Card**](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "  install_SDXL = Switcher(label=\"Install Stable Diffusion XL 1.0 text2image, image2image & Inpaint Pipeline\", value=prefs['install_SDXL'], disabled=status['installed_SDXL'], on_change=toggle_SDXL, tooltip=\"Latest SDXL v1.0 trained on 1080p images.\")\n",
        "  SDXL_compel = Checkbox(label=\"Use Compel Long Prompt Weighting Embeds with SDXL\", tooltip=\"Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.\", value=prefs['SDXL_compel'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SDXL_compel'))\n",
        "  #SDXL_compel = Switcher(label=\"Use Compel Long Prompt Weighting Embeds with SDXL\", tooltip=\"Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.\", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))\n",
        "  SDXL_params = Container(Column([SDXL_compel]), padding=padding.only(top=0, left=32), height=None if prefs['install_SDXL'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  #SDXL_params.visible = False\n",
        "  install_img2img = Switcher(label=\"Install Stable Diffusion Specialized Inpainting Model for image2image & Inpaint Pipeline\", value=prefs['install_img2img'], disabled=status['installed_img2img'], on_change=lambda e:changed(e, 'install_img2img'), tooltip=\"Gets more coherant results modifying Inpaint init & mask images\")\n",
        "  #install_repaint = Tooltip(message=\"Without using prompts, redraw masked areas to remove and repaint.\", content=Switcher(label=\"Install Stable Diffusion RePaint Pipeline\", value=prefs['install_repaint'], disabled=status['installed_repaint'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_repaint')))\n",
        "  install_interpolation = Switcher(label=\"Install Stable Diffusion Prompt Walk Interpolation Pipeline\", value=prefs['install_interpolation'], disabled=status['installed_interpolation'], on_change=lambda e:changed(e, 'install_interpolation'), tooltip=\"Create multiple tween images between prompts latent space. Almost animation.\")\n",
        "  #install_dreamfusion = Tooltip(message=\"Generate interesting mesh .obj, texture and preview video from a prompt.\", content=Switcher(label=\"Install Stable Diffusion DreamFusion 3D Pipeline\", value=prefs['install_dreamfusion'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_dreamfusion'], on_change=lambda e:changed(e, 'install_dreamfusion')))\n",
        "  install_alt_diffusion = Switcher(label=\"Install AltDiffusion text2image & image2image Multilingual Pipeline\", value=prefs['install_alt_diffusion'], disabled=status['installed_alt_diffusion'], on_change=lambda e:changed(e, 'install_alt_diffusion'), tooltip=\"Multilingual Stable Diffusion supporting English, Chinese, Spanish, French, Russian, Japanese, Korean, Arabic and Italian.\")\n",
        "  install_attend_and_excite = Switcher(label=\"Install Attend and Excite text2image Pipeline\", value=prefs['install_attend_and_excite'], disabled=status['installed_attend_and_excite'], on_change=lambda e:changed(e, 'install_attend_and_excite'), tooltip=\"Provides textual Attention-Based Semantic Guidance control over the image generation.\")\n",
        "  install_SAG = Switcher(label=\"Install Self-Attention Guidance (SAG) text2image Pipeline\", value=prefs['install_SAG'], disabled=status['installed_SAG'], on_change=lambda e:changed(e, 'install_SAG'), tooltip=\"Intelligent guidance that can plugged into any diffusion model using their self-attention map, improving sample quality.\")\n",
        "  install_panorama = Switcher(label=\"Install MultiDiffusion Panorama text2image Pipeline\", value=prefs['install_panorama'], disabled=status['installed_panorama'], on_change=lambda e:changed(e, 'install_panorama'), tooltip=\"Generate panorama-like wide images, Fusing Diffusion Paths for Controlled Image Generation\")\n",
        "  install_imagic = Switcher(label=\"Install Stable Diffusion iMagic image2image Pipeline\", value=prefs['install_imagic'], disabled=status['installed_imagic'], on_change=lambda e:changed(e, 'install_imagic'), tooltip=\"Edit your image according to the prompted instructions like magic.\")\n",
        "  install_depth2img = Switcher(label=\"Install Stable Diffusion Depth2Image Pipeline\", value=prefs['install_depth2img'], disabled=status['installed_depth2img'], on_change=lambda e:changed(e, 'install_depth2img'), tooltip=\"Uses Depth-map of init image for text-guided image to image generation.\")\n",
        "  install_composable = Switcher(label=\"Install Stable Diffusion Composable text2image Pipeline\", value=prefs['install_composable'], disabled=status['installed_composable'], on_change=lambda e:changed(e, 'install_composable'), tooltip=\"Craft your prompts with | precise | weights AND composed together components | with AND NOT negatives.\")\n",
        "  install_safe = Switcher(label=\"Install Stable Diffusion Safe text2image Pipeline\", value=prefs['install_safe'], disabled=status['installed_safe'], on_change=toggle_safe, tooltip=\"Use a content quality tuned safety model, providing levels of NSFW protection.\")\n",
        "  safety_config = Container(Dropdown(label=\"Model Safety Level\", width=350, options=[dropdown.Option(\"Weak\"), dropdown.Option(\"Medium\"), dropdown.Option(\"Strong\"), dropdown.Option(\"Max\")], value=prefs['safety_config'], on_change=lambda e:changed(e, 'safety_config')), padding=padding.only(left=32))\n",
        "  safety_config.visible = prefs['install_safe']\n",
        "  install_versatile = Switcher(label=\"Install Versatile Diffusion text2image, Dual Guided & Image Variation Pipeline\", value=prefs['install_versatile'], disabled=status['installed_versatile'], on_change=lambda e:changed(e, 'install_versatile'), tooltip=\"Multi-flow model that provides both image and text data streams and conditioned on both text and image.\")\n",
        "\n",
        "  def toggle_clip(e):\n",
        "      prefs['install_CLIP_guided'] = e.control.value\n",
        "      status['changed_installers'] = True\n",
        "      clip_settings.height=None if prefs['install_CLIP_guided'] else 0\n",
        "      clip_settings.update()\n",
        "  install_CLIP_guided = Switcher(label=\"Install Stable Diffusion CLIP-Guided Pipeline\", value=prefs['install_CLIP_guided'], disabled=status['installed_clip'], on_change=toggle_clip, tooltip=\"Uses alternative LAION & OpenAI ViT diffusion. Takes more VRAM, so may need to make images smaller\")\n",
        "  clip_model_id = Dropdown(label=\"CLIP Model ID\", width=350,\n",
        "            options=[\n",
        "                dropdown.Option(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\"),\n",
        "                dropdown.Option(\"openai/clip-vit-base-patch32\"),\n",
        "                dropdown.Option(\"openai/clip-vit-base-patch16\"),\n",
        "                dropdown.Option(\"openai/clip-vit-large-patch14\"),\n",
        "            ], value=prefs['clip_model_id'], autofocus=False, on_change=lambda e:changed(e, 'clip_model_id'),\n",
        "        )\n",
        "  clip_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=4), content=Column([clip_model_id]))\n",
        "\n",
        "  def toggle_conceptualizer(e):\n",
        "      changed(e, 'install_conceptualizer')\n",
        "      conceptualizer_settings.height = None if e.control.value else 0\n",
        "      conceptualizer_settings.update()\n",
        "  def change_concepts_model(e):\n",
        "      nonlocal concept\n",
        "      changed(e, 'concepts_model')\n",
        "      concept = get_concept(e.control.value)\n",
        "      concepts_info.value = f\"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})\"\n",
        "      concepts_info.update()\n",
        "  def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "  def copy_token(e):\n",
        "      nonlocal concept\n",
        "      page.set_clipboard(f\"<{concept['token']}>\")\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã  Token <{concept['token']}> copied to clipboard... Paste as word in your Prompt Text.\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "  install_conceptualizer = Switcher(label=\"Install Stable Diffusion Textual-Inversion Conceptualizer Pipeline\", value=prefs['install_conceptualizer'], on_change=toggle_conceptualizer, tooltip=\"Loads specially trained concept models to include in prompt with token\")\n",
        "  concept = get_concept(prefs['concepts_model'])\n",
        "  concepts_model = Dropdown(label=\"SD-Concepts Library Model\", hint_text=\"Specially trained community models made with Textual-Inversion\", width=451, options=[], value=prefs['concepts_model'], on_change=change_concepts_model)\n",
        "  copy_token_btn = IconButton(icon=icons.CONTENT_COPY, tooltip=\"Copy Token to Clipboard\", on_click=copy_token)\n",
        "  concepts_row = Row([concepts_model, copy_token_btn])\n",
        "  concepts_info = Markdown(f\"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})\", selectable=True, on_tap_link=open_url)\n",
        "  conceptualizer_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=5), content=Column([concepts_row, concepts_info]))\n",
        "  conceptualizer_settings.height = None if prefs['install_conceptualizer'] else 0\n",
        "  for c in concepts: concepts_model.options.append(dropdown.Option(c['name']))\n",
        "  install_upscale = Switcher(label=\"Install Stable Diffusion v2 Upscale 4X Pipeline\", value=prefs['install_upscale'], disabled=status['installed_upscale'], on_change=lambda e:changed(e, 'install_upscale'), tooltip=\"Allows you to enlarge images with prompts. Note: Will run out of mem for images larger than 512px, start small.\")\n",
        "\n",
        "  diffusers_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, content=\n",
        "                                 Column([Container(Column([Container(None, height=3), model_row, SDXL_model_row,\n",
        "                                Container(content=None, height=4), Row([scheduler_mode, scheduler_help_btn]),\n",
        "                                 Row([enable_attention_slicing, higher_vram_mode, enable_xformers,#memory_optimization,\n",
        "                                      ]),\n",
        "                                 Row([enable_vae_slicing, enable_vae_tiling, enable_bitsandbytes]),\n",
        "                                 #enable_attention_slicing,\n",
        "                                 #Row([sequential_cpu_offload, enable_vae_tiling]),\n",
        "                                 Row([enable_freeu, enable_tome, enable_deepcache, enable_torch_compile]),\n",
        "                                 ]), padding=padding.only(left=32, top=4)),\n",
        "                                         install_text2img, SD_params, install_SDXL, SDXL_params, install_img2img, #install_repaint, #install_megapipe, install_alt_diffusion,\n",
        "                                         install_interpolation, install_CLIP_guided, clip_settings, install_conceptualizer, conceptualizer_settings, install_safe, safety_config,\n",
        "                                         install_versatile, install_SAG, install_attend_and_excite, install_panorama, install_imagic, install_depth2img, install_composable, install_upscale]))\n",
        "  def toggle_stability(e):\n",
        "      prefs['install_Stability_api'] = e.control.value\n",
        "      has_changed=True\n",
        "      stability_settings.height=None if prefs['install_Stability_api'] else 0\n",
        "      stability_settings.update()\n",
        "      page.update()\n",
        "  install_Stability_api = Switcher(label=\"Install Stability-API DreamStudio Pipeline\", value=prefs['install_Stability_api'], disabled=status['installed_stability'], on_change=toggle_stability, tooltip=\"Use DreamStudio.com servers without your GPU to create images on CPU.\")\n",
        "  use_Stability_api = Checkbox(label=\"Use Stability-ai API by default\", tooltip=\"Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..\", value=prefs['use_Stability_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_Stability_api'))\n",
        "  model_checkpoint = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=350, options=[dropdown.Option(\"stable-diffusion-xl-1024-v1-0\"), dropdown.Option(\"stable-diffusion-xl-1024-v0-9\"), dropdown.Option(\"stable-diffusion-xl-beta-v2-2-2\"), dropdown.Option(\"stable-diffusion-768-v2-1\"), dropdown.Option(\"stable-diffusion-512-v2-1\"), dropdown.Option(\"stable-diffusion-768-v2-0\"), dropdown.Option(\"stable-diffusion-512-v2-0\"), dropdown.Option(\"stable-diffusion-v1-5\"), dropdown.Option(\"stable-diffusion-v1\"), dropdown.Option(\"stable-inpainting-512-v2-0\"), dropdown.Option(\"stable-inpainting-v1-0\")], value=prefs['model_checkpoint'], autofocus=False, on_change=lambda e:changed(e, 'model_checkpoint'))\n",
        "  clip_guidance_preset = Dropdown(label=\"Clip Guidance Preset\", width=350, options=[dropdown.Option(\"SIMPLE\"), dropdown.Option(\"FAST_BLUE\"), dropdown.Option(\"FAST_GREEN\"), dropdown.Option(\"SLOW\"), dropdown.Option(\"SLOWER\"), dropdown.Option(\"SLOWEST\"), dropdown.Option(\"NONE\")], value=prefs['clip_guidance_preset'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance_preset'))\n",
        "  #generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"ddim\"), dropdown.Option(\"plms\"), dropdown.Option(\"k_euler\"), dropdown.Option(\"k_euler_ancestral\"), dropdown.Option(\"k_heun\"), dropdown.Option(\"k_dpm_2\"), dropdown.Option(\"k_dpm_2_ancestral\"), dropdown.Option(\"k_lms\")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))\n",
        "  generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"DDPM\"), dropdown.Option(\"K_EULER\"), dropdown.Option(\"K_EULER_ANCESTRAL\"), dropdown.Option(\"K_HEUN\"), dropdown.Option(\"K_DPMPP_2M\"), dropdown.Option(\"K_DPM_2_ANCESTRAL\"), dropdown.Option(\"K_LMS\"), dropdown.Option(\"K_DPMPP_2S_ANCESTRAL\"), dropdown.Option(\"K_DPM_2\")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))\n",
        "  #\"K_EULER\" \"K_DPM_2\" \"K_LMS\" \"K_DPMPP_2S_ANCESTRAL\" \"K_DPMPP_2M\" \"DDIM\" \"DDPM\" \"K_EULER_ANCESTRAL\" \"K_HEUN\" \"K_DPM_2_ANCESTRAL\"\n",
        "  stability_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_Stability_api, model_checkpoint, generation_sampler, clip_guidance_preset]))\n",
        "\n",
        "  def toggle_AIHorde(e):\n",
        "      prefs['install_AIHorde_api'] = e.control.value\n",
        "      AIHorde_settings.height=None if prefs['install_AIHorde_api'] else 0\n",
        "      AIHorde_settings.update()\n",
        "      page.update()\n",
        "  install_AIHorde = Switcher(label=\"Install AIHorde Crowdsorced Pipeline\", value=prefs['install_AIHorde_api'], on_change=toggle_AIHorde, tooltip=\"Use AIHorde.net Crowdsourced cloud without your GPU to create images on CPU.\")\n",
        "  use_AIHorde = Checkbox(label=\"Use Stable Horde API by default\", tooltip=\"Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..\", value=prefs['use_AIHorde_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_AIHorde_api'))\n",
        "  AIHorde_model = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=350, options=[dropdown.Option(\"3DKX\"), dropdown.Option(\"Abyss OrangeMix\"), dropdown.Option(\"AbyssOrangeMix-AfterDark\"), dropdown.Option(\"ACertainThing\"), dropdown.Option(\"AIO Pixel Art\"), dropdown.Option(\"Analog Diffusion\"), dropdown.Option(\"Anime Pencil Diffusion\"), dropdown.Option(\"Anygen\"), dropdown.Option(\"Anything Diffusion\"), dropdown.Option(\"Anything v3\"), dropdown.Option(\"App Icon Diffusion\"), dropdown.Option(\"Arcane Diffusion\"), dropdown.Option(\"Archer Diffusion\"), dropdown.Option(\"Asim Simpsons\"), dropdown.Option(\"A to Zovya RPG\"), dropdown.Option(\"Balloon Art\"), dropdown.Option(\"Borderlands\"), dropdown.Option(\"BPModel\"), dropdown.Option(\"BubblyDubbly\"), dropdown.Option(\"Char\"), dropdown.Option(\"CharHelper\"), dropdown.Option(\"Cheese Daddys Landscape Mix\"), dropdown.Option(\"ChilloutMix\"), dropdown.Option(\"ChromaV5\"), dropdown.Option(\"Classic Animation Diffusion\"), dropdown.Option(\"Clazy\"), dropdown.Option(\"Colorful\"), dropdown.Option(\"Coloring Book\"), dropdown.Option(\"Comic-Diffusion\"), dropdown.Option(\"Concept Sheet\"), dropdown.Option(\"Counterfeit\"), dropdown.Option(\"Cyberpunk Anime Diffusion\"), dropdown.Option(\"CyriousMix\"), dropdown.Option(\"Dan Mumford Style\"), dropdown.Option(\"Darkest Diffusion\"), dropdown.Option(\"Dark Victorian Diffusion\"), dropdown.Option(\"Deliberate\"), dropdown.Option(\"DGSpitzer Art Diffusion\"), dropdown.Option(\"Disco Elysium\"), dropdown.Option(\"DnD Item\"), dropdown.Option(\"Double Exposure Diffusion\"), dropdown.Option(\"Dreamlike Diffusion\"), dropdown.Option(\"Dreamlike Photoreal\"), dropdown.Option(\"DreamLikeSamKuvshinov\"), dropdown.Option(\"Dreamshaper\"), dropdown.Option(\"DucHaiten\"), dropdown.Option(\"DucHaiten Classic Anime\"), dropdown.Option(\"Dungeons and Diffusion\"), dropdown.Option(\"Dungeons n Waifus\"), dropdown.Option(\"Eimis Anime Diffusion\"), dropdown.Option(\"Elden Ring Diffusion\"), dropdown.Option(\"Elldreth's Lucid Mix\"), dropdown.Option(\"Elldreths Retro Mix\"), dropdown.Option(\"Epic Diffusion\"), dropdown.Option(\"Eternos\"), dropdown.Option(\"Experience\"), dropdown.Option(\"ExpMix Line\"), dropdown.Option(\"FaeTastic\"), dropdown.Option(\"Fantasy Card Diffusion\"), dropdown.Option(\"FKing SciFi\"), dropdown.Option(\"Funko Diffusion\"), dropdown.Option(\"Furry Epoch\"), dropdown.Option(\"Future Diffusion\"), dropdown.Option(\"Ghibli Diffusion\"), dropdown.Option(\"GorynichMix\"), dropdown.Option(\"Grapefruit Hentai\"), dropdown.Option(\"Graphic-Art\"), dropdown.Option(\"GTA5 Artwork Diffusion\"), dropdown.Option(\"GuoFeng\"), dropdown.Option(\"Guohua Diffusion\"), dropdown.Option(\"HASDX\"), dropdown.Option(\"Hassanblend\"), dropdown.Option(\"Healy's Anime Blend\"), dropdown.Option(\"Hentai Diffusion\"), dropdown.Option(\"HRL\"), dropdown.Option(\"iCoMix\"), dropdown.Option(\"Illuminati Diffusion\"), dropdown.Option(\"Inkpunk Diffusion\"), dropdown.Option(\"Jim Eidomode\"), dropdown.Option(\"JWST Deep Space Diffusion\"), dropdown.Option(\"Kenshi\"), dropdown.Option(\"Knollingcase\"), dropdown.Option(\"Korestyle\"), dropdown.Option(\"kurzgesagt\"), dropdown.Option(\"Laolei New Berry Protogen Mix\"), dropdown.Option(\"Lawlas's yiff mix\"), dropdown.Option(\"Liberty\"), dropdown.Option(\"Marvel Diffusion\"), dropdown.Option(\"Mega Merge Diffusion\"), dropdown.Option(\"Microcasing\"), dropdown.Option(\"Microchars\"), dropdown.Option(\"Microcritters\"), dropdown.Option(\"Microscopic\"), dropdown.Option(\"Microworlds\"), dropdown.Option(\"Midjourney Diffusion\"), dropdown.Option(\"Midjourney PaintArt\"), dropdown.Option(\"Min Illust Background\"), dropdown.Option(\"ModernArt Diffusion\"), dropdown.Option(\"mo-di-diffusion\"), dropdown.Option(\"Moedel\"), dropdown.Option(\"MoistMix\"), dropdown.Option(\"Movie Diffusion\"), dropdown.Option(\"NeverEnding Dream\"), dropdown.Option(\"Nitro Diffusion\"), dropdown.Option(\"Openniji\"), dropdown.Option(\"OrbAI\"), dropdown.Option(\"Papercutcraft\"), dropdown.Option(\"Papercut Diffusion\"), dropdown.Option(\"Pastel Mix\"), dropdown.Option(\"Perfect World\"), dropdown.Option(\"PFG\"), dropdown.Option(\"pix2pix\"), dropdown.Option(\"PIXHELL\"), dropdown.Option(\"Poison\"), dropdown.Option(\"Pokemon3D\"), dropdown.Option(\"PortraitPlus\"), dropdown.Option(\"PPP\"), dropdown.Option(\"Pretty 2.5D\"), dropdown.Option(\"PRMJ\"), dropdown.Option(\"Project Unreal Engine 5\"), dropdown.Option(\"ProtoGen\"), dropdown.Option(\"Protogen Anime\"), dropdown.Option(\"Protogen Infinity\"), dropdown.Option(\"Pulp Vector Art\"), dropdown.Option(\"PVC\"), dropdown.Option(\"Rachel Walker Watercolors\"), dropdown.Option(\"Rainbowpatch\"), dropdown.Option(\"Ranma Diffusion\"), dropdown.Option(\"RCNZ Dumb Monkey\"), dropdown.Option(\"RCNZ Gorilla With A Brick\"), dropdown.Option(\"RealBiter\"), dropdown.Option(\"Realism Engine\"), dropdown.Option(\"Realistic Vision\"), dropdown.Option(\"Redshift Diffusion\"), dropdown.Option(\"Rev Animated\"), dropdown.Option(\"Robo-Diffusion\"), dropdown.Option(\"Rodent Diffusion\"), dropdown.Option(\"RPG\"), dropdown.Option(\"Samdoesarts Ultmerge\"), dropdown.Option(\"Sci-Fi Diffusion\"), dropdown.Option(\"SD-Silicon\"), dropdown.Option(\"Seek.art MEGA\"), dropdown.Option(\"Smoke Diffusion\"), dropdown.Option(\"Something\"), dropdown.Option(\"Sonic Diffusion\"), dropdown.Option(\"Spider-Verse Diffusion\"), dropdown.Option(\"Squishmallow Diffusion\"), dropdown.Option(\"SDXL_beta\"), dropdown.Option(\"stable_diffusion\"), dropdown.Option(\"stable_diffusion_2.1\"), dropdown.Option(\"stable_diffusion_inpainting\"), dropdown.Option(\"Supermarionation\"), dropdown.Option(\"Sygil-Dev Diffusion\"), dropdown.Option(\"Synthwave\"), dropdown.Option(\"SynthwavePunk\"), dropdown.Option(\"TrexMix\"), dropdown.Option(\"trinart\"), dropdown.Option(\"Trinart Characters\"), dropdown.Option(\"Tron Legacy Diffusion\"), dropdown.Option(\"T-Shirt Diffusion\"), dropdown.Option(\"T-Shirt Print Designs\"), dropdown.Option(\"Uhmami\"), dropdown.Option(\"Ultraskin\"), dropdown.Option(\"UMI Olympus\"), dropdown.Option(\"Unstable Ink Dream\"), dropdown.Option(\"URPM\"), dropdown.Option(\"Valorant Diffusion\"), dropdown.Option(\"Van Gogh Diffusion\"), dropdown.Option(\"Vector Art\"), dropdown.Option(\"vectorartz\"), dropdown.Option(\"Vintedois Diffusion\"), dropdown.Option(\"VinteProtogenMix\"), dropdown.Option(\"Vivid Watercolors\"), dropdown.Option(\"Voxel Art Diffusion\"), dropdown.Option(\"waifu_diffusion\"), dropdown.Option(\"Wavyfusion\"), dropdown.Option(\"Woop-Woop Photo\"), dropdown.Option(\"Xynthii-Diffusion\"), dropdown.Option(\"Yiffy\"), dropdown.Option(\"Zack3D\"), dropdown.Option(\"Zeipher Female Model\"), dropdown.Option(\"Zelda BOTW\")], value=prefs['AIHorde_model'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_model'))\n",
        "  AIHorde_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"k_lms\"), dropdown.Option(\"k_heun\"), dropdown.Option(\"k_euler\"), dropdown.Option(\"k_euler_a\"), dropdown.Option(\"k_dpm_2\"), dropdown.Option(\"k_dpm_2_a\"), dropdown.Option(\"k_dpm_fast\"), dropdown.Option(\"k_dpm_adaptive\"), dropdown.Option(\"k_dpmpp_2s_a\"), dropdown.Option(\"k_dpmpp_2m\"), dropdown.Option(\"dpmsolver\"), dropdown.Option(\"k_dpmpp_sde\"), dropdown.Option(\"DDIM\")], value=prefs['AIHorde_sampler'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_sampler'))\n",
        "  AIHorde_post_processing = Dropdown(label=\"Post-Processing\", hint_text=\"\", width=350, options=[dropdown.Option(\"None\"), dropdown.Option(\"GFPGAN\"), dropdown.Option(\"RealESRGAN_x4plus\"), dropdown.Option(\"RealESRGAN_x2plus\"), dropdown.Option(\"RealESRGAN_x4plus_anime_6B\"), dropdown.Option(\"NMKD_Siax\"), dropdown.Option(\"4x_AnimeSharp\"), dropdown.Option(\"CodeFormers\"), dropdown.Option(\"strip_background\")], value=prefs['AIHorde_post_processing'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_post_processing'))\n",
        "  AIHorde_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_AIHorde, AIHorde_model, AIHorde_sampler, AIHorde_post_processing]))\n",
        "  AIHorde_settings.height = None if prefs['install_AIHorde_api'] else 0\n",
        "  def toggle_upscale(e):\n",
        "      prefs['install_ESRGAN'] = e.control.value\n",
        "      upscale_settings.height=None if prefs['install_ESRGAN'] else 0\n",
        "      upscale_settings.update()\n",
        "      page.update()\n",
        "  def change_upscale_model(e):\n",
        "      prefs['upscale_model'] = e.control.value\n",
        "      for u in Real_ESRGAN_models:\n",
        "        if u['name'] == prefs['upscale_model']:\n",
        "          model_info.value = f\"  [**Model Card**]({u['info']})\"\n",
        "          model_info.update()\n",
        "  install_ESRGAN = Switcher(label=\"Install Real-ESRGAN AI Upscaler\", value=prefs['install_ESRGAN'], disabled=status['installed_ESRGAN'], on_change=toggle_upscale, tooltip=\"Recommended to enlarge & sharpen all images as they're made.\")\n",
        "  upscale_model = Dropdown(label=\"ESRGAN Upscale Model\", hint_text=\"\", width=300, options=[], value=prefs['upscale_model'], autofocus=False, on_change=change_upscale_model)\n",
        "  for u in Real_ESRGAN_models:\n",
        "    upscale_model.options.append(dropdown.Option(u['name']))\n",
        "    if u['name'] == prefs['upscale_model']:\n",
        "      current_model = u\n",
        "  model_info = Markdown(f\"  [**Model Info**]({current_model['info']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "  upscale_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=4), content=Row([upscale_model, model_info]))\n",
        "  upscale_settings.height = None if prefs['install_ESRGAN'] else 0\n",
        "  \n",
        "  install_OpenAI = Switcher(label=\"Install OpenAI GPT-3 Text Engine\", value=prefs['install_OpenAI'], disabled=status['installed_OpenAI'], on_change=lambda e:changed(e, 'install_OpenAI'), tooltip=\"Use advanced AI to help make creative prompts. Also enables DALL-E 2 generation.\")\n",
        "  install_TextSynth = Switcher(label=\"Install TextSynth GPT-J Text Engine\", value=prefs['install_TextSynth'], disabled=status['installed_TextSynth'], on_change=lambda e:changed(e, 'install_TextSynth'), tooltip=\"Alternative Text AI for brainstorming & rewriting your prompts. Pretty smart..\")\n",
        "  diffusers_settings.height = None if prefs['install_diffusers'] else 0\n",
        "  stability_settings.height = None if prefs['install_Stability_api'] else 0\n",
        "  clip_settings.height = None if prefs['install_CLIP_guided'] else 0\n",
        "\n",
        "  def run_installers(e):\n",
        "      global force_updates\n",
        "      def console_clear():\n",
        "        page.banner.content.controls = []\n",
        "        page.update()\n",
        "      def console_msg(msg, clear=True, show_progress=True):\n",
        "        if not page.banner.open:\n",
        "          page.banner.open = True\n",
        "        if clear:\n",
        "          page.banner.content.controls = []\n",
        "        if show_progress:\n",
        "          page.banner.content.controls.append(Row([Stack([Icon(icons.DOWNLOADING, color=colors.AMBER, size=48), Container(content=ProgressRing(), padding=padding.only(top=6, left=6), alignment=alignment.center)]), Container(content=Text(\"  \" + msg.strip() , weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)) ]))\n",
        "          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center if (page.width if page.web else page.window_width) > 768 else alignment.center_right)]))\n",
        "          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center)]))\n",
        "          #page.banner.content.controls.append(Row([Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.GREEN_600), ProgressRing()]))\n",
        "        else:\n",
        "          page.banner.content.controls.append(Text(msg.strip(), weight=FontWeight.BOLD, color=colors.GREEN_600))\n",
        "        page.update()\n",
        "      page.console_msg = console_msg\n",
        "      if status['changed_installers']:\n",
        "        save_settings_file(page, change_icon=False)\n",
        "        status['changed_installers'] = False\n",
        "      # Temporary until I get Xformers to work\n",
        "      #prefs['memory_optimization'] = 'Attention Slicing' if prefs['enable_attention_slicing'] else 'None'\n",
        "      if prefs['install_diffusers'] and not bool(prefs['HuggingFace_api_key']):\n",
        "        alert_msg(e.page, \"You must provide your HuggingFace API Key to use Diffusers.\")\n",
        "        return\n",
        "      if prefs['install_Stability_api'] and not bool(prefs['Stability_api_key']):\n",
        "        alert_msg(e.page, \"You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will use your tokens.\")\n",
        "        return\n",
        "      if prefs['install_OpenAI'] and not bool(prefs['OpenAI_api_key']):\n",
        "        alert_msg(e.page, \"You must have your OpenAI API Key to use GPT-3 Text AI.\")\n",
        "        return\n",
        "      if prefs['install_TextSynth'] and not bool(prefs['TextSynth_api_key']):\n",
        "        alert_msg(e.page, \"You must have your TextSynth API Key to use GPT-J Text AI.\")\n",
        "        return\n",
        "      if prefs['install_AIHorde_api'] and not bool(prefs['AIHorde_api_key']):\n",
        "        alert_msg(e.page, \"You must have your AIHorde.net API Key to use Stable Horde.  Note that it will use your Kudos.\")\n",
        "        return\n",
        "      page.banner.content = Column([], scroll=ScrollMode.AUTO, auto_scroll=True, tight=True, spacing=0, alignment=MainAxisAlignment.END)\n",
        "      page.banner.open = True\n",
        "      page.update()\n",
        "      if prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "        get_diffusers(page)\n",
        "        status['installed_diffusers'] = True\n",
        "      if prefs['install_text2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Text2Image, Image2Image & Inpaint Pipeline...\")\n",
        "        #with io.StringIO() as buf, redirect_stdout(buf):\n",
        "        #print('redirected')\n",
        "        get_text2image(page)\n",
        "        #output = buf.getvalue()\n",
        "        #page.banner.content.controls.append(Text(output.strip()))\n",
        "        status['installed_txt2img'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()\n",
        "        page.update()\n",
        "      if prefs['install_img2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Inpaint Model & Image2Image Pipeline...\")\n",
        "        get_image2image(page)\n",
        "        status['installed_img2img'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()\n",
        "        page.use_inpaint_model.visible = True\n",
        "        page.use_inpaint_model.update()\n",
        "        if not status['installed_txt2img']:\n",
        "          prefs['use_inpaint_model'] = True\n",
        "      '''if prefs['install_megapipe'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Unified Mega Pipeline...\")\n",
        "        get_text2image(page)\n",
        "        status['installed_megapipe'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()'''\n",
        "      if prefs['install_SDXL'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion XL Text2Image, Image2Image & Inpaint Pipeline...\")\n",
        "        if get_SDXL(page):\n",
        "          status['installed_SDXL'] = True\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "          page.use_SDXL.visible = True\n",
        "          page.use_SDXL.update()\n",
        "          page.SDXL_params.visible = True\n",
        "          page.SDXL_params.update()\n",
        "      if prefs['install_alt_diffusion'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing AltDiffusion Text2Image & Image2Image Pipeline...\")\n",
        "        get_alt_diffusion(page)\n",
        "        status['installed_alt_diffusion'] = True\n",
        "        page.use_alt_diffusion.visible = True\n",
        "        page.use_alt_diffusion.update()\n",
        "      if prefs['install_interpolation'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Walk Interpolation Pipeline...\")\n",
        "        get_interpolation(page)\n",
        "        status['installed_interpolation'] = True\n",
        "        page.interpolation_block.visible = True\n",
        "        page.interpolation_block.update()\n",
        "      if prefs['install_CLIP_guided'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion CLIP-Guided Pipeline...\")\n",
        "        get_clip(page)\n",
        "        status['installed_clip'] = True\n",
        "        page.use_clip_guided_model.visible = True\n",
        "        page.use_clip_guided_model.update()\n",
        "        page.clip_block.height = None if prefs['use_clip_guided_model'] else 0\n",
        "        page.clip_block.update()\n",
        "        if prefs['use_clip_guided_model']:\n",
        "          page.img_block.height = 0\n",
        "          page.img_block.update()\n",
        "      if prefs['install_conceptualizer'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing SD Concepts Library Textual Inversion Pipeline...\")\n",
        "        get_conceptualizer(page)\n",
        "        page.use_conceptualizer_model.visible = True\n",
        "        page.use_conceptualizer_model.update()\n",
        "        if prefs['use_conceptualizer']:\n",
        "          page.img_block.height = 0\n",
        "          page.img_block.update()\n",
        "        status['installed_conceptualizer'] = True\n",
        "      if prefs['install_repaint'] and not status['installed_repaint'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion RePaint Pipeline...\")\n",
        "        get_repaint(page)\n",
        "        status['installed_repaint'] = True\n",
        "      if prefs['install_depth2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion 2 Depth2Image Pipeline...\")\n",
        "        get_depth2img(page)\n",
        "        status['installed_depth2img'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_depth2img.visible = True\n",
        "        page.use_depth2img.update()\n",
        "      if prefs['install_SAG'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Self-Attention Guidance Text2Image Pipeline...\")\n",
        "        get_SAG(page)\n",
        "        status['installed_SAG'] = True\n",
        "        page.use_SAG.visible = True\n",
        "        page.use_SAG.update()\n",
        "      if prefs['install_attend_and_excite'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Attend and Excite Text2Image Pipeline...\")\n",
        "        get_attend_and_excite(page)\n",
        "        status['installed_attend_and_excite'] = True\n",
        "        page.use_attend_and_excite.visible = True\n",
        "        page.use_attend_and_excite.update()\n",
        "      if prefs['install_imagic'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion iMagic Image2Image Pipeline...\")\n",
        "        get_imagic(page)\n",
        "        status['installed_imagic'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_imagic.visible = True\n",
        "        page.use_imagic.update()\n",
        "      if prefs['install_composable'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Composable Text2Image Pipeline...\")\n",
        "        get_composable(page)\n",
        "        status['installed_composable'] = True\n",
        "        page.use_composable.visible = True\n",
        "        page.use_composable.update()\n",
        "      if prefs['install_versatile'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Versatile Text2Image, Variation & Inpaint Pipeline...\")\n",
        "        get_versatile(page)\n",
        "        status['installed_versatile'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_versatile.visible = True\n",
        "        page.use_versatile.update()\n",
        "      if prefs['install_safe'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Safe Text2Image Pipeline...\")\n",
        "        get_safe(page)\n",
        "        status['installed_safe'] = True\n",
        "        page.use_safe.visible = True\n",
        "        page.use_safe.update()\n",
        "      if prefs['install_panorama'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing MultiDiffusion Panorama Text2Image Pipeline...\")\n",
        "        get_panorama(page)\n",
        "        status['installed_panorama'] = True\n",
        "        page.use_panorama.visible = True\n",
        "        page.use_panorama.update()\n",
        "      if prefs['install_upscale'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion 4X Upscale Pipeline...\")\n",
        "        get_upscale(page)\n",
        "        status['installed_upscale'] = True\n",
        "        page.use_upscale.visible = True\n",
        "        page.use_upscale.update()\n",
        "      if prefs['install_dreamfusion'] and not status['installed_dreamfusion'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion DreamFusion 3D Pipeline...\")\n",
        "        get_dreamfusion(page) # No longer installing from here\n",
        "        status['installed_dreamfusion'] = True\n",
        "      if prefs['install_Stability_api']:\n",
        "        console_msg(\"Installing Stability-API DreamStudio.ai Pipeline...\")\n",
        "        get_stability(page)\n",
        "        status['installed_stability'] = True\n",
        "      if prefs['install_AIHorde_api']:\n",
        "        console_msg(\"Installing Stable Horde AIHorde.net Pipeline...\")\n",
        "        get_AIHorde(page)\n",
        "      if prefs['install_ESRGAN'] and not status['installed_ESRGAN']:\n",
        "        if not os.path.isdir(os.path.join(dist_dir, 'Real-ESRGAN')):\n",
        "          get_ESRGAN(page)\n",
        "          console_msg(\"Installing Real-ESRGAN Upscaler...\")\n",
        "        status['installed_ESRGAN'] = True\n",
        "      if prefs['install_ESRGAN']:\n",
        "        ESRGAN_blocks = [\n",
        "          page.ESRGAN_block,\n",
        "          page.ESRGAN_block_material,\n",
        "          page.ESRGAN_block_dalle,\n",
        "          page.ESRGAN_block_kandinsky,\n",
        "          page.ESRGAN_block_kandinsky_fuse,\n",
        "          page.ESRGAN_block_kandinsky_controlnet,\n",
        "          #page.ESRGAN_block_kandinsky21,\n",
        "          #page.ESRGAN_block_kandinsky21_fuse,\n",
        "          page.ESRGAN_block_deepfloyd,\n",
        "          page.ESRGAN_block_amused,\n",
        "          page.ESRGAN_block_wuerstchen,\n",
        "          page.ESRGAN_block_pixart_alpha,\n",
        "          page.ESRGAN_block_lmd_plus,\n",
        "          page.ESRGAN_block_blip_diffusion,\n",
        "          page.ESRGAN_block_reference,\n",
        "          page.ESRGAN_block_unCLIP,\n",
        "          page.ESRGAN_block_unCLIP_image_variation,\n",
        "          page.ESRGAN_block_unCLIP_interpolation,\n",
        "          page.ESRGAN_block_unCLIP_image_interpolation,\n",
        "          page.ESRGAN_block_semantic,\n",
        "          page.ESRGAN_block_EDICT,\n",
        "          page.ESRGAN_block_DiffEdit,\n",
        "          page.ESRGAN_block_null_text,\n",
        "          page.ESRGAN_block_magic_mix,\n",
        "          page.ESRGAN_block_paint_by_example,\n",
        "          page.ESRGAN_block_instruct_pix2pix,\n",
        "          page.ESRGAN_block_controlnet,\n",
        "          page.ESRGAN_block_controlnet_qr,\n",
        "          page.ESRGAN_block_controlnet_segment,\n",
        "          page.ESRGAN_block_styler,\n",
        "          page.ESRGAN_block_deep_daze,\n",
        "          page.ESRGAN_block_DiT,\n",
        "          page.ESRGAN_block_text_to_video,\n",
        "          page.ESRGAN_block_text_to_video_zero,\n",
        "          page.ESRGAN_block_stable_animation,\n",
        "        ]\n",
        "        for b in ESRGAN_blocks:\n",
        "          b.height = None\n",
        "          b.update()\n",
        "      if prefs['install_OpenAI'] and not status['installed_OpenAI']:\n",
        "        try:\n",
        "          import openai\n",
        "        except ModuleNotFoundError as e:\n",
        "          console_msg(\"Installing OpenAI GPT-3 Libraries...\")\n",
        "          run_process(\"pip install openai -qq\", page=page)\n",
        "          pass\n",
        "        status['installed_OpenAI'] = True\n",
        "      if prefs['install_TextSynth'] and not status['installed_TextSynth']:\n",
        "        try:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        except ModuleNotFoundError as e:\n",
        "          console_msg(\"Installing TextSynth GPT-J Libraries...\")\n",
        "          run_process(\"pip install textsynthpy -qq\", page=page)\n",
        "          pass\n",
        "        status['installed_TextSynth'] = True\n",
        "      #print('Done Installing...')\n",
        "      if prefs['enable_sounds']: page.snd_done.play()\n",
        "      console_clear()\n",
        "      page.banner.open = False\n",
        "      page.banner.update()\n",
        "      page.update()\n",
        "      install_diffusers.update()\n",
        "      #install_text2img.update()\n",
        "      #install_img2img.update()\n",
        "      install_Stability_api.update()\n",
        "      install_CLIP_guided.update()\n",
        "      install_ESRGAN.update()\n",
        "      install_OpenAI.update()\n",
        "      install_TextSynth.update()\n",
        "      update_parameters(page)\n",
        "      page.Parameters.controls[0].content.update()\n",
        "      #page.Parameters.updater()\n",
        "      if force_updates:\n",
        "          prefs['last_updated'] = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "          force_updates = False\n",
        "      if current_tab==1:\n",
        "        page.Installers.controls[0].content.update()\n",
        "        page.Installers.update()\n",
        "        page.show_install_fab(False)\n",
        "        page.tabs.selected_index = 2\n",
        "        page.tabs.update()\n",
        "        page.update()\n",
        "  def show_install_fab(show = True):\n",
        "    if show:\n",
        "      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.FILE_DOWNLOAD), Text(\"Run Installations\", size=18)], alignment=\"center\", spacing=5), width=205, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_installers)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.FILE_DOWNLOAD, text=\"Run Installations\", on_click=run_installers)\n",
        "      page.update()\n",
        "    else:\n",
        "      if page.floating_action_button is not None:\n",
        "        page.floating_action_button = None\n",
        "        page.update()\n",
        "  page.show_install_fab = show_install_fab\n",
        "  install_button = ElevatedButton(content=Text(value=\"‚è¨   Run Installations \", size=20), on_click=run_installers)\n",
        "\n",
        "  #image_output = TextField(label=\"Image Output Path\", value=prefs['image_output'], on_change=changed)\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "                content=Column([\n",
        "        Header(\"üì•  Stable Diffusion Required & Optional Installers\", subtitle=\"Run this every time you Start the App or make Changes. Only pick what you plan to use this session...\"),\n",
        "        install_diffusers,\n",
        "        diffusers_settings,\n",
        "        #install_text2img,\n",
        "        #install_img2img,\n",
        "        install_Stability_api,\n",
        "        stability_settings,\n",
        "        install_AIHorde,\n",
        "        AIHorde_settings,\n",
        "        #install_CLIP_guided,\n",
        "        #clip_settings,\n",
        "        install_ESRGAN, upscale_settings,\n",
        "        install_OpenAI,\n",
        "        install_TextSynth,\n",
        "        #install_button,\n",
        "        Container(content=None, height=32),\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)\n",
        "  def init_boxes():\n",
        "    diffusers_settings.height = None if prefs['install_diffusers'] else 0\n",
        "    stability_settings.height = None if prefs['install_Stability_api'] else 0\n",
        "    clip_settings.height = None if prefs['install_CLIP_guided'] else 0\n",
        "    diffusers_settings.update()\n",
        "    stability_settings.update()\n",
        "    clip_settings.update()\n",
        "    page.update()\n",
        "  #init_boxes()\n",
        "  return c\n",
        "\n",
        "def update_parameters(page):\n",
        "  #page.img_block.height = None if status['installed_img2img'] or status['installed_megapipe'] or status['installed_stability'] else 0\n",
        "  page.img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_AIHorde'] or status['installed_SDXL']) and not (status['installed_clip'] and prefs['use_clip_guided_model']) else 0\n",
        "  page.clip_block.height = None if status['installed_clip']  and prefs['use_clip_guided_model'] else 0\n",
        "  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0\n",
        "  page.img_block.update()\n",
        "  page.clip_block.update()\n",
        "  page.ESRGAN_block.update()\n",
        "  page.Parameters.update()\n",
        "  #print(\"Updated Parameters\")\n",
        "\n",
        "if is_Colab:\n",
        "    from google.colab import files\n",
        "\n",
        "#LoRA_models = [{'name': 'Von Platen LoRA', 'path': 'patrickvonplaten/lora'}, {'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'Trauter LoRAs', 'path': 'YoungMasterFromSect/Trauter_LoRAs'}, {'name': 'Capitalize T5', 'path': 'ShengdingHu/Capitalize_T5-LoRA'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}]\n",
        "#[{'name': 'sample-dog', 'path': 'lora-library/lora-dreambooth-sample-dog', 'prefix': 'sksdog'}, {'name': 'kdekuni', 'path': 'lora-library/kdekuni', 'prefix': 'a kdekuni golden funkopop'}, {'name': 'yarosnnv', 'path': 'lora-library/yarosnnv', 'prefix': 'yarosnnv'}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, ]\n",
        "LoRA_models = [{'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}, {'name':'Openjourney LoRA', 'path':'prompthero/openjourney-lora', 'prefix': ''}, {'name':'Analog Diffusion', 'path':'https://replicate.delivery/pbxt/IzbeguwVsW3PcC1gbiLy5SeALwk4sGgWroHagcYIn9I960bQA/tmpjlodd7vazekezip.safetensors', 'prefix':'<1> '}, {'name': 'Analog.Redmond', 'path': 'artificialguybr/analogredmond', 'prefix':'AnalogRedmAF'}, {'name': 'LogoLoraForSDXL', 'path': 'artificialguybr/LogoRedmond-LogoLoraForSDXL', 'prefix':'LogoRedAF'}]\n",
        "\n",
        "def buildParameters(page):\n",
        "  global prefs, status, args\n",
        "  def changed(e, pref=None, asInt=False, apply=True):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value if not asInt else int(e.control.value)\n",
        "      if page.floating_action_button is None and apply:\n",
        "        show_apply_fab(len(prompts) > 0)\n",
        "      #if apply_changes_button.visible != (len(prompts) > 0): #status['changed_parameters']:\n",
        "      #  apply_changes_button.visible = len(prompts) > 0\n",
        "      #  apply_changes_button.update()\n",
        "      status['changed_parameters'] = True\n",
        "      #page.update()\n",
        "  def change(e):\n",
        "      if page.floating_action_button is None:\n",
        "        show_apply_fab(len(prompts) > 0)\n",
        "      status['changed_parameters'] = True\n",
        "  def run_parameters(e):\n",
        "      save_parameters()\n",
        "      #page.tabs.current_tab = 3\n",
        "      page.show_apply_fab(False)\n",
        "      page.tabs.selected_index = 3\n",
        "      page.tabs.update()\n",
        "      page.update()\n",
        "  def save_parameters():\n",
        "      update_args()\n",
        "      page.update_prompts()\n",
        "      save_settings_file(page)\n",
        "      status['changed_parameters'] = False\n",
        "  def apply_to_prompts(e):\n",
        "      update_args()\n",
        "      page.apply_changes(e)\n",
        "      save_settings_file(page)\n",
        "      show_apply_fab(False)\n",
        "      #apply_changes_button.visible = False\n",
        "      #apply_changes_button.update()\n",
        "  def pick_files_result(e: FilePickerResultEvent):\n",
        "      # TODO: This is not working on Colab, maybe it can get_upload_url on other platform?\n",
        "      if e.files:\n",
        "        img = e.files\n",
        "        uf = []\n",
        "        fname = img[0]\n",
        "        print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "        #print(os.path.join(fname.path, fname.name))\n",
        "        #src_path = os.path.join(fname.path, fname.name)\n",
        "        #for f in pick_files_dialog.result.files:\n",
        "        src_path = page.get_upload_url(fname.name, 600)\n",
        "        uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "        pick_files_dialog.upload(uf)\n",
        "        print(str(src_path))\n",
        "        #src_path = ''.join(src_path)\n",
        "        print(str(uf[0]))\n",
        "        dst_path = os.path.join(root_dir, fname.name)\n",
        "        print(f'Copy {src_path} to {dst_path}')\n",
        "        #shutil.copy(src_path, dst_path)\n",
        "        # TODO: is init or mask?\n",
        "        init_image.value = dst_path\n",
        "      #selected_files.value = (\", \".join(map(lambda f: f.name, e.files)) if e.files else \"Cancelled!\")\n",
        "      #selected_files.update()\n",
        "\n",
        "  pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "  page.overlay.append(pick_files_dialog)\n",
        "  #selected_files = Text()\n",
        "\n",
        "  def file_picker_result(e: FilePickerResultEvent):\n",
        "      if e.files != None:\n",
        "        upload_files(e)\n",
        "  def on_upload_progress(e: FilePickerUploadEvent):\n",
        "    nonlocal pick_type\n",
        "    if e.progress == 1:\n",
        "      if not slash in e.file_name:\n",
        "        fname = os.path.join(root_dir, e.file_name)\n",
        "      else:\n",
        "        fname = e.file_name\n",
        "      if pick_type == \"init\":\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        prefs['init_image'] = fname\n",
        "      elif pick_type == \"mask\":\n",
        "        mask_image.value = fname\n",
        "        mask_image.update()\n",
        "        prefs['mask_image'] = fname\n",
        "      page.update()\n",
        "  file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "  def upload_files(e):\n",
        "      uf = []\n",
        "      if file_picker.result != None and file_picker.result.files != None:\n",
        "          for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "          file_picker.upload(uf)\n",
        "  page.overlay.append(file_picker)\n",
        "  pick_type = \"\"\n",
        "  #page.overlay.append(pick_files_dialog)\n",
        "  def pick_init(e):\n",
        "      nonlocal pick_type\n",
        "      pick_type = \"init\"\n",
        "      file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "  def pick_mask(e):\n",
        "      nonlocal pick_type\n",
        "      pick_type = \"mask\"\n",
        "      file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "  def toggle_ESRGAN(e):\n",
        "      ESRGAN_settings.height = None if e.control.value else 0\n",
        "      prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "      ESRGAN_settings.update()\n",
        "      has_changed = True\n",
        "  def toggle_clip(e):\n",
        "      if e.control.value:\n",
        "        page.img_block.height = 0\n",
        "        page.clip_block.height = None if status['installed_clip'] else 0\n",
        "      else:\n",
        "        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "        page.clip_block.height = 0\n",
        "      page.img_block.update()\n",
        "      page.clip_block.update()\n",
        "      changed(e, 'use_clip_guided_model')\n",
        "  def change_use_cutouts(e):\n",
        "      num_cutouts.visible = e.control.value\n",
        "      num_cutouts.update()\n",
        "      changed(e, 'use_cutouts')\n",
        "  def toggle_interpolation(e):\n",
        "      interpolation_steps_slider.height = None if e.control.value else 0\n",
        "      interpolation_steps_slider.update()\n",
        "      if e.control.value: page.img_block.height = 0\n",
        "      else: page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "      page.img_block.update()\n",
        "      changed(e, 'use_interpolation', apply=False)\n",
        "  def change_interpolation_steps(e):\n",
        "      interpolation_steps_value.value = f\" {int(e.control.value)} steps\"\n",
        "      interpolation_steps_value.update()\n",
        "      changed(e, 'num_interpolation_steps', asInt=True, apply=False)\n",
        "  def toggle_SAG(e):\n",
        "      sag_scale_slider.height = None if e.control.value else 0\n",
        "      sag_scale_slider.update()\n",
        "      changed(e, 'use_SAG', apply=False)\n",
        "  def change_sag_scale(e):\n",
        "      sag_scale_value.value = f\" {float(e.control.value)}\"\n",
        "      sag_scale_value.update()\n",
        "      changed(e, 'sag_scale', apply=False)\n",
        "  def toggle_attend_and_excite(e):\n",
        "      max_iter_to_alter_slider.height = None if e.control.value else 0\n",
        "      max_iter_to_alter_slider.update()\n",
        "      changed(e, 'use_attend_and_excite', apply=False)\n",
        "  def change_max_iter_to_alter(e):\n",
        "      max_iter_to_alter_value.value = f\" {int(e.control.value)} Iterations\"\n",
        "      max_iter_to_alter_value.update()\n",
        "      changed(e, 'max_iter_to_alter', asInt=True, apply=False)\n",
        "  def change_enlarge_scale(e):\n",
        "      enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "      enlarge_scale_slider.update()\n",
        "      changed(e, 'enlarge_scale', apply=False)\n",
        "  def change_strength(e):\n",
        "      strength_value.value = f\" {int(e.control.value * 100)}%\"\n",
        "      strength_value.update()\n",
        "      changed(e, 'init_image_strength')\n",
        "  def toggle_SDXL(e):\n",
        "      changed(e,'use_SDXL', apply=False)\n",
        "      page.SDXL_params.height = None if e.control.value else 0\n",
        "      page.SDXL_params.update()\n",
        "  def toggle_conceptualizer(e):\n",
        "      if e.control.value:\n",
        "        page.img_block.height = 0\n",
        "      else:\n",
        "        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "      page.img_block.update()\n",
        "      changed(e, 'use_conceptualizer')\n",
        "  def toggle_centipede(e):\n",
        "      changed(e,'centipede_prompts_as_init_images', apply=False)\n",
        "      image_pickers.height = None if not e.control.value else 0\n",
        "      image_pickers.update()\n",
        "  def toggle_LoRA(e):\n",
        "      changed(e,'use_LoRA_model', apply=False)\n",
        "      LoRA_block.height = None if e.control.value else 0\n",
        "      LoRA_block.update()\n",
        "  def changed_LoRA(e):\n",
        "      changed(e, 'LoRA_model', apply=False)\n",
        "      custom_LoRA_model.visible = True if prefs['LoRA_model'] == \"Custom LoRA Path\" else False\n",
        "      custom_LoRA_model.update()\n",
        "  def changed_SDXL_LoRA(e):\n",
        "      changed(e, 'SDXL_LoRA_model', apply=False)\n",
        "      custom_SDXL_LoRA_model.visible = True if prefs['SDXL_LoRA_model'] == \"Custom SDXL LoRA Path\" else False\n",
        "      custom_SDXL_LoRA_model.update()\n",
        "  batch_folder_name = TextField(label=\"Batch Folder Name\", value=prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name', apply=False))\n",
        "  #batch_size = TextField(label=\"Batch Size\", value=prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'batch_size'))\n",
        "  #n_iterations = TextField(label=\"Number of Iterations\", value=prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations'))\n",
        "  batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=10, value=prefs['batch_size'], tooltip=\"Generates multiple images at the same time. Uses more memory...\", on_change=lambda e: changed(e, 'batch_size'))\n",
        "  n_iterations = NumberPicker(label=\"Number of Iterations: \", min=1, max=30, value=prefs['n_iterations'], tooltip=\"Creates multiple images in batch seperately\", on_change=lambda e: changed(e, 'n_iterations'))\n",
        "  #steps = TextField(label=\"Steps\", value=prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', asInt=True))\n",
        "  steps = SliderRow(label=\"Steps\", min=0, max=200, divisions=200, pref=prefs, key='steps', on_change=change)\n",
        "  #eta = TextField(label=\"DDIM ETA\", value=prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta'))\n",
        "  eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=20, round=1, pref=prefs, key='eta', tooltip=\"\", visible=False, on_change=change)\n",
        "  page.etas.append(eta)\n",
        "  seed = TextField(label=\"Seed\", value=prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 160, on_change=lambda e:changed(e,'seed'))\n",
        "  param_rows = Row([Column([batch_folder_name, seed, batch_size]), Column([steps, eta, n_iterations])])\n",
        "  batch_row = Row([batch_folder_name, seed])\n",
        "  number_row = Row([batch_size, n_iterations])\n",
        "  guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=prefs, key='guidance_scale', on_change=change)\n",
        "  width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=prefs, key='width', on_change=change)\n",
        "  height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=prefs, key='height', on_change=change)\n",
        "\n",
        "  init_image = TextField(label=\"Init Image\", value=prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "  mask_image = TextField(label=\"Mask Image\", value=prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "  alpha_mask = Checkbox(label=\"Alpha Mask\", value=prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "  invert_mask = Checkbox(label=\"Invert Mask\", value=prefs['invert_mask'], tooltip=\"Reverse Black & White of Image Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "  image_pickers = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={\"lg\":6}), Row([mask_image, invert_mask], col={\"lg\":6})]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  image_pickers.height = None if not prefs['centipede_prompts_as_init_images'] else 0\n",
        "  init_image_strength = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", round=2, value=prefs['init_image_strength'], on_change=change_strength, expand=True)\n",
        "  strength_value = Text(f\" {int(prefs['init_image_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "  strength_slider = Row([Text(\"Init Image Strength: \"), strength_value, init_image_strength])\n",
        "  page.use_SDXL = Switcher(label=\"Use Stable Diffusion XL Model/Pipeline Instead\", tooltip=\"The latest SDXL base model, with img2img Refiner. It's tasty..\", value=prefs['use_SDXL'], on_change=toggle_SDXL)\n",
        "  page.use_SDXL.visible = status['installed_SDXL']\n",
        "  #SDXL_compel = Switcher(label=\"Use Compel Long Prompt Weighting Embeds\", tooltip=\"Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.\", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))\n",
        "  SDXL_high_noise_frac = SliderRow(label=\"SDXL High Noise Fraction\", min=0, max=1, divisions=20, round=2, pref=prefs, key='SDXL_high_noise_frac', tooltip=\"Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.\", on_change=lambda e:changed(e,'SDXL_high_noise_frac', apply=False))\n",
        "  SDXL_negative_conditions = Switcher(label=\"Use Negative Conditions on Image Size\", tooltip=\"Pass negative conditions about an image's size and position to avoid undesirable cropping behavior in the generated image, and improve image resolution.\", value=prefs['SDXL_negative_conditions'], on_change=lambda e:changed(e,'SDXL_negative_conditions', apply=False))\n",
        "  page.SDXL_params = Container(Column([SDXL_high_noise_frac, SDXL_negative_conditions]), padding=padding.only(top=5, left=20), height=None if prefs['use_SDXL'] else 0, visible=status['installed_SDXL'], animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_inpaint_model = Switcher(label=\"Use Specialized Inpaint Model Instead\", tooltip=\"When using init_image and/or mask, use the newer pipeline for potentially better results\", value=prefs['use_inpaint_model'], on_change=lambda e:changed(e,'use_inpaint_model', apply=False))\n",
        "  page.use_inpaint_model.visible = status['installed_img2img']\n",
        "  page.use_alt_diffusion = Switcher(label=\"Use AltDiffusion Pipeline Model Instead\", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip=\"Supports 9 different languages for text2image & image2image\")\n",
        "  page.use_alt_diffusion.visible = status['installed_alt_diffusion']\n",
        "  page.use_versatile = Switcher(label=\"Use Versatile Pipeline Model Instead\", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip=\"Dual Guided between prompt & image, or create Image Variation.\")\n",
        "  page.use_versatile.visible = status['installed_versatile']\n",
        "  use_LoRA_model = Switcher(label=\"Use LoRA Model Adapter Layer \", value=prefs['use_LoRA_model'], on_change=toggle_LoRA, tooltip=\"Applies custom trained weighted attention model on top of loaded model\")\n",
        "  def delete_lora_layer(e):\n",
        "      for l in prefs['active_LoRA_layers']:\n",
        "        if l['name'] == e.control.data['name']:\n",
        "          prefs['active_LoRA_layers'].remove(l)\n",
        "      for c in active_LoRA_layers.controls:\n",
        "        if c.data['name'] == e.control.data['name']:\n",
        "            active_LoRA_layers.controls.remove(c)\n",
        "            break\n",
        "      active_LoRA_layers.update()\n",
        "  def delete_all_lora_layers(e):\n",
        "      prefs['active_LoRA_layers'].clear()\n",
        "      active_LoRA_layers.controls.clear()\n",
        "      active_LoRA_layers.update()\n",
        "  def change_LoRA(e):\n",
        "      for l in prefs['active_LoRA_layers']:\n",
        "        if l['name'] == e.control.data['name']:\n",
        "          l['scale'] = e.control.value\n",
        "  def add_LoRA(e, LoRA_map=None):\n",
        "      if LoRA_map != None:\n",
        "        lora = LoRA_map['name']\n",
        "        lora_scale = LoRA_map['scale']\n",
        "        lora_layer = LoRA_map\n",
        "      else:\n",
        "        lora = prefs['LoRA_model']\n",
        "        lora_scale = 1.\n",
        "        lora_layer = {}\n",
        "        if lora.startswith(\"Custom\"):\n",
        "          num = 1\n",
        "          for c in prefs['active_LoRA_layers']:\n",
        "              if c['name'].startswith(\"Custom\"):\n",
        "                  if num == int(c['name'].rpartition('-')[2]):\n",
        "                      num += 1\n",
        "          lora_layer = {'name': f'Custom-{num}', 'file':'', 'path':prefs['custom_LoRA_model'], 'scale': lora_scale}\n",
        "        else:\n",
        "          for l in prefs['active_LoRA_layers']:\n",
        "            if l['name'] == lora:\n",
        "              return\n",
        "          lora_layer = get_LoRA_model(lora)\n",
        "          lora_layer['scale'] = lora_scale\n",
        "        prefs['active_LoRA_layers'].append(lora_layer)\n",
        "      lora_scaler = SliderRow(label=\"Scale\", min=-3, max=3, divisions=12, round=1, pref=lora_layer, key='scale', tooltip=\"\", expand=True, data=lora_layer, on_change=change_LoRA)\n",
        "      title_link = Markdown(f\"[**{lora_layer['name']}**](https://huggingface.co/{lora_layer['path']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "      title = Row([title_link, lora_scaler])#Text(lora_layer['name'])\n",
        "      active_LoRA_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "        items=[\n",
        "            PopupMenuItem(icon=icons.DELETE, text=\"Delete LoRA Layer\", on_click=delete_lora_layer, data=lora_layer),\n",
        "            PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_lora_layers, data=lora_layer),\n",
        "        ]), data=lora_layer))\n",
        "      if LoRA_map == None:\n",
        "        active_LoRA_layers.update()\n",
        "  \n",
        "  def delete_SDXL_lora_layer(e):\n",
        "      for l in prefs['active_SDXL_LoRA_layers']:\n",
        "        if l['name'] == e.control.data['name']:\n",
        "          prefs['active_SDXL_LoRA_layers'].remove(l)\n",
        "      for c in active_SDXL_LoRA_layers.controls:\n",
        "        if c.data['name'] == e.control.data['name']:\n",
        "            active_SDXL_LoRA_layers.controls.remove(c)\n",
        "            break\n",
        "      active_SDXL_LoRA_layers.update()\n",
        "  def delete_all_SDXL_lora_layers(e):\n",
        "      prefs['active_SDXL_LoRA_layers'].clear()\n",
        "      active_SDXL_LoRA_layers.controls.clear()\n",
        "      active_SDXL_LoRA_layers.update()\n",
        "  def change_SDXL_LoRA(e):\n",
        "      for l in prefs['active_SDXL_LoRA_layers']:\n",
        "        if l['name'] == e.control.data['name']:\n",
        "          l['scale'] = e.control.value\n",
        "  def add_SDXL_LoRA(e, LoRA_map=None):\n",
        "      if LoRA_map != None:\n",
        "        lora = LoRA_map['name']\n",
        "        lora_scale = LoRA_map['scale']\n",
        "        lora_layer = LoRA_map\n",
        "      else:\n",
        "        lora = prefs['SDXL_LoRA_model']\n",
        "        lora_scale = 1.\n",
        "        lora_layer = {}\n",
        "        if lora.startswith(\"Custom\"):\n",
        "          num = 1\n",
        "          for c in prefs['active_SDXL_LoRA_layers']:\n",
        "              if c['name'].startswith(\"Custom\"):\n",
        "                  if num == int(c['name'].rpartition('-')[2]):\n",
        "                      num += 1\n",
        "          lora_layer = {'name': f'Custom-{num}', 'file':'', 'path':prefs['custom_SDXL_LoRA_model'], 'scale': lora_scale}\n",
        "        else:\n",
        "          for l in prefs['active_SDXL_LoRA_layers']:\n",
        "            if l['name'] == lora:\n",
        "              return\n",
        "          lora_layer = get_SDXL_LoRA_model(lora)\n",
        "          lora_layer['scale'] = lora_scale\n",
        "        prefs['active_SDXL_LoRA_layers'].append(lora_layer)\n",
        "      lora_scaler = SliderRow(label=\"Scale\", min=-3, max=3, divisions=12, round=1, pref=lora_layer, key='scale', tooltip=\"\", expand=True, data=lora_layer, on_change=change_SDXL_LoRA)\n",
        "      title_link = Markdown(f\"[**{lora_layer['name']}**](https://huggingface.co/{lora_layer['path']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "      title = Row([title_link, lora_scaler])\n",
        "      active_SDXL_LoRA_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "        items=[\n",
        "            PopupMenuItem(icon=icons.DELETE, text=\"Delete LoRA Layer\", on_click=delete_SDXL_lora_layer, data=lora_layer),\n",
        "            PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_SDXL_lora_layers, data=lora_layer),\n",
        "        ]), data=lora_layer))\n",
        "      if LoRA_map == None:\n",
        "        active_SDXL_LoRA_layers.update()\n",
        "  page.LoRA_model = Dropdown(label=\"LoRA Model Weights\", width=235, options=[], value=prefs['LoRA_model'], on_change=changed_LoRA)\n",
        "  if len(prefs['custom_LoRA_models']) > 0:\n",
        "    for l in prefs['custom_LoRA_models']:\n",
        "      page.LoRA_model.options.append(dropdown.Option(l['name']))\n",
        "  for m in LoRA_models:\n",
        "      page.LoRA_model.options.append(dropdown.Option(m['name']))\n",
        "  page.LoRA_model.options.append(dropdown.Option(\"Custom LoRA Path\"))\n",
        "  custom_LoRA_model = TextField(label=\"Custom LoRA Model Path\", value=prefs['custom_LoRA_model'], expand=True, on_change=lambda e:changed(e, 'custom_LoRA_model', apply=False))\n",
        "  custom_LoRA_model.visible = True if prefs['LoRA_model'] == \"Custom LoRA Path\" else False\n",
        "  \n",
        "  page.SDXL_LoRA_model = Dropdown(label=\"SDXL LoRA Model Weights\", width=235, options=[], value=prefs['SDXL_LoRA_model'], on_change=changed_SDXL_LoRA)\n",
        "  if len(prefs['custom_SDXL_LoRA_models']) > 0:\n",
        "    for l in prefs['custom_SDXL_LoRA_models']:\n",
        "      page.SDXL_LoRA_model.options.append(dropdown.Option(l['name']))\n",
        "  for m in SDXL_LoRA_models:\n",
        "      page.SDXL_LoRA_model.options.append(dropdown.Option(m['name']))\n",
        "  page.SDXL_LoRA_model.options.append(dropdown.Option(\"Custom SDXL LoRA Path\"))\n",
        "  custom_SDXL_LoRA_model = TextField(label=\"Custom SDXL LoRA Model Path\", value=prefs['custom_SDXL_LoRA_model'], expand=True, on_change=lambda e:changed(e, 'custom_SDXL_LoRA_model', apply=False))\n",
        "  custom_SDXL_LoRA_model.visible = True if prefs['SDXL_LoRA_model'] == \"Custom SDXL LoRA Path\" else False\n",
        "  active_LoRA_layers = Column([], spacing=0, tight=True)\n",
        "  active_SDXL_LoRA_layers = Column([], spacing=0, tight=True)\n",
        "  add_LoRA_layer = ft.FilledButton(\"‚ûï Add LoRA\", on_click=add_LoRA)\n",
        "  add_SDXL_LoRA_layer = ft.FilledButton(\"‚ûï Add SDXL LoRA\", on_click=add_SDXL_LoRA)\n",
        "  for l in prefs['active_LoRA_layers']:\n",
        "      add_LoRA(None, l)\n",
        "  for l in prefs['active_SDXL_LoRA_layers']:\n",
        "      add_SDXL_LoRA(None, l)\n",
        "  LoRA_block = Container(ResponsiveRow([Column([Row([page.LoRA_model, custom_LoRA_model, add_LoRA_layer]), active_LoRA_layers], col={'lg': 6}), \n",
        "                                        Column([Row([page.SDXL_LoRA_model, custom_SDXL_LoRA_model, add_SDXL_LoRA_layer]), active_SDXL_LoRA_layers], col={'lg': 6})]), padding=padding.only(top=6, left=10), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  LoRA_block.height = None if prefs['use_LoRA_model'] else 0\n",
        "  def toggle_ip_adapter(e):\n",
        "      prefs['use_ip_adapter'] = e.control.value\n",
        "      ip_adapter_container.height = None if e.control.value else 0\n",
        "      ip_adapter_container.update()\n",
        "      ip_adapter_model.visible = e.control.value\n",
        "      ip_adapter_model.update()\n",
        "      ip_adapter_SDXL_model.visible = e.control.value\n",
        "      ip_adapter_SDXL_model.update()\n",
        "  use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip=\"Applies to Pipelines for SD 1.5, SDXL, Inpainting, Self-Attention Guidance, Attend & Excite, Safe and Panorama\")\n",
        "  ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=prefs['ip_adapter_model'], visible=prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "  for m in ip_adapter_models:\n",
        "      ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "  ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=prefs['ip_adapter_SDXL_model'], visible=prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "  for m in ip_adapter_SDXL_models:\n",
        "      ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "  ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=prefs, key='ip_adapter_image', page=page)\n",
        "  ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "  ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "  centipede_prompts_as_init_images = Switcher(label=\"Centipede Prompts as Init Images\", tooltip=\"Feeds each image to the next prompt sequentially down the line\", value=prefs['centipede_prompts_as_init_images'], on_change=toggle_centipede)\n",
        "  use_interpolation = Switcher(label=\"Use Interpolation to Walk Latent Space between Prompts\", tooltip=\"Creates animation frames transitioning, but it's not always perfect.\", value=prefs['use_interpolation'], on_change=toggle_interpolation)\n",
        "  interpolation_steps = Slider(min=1, max=100, divisions=99, label=\"{value}\", value=prefs['num_interpolation_steps'], on_change=change_interpolation_steps, expand=True)\n",
        "  interpolation_steps_value = Text(f\" {int(prefs['num_interpolation_steps'])} steps\", weight=FontWeight.BOLD)\n",
        "  interpolation_steps_slider = Container(Row([Text(f\"Number of Interpolation Steps between Prompts: \"), interpolation_steps_value, interpolation_steps]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  Row([Text(f\"Number of Interpolation Steps between Prompts: \"), interpolation_steps_value, interpolation_steps])\n",
        "  if not bool(prefs['use_interpolation']):\n",
        "    interpolation_steps_slider.height = 0\n",
        "  page.interpolation_block = Column([use_interpolation, interpolation_steps_slider])\n",
        "  page.img_block = Container(Column([image_pickers, strength_slider, page.use_inpaint_model, centipede_prompts_as_init_images, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  if not status['installed_interpolation']:\n",
        "    page.interpolation_block.visible = False\n",
        "  elif bool(prefs['use_interpolation']):\n",
        "    page.img_block.height = 0\n",
        "  use_SAG = Switcher(label=\"Use Self-Attention Guidance (SAG) Text-to-Image\", value=prefs['use_SAG'], on_change=toggle_SAG, tooltip=\"Can drastically boost the performance and quality. Extracts the intermediate attention map from a diffusion model at every iteration and selects tokens above a certain attention score for masking and blurring to obtain a partially blurred input.\")\n",
        "  sag_scale = Slider(min=0, max=1, divisions=20, label=\"{value}\", value=prefs['sag_scale'], tooltip=\"How much Self-Attention Guidance to apply.\", on_change=change_sag_scale, expand=True)\n",
        "  sag_scale_value = Text(f\" {float(prefs['sag_scale'])}\", weight=FontWeight.BOLD)\n",
        "  sag_scale_slider = Container(Row([Text(f\"SAG Scale: \"),sag_scale_value, sag_scale]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_SAG = Column([use_SAG, sag_scale_slider])\n",
        "  if not bool(prefs['use_SAG']):\n",
        "    sag_scale_slider.height = 0\n",
        "  if not status['installed_SAG']:\n",
        "    page.use_SAG.visible = False\n",
        "  use_attend_and_excite = Switcher(label=\"Use Attend and Excite\", value=prefs['use_attend_and_excite'], on_change=toggle_attend_and_excite, tooltip=\"To use, include plus signs before subject words in prompt to indicate token indices, like 'a +cat and a +frog'.\")\n",
        "  max_iter_to_alter = Slider(min=1, max=100, divisions=99, label=\"{value}\", value=prefs['max_iter_to_alter'], tooltip=\"The first denoising steps are where the attend-and-excite is applied. I.e. if `max_iter_to_alter` is 25 and there are a total of `30` denoising steps, the first 25 denoising steps will apply attend-and-excite and the last 5 will not apply attend-and-excite.\", on_change=change_max_iter_to_alter, expand=True)\n",
        "  max_iter_to_alter_value = Text(f\" {int(prefs['max_iter_to_alter'])} iterations\", weight=FontWeight.BOLD)\n",
        "  max_iter_to_alter_slider = Container(Row([Text(f\"Max Iterations to Alter: \"), max_iter_to_alter_value, max_iter_to_alter]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_attend_and_excite = Column([use_attend_and_excite, max_iter_to_alter_slider])\n",
        "  if not bool(prefs['use_attend_and_excite']):\n",
        "    max_iter_to_alter_slider.height = 0\n",
        "  if not status['installed_attend_and_excite']:\n",
        "    page.use_attend_and_excite.visible = False\n",
        "  page.use_clip_guided_model = Switcher(label=\"Use CLIP-Guided Model\", tooltip=\"Uses more VRAM, so you'll probably need to make image size smaller\", value=prefs['use_clip_guided_model'], on_change=toggle_clip)\n",
        "  clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label=\"{value}\", value=prefs['clip_guidance_scale'], on_change=lambda e:changed(e,'clip_guidance_scale'), expand=True)\n",
        "  clip_guidance_scale_slider = Row([Text(\"CLIP Guidance Scale: \"), clip_guidance_scale])\n",
        "  use_cutouts = Checkbox(label=\"Use Cutouts\", value=bool(prefs['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_use_cutouts)\n",
        "  num_cutouts = NumberPicker(label=\"    Number of Cutouts: \", min=1, max=10, value=prefs['num_cutouts'], on_change=lambda e: changed(e, 'num_cutouts', asInt=True))\n",
        "  num_cutouts.visible = bool(prefs['use_cutouts'])\n",
        "  #num_cutouts = TextField(label=\"Number of Cutouts\", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))\n",
        "  unfreeze_unet = Checkbox(label=\"Unfreeze UNET\", value=prefs['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_unet', apply=False))\n",
        "  unfreeze_vae = Checkbox(label=\"Unfreeze VAE\", value=prefs['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_vae', apply=False))\n",
        "  page.clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_conceptualizer_model = Switcher(label=\"Use Custom Conceptualizer Model\", tooltip=\"Use Textual-Inversion Community Model Concepts\", value=prefs['use_conceptualizer'], on_change=toggle_conceptualizer)\n",
        "  page.use_conceptualizer_model.visible = bool(status['installed_conceptualizer'])\n",
        "  page.use_depth2img = Switcher(label=\"Use Depth2Image Pipeline for img2img init image generation\", value=prefs['use_depth2img'], on_change=lambda e:changed(e,'use_depth2img', apply=False), tooltip=\"To use, provide init_image with a good composition and prompts to approximate same depth.\")\n",
        "  page.use_depth2img.visible = bool(status['installed_depth2img'])\n",
        "  page.use_imagic = Switcher(label=\"Use iMagic for img2img init image editing\", value=prefs['use_imagic'], on_change=lambda e:changed(e,'use_imagic', apply=False), tooltip=\"Allows you to edit an image with prompt text.\")\n",
        "  page.use_imagic.visible = bool(status['installed_imagic'])\n",
        "  page.use_composable = Switcher(label=\"Use Composable Prompts for txt2img Weight | Segments\", value=prefs['use_composable'], on_change=lambda e:changed(e,'use_composable', apply=False), tooltip=\"Allows conjunction and negation operators for compositional generation with conditional diffusion models\")\n",
        "  page.use_composable.visible = bool(status['installed_composable'])\n",
        "  page.use_panorama = Column([Switcher(label=\"Use Panorama text2image Pipeline Instead\", value=prefs['use_panorama'], on_change=lambda e:changed(e,'use_panorama', apply=False), tooltip=\"Fuses together images to make extra-wide 2048x512\"),\n",
        "                              Checkbox(label=\"Use Circular Padding to remove stitching artifacts\", value=prefs['panorama_circular_padding'], tooltip=\"To seamlessly generate a transition from the right to left, maintaining consistency in a 360-degree sense.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'panorama_circular_padding', apply=False)),\n",
        "                              Row([Text(\"Panoramic Width x 512:\"), Slider(min=1024, max=4608, divisions=28, label=\"{value}px\", expand=True, value=prefs['panorama_width'], on_change=lambda e:changed(e, 'panorama_width', asInt=True, apply=False))])])\n",
        "  page.use_panorama.visible = status['installed_panorama']\n",
        "  page.use_safe = Switcher(label=\"Use Safe Diffusion Pipeline instead\", value=prefs['use_safe'], on_change=lambda e:changed(e,'use_safe', apply=False), tooltip=\"Models trained only on Safe images\")\n",
        "  page.use_safe.visible = bool(status['installed_safe'])\n",
        "  page.use_upscale = Switcher(label=\"Upscale 4X with Stable Diffusion 2\", value=prefs['use_upscale'], on_change=lambda e:changed(e,'use_upscale', apply=False), tooltip=\"Enlarges your Image Generations guided by the same Prompt.\")\n",
        "  page.use_upscale.visible = bool(status['installed_upscale'])\n",
        "  apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=prefs['apply_ESRGAN_upscale'], on_change=toggle_ESRGAN)\n",
        "  enlarge_scale_value = Text(f\" {float(prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "  enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "  enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "  face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance', apply=False))\n",
        "  display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image', apply=False))\n",
        "  ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.ESRGAN_block = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  #page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "  page.use_clip_guided_model.visible = status['installed_clip']\n",
        "  page.clip_block.height = None if status['installed_clip'] and prefs['use_clip_guided_model'] else 0\n",
        "  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0\n",
        "  if not prefs['apply_ESRGAN_upscale']:\n",
        "    ESRGAN_settings.height = 0\n",
        "  parameters_button = ElevatedButton(content=Text(value=\"üìú   Continue to Image Prompts\", size=20), on_click=run_parameters)\n",
        "  parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "  #apply_changes_button = ElevatedButton(content=Text(value=\"üîÄ   Apply Changes to Current Prompts\", size=20), on_click=apply_to_prompts)\n",
        "  #apply_changes_button.visible = len(prompts) > 0 and status['changed_parameters']\n",
        "  def show_apply_fab(show = True):\n",
        "    if show:\n",
        "      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.TRANSFORM), Text(\"Apply Changes to Current Prompts\", size=18)], alignment=\"center\", spacing=5), width=333, shape=ft.RoundedRectangleBorder(radius=22), on_click=apply_to_prompts)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.TRANSFORM, text=\"Apply Changes to Current Prompts\", on_click=apply_to_prompts)\n",
        "      page.update()\n",
        "    else:\n",
        "      if page.floating_action_button is not None:\n",
        "        page.floating_action_button = None\n",
        "        page.update()\n",
        "  show_apply_fab(len(prompts) > 0 and status['changed_parameters'])\n",
        "  page.show_apply_fab = show_apply_fab\n",
        "  def updater():\n",
        "      #parameters.update()\n",
        "      c.update()\n",
        "      page.update()\n",
        "      #print(\"Updated Parameters Page\")\n",
        "\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "        Header(\"üìù  Stable Diffusion Image Parameters\"),\n",
        "        #param_rows,\n",
        "        batch_row,\n",
        "        number_row,\n",
        "        steps,\n",
        "        guidance,\n",
        "        eta,\n",
        "        width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "        page.interpolation_block, page.img_block, page.use_safe, page.use_SDXL, page.SDXL_params, page.use_alt_diffusion, page.use_clip_guided_model, page.clip_block, page.use_versatile, page.use_SAG, page.use_attend_and_excite, page.use_panorama, page.use_conceptualizer_model,\n",
        "        use_LoRA_model, LoRA_block,\n",
        "        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        ip_adapter_container,\n",
        "        page.use_imagic, page.use_depth2img, page.use_composable, page.use_upscale, page.ESRGAN_block,\n",
        "        #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "        #parameters_row,\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,\n",
        "  return c\n",
        "\n",
        "prompts = []\n",
        "args = {}\n",
        "\n",
        "def update_args():\n",
        "    global args\n",
        "    args = {\n",
        "        \"batch_size\":int(prefs['batch_size']),\n",
        "        \"n_iterations\":int(prefs['n_iterations']),\n",
        "        \"steps\":int(prefs['steps']),\n",
        "        \"eta\":float(prefs['eta']),\n",
        "        \"width\":int(prefs['width']),\n",
        "        \"height\":int(prefs['height']),\n",
        "        \"guidance_scale\":float(prefs['guidance_scale']),\n",
        "        \"seed\":int(prefs['seed']),\n",
        "        \"precision\":prefs['precision'],\n",
        "        \"init_image\": prefs['init_image'],\n",
        "        \"init_image_strength\": prefs['init_image_strength'],\n",
        "        \"mask_image\": prefs['mask_image'],\n",
        "        \"alpha_mask\": prefs['alpha_mask'],\n",
        "        \"invert_mask\": prefs['invert_mask'],\n",
        "        \"prompt2\": None, \"tweens\": 10,\n",
        "        \"negative_prompt\": None,\n",
        "        \"use_clip_guided_model\": prefs['use_clip_guided_model'],\n",
        "        \"clip_prompt\": \"\",\n",
        "        \"clip_guidance_scale\": float(prefs['clip_guidance_scale']),\n",
        "        \"use_cutouts\": bool(prefs['use_cutouts']),\n",
        "        \"num_cutouts\": int(prefs['num_cutouts']),\n",
        "        \"unfreeze_unet\": prefs['unfreeze_unet'],\n",
        "        \"unfreeze_vae\": prefs['unfreeze_vae'],\n",
        "        \"use_Stability\": False,\n",
        "        \"use_conceptualizer\": False}\n",
        "\n",
        "update_args()\n",
        "\n",
        "class Dream:\n",
        "    def __init__(self, prompt, **kwargs):\n",
        "        self.prompt = prompt\n",
        "        self.arg = args.copy()\n",
        "        for key, value in kwargs.items():\n",
        "          if key=='arg': self.arg = value\n",
        "          elif key==\"batch_size\": self.arg[key] = int(value)\n",
        "          elif key==\"n_iterations\": self.arg[key] = int(value)\n",
        "          elif key==\"steps\": self.arg[key] = int(value)\n",
        "          elif key==\"eta\": self.arg[key] = float(value)\n",
        "          elif key==\"width\": self.arg[key] = int(value)\n",
        "          elif key==\"height\": self.arg[key] = int(value)\n",
        "          elif key==\"guidance_scale\": self.arg[key] = float(value)\n",
        "          elif key==\"seed\": self.arg[key] = int(value)\n",
        "          elif key==\"precision\": self.arg[key] = value\n",
        "          elif key==\"init_image\": self.arg[key] = value\n",
        "          elif key==\"init_image_strength\": self.arg[key] = value\n",
        "          elif key==\"mask_image\": self.arg[key] = value\n",
        "          elif key==\"alpha_mask\": self.arg[key] = value\n",
        "          elif key==\"invert_mask\": self.arg[key] = value\n",
        "          elif key==\"prompt2\": self.arg[key] = value\n",
        "          elif key==\"tweens\": self.arg[key] = int(value)\n",
        "          elif key==\"negative_prompt\": self.arg[key] = value\n",
        "          elif key==\"clip_prompt\": self.arg[key] = value\n",
        "          elif key==\"use_clip_guided_model\": self.arg[key] = value\n",
        "          elif key==\"clip_guidance_scale\": self.arg[key] = float(value)\n",
        "          elif key==\"use_cutouts\": self.arg[key] = value\n",
        "          elif key==\"num_cutouts\": self.arg[key] = int(value)\n",
        "          elif key==\"unfreeze_unet\": self.arg[key] = value\n",
        "          elif key==\"unfreeze_vae\": self.arg[key] = value\n",
        "          elif key==\"use_Stability\": self.arg[key] = value\n",
        "          elif key==\"use_conceptualizer\": self.arg[key] = value\n",
        "          elif key==\"prompt\": self.prompt = value\n",
        "          else: print(f\"Unknown argument: {key} = {value}\")\n",
        "    def __getitem__(self, key):\n",
        "        if key in self.arg:\n",
        "          return self.arg[key]\n",
        "        else:\n",
        "          print(f\"Error getting arg {key} from Dream\")\n",
        "          return \"\"\n",
        "    def __setitem__(self, key, value):\n",
        "        self.arg[key] = value\n",
        "\n",
        "\n",
        "def format_filename(s, force_underscore=False, use_dash=False, max_length=None):\n",
        "    if prefs['file_datetime'] and not use_dash and not force_underscore:\n",
        "        return datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    file_max_length = int(prefs['file_max_length']) if max_length == None else int(max_length)\n",
        "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
        "    filename = ''.join(c for c in s if c in valid_chars)\n",
        "    if use_dash:\n",
        "        filename = filename.replace(' ','-')\n",
        "    else:\n",
        "        if not prefs['file_allowSpace'] or force_underscore: filename = filename.replace(' ','_')\n",
        "    return filename[:file_max_length]\n",
        "\n",
        "def to_title(s, sentence=False, clean=True):\n",
        "    if clean:\n",
        "        s = s.replace('_',' ')\n",
        "        s = s.replace('-',' ')\n",
        "    if sentence:\n",
        "        sentences = s.split(\". \")\n",
        "        sentences2 = [sentence[0].capitalize() + sentence[1:] for sentence in sentences]\n",
        "        s2 = '. '.join(sentences2)\n",
        "        return s2\n",
        "    else:\n",
        "        return string.capwords(s)\n",
        "\n",
        "'''from collections import ChainMap\n",
        "def merge_dict(*dicts):\n",
        "    all_keys  = set(k for d in dicts for k in d.keys())\n",
        "    chain_map = ChainMap(*reversed(dicts))\n",
        "    return {k: chain_map[k] for k in all_keys}\n",
        "def merge_dict(dict1, dict2):\n",
        "    merged_dict = {**dict1, **dict2}\n",
        "    return merged_dict\n",
        "'''\n",
        "def merge_dict(dict1, dict2):\n",
        "    new_dict = {}\n",
        "    for key in dict1:\n",
        "        new_dict[key] = dict1[key]\n",
        "    for key in dict2:\n",
        "        new_dict[key] = dict2[key]\n",
        "    return new_dict\n",
        "\n",
        "import copy\n",
        "\n",
        "def editPrompt(e):\n",
        "    global prompts, prefs, status\n",
        "    open_dream = e.control.data\n",
        "    idx = prompts.index(open_dream)\n",
        "    def changed_tweening(e):\n",
        "        status['changed_prompts'] = True\n",
        "        tweening_params.height = None if e.control.value else 0\n",
        "        tweening_params.update()\n",
        "        #prompt2.visible = e.control.value\n",
        "        #tweens.visible = e.control.value\n",
        "        prompt_tweening = e.control.value\n",
        "        e.page.update()\n",
        "    def changed_tweens(e):\n",
        "        prefs['tweens'] = int(e.control.value)\n",
        "    def close_dlg(e):\n",
        "        nonlocal edit_dlg\n",
        "        edit_dlg.open = False\n",
        "        #e.page.dialog.open = False\n",
        "        e.page.update()\n",
        "        #del edit_dlg\n",
        "        #e.page.dialog = None\n",
        "    def open_dlg(e):\n",
        "        nonlocal edit_dlg\n",
        "        e.page.dialog = edit_dlg\n",
        "        edit_dlg.open = True\n",
        "        e.page.update()\n",
        "    def save_dlg(e):\n",
        "        nonlocal arg, open_dream, edit_dlg\n",
        "        dream = open_dream #e.control.data\n",
        "        dream.prompt = edit_text.value\n",
        "        arg['batch_size'] = int(batch_size.value)\n",
        "        arg['n_iterations'] = int(n_iterations.value)\n",
        "        arg['steps'] = int(steps.value)\n",
        "        arg['eta'] = float(eta.value)\n",
        "        arg['seed'] = int(seed.value)\n",
        "        arg['guidance_scale'] = float(guidance_scale.value)\n",
        "        arg['width'] = int(width_slider.value)\n",
        "        arg['height'] = int(height_slider.value)\n",
        "        arg['init_image'] = init_image.value\n",
        "        arg['mask_image'] = mask_image.value\n",
        "        arg['init_image_strength'] = float(init_image_strength.value)\n",
        "        arg['alpha_mask'] = alpha_mask.value\n",
        "        arg['invert_mask'] = invert_mask.value\n",
        "        arg['prompt2'] = prompt2.value if bool(use_prompt_tweening.value) else None\n",
        "        arg['tweens'] = int(tweens.value)\n",
        "        arg['negative_prompt'] = negative_prompt.value if bool(negative_prompt.value) else None\n",
        "        arg['use_clip_guided_model'] = use_clip_guided_model.value\n",
        "        arg['clip_guidance_scale'] = float(clip_guidance_scale.value)\n",
        "        arg['use_cutouts'] = use_cutouts.value\n",
        "        arg['num_cutouts'] = int(num_cutouts.value)\n",
        "        arg['unfreeze_unet'] = unfreeze_unet.value\n",
        "        arg['unfreeze_vae'] = unfreeze_vae.value\n",
        "        dream.arg = arg\n",
        "        diffs = arg_diffs(arg, args)\n",
        "        if bool(diffs):\n",
        "          e.page.prompts_list.controls[idx].subtitle = Text(\"    \" + diffs)\n",
        "        else:\n",
        "          e.page.prompts_list.controls[idx].subtitle = None\n",
        "        e.page.prompts_list.controls[idx].title.value = dream.prompt # = Text(edit_text.value)\n",
        "        status['changed_prompts'] = True\n",
        "        edit_dlg.open = False\n",
        "        e.page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        if pick_type == \"init\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          prefs['init_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          prefs['mask_image'] = fname\n",
        "        e.page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if e.page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    e.page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_clip(e):\n",
        "        if e.control.value:\n",
        "          img_block.height = 0\n",
        "          clip_block.height = None if status['installed_clip'] else 0\n",
        "        else:\n",
        "          img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "          clip_block.height = 0\n",
        "        img_block.update()\n",
        "        clip_block.update()\n",
        "        #changed(e)\n",
        "    arg = open_dream.arg #e.control.data.arg\n",
        "    edit_text = TextField(label=\"Composable | Prompt | Text\" if prefs['use_composable'] and status['installed_composable'] else \"Prompt Text\", col={'lg':9}, value=open_dream.prompt, multiline=True)\n",
        "    negative_prompt = TextField(label=\"Segmented Weights 1 | -0.7 | 1.2\" if prefs['use_composable'] and status['installed_composable'] else \"Negative Prompt Text\", col={'lg':3}, value=str((arg['negative_prompt'] or '') if 'negative_prompt' in arg else ''))\n",
        "    #batch_folder_name = TextField(label=\"Batch Folder Name\", value=arg['batch_folder_name'], on_change=changed)\n",
        "    #print(str(arg))\n",
        "    prompt_tweening = bool(arg['prompt2']) if 'prompt2' in arg else False\n",
        "    use_prompt_tweening = Switcher(label=\"Prompt Tweening\", value=prompt_tweening, on_change=changed_tweening)\n",
        "    use_prompt_tweening.visible = True if status['installed_txt2img'] and prefs['higher_vram_mode'] else False\n",
        "#TODO: Fix tweening code for float16 lpw pipeline to reactivate tweening\n",
        "    prompt2 = TextField(label=\"Prompt 2 Transition Text\", expand=True, value=arg['prompt2'] if 'prompt2' in arg else '')\n",
        "    tweens = TextField(label=\"# of Tweens\", value=str(arg['tweens'] if 'tweens' in arg else 8), keyboard_type=KeyboardType.NUMBER, width = 90)\n",
        "    #tweens =  NumberPicker(label=\"# of Tweens: \", min=2, max=300, value=int(arg['tweens'] if 'tweens' in arg else 8), on_change=changed_tweens),\n",
        "    #prompt2.visible = prompt_tweening\n",
        "    #tweens.visible = prompt_tweening\n",
        "    tweening_params = Container(Row([Container(content=None, width=8), prompt2, tweens]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    tweening_params.height = None if prompt_tweening else 0\n",
        "    #tweening_row = Row([use_prompt_tweening, ])#tweening_params\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=10, value=arg['batch_size'])\n",
        "    n_iterations = NumberPicker(label=\"Number of Iterations: \", min=1, max=30, value=arg['n_iterations'])\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=str(arg['batch_size']), keyboard_type=KeyboardType.NUMBER)\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=str(arg['n_iterations']), keyboard_type=KeyboardType.NUMBER)\n",
        "    steps = TextField(label=\"Steps\", value=str(arg['steps']), keyboard_type=KeyboardType.NUMBER)\n",
        "    eta = TextField(label=\"DDIM ETA\", value=str(arg['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise (only with DDIM sampler)\")\n",
        "    seed = TextField(label=\"Seed\", value=str(arg['seed']), keyboard_type=KeyboardType.NUMBER, hint_text=\"0 or -1 picks a Random seed\")\n",
        "    guidance_scale = TextField(label=\"Guidance Scale\", value=str(arg['guidance_scale']), keyboard_type=KeyboardType.NUMBER)\n",
        "    param_columns = Row([Column([steps, seed, batch_size]), Column([guidance_scale, eta, n_iterations])])\n",
        "    #guidance_scale = Slider(min=0, max=50, divisions=100, label=\"{value}\", value=arg['guidance_scale'], expand=True)\n",
        "    #guidance = Row([Text(\"Guidance Scale: \"), guidance_scale])\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=arg, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=arg, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=arg['init_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=arg['mask_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=arg['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    invert_mask = Checkbox(label=\"Invert Mask\", value=arg['invert_mask'], tooltip=\"Reverse Black & White of Image Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    image_row = ResponsiveRow([Row([init_image, alpha_mask], col={\"lg\":6}), Row([mask_image, invert_mask], col={\"lg\":6})])\n",
        "    init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}\", round=2, value=float(arg['init_image_strength']), expand=True)\n",
        "    strength_slider = Row([Text(\"Init Image Strength: \"), init_image_strength])\n",
        "    img_block = Container(content=Column([image_row, strength_slider]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_SDXL']) else 0\n",
        "    use_clip_guided_model = Switcher(label=\"Use CLIP-Guided Model\", tooltip=\"Uses more VRAM, so you'll probably need to make image size smaller\", value=arg['use_clip_guided_model'], on_change=toggle_clip)\n",
        "    clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label=\"{value}\", value=arg['clip_guidance_scale'], expand=True)\n",
        "    clip_guidance_scale_slider = Row([Text(\"CLIP Guidance Scale: \"), clip_guidance_scale])\n",
        "    use_cutouts = Checkbox(label=\"Use Cutouts\", value=bool(arg['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    num_cutouts = NumberPicker(label=\"    Number of Cutouts: \", min=1, max=10, value=arg['num_cutouts'])\n",
        "    #num_cutouts.visible = bool(prefs['use_cutouts'])\n",
        "    #num_cutouts = TextField(label=\"Number of Cutouts\", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))\n",
        "    unfreeze_unet = Checkbox(label=\"Unfreeze UNET\", value=arg['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    unfreeze_vae = Checkbox(label=\"Unfreeze VAE\", value=arg['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    if not status['installed_clip']:\n",
        "      use_clip_guided_model.visible = False\n",
        "      clip_block.height = 0\n",
        "    elif not arg['use_clip_guided_model']:\n",
        "      clip_block.height = 0\n",
        "    edit_dlg = AlertDialog(title=Text(\"üìù  Edit Prompt Dream Parameters\"), content=Container(Column([\n",
        "          Container(content=None, height=7),\n",
        "          ResponsiveRow([\n",
        "            edit_text,\n",
        "            negative_prompt,\n",
        "          ]),\n",
        "          #Text(\"Override any Default Parameters\"),\n",
        "          #use_prompt_tweening,\n",
        "          #tweening_params,\n",
        "          #batch_size, n_iterations, steps, eta, seed, guidance,\n",
        "          param_columns,\n",
        "          width_slider, height_slider, img_block,\n",
        "          use_clip_guided_model, clip_block,\n",
        "          #Row([Column([batch_size, n_iterations, steps, eta, seed,]), Column([guidance, width_slider, height_slider, Divider(height=9, thickness=2), (img_block if prefs['install_img2img'] else Container(content=None))])],),\n",
        "        ], alignment=MainAxisAlignment.START, width=(e.page.width if e.page.web else e.page.window_width) - 200, height=(e.page.height if e.page.web else e.page.window_height) - 100, scroll=ScrollMode.AUTO), width=(e.page.width if e.page.web else e.page.window_width) - 200, height=(e.page.height if e.page.web else e.page.window_height) - 100),\n",
        "        actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "    #open_dlg(e)\n",
        "    e.page.dialog = edit_dlg\n",
        "    edit_dlg.open = True\n",
        "    e.page.update()\n",
        "\n",
        "def buildPromptsList(page):\n",
        "  parameter = Ref[ListTile]()\n",
        "  global prompts, args, prefs\n",
        "  def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "          prefs[pref] = e.control.value\n",
        "  def prompt_help(e):\n",
        "      def close_help_dlg(e):\n",
        "        nonlocal prompt_help_dlg\n",
        "        prompt_help_dlg.open = False\n",
        "        page.update()\n",
        "      prompt_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Prompt Creations\"), content=Column([\n",
        "          Text(\"You can keep your text prompts simple, or get really complex with it. Just describe the image you want it to dream up with as many details as you can think of. Add artists, styles, colors, adjectives and get creative...\"),\n",
        "          Text('Now you can add prompt weighting, so you can emphasize the strength of certain words between parentheses, and de-emphasize words between brackets. For example: \"A (hyper realistic) painting of (magical:1.8) owl with the (((face of a cat))), without [[tail]], in a [twisted:0.6] tree, by Thomas Kinkade\"'),\n",
        "          Text('After adding your prompts, click on a prompt line to edit all the parameters of it. There you can add negative prompts like \"lowres, bad_anatomy, error_body, bad_fingers, missing_fingers, error_lighting, jpeg_artifacts, signature, watermark, username, blurry\" or anything else you don\\'t want.'),\n",
        "          Text('Then you can override all the parameters for each individual prompt, playing with variations of sizes, steps, guidance scale, init & mask image, seeds, etc.  In the prompts list, you can press the ... options button to duplicate, delete and move prompts in the batch queue.  When ready, Run Diffusion on Prompts...')\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòÄ  Very nice... \", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = prompt_help_dlg\n",
        "      prompt_help_dlg.open = True\n",
        "      page.update()\n",
        "  def paste_prompts(e):\n",
        "      def save_prompts_list(e):\n",
        "        plist = enter_text.value.strip()\n",
        "        prompts_list = plist.split('\\n')\n",
        "        negative_prompt = negative_prompt_text.value\n",
        "        negative = None\n",
        "        if bool(negative_prompt):\n",
        "            if '_' in negative_prompt:\n",
        "                negative_prompt = nsp_parse(negative_prompt)\n",
        "            negative_prompt = prompt_parse(negative_prompt)\n",
        "            negative = {'negative_prompt': negative_prompt}\n",
        "        for pr in prompts_list:\n",
        "            if bool(pr.strip()):\n",
        "                if '_' in pr:\n",
        "                    pr = nsp_parse(pr)\n",
        "                pr = prompt_parse(pr)\n",
        "                add_to_prompts(pr.strip(), negative)\n",
        "        status['changed_prompts'] = True\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_paste.open = False\n",
        "          page.update()\n",
        "      enter_text = TextField(label=\"Enter Prompts List with multiple lines\", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)\n",
        "      dlg_paste = AlertDialog(modal=False, title=Text(\"üìù  Paste or Write Prompts List from Simple Text\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll=\"none\"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Add to Prompts List \", size=19, weight=FontWeight.BOLD), on_click=save_prompts_list)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_paste\n",
        "      dlg_paste.open = True\n",
        "      page.update()\n",
        "  def copy_prompts(e):\n",
        "      def copy_prompts_list(pl):\n",
        "          nonlocal text_list, enter_text\n",
        "          page.set_clipboard(enter_text.value)\n",
        "          page.snack_bar = SnackBar(content=Text(f\"üìã   Prompt Text copied to clipboard...\"))\n",
        "          page.snack_bar.open = True\n",
        "          close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_copy.open = False\n",
        "          page.update()\n",
        "      text_list = \"\"\n",
        "      for d in prompts:\n",
        "          p = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d['prompt'] if 'prompt' in d else d.arg['prompt'] if 'prompt' in d.arg else ''\n",
        "          text_list += f\"{p}\\n\"\n",
        "      enter_text = TextField(label=\"Prompts on multiple lines\", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)\n",
        "      dlg_copy = AlertDialog(modal=False, title=Text(\"üìù  Prompts List as Plain Text\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll=\"none\"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Copy Prompts List to Clipboard\", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_prompts_list(text_list))], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_copy\n",
        "      dlg_copy.open = True\n",
        "      page.update()\n",
        "  def delete_prompt(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      prompts.pop(idx)\n",
        "      prompts_list.controls.pop(idx)\n",
        "      prompts_list.update()\n",
        "      status['changed_prompts'] = True\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "  def copy_prompt(e):\n",
        "      open_dream = e.control.data\n",
        "      page.set_clipboard(open_dream.prompt)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã   Prompt Text copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "  def duplicate_prompt(e):\n",
        "      open_dream = e.control.data\n",
        "      add_to_prompts(open_dream.prompt, open_dream.arg)\n",
        "  def duplicate_multiple(e):\n",
        "      open_dream = e.control.data\n",
        "      num_times = 2\n",
        "      def close_dlg(e):\n",
        "          duplicate_modal.open = False\n",
        "          page.update()\n",
        "      def save_dlg(e):\n",
        "          for i in range(num_times):\n",
        "            add_to_prompts(open_dream.prompt, open_dream.arg)\n",
        "          duplicate_modal.open = False\n",
        "          page.update()\n",
        "      def change_num(e):\n",
        "          nonlocal num_times\n",
        "          num_times = int(e.control.value)\n",
        "      duplicate_modal = AlertDialog(modal=False, title=Text(\"üåÄ  Duplicate Prompt Multiple Times\"), content=Container(Column([\n",
        "            Container(content=None, height=7),\n",
        "            NumberPicker(label=\"Number of Copies: \", min=1, max=99, value=num_times, on_change=change_num),\n",
        "          ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":bowling:\") + \"  Duplicate Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      e.page.dialog = duplicate_modal\n",
        "      duplicate_modal.open = True\n",
        "      e.page.update()\n",
        "  def move_down(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      if idx < (len(prompts) - 1):\n",
        "        d = prompts.pop(idx)\n",
        "        prompts.insert(idx+1, d)\n",
        "        dr = prompts_list.controls.pop(idx)\n",
        "        prompts_list.controls.insert(idx+1, dr)\n",
        "        prompts_list.update()\n",
        "  def move_up(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      if idx > 0:\n",
        "        d = prompts.pop(idx)\n",
        "        prompts.insert(idx-1, d)\n",
        "        dr = prompts_list.controls.pop(idx)\n",
        "        prompts_list.controls.insert(idx-1, dr)\n",
        "        prompts_list.update()\n",
        "  def add_prompt(e):\n",
        "      positive_prompt = prompt_text.value\n",
        "      negative_prompt = negative_prompt_text.value\n",
        "      if '_' in positive_prompt:\n",
        "        positive_prompt = nsp_parse(positive_prompt)\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "        add_to_prompts(positive_prompt, {'negative_prompt': negative_prompt})\n",
        "      else:\n",
        "        add_to_prompts(positive_prompt)\n",
        "  def add_to_prompts(p, arg=None):\n",
        "      global prompts\n",
        "      update_args()\n",
        "      p = prompt_parse(p)\n",
        "      dream = Dream(p)\n",
        "      if arg is not None:\n",
        "        if 'prompt' in arg: del arg['prompt']\n",
        "        arg = merge_dict(args, arg)\n",
        "        dream.arg = arg\n",
        "      #if prefs['']\n",
        "      prompts.append(dream)\n",
        "      prompts_list.controls.append(ListTile(title=Text(p, max_lines=6, style=TextThemeStyle.BODY_LARGE), dense=True, data=dream, on_click=editPrompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Prompt\", on_click=editPrompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Prompt\", on_click=delete_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE, text=\"Duplicate Prompt\", on_click=duplicate_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE_SHARP, text=\"Duplicate Multiple\", on_click=duplicate_multiple, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTENT_COPY, text=\"Copy to Clipboard\", on_click=copy_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=dream),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=dream),\n",
        "          ],\n",
        "      )))\n",
        "      #prompts_list.controls.append(Text(\"Prompt 1 added to the list of prompts\"))\n",
        "      prompts_list.update()\n",
        "      if prompts_buttons.visible==False:\n",
        "          prompts_buttons.visible=True\n",
        "          prompts_buttons.update()\n",
        "          if current_tab == 3:\n",
        "            show_run_diffusion_fab(True)\n",
        "      if arg is not None:\n",
        "        update_prompts()\n",
        "      else:\n",
        "        prompt_text.focus()\n",
        "      page.update()\n",
        "      status['changed_prompts'] = True\n",
        "  page.add_to_prompts = add_to_prompts\n",
        "\n",
        "  def save_prompts():\n",
        "      if len(prompts) > 0:\n",
        "          #print(\"Saving your Prompts List\")\n",
        "          prompts_prefs = []\n",
        "          for d in prompts:\n",
        "            a = d.arg.copy()\n",
        "            #a['prompt'] = d.prompt if bool(d.prompt) else d['prompt']\n",
        "            a['prompt'] = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d.arg['prompt'] if 'prompt' in d.arg else ''\n",
        "            if 'batch_size' in a: del a['batch_size']\n",
        "            if 'n_iterations' in a: del a['n_iterations']\n",
        "            if 'precision' in a: del a['precision']\n",
        "            #a['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "            if prefs['use_Stability_api']: del a['eta']\n",
        "            if 'use_Stability' in a: del a['use_Stability']\n",
        "            if 'negative_prompt' in a:\n",
        "              if not bool(a['negative_prompt']): del a['negative_prompt']\n",
        "            if 'prompt2' in a:\n",
        "              if not bool(a['prompt2']):\n",
        "                del a['prompt2']\n",
        "                del a['tweens']\n",
        "            if 'init_image' in a:\n",
        "              if not bool(a['init_image']):\n",
        "                del a['init_image']\n",
        "                del a['init_image_strength']\n",
        "                del a['invert_mask']\n",
        "                if 'alpha_image' in a:\n",
        "                  del a['alpha_mask']\n",
        "              elif bool(a['mask_image']) and 'alpha_image' in a:\n",
        "                del a['alpha_mask']\n",
        "            if 'mask_image' in a:\n",
        "              if not bool(a['mask_image']):\n",
        "                del a['mask_image']\n",
        "                if 'alpha_image' in a:\n",
        "                  del a['alpha_mask']\n",
        "            if 'use_clip_guided_model' in a:\n",
        "              if not bool(a['use_clip_guided_model']):\n",
        "                del a[\"use_clip_guided_model\"]\n",
        "                del a[\"clip_prompt\"]\n",
        "                del a[\"clip_guidance_scale\"]\n",
        "                del a[\"num_cutouts\"]\n",
        "                del a[\"use_cutouts\"]\n",
        "                del a[\"unfreeze_unet\"]\n",
        "                del a[\"unfreeze_vae\"]\n",
        "              else:\n",
        "                a[\"clip_model_id\"] = prefs['clip_model_id']\n",
        "            if 'use_conceptualizer' in a:\n",
        "              if not bool(a['use_conceptualizer']):\n",
        "                del a['use_conceptualizer']\n",
        "            prompts_prefs.append(a)\n",
        "            #j = json.dumps(a)\n",
        "          prefs['prompt_list'] = prompts_prefs\n",
        "  page.save_prompts = save_prompts\n",
        "  def load_prompts():\n",
        "      saved_prompts = prefs['prompt_list']\n",
        "      if len(saved_prompts) > 0:\n",
        "          for d in saved_prompts:\n",
        "            #print(f'Loading {d}')\n",
        "            if 'prompt' not in d: continue\n",
        "            p = d['prompt']\n",
        "            page.add_to_prompts(p, d)\n",
        "          page.update()\n",
        "\n",
        "  page.load_prompts = load_prompts\n",
        "\n",
        "  def update_prompts():\n",
        "      if len(prompts_list.controls) > 0:\n",
        "        for p in prompts_list.controls:\n",
        "          diffs = arg_diffs(p.data.arg, args)\n",
        "          if bool(diffs):\n",
        "            subtitle = Text(\"    \" + diffs)\n",
        "          else: subtitle = None\n",
        "          p.subtitle = subtitle\n",
        "          p.update()\n",
        "        prompts_list.update()\n",
        "  page.update_prompts = update_prompts\n",
        "\n",
        "  def apply_changes(e):\n",
        "      global prompts\n",
        "      if len(prompts_list.controls) > 0:\n",
        "        i = 0\n",
        "        for p in prompts_list.controls:\n",
        "          negative = prompts[i].arg['negative_prompt']\n",
        "          init = prompts[i].arg['init_image']\n",
        "          mask = prompts[i].arg['mask_image']\n",
        "          prompts[i].arg = merge_dict(prompts[i].arg, args)\n",
        "          prompts[i].arg['negative_prompt'] = negative\n",
        "          if not bool(args['init_image']):\n",
        "            prompts[i].arg['init_image'] = init\n",
        "          if not bool(args['mask_image']):\n",
        "            prompts[i].arg['mask_image'] = mask\n",
        "          p.data = prompts[i]\n",
        "          i += 1\n",
        "        update_prompts()\n",
        "\n",
        "  page.apply_changes = apply_changes\n",
        "  def clear_prompt(e):\n",
        "      prompt_text.value = \"\"\n",
        "      prompt_text.update()\n",
        "  def clear_negative_prompt(e):\n",
        "      negative_prompt_text.value = \"\"\n",
        "      negative_prompt_text.update()\n",
        "  def clear_list(e):\n",
        "      global prompts\n",
        "      prompts_list.controls = []\n",
        "      prompts_list.update()\n",
        "      prompts = []\n",
        "      prefs['prompt_list'] = []\n",
        "      prompts_buttons.visible=False\n",
        "      prompts_buttons.update()\n",
        "      show_run_diffusion_fab(False)\n",
        "      e.page.save_prompts()\n",
        "      save_settings_file(e.page)\n",
        "      #status['changed_prompts'] = True\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "  page.clear_prompts_list = clear_list\n",
        "  def on_keyboard (e: KeyboardEvent):\n",
        "      if e.key == \"Escape\":\n",
        "        if current_tab == 3:\n",
        "          clear_prompt(None)\n",
        "  page.on_keyboard_event = on_keyboard\n",
        "  def run_diffusion(e):\n",
        "      if not status['installed_diffusers'] and not status['installed_stability'] and not status['installed_AIHorde']:\n",
        "        alert_msg(e.page, \"You must Install the required Diffusers or Stability api first...\")\n",
        "        return\n",
        "      if prefs['use_interpolation'] and prefs['install_interpolation'] and not status['installed_interpolation']:\n",
        "        alert_msg(e.page, \"You must Install Walk Interpolation Pipeline first...\")\n",
        "        return\n",
        "      if len(prompts) < 1:\n",
        "        if not bool(prompt_text.value):\n",
        "          alert_msg(p.page, \"Add some Prompts to the Batch List before running, or at least fill in Prompt Text first.\")\n",
        "          return\n",
        "        add_prompt(e)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      show_run_diffusion_fab(False)\n",
        "      if status['changed_prompts']:\n",
        "        page.save_prompts()\n",
        "        save_settings_file(page)\n",
        "        status['changed_prompts'] = False\n",
        "      page.update()\n",
        "      start_diffusion(page)\n",
        "  has_changed = False\n",
        "  prompts_list = Column([],spacing=1)\n",
        "  page.prompts_list = prompts_list\n",
        "  prompt_text = TextField(label=\"Prompt Text\", suffix=IconButton(icons.CLEAR, on_click=clear_prompt), autofocus=True, filled=True, multiline=True, max_lines=6, on_submit=add_prompt, col={'lg':9})\n",
        "  negative_prompt_text = TextField(label=\"Segmented Weights 1 | -0.7 | 1.2\" if prefs['use_composable'] and status['installed_composable'] else \"Negative Prompt Text\", filled=True, multiline=True, max_lines=4, value=prefs['negative_prompt'], on_change=lambda e:changed(e,'negative_prompt'), suffix=IconButton(icons.CLEAR, on_click=clear_negative_prompt), col={'lg':3})\n",
        "  add_prompt_button = ElevatedButton(content=Text(value=\"‚ûï  Add\" + (\" Prompt\" if (page.width if page.web else page.window_width) > 720 else \"\"), size=17, weight=FontWeight.BOLD), height=52, on_click=add_prompt)\n",
        "  prompt_help_button = IconButton(icons.HELP_OUTLINE, tooltip=\"Help with Prompt Creation\", on_click=prompt_help)\n",
        "  copy_prompts_button = IconButton(icons.COPY_ALL, tooltip=\"Save Prompts as Plain-Text List\", on_click=copy_prompts)\n",
        "  paste_prompts_button = IconButton(icons.CONTENT_PASTE, tooltip=\"Load Prompts from Plain-Text List\", on_click=paste_prompts)\n",
        "  prompt_row = Row([ResponsiveRow([prompt_text, negative_prompt_text], expand=True, vertical_alignment=CrossAxisAlignment.START), add_prompt_button])\n",
        "  #diffuse_prompts_button = ElevatedButton(content=Text(value=\"‚ñ∂Ô∏è    Run Diffusion on Prompts \", size=20), on_click=run_diffusion)\n",
        "  clear_prompts_button = ElevatedButton(content=Text(\"‚ùå   Clear Prompts List\", size=18), on_click=clear_list)\n",
        "  prompts_buttons = Row([clear_prompts_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "  def show_run_diffusion_fab(show = True, p = None):\n",
        "    if p is None:\n",
        "      p = page\n",
        "    if show:\n",
        "      p.floating_action_button = FloatingActionButton(content=Row([Icon(icons.PLAY_ARROW), Text(\"Run Diffusion on Prompts\", size=18)], alignment=\"center\", spacing=5), width=270, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_diffusion)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.PLAY_ARROW, text=\"Run Diffusion on Prompts\", on_click=run_diffusion)\n",
        "      p.update()\n",
        "    else:\n",
        "      if p.floating_action_button is not None:\n",
        "        p.floating_action_button = None\n",
        "        try:\n",
        "          p.update()\n",
        "        except Exception:\n",
        "          print(\"Problem updating page while showing Run Diffusion FAB\")\n",
        "          pass\n",
        "\n",
        "  page.show_run_diffusion_fab = show_run_diffusion_fab\n",
        "  show_run_diffusion_fab(False)#(len(prompts_list.controls) > 0)\n",
        "  #page.load_prompts()\n",
        "  if len(prompts_list.controls) < 1:\n",
        "    prompts_buttons.visible=False\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "        Header(\"üóíÔ∏è   List of Prompts to Diffuse\", actions=[prompt_help_button, copy_prompts_button, paste_prompts_button]),\n",
        "        #add_prompt_button,\n",
        "        prompt_row,\n",
        "        prompts_list,\n",
        "        prompts_buttons,\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)\n",
        "  return c\n",
        "\n",
        "def buildImages(page):\n",
        "    auto_scroll = True\n",
        "    def auto_scrolling(auto):\n",
        "      page.imageColumn.auto_scroll = auto\n",
        "      page.imageColumn.update()\n",
        "      c.update()\n",
        "    page.auto_scrolling = auto_scrolling\n",
        "    page.imageColumn = Column([Text(\"‚ñ∂Ô∏è   Start Run from Prompts List.  Get ready...\", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT)], scroll=ScrollMode.AUTO, auto_scroll=True)\n",
        "    c = Container(padding=padding.only(18, 12, 0, 0), content=page.imageColumn)\n",
        "    return c\n",
        "\n",
        "def buildPromptGenerator(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_generator'][pref] = e.control.value\n",
        "      status['changed_prompt_generator'] = True\n",
        "    page.prompt_generator_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_generator(p):\n",
        "      page.prompt_generator_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_generator_list.update()\n",
        "      generator_list_buttons.visible = True\n",
        "      generator_list_buttons.update()\n",
        "    page.add_to_prompt_generator = add_to_prompt_generator\n",
        "    def click_prompt_generator(e):\n",
        "      if status['installed_OpenAI']:\n",
        "        run_prompt_generator(page)\n",
        "      else:\n",
        "        alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using...\")\n",
        "    def add_to_list(e):\n",
        "      for p in page.prompt_generator_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def clear_prompts(e):\n",
        "      page.prompt_generator_list.controls = []\n",
        "      page.prompt_generator_list.update()\n",
        "      #prompts = []\n",
        "      generator_list_buttons.visible = False\n",
        "      generator_list_buttons.update()\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def changed_request(e):\n",
        "      request_slider.label = generator_request_modes[int(request_slider.value)]\n",
        "      request_slider.update()\n",
        "      changed(e, 'request_mode')\n",
        "    request_slider = Slider(label=\"{value}\", min=0, max=7, divisions=7, expand=True, value=prefs['prompt_generator']['request_mode'], on_change=changed_request)\n",
        "    request_slider.label = generator_request_modes[int(prefs['prompt_generator']['request_mode'])]\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"OpenAI GPT-4\"), dropdown.Option(\"GPT-4 Turbo\"), dropdown.Option(\"Google Gemini\")], value=prefs['prompt_generator']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    generator_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), on_click=add_to_list)\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_generator_list.controls) < 1:\n",
        "      generator_list_buttons.visible = False\n",
        "      #generator_list_buttons.update()\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üß†  OpenAI GPT-3/4/PaLM Prompt Genenerator\", \"Enter a phrase each prompt should start with and the amount of prompts to generate. 'Subject Details' is optional to influence the output. 'Phase as subject' makes it about phrase and subject detail. 'Request mode' is the way it asks for the visual description. Just experiment, AI will continue to surprise.\"),\n",
        "        Row([TextField(label=\"Subject Phrase\", expand=True, value=prefs['prompt_generator']['phrase'], multiline=True, on_change=lambda e: changed(e, 'phrase')),\n",
        "             TextField(label=\"Subject Detail (optional)\", expand=True, hint_text=\"About Details (optional)\", value=prefs['prompt_generator']['subject_detail'], multiline=True, on_change=lambda e: changed(e, 'subject_detail')), \n",
        "             Checkbox(label=\"Phrase as Subject\", value=prefs['prompt_generator']['phrase_as_subject'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'phrase_as_subject'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_generator']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_generator']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_generator']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_generator']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        AI_engine,\n",
        "        ResponsiveRow([\n",
        "          Row([Text(\"Request Mode:\"), request_slider,], col={'lg':6}),\n",
        "          Row([Text(\" AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, value=prefs['prompt_generator']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üí≠   Generate Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_generator(page)),\n",
        "        page.prompt_generator_list,\n",
        "        generator_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptRemixer(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_remixer'][pref] = e.control.value\n",
        "      status['changed_prompt_remixer'] = True\n",
        "    page.prompt_remixer_list = Column([], spacing=0)\n",
        "    def click_prompt_remixer(e):\n",
        "      if status['installed_OpenAI']:\n",
        "        run_prompt_remixer(page)\n",
        "      else:\n",
        "        alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using...\")\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_remixer(p):\n",
        "      page.prompt_remixer_list.controls.append(ListTile(title=Text(p, max_lines=4, style=TextThemeStyle.BODY_LARGE), dense=True, data=p, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_remixer_list.update()\n",
        "      remixer_list_buttons.visible = True\n",
        "      remixer_list_buttons.update()\n",
        "    page.add_to_prompt_remixer = add_to_prompt_remixer\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.prompt_remixer_list.controls:\n",
        "        page.add_to_prompts(p.data)#(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.prompt_remixer_list.controls = []\n",
        "      page.prompt_remixer_list.update()\n",
        "      remixer_list_buttons.visible = False\n",
        "      remixer_list_buttons.update()\n",
        "    def changed_request(e):\n",
        "      request_slider.label = remixer_request_modes[int(request_slider.value)]\n",
        "      request_slider.update()\n",
        "      changed(e, 'request_mode')\n",
        "    request_slider = Slider(label=\"{value}\", min=0, max=8, divisions=8, expand=True, value=prefs['prompt_remixer']['request_mode'], on_change=changed_request)\n",
        "    request_slider.label = remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"OpenAI GPT-4\"), dropdown.Option(\"GPT-4 Turbo\"), dropdown.Option(\"Google Gemini\")], value=prefs['prompt_remixer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    remixer_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_remixer_list.controls) < 1:\n",
        "      remixer_list_buttons.visible = False\n",
        "\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîÑ  Prompt Remixer - GPT-3/4/PaLM AI Helper\", \"Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friend. Experiment.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Seed Prompt\", expand=True, value=prefs['prompt_remixer']['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt')), \n",
        "             TextField(label=\"About Detail (optional)\", expand=True, hint_text=\"Subject Details (optional)\", value=prefs['prompt_remixer']['optional_about_influencer'], multiline=True, on_change=lambda e: changed(e, 'optional_about_influencer'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_remixer']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_remixer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_remixer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_remixer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        AI_engine,\n",
        "        ResponsiveRow([\n",
        "          Row([Text(\"Request Mode:\"), request_slider,], col={'lg':6}),\n",
        "          Row([Text(\" AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, value=prefs['prompt_remixer']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üçπ   Remix Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_remixer(page)),\n",
        "        page.prompt_remixer_list,\n",
        "        remixer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptBrainstormer(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_brainstormer'][pref] = e.control.value\n",
        "      status['changed_prompt_brainstormer'] = True\n",
        "    def click_prompt_brainstormer(e):\n",
        "      if prefs['prompt_brainstormer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "        if status['installed_OpenAI']:\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "        if status['installed_TextSynth']:\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must Install TextSynth GPT-J Library first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\":\n",
        "        if bool(prefs['HuggingFace_api_key']):\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must provide your HuggingFace API Key in settings first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5 XXL\":\n",
        "        if bool(prefs['HuggingFace_api_key']):\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must provide your HuggingFace API Key in settings first before using this Request Mode...\")\n",
        "    page.prompt_brainstormer_list = Column([], spacing=0)\n",
        "    def add_to_prompt_brainstormer(p):\n",
        "      page.prompt_brainstormer_list.controls.append(Text(p, style=TextThemeStyle.BODY_LARGE, selectable=True))\n",
        "      page.prompt_brainstormer_list.update()\n",
        "      brainstormer_list_buttons.visible = True\n",
        "      brainstormer_list_buttons.update()\n",
        "    page.add_to_prompt_brainstormer = add_to_prompt_brainstormer\n",
        "    def add_to_prompts(e):\n",
        "      page.add_to_prompts(new_prompt_text.value)\n",
        "    def clear_prompts(e):\n",
        "      page.prompt_brainstormer_list.controls = []\n",
        "      page.prompt_brainstormer_list.update()\n",
        "      brainstormer_list_buttons.visible = False\n",
        "      brainstormer_list_buttons.update()\n",
        "    def clear_prompt_text(e):\n",
        "      new_prompt_text.value = \"\"\n",
        "      new_prompt_text.update()\n",
        "\n",
        "    new_prompt_text = TextField(label=\"New Prompt Text\", expand=True, suffix=IconButton(icons.CLEAR, on_click=clear_prompt_text), autofocus=True, on_submit=add_to_prompts)\n",
        "    add_to_prompts_button = ElevatedButton(\"‚ûï  Add to Prompts\", on_click=add_to_prompts)#, icon=icons.ADD_ROUNDED\n",
        "    brainstormer_list_buttons = Row([\n",
        "        new_prompt_text, add_to_prompts_button,\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Brainstorms\"), on_click=clear_prompts),\n",
        "    ], alignment=MainAxisAlignment.END)\n",
        "\n",
        "    if len(page.prompt_brainstormer_list.controls) < 1:\n",
        "      brainstormer_list_buttons.visible = False\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§î  Prompt Brainstormer - TextSynth GPT-J-6B, OpenAI GPT-3 & HuggingFace Bloom AI\",\n",
        "               \"Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friends. Experiment, each has different personalities.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"TextSynth GPT-J\"), dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"OpenAI GPT-4\"), dropdown.Option(\"GPT-4 Turbo\"), dropdown.Option(\"HuggingFace Bloom 176B\"), dropdown.Option(\"HuggingFace Flan-T5 XXL\"), dropdown.Option(\"StableLM 7b\"), dropdown.Option(\"StableLM 3b\"), dropdown.Option(\"Google Gemini\")], value=prefs['prompt_brainstormer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine')),\n",
        "          Dropdown(label=\"Request Mode\", width=250, options=[dropdown.Option(\"Brainstorm\"), dropdown.Option(\"Write\"), dropdown.Option(\"Rewrite\"), dropdown.Option(\"Edit\"), dropdown.Option(\"Story\"), dropdown.Option(\"Description\"), dropdown.Option(\"Picture\"), dropdown.Option(\"Raw Request\")], value=prefs['prompt_brainstormer']['request_mode'], on_change=lambda e: changed(e, 'request_mode')),\n",
        "        ], alignment=MainAxisAlignment.START),\n",
        "        Row([TextField(label=\"About Prompt\", expand=True, value=prefs['prompt_brainstormer']['about_prompt'], multiline=True, on_change=lambda e: changed(e, 'about_prompt')),]),\n",
        "        ElevatedButton(content=Text(\"‚õàÔ∏è    Brainstorm Prompt\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_brainstormer(page)),\n",
        "        page.prompt_brainstormer_list,\n",
        "        brainstormer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptWriter(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_writer'][pref] = e.control.value\n",
        "      status['changed_prompt_writer'] = True\n",
        "    page.prompt_writer_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      negative_prompt = prefs['prompt_writer']['negative_prompt']\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "        page.add_to_prompts(p, {'negative_prompt':negative_prompt})\n",
        "      else:\n",
        "        page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_writer(p):\n",
        "      page.prompt_writer_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_writer_list.update()\n",
        "      writer_list_buttons.visible = True\n",
        "      writer_list_buttons.update()\n",
        "    page.add_to_prompt_writer = add_to_prompt_writer\n",
        "\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      negative_prompt = prefs['prompt_writer']['negative_prompt']\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "      for p in page.prompt_writer_list.controls:\n",
        "        if bool(negative_prompt):\n",
        "          page.add_to_prompts(p.title.value, {'negative_prompt':negative_prompt})\n",
        "        else:\n",
        "          page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.prompt_writer_list.controls = []\n",
        "      page.prompt_writer_list.update()\n",
        "      writer_list_buttons.visible = False\n",
        "      writer_list_buttons.update()\n",
        "    writer_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_writer_list.controls) < 1:\n",
        "      writer_list_buttons.visible = False\n",
        "\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìú Advanced Prompt Writer with Noodle Soup Prompt random variables \", \"Construct your Art descriptions easier, with all the extras you need to engineer perfect prompts faster. Note, you don't have to use any randoms if you rather do all custom.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        ResponsiveRow([\n",
        "          TextField(label=\"Prompt Art Subjects\", value=prefs['prompt_writer']['art_Subjects'], on_change=lambda e: changed(e, 'art_Subjects'), multiline=True, max_lines=4, col={'lg':9}),\n",
        "          TextField(label=\"Negative Prompt (optional)\", value=prefs['prompt_writer']['negative_prompt'], on_change=lambda e: changed(e, 'negative_prompt'), multiline=True, max_lines=4, col={'lg':3}),\n",
        "        ]),\n",
        "        Row([TextField(label=\"by Artists\", value=prefs['prompt_writer']['by_Artists'], on_change=lambda e: changed(e, 'by_Artists')),\n",
        "             TextField(label=\"Art Styles\", value=prefs['prompt_writer']['art_Styles'], on_change=lambda e: changed(e, 'art_Styles')),]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_writer']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_writer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_writer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_writer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"‚úçÔ∏è   Write Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_writer(page)),\n",
        "        page.prompt_writer_list,\n",
        "        writer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "magic_prompt_prefs = {\n",
        "    'seed_prompt': '',\n",
        "    'seed': 0,\n",
        "    'amount': 8,\n",
        "    'random_artists': 0,\n",
        "    'random_styles': 0,\n",
        "    'permutate_artists': True,\n",
        "}\n",
        "\n",
        "def buildMagicPrompt(page):\n",
        "    global magic_prompt_prefs, prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            magic_prompt_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            magic_prompt_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            magic_prompt_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    page.magic_prompt_list = Column([], spacing=0)\n",
        "    page.magic_prompt_output = Column([])\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_magic_prompt(p):\n",
        "      page.magic_prompt_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.magic_prompt_list.update()\n",
        "      magic_list_buttons.visible = True\n",
        "      magic_list_buttons.update()\n",
        "    page.add_to_magic_prompt = add_to_magic_prompt\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.magic_prompt_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.magic_prompt_list.controls = []\n",
        "      page.magic_prompt_list.update()\n",
        "      magic_list_buttons.visible = False\n",
        "      magic_list_buttons.update()\n",
        "    seed = TextField(label=\"Seed\", value=magic_prompt_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    magic_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.magic_prompt_list.controls) < 1:\n",
        "      magic_list_buttons.visible = False\n",
        "\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé©  Magic Prompt Generator - GPT-2 AI Helper\", \"Generates new Image Prompts made for Stable Diffusion with a specially trained GPT-2 Text AI by Gustavosta...\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Starter Prompt Text\", expand=True, value=magic_prompt_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=40, value=magic_prompt_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=magic_prompt_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=magic_prompt_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=magic_prompt_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üßô   Make Magic Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_prompt(page)),\n",
        "        page.magic_prompt_output,\n",
        "        page.magic_prompt_list,\n",
        "        magic_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "distil_gpt2_prefs = {\n",
        "    'seed_prompt': '',\n",
        "    'seed': 0,\n",
        "    'amount': 8,\n",
        "    'AI_temperature': 0.9,\n",
        "    'top_k': 8,\n",
        "    'max_length': 80,\n",
        "    'repetition_penalty': 1.2,\n",
        "    'penalty_alpha': 0.6,\n",
        "    'no_repeat_ngram_size': 1,\n",
        "    'random_artists': 0,\n",
        "    'random_styles': 0,\n",
        "    'permutate_artists': True,\n",
        "}\n",
        "\n",
        "def buildDistilGPT2(page):\n",
        "    global distil_gpt2_prefs, prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            distil_gpt2_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            distil_gpt2_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            distil_gpt2_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    page.distil_gpt2_list = Column([], spacing=0)\n",
        "    page.distil_gpt2_output = Column([])\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_distil_gpt2(p):\n",
        "      page.distil_gpt2_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.distil_gpt2_list.update()\n",
        "      distil_list_buttons.visible = True\n",
        "      distil_list_buttons.update()\n",
        "    page.add_to_distil_gpt2 = add_to_distil_gpt2\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.distil_gpt2_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.distil_gpt2_list.controls = []\n",
        "      page.distil_gpt2_list.update()\n",
        "      distil_list_buttons.visible = False\n",
        "      distil_list_buttons.update()\n",
        "    AI_temperature = Row([Text(\"AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, tooltip=\"The value used to module the next token probabilities\", value=distil_gpt2_prefs['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6})\n",
        "    top_k = Row([Text(\"Top-K Samples:\"), Slider(label=\"{value}\", min=0, max=50, divisions=50, expand=True, tooltip=\"Number of highest probability vocabulary tokens to keep for top-k-filtering\", value=distil_gpt2_prefs['top_k'], on_change=lambda e: changed(e, 'top_k'))], col={'lg':6})\n",
        "    #max_length = Row([Text(\"Max Length:\"), Slider(label=\"{value}\", min=0, max=1024, divisions=1024, expand=True, tooltip=\"The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens.\", value=distil_gpt2_prefs['max_length'], on_change=lambda e: changed(e, 'max_length', ptype=\"int\"))], col={'lg':6})\n",
        "    max_length = SliderRow(label=\"Max Length\", min=0, max=1024, divisions=1024, pref=distil_gpt2_prefs, key='max_length')\n",
        "    repetition_penalty = Row([Text(\"Repetition Penalty:\"), Slider(label=\"{value}\", min=1.0, max=3.0, divisions=20, round=2, expand=True, tooltip=\"Penalizes repetition by discounting the scores of previously generated tokens\", value=distil_gpt2_prefs['repetition_penalty'], on_change=lambda e: changed(e, 'repetition_penalty'))], col={'lg':6})\n",
        "    penalty_alpha = Row([Text(\"Penalty Alpha:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, tooltip=\"The degeneration penalty for contrastive search; activate when it is larger than 0\", value=distil_gpt2_prefs['penalty_alpha'], on_change=lambda e: changed(e, 'penalty_alpha', ptype=\"float\"))], col={'lg':6})\n",
        "    no_repeat_ngram_size = Row([Text(\"No Repeat NGRAM Size:\"), Slider(label=\"{value}\", min=0, max=50, expand=True, divisions=50, tooltip=\"If set > 0, all ngrams of that size can only occur once. 0 adds more commas.\", value=distil_gpt2_prefs['no_repeat_ngram_size'], on_change=lambda e: changed(e, 'no_repeat_ngram_size', ptype=\"int\"))], col={'lg':6})\n",
        "    seed = TextField(label=\"Seed\", value=distil_gpt2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    distil_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.distil_gpt2_list.controls) < 1:\n",
        "      distil_list_buttons.visible = False\n",
        "\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚öóÔ∏è  Distilled GPT-2 Generator - GPT-2 AI Helper\", \"Generates new Image Prompts with a model trained on 2,470,000 descriptive Stable Diffusion prompts...\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Starter Prompt Text\", expand=True, value=distil_gpt2_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),\n",
        "        AI_temperature,\n",
        "        ResponsiveRow([top_k, no_repeat_ngram_size]),\n",
        "        ResponsiveRow([repetition_penalty, penalty_alpha]),\n",
        "        max_length,\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=40, value=distil_gpt2_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=distil_gpt2_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=distil_gpt2_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=distil_gpt2_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üìù   Make Distil GPT-2 Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_distil_gpt2(page)),\n",
        "        page.distil_gpt2_output,\n",
        "        page.distil_gpt2_list,\n",
        "        distil_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "def NSP_instructions(page):\n",
        "    def open_url(e):\n",
        "        if e.data.startswith('http'):\n",
        "          page.launch_url(e.data)\n",
        "        else:\n",
        "          page.set_clipboard(e.data)\n",
        "          page.snack_bar = SnackBar(content=Text(f\"üìã   NSP variable {e.data} copied to clipboard...\"))\n",
        "          page.snack_bar.open = True\n",
        "          page.update()\n",
        "    NSP_markdown = '''To use a term database, simply use any of the keys below in sentence. Copy to Clipboard with click.\n",
        "\n",
        "For example if you wanted beauty adjective, you would write `_adj-beauty_` in your prompt.\n",
        "\n",
        "## Terminology Keys (by [@WAS](https://rebrand.ly/easy-diffusion))\n",
        "\n",
        "### Adjective Types\n",
        "   - [\\_adj-architecture\\_](_adj-architecture_) - A list of architectural adjectives and styles\n",
        "   - [\\_adj-beauty\\_](_adj-beauty_) - A list of beauty adjectives for people (maybe things?)\n",
        "   - [\\_adj-general\\_](_adj-general_) - A list of general adjectives for people/things.\n",
        "   - [\\_adj-horror\\_](_adj-horror_) - A list of horror adjectives\n",
        "### Art Types\n",
        "   - [\\_artist\\_](_artist_) - A comprehensive list of artists by [**MisterRuffian**](https://docs.google.com/spreadsheets/d/1_jgQ9SyvUaBNP1mHHEzZ6HhL_Es1KwBKQtnpnmWW82I/edit) (Discord _Misterruffian#2891_)\n",
        "   - [\\_color\\_](_color_) - A comprehensive list of colors\n",
        "   - [\\_portrait-type\\_](_portrait-type_) - A list of common portrait types/poses\n",
        "   - [\\_style\\_](_style_) - A list of art styles and mediums\n",
        "### Computer Graphics Types\n",
        "   - [\\_3d-terms\\_](_3d-terms_) - A list of 3D graphics terminology\n",
        "   - [\\_color-palette\\_](_color-palette_) - A list of computer and video game console color palettes\n",
        "   - [\\_hd\\_](_hd_) - A list of high definition resolution terms\n",
        "### Miscellaneous Types\n",
        "   - [\\_details\\_](_details_) - A list of detail descriptors\n",
        "   - [\\_site\\_](_site_) - A list of websites to query\n",
        "   - [\\_gen-modififer\\_](_gen-modififer_) - A list of general modifiers adopted from [Weird Wonderful AI Art](https://weirdwonderfulai.art/)\n",
        "   - [\\_neg-weight\\_](_neg-weight_) - A lsit of negative weight ideas\n",
        "   - [\\_punk\\_](_punk_) - A list of punk modifier (eg. cyberpunk)\n",
        "   - [ _pop-culture\\_](_pop-culture_) - A list of popular culture movies, shows, etc\n",
        "   - [\\_pop-location\\_](_pop-location_) - A list of popular tourist locations\n",
        "   - [\\_fantasy-setting\\_](_fantasy-setting_) - A list of fantasy location settings\n",
        "   - [\\_fantasy-creature\\_](_fantasy-creature_) - A list of fantasy creatures\n",
        "   - [\\_animals\\_](_animals_) - A list of modern animals\n",
        "### Noun Types\n",
        "   - [\\_noun-beauty\\_](_noun-beauty_) - A list of beauty related nouns\n",
        "   - [\\_noun-emote\\_](_noun-emote_) - A list of emotions and expressions\n",
        "   - [\\_noun-fantasy\\_](_noun-fantasy_) - A list of fantasy nouns\n",
        "   - [\\_noun-general\\_](_noun-general_) - A list of general nouns\n",
        "   - [\\_noun-horror\\_](_noun-horror_) - A list of horror nouns\n",
        "### People Types\n",
        "   - [\\_bodyshape\\_](_bodyshape_) - A list of body shapes\n",
        "   - [\\_celeb\\_](_celeb_) - A list of celebrities\n",
        "   - [\\_eyecolor\\_](_eyecolor_) - A list of eye colors\n",
        "   - [\\_hair\\_](_hair_) - A list of hair types\n",
        "   - [\\_nationality\\_](_nationality_) - A list of nationalities\n",
        "   - [\\_occputation\\_](_occputation_) A list of occupation types\n",
        "   - [\\_skin-color\\_](_skin-color_) - A list of skin tones\n",
        "   - [\\_identity-young\\_](_identity-young_) A list of young identifiers\n",
        "   - [\\_identity-adult\\_](_identity-adult_) A list of adult identifiers\n",
        "   - [\\_identity\\_](_identity_) A list of general identifiers\n",
        "### Photography / Image / Film Types\n",
        "   - [\\_aspect-ratio\\_](_aspect-ratio_) - A list of common aspect ratios\n",
        "   - [\\_cameras\\_](_cameras_) - A list of camera models *(including manufactuerer)*\n",
        "   - [\\_camera-manu\\_](_camera-manu_) - A list of camera manufacturers\n",
        "   - [\\_f-stop\\_](_f-stop_) - A list of camera aperture f-stop\n",
        "   - [\\_focal-length\\_](_focal-length_) - A list of focal length ranges\n",
        "   - [\\_photo-term\\_](_photo-term_) - A list of photography terms relating to photos\n",
        "\n",
        "So in Subject try something like: `A _color_ _noun-general_ that is _adj-beauty_ and _adj-general_ with a _noun-emote_ _noun-fantasy_`\n",
        "'''\n",
        "    def close_NSP_dlg(e):\n",
        "      instruction_alert.open = False\n",
        "      page.update()\n",
        "    instruction_alert = AlertDialog(title=Text(\"üçú  Noodle Soup Prompt Variables Instructions\"), content=Column([Markdown(NSP_markdown, extension_set=\"gitHubWeb\", on_tap_link=open_url)], scroll=ScrollMode.AUTO), actions=[TextButton(\"üç≤  Good Soup! \", on_click=close_NSP_dlg)], actions_alignment=MainAxisAlignment.END,)\n",
        "    page.dialog = instruction_alert\n",
        "    instruction_alert.open = True\n",
        "    page.update()\n",
        "\n",
        "negatives = {\n",
        "    'Blurry': 'blurry, blur, out of focus, jpeg artifacts, pixelated',\n",
        "    'Text': 'text, words, signature, watermark',\n",
        "    'Bad Quality': 'bad quality, low-res, worst quality, grainy',\n",
        "    'Ugly': 'ugly, deformed, distorted, poorly drawn',\n",
        "    'Black & White': 'black and white, monochrome, monotone, colorless',\n",
        "    'Bad Hands': 'bad hands, deformed fingers, fewer digits, fused fingers',\n",
        "    'Bad Body': 'bad body, disfigured, amputation, wrong anatomy, missing limbs, extra limbs',\n",
        "    'Bad Eyes': 'bad eyes, deformed iris, deformed pupils, unclear eyes, cross-eyed',\n",
        "    'Bad Legs': 'bad legs, missing legs, extra legs, missing feet, imperfect feet, bad knee, fused calf',\n",
        "    'Mutilated': 'mutilated, mutilation, mutated, morbid, bad anatomy',\n",
        "    'Proportion': 'bad proportions, gross proportions, long neck, long body, malformed',\n",
        "    'Saturation': 'over saturated, unsaturated, washed out, bad saturation',\n",
        "    'Contrast': 'high contrast, low contrast, High pass filter',\n",
        "    'Boring': 'boring, unappealing, tasteless, tacky, lackluster',\n",
        "    'Simple': 'simple, simplistic, sketch, amateur, plain background',\n",
        "    'Unrealistic': 'unrealistic, vector, cartoon',\n",
        "    'Cropped': 'cropped, crop, out of frame, cut off',\n",
        "    'NSFW': 'NSFW, nude, naked, censored, censor_bar, nipples',\n",
        "}\n",
        "\n",
        "def buildNegatives(page):\n",
        "    global prefs, negatives, status\n",
        "    def change_neg(e):\n",
        "        if e.control.data in prefs['negatives']:\n",
        "            prefs['negatives'].remove(e.control.data)\n",
        "        else:\n",
        "            prefs['negatives'].append(e.control.data)\n",
        "        update_negs()\n",
        "        status['changed_prompt_generator'] = True\n",
        "    negs = \"\"\n",
        "    def changed_custom(e):\n",
        "        nonlocal negs\n",
        "        prefs['custom_negatives'] = e.control.value\n",
        "        cust = f\", {prefs['custom_negatives']}\"\n",
        "        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')\n",
        "        neg_text.update()\n",
        "    def update_negs(update=True):\n",
        "        nonlocal negs\n",
        "        negs_list = []\n",
        "        for n in prefs['negatives']:\n",
        "            negs_list.append(negatives[n])\n",
        "        negs = ', '.join(negs_list)\n",
        "        cust = f\", {prefs['custom_negatives']}\"\n",
        "        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')\n",
        "        if update: neg_text.update()\n",
        "    def copy_clip(e):\n",
        "        page.set_clipboard(neg_text.value)\n",
        "        page.snack_bar = SnackBar(content=Text(f\"üìã  Copied to clipboard... Paste into your Negative Prompt Text.\"))\n",
        "        page.snack_bar.open = True\n",
        "        page.update()\n",
        "    neg_list = ResponsiveRow(controls=[])\n",
        "    for k, v in negatives.items():\n",
        "        neg_list.controls.append(Checkbox(label=k, tooltip=v, data=k, value=k in prefs['negatives'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_neg, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    custom_negatives = TextField(label=\"Custom Negative Text\", value=prefs['custom_negatives'], on_change=changed_custom)\n",
        "    neg_text = Text(negs, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)\n",
        "    update_negs(False)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üö´   Negative Prompt Builder\", \"Generate your Negatives with ease to subtract what you don't want in your images.\"),\n",
        "        neg_list,\n",
        "        custom_negatives,\n",
        "        ElevatedButton(content=Text(\"‚ûñ  Copy Negs to Clipboard\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=copy_clip),\n",
        "        Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), alignment = alignment.center, margin=margin.only(top=10)),#], alignment=CrossAxisAlignment.CENTER),\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptStyler(page):\n",
        "    global prefs, status\n",
        "    def styler_help(e):\n",
        "        def close_styler_dlg(e):\n",
        "          nonlocal styler_help_dlg\n",
        "          styler_help_dlg.open = False\n",
        "          page.update()\n",
        "        styler_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Prompt Styler\"), content=Column([\n",
        "            Text(\"This allows you to take a simple base prompt and apply a preset style to the the positive and negative prompt variables.  You can then add that stylized prompt to your Prompts List, or copy/paste it for other Image Generators.\"),\n",
        "            Markdown(\"Credit goes to [Prompt Styler](https://github.com/twri/sdxl_prompt_styler) by twri and [Fooocus UI](https://github.com/lllyasviel/Fooocus) by Illyasviel for the Styler presets, which is a pretty good GUI alternative for easy SDXL generation. Launch [Fooocus Colab](https://colab.research.google.com/github/camenduru/Fooocus-colab/blob/main/Fooocus_colab.ipynb) and read this [Fooocus Style Reference Doc](https://docs.google.com/spreadsheets/d/1AF5bd-fALxlu0lguZQiQVn1yZwxUiBJGyh2eyJJWl74/edit#gid=0)...\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üíÖ  So Stylish... \", on_click=close_styler_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = styler_help_dlg\n",
        "        styler_help_dlg.open = True\n",
        "        page.update()\n",
        "    def toggle_multi(e):\n",
        "        prefs['prompt_styler_multi'] = e.control.value\n",
        "        styler_radio_container.visible=not prefs['prompt_styler_multi']\n",
        "        styler_checkbox_container.visible=prefs['prompt_styler_multi']\n",
        "        styler_checkbox_container.update()\n",
        "        styler_radio_container.update()\n",
        "        status['changed_prompt_generator'] = True\n",
        "    def change_style(e):\n",
        "        prefs['prompt_style'] = e.control.value\n",
        "        update_style()\n",
        "        status['changed_prompt_generator'] = True\n",
        "    negative = \"\"\n",
        "    prompt = \"\"\n",
        "    def changed_custom(e):\n",
        "        nonlocal prompt\n",
        "        prefs['prompt_styler'] = e.control.value\n",
        "        styler = aeionic_utils.prompt_styles[prefs['prompt_style']]\n",
        "        prompt = styler[0].replace(\"{prompt}\", prefs['prompt_styler'])\n",
        "        prompt = to_title(prompt, sentence=True, clean=False)\n",
        "        prompt_text.value = prompt\n",
        "        prompt_text.update()\n",
        "        status['changed_prompt_generator'] = True\n",
        "    def changed_checkbox(e):\n",
        "        on = e.control.value\n",
        "        if e.control.data in prefs['prompt_styles']:\n",
        "            prefs['prompt_styles'].remove(e.control.data)\n",
        "        else:\n",
        "            prefs['prompt_styles'].append(e.control.data)\n",
        "        status['changed_prompt_generator'] = True\n",
        "    def update_style(update=True):\n",
        "        nonlocal negative, prompt\n",
        "        styler = aeionic_utils.prompt_styles[prefs['prompt_style']]\n",
        "        prompt = styler[0].replace(\"{prompt}\", prefs['prompt_styler'])\n",
        "        prompt = to_title(prompt, sentence=True, clean=False)\n",
        "        negative = styler[1]\n",
        "        prompt_text.value = prompt\n",
        "        neg_text.value = negative\n",
        "        if update:\n",
        "          prompt_text.update()\n",
        "          neg_text.update()\n",
        "    def add_to_prompts(e):\n",
        "        nonlocal negative, prompt\n",
        "        page.add_to_prompts(prompt, {'negative_prompt': negative})\n",
        "        if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def copy_clip(e):\n",
        "        page.set_clipboard(neg_text.value)\n",
        "        page.snack_bar = SnackBar(content=Text(f\"üìã  Copied to clipboard... Paste into your Negative Prompt Text.\"))\n",
        "        page.snack_bar.open = True\n",
        "        page.update()\n",
        "    def add_to_prompt_list(e):\n",
        "        styler = aeionic_utils.prompt_styles[e.control.data]\n",
        "        pr = styler[0].replace(\"{prompt}\", prefs['prompt_styler'])\n",
        "        pr = to_title(pr, sentence=True, clean=False)\n",
        "        negative = styler[1]\n",
        "        arg = {'negative_prompt': negative}\n",
        "        page.add_to_prompts(pr, arg)\n",
        "        if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_list(e):\n",
        "        if prefs['enable_sounds']: page.snd_drop.play()\n",
        "        for s in prefs['prompt_styles']:\n",
        "            styler = aeionic_utils.prompt_styles[s]\n",
        "            pr = styler[0].replace(\"{prompt}\", prefs['prompt_styler'])\n",
        "            pr = to_title(pr, sentence=True, clean=False)\n",
        "            negative = styler[1]\n",
        "            arg = {'negative_prompt': negative}\n",
        "            page.add_to_prompts(pr, arg)\n",
        "    def clear_prompts(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        styler_results.controls.clear()\n",
        "        styler_results.update()\n",
        "    def generate_styles(e):\n",
        "        styler_results.controls.clear()\n",
        "        for s in prefs['prompt_styles']:\n",
        "            styler = aeionic_utils.prompt_styles[s]\n",
        "            pr = styler[0].replace(\"{prompt}\", prefs['prompt_styler'])\n",
        "            pr = to_title(pr, sentence=True, clean=False)\n",
        "            negative = styler[1]\n",
        "            styler_results.controls.append(ListTile(title=Text(pr, max_lines=3, style=TextThemeStyle.BODY_LARGE), subtitle=Text(f\"Negative: {negative}\", max_lines=3), data=s, dense=True, on_click=add_to_prompt_list))\n",
        "        styler_list_buttons = Row([\n",
        "            ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "            add_all_to_prompts_btn,\n",
        "        ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "        styler_results.controls.append(styler_list_buttons)\n",
        "        styler_results.update()\n",
        "    style_list = ResponsiveRow(controls=[], run_spacing=0)\n",
        "    style_checkboxes = ResponsiveRow(controls=[], run_spacing=0)\n",
        "    for k in aeionic_utils.style_keys:\n",
        "        style_list.controls.append(ft.Radio(label=k, value=k, fill_color=colors.PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "        style_checkboxes.controls.append(Checkbox(label=k, data=k, value=k in prefs['prompt_styles'], on_change=changed_checkbox, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    prompt_styler_multi = Switcher(label=\"Multi-Select \", value=prefs['prompt_styler_multi'], tooltip=\"Toggle between Single Style Selection to Multiple Prompt Generator.\", label_position=ft.LabelPosition.LEFT, on_change=toggle_multi)\n",
        "    prompt_styler = TextField(label=\"Subject Prompt Text\", value=prefs['prompt_styler'], filled=True, on_change=changed_custom)\n",
        "    prompt_text = Text(prompt, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)\n",
        "    neg_text = Text(negative, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)\n",
        "    style_radio = ft.RadioGroup(content=style_list, value=prefs['prompt_style'], on_change=change_style)\n",
        "    styler_results = Column([], tight=True, spacing=0)\n",
        "    generate_styles_btn = ElevatedButton(content=Text(\"üëó  Generate Styles\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=generate_styles)\n",
        "    add_all_to_prompts_btn = FilledButton(content=Text(\"‚ûï  Add to Prompts List\", size=20), height=45, on_click=add_to_list)\n",
        "    add_to_prompts_btn = ElevatedButton(content=Text(\"‚ûï  Add to Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=add_to_prompts)\n",
        "    styler_result = ResponsiveRow([\n",
        "          Container(prompt_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 8}),\n",
        "          Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 4}),\n",
        "        ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    styler_checkbox_container = Container(Column([\n",
        "      style_checkboxes,\n",
        "      prompt_styler,\n",
        "      generate_styles_btn,\n",
        "      styler_results,\n",
        "    ]), visible=prefs['prompt_styler_multi'])\n",
        "    styler_radio_container = Container(Column([\n",
        "      style_radio,\n",
        "      prompt_styler,\n",
        "      add_to_prompts_btn,\n",
        "      styler_result,\n",
        "    ]), visible=not prefs['prompt_styler_multi'])\n",
        "    update_style(False)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëì   Prompt Styler\", \"Generate your Prompts with Premade Style Templates.\", actions=[prompt_styler_multi, IconButton(icon=icons.HELP, tooltip=\"Help with Prompt Styler\", on_click=styler_help)]),\n",
        "        #prompt_styler_multi,\n",
        "        styler_checkbox_container,\n",
        "        styler_radio_container,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "ESRGAN_prefs = {\n",
        "    'enlarge_scale': 1.5,\n",
        "    'face_enhance': False,\n",
        "    'image_path': '',\n",
        "    'save_to_GDrive': True,\n",
        "    'upload_file': False,\n",
        "    'download_locally': False,\n",
        "    'display_image': False,\n",
        "    'dst_image_path': '',\n",
        "    'filename_suffix': '',\n",
        "    'split_image_grid': False,\n",
        "    'rows': 3,\n",
        "    'cols': 3,\n",
        "}\n",
        "def buildESRGANupscaler(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        ESRGAN_prefs[pref] = e.control.value\n",
        "    def add_to_ESRGAN_output(o):\n",
        "      ESRGAN_output.controls.append(o)\n",
        "      ESRGAN_output.update()\n",
        "      if clear_button.visible == False:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "      #generator_list_buttons.visible = True\n",
        "      #generator_list_buttons.update()\n",
        "    page.add_to_ESRGAN_output = add_to_ESRGAN_output\n",
        "    def toggle_split(e):\n",
        "      split_container.height = None if e.control.value else 0\n",
        "      changed(e, 'split_image_grid')\n",
        "      split_container.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      ESRGAN_output.controls = []\n",
        "      ESRGAN_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    page.clear_ESRGAN_output = clear_output\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        image_path.value = fname\n",
        "        image_path.update()\n",
        "        ESRGAN_prefs['image_path'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    def pick_destination(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to Save Images into, Right Click and Copy Path, then Paste here\")\n",
        "    page.overlay.append(file_picker)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=ESRGAN_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=ESRGAN_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    image_path = TextField(label=\"Image File or Folder Path\", value=ESRGAN_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))\n",
        "    dst_image_path = TextField(label=\"Destination Image Path\", value=ESRGAN_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))\n",
        "    filename_suffix = TextField(label=\"Optional Filename Suffix\", hint_text=\"-big\", value=ESRGAN_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)\n",
        "    download_locally = Checkbox(label=\"Download Images Locally\", value=ESRGAN_prefs['download_locally'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'download_locally'))\n",
        "    display_image = Checkbox(label=\"Display Upscaled Image\", value=ESRGAN_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))\n",
        "    split_image_grid = Switcher(label=\"Split Image Grid\", value=ESRGAN_prefs['split_image_grid'], on_change=toggle_split)\n",
        "    rows = NumberPicker(label=\"Rows: \", min=1, max=8, value=ESRGAN_prefs['rows'], on_change=lambda e: changed(e, 'rows'))\n",
        "    cols = NumberPicker(label=\"Columns: \", min=1, max=8, value=ESRGAN_prefs['cols'], on_change=lambda e: changed(e, 'cols'))\n",
        "    split_container = Container(Row([rows, Container(content=None, width=25), cols]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=28), height=0)\n",
        "    ESRGAN_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(ESRGAN_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚ÜïÔ∏è   Real-ESRGAN AI Upscale Enlarging\", \"Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download.\"),\n",
        "        enlarge_scale_slider,\n",
        "        face_enhance,\n",
        "        ResponsiveRow([image_path, dst_image_path]),\n",
        "        filename_suffix,\n",
        "        Row([download_locally, display_image]),\n",
        "        #Divider(thickness=2, height=4),\n",
        "        split_image_grid,\n",
        "        split_container,\n",
        "        ElevatedButton(content=Text(\"üêò  Run AI Upscaling\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),\n",
        "        ESRGAN_output,\n",
        "        clear_button,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "#TODO: https://colab.research.google.com/github/AhabbscienceStudioPak/ESRGAN/blob/master/ESRGAN_Colab.ipynb\n",
        "upscale_video_prefs = {\n",
        "    'enlarge_scale': 1.5,\n",
        "    'face_enhance': False,\n",
        "    'image_path': '',\n",
        "    'save_to_GDrive': True,\n",
        "    'upload_file': False,\n",
        "    'download_locally': False,\n",
        "    'display_image': False,\n",
        "    'dst_image_path': '',\n",
        "    'filename_suffix': '',\n",
        "}\n",
        "def buildUpscaleVideo(page):\n",
        "    global upscale_video_prefs\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        upscale_video_prefs[pref] = e.control.value\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=upscale_video_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=upscale_video_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    #image_path = TextField(label=\"Image File or Folder Path\", value=upscale_video_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))\n",
        "    #dst_image_path = TextField(label=\"Destination Image Path\", value=upscale_video_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))\n",
        "    filename_suffix = TextField(label=\"Optional Filename Suffix\", hint_text=\"-big\", value=upscale_video_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚ÜïÔ∏è   Upscale Videos with AI Enlarging\", \"Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download.\"),\n",
        "        enlarge_scale_slider,\n",
        "        face_enhance,\n",
        "        #ResponsiveRow([image_path, dst_image_path]),\n",
        "        filename_suffix,\n",
        "        #Row([download_locally, display_image]),\n",
        "        #Divider(thickness=2, height=4),\n",
        "        ElevatedButton(content=Text(\"üêò  Run Video Upscaling\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "retrieve_prefs = {\n",
        "    'image_path': '',\n",
        "    'add_to_prompts': True,\n",
        "    'display_full_metadata': False,\n",
        "    'display_image': False,\n",
        "    'upload_file': False,\n",
        "}\n",
        "def buildRetrievePrompts(page):\n",
        "    def changed(e, pref=None):\n",
        "        if pref is not None:\n",
        "          retrieve_prefs[pref] = e.control.value\n",
        "    def add_to_retrieve_output(o):\n",
        "      retrieve_output.controls.append(o)\n",
        "      retrieve_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      retrieve_output.controls = []\n",
        "      retrieve_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def pick_image(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to Retrieve, Right Click and Copy Path, then Paste here\")\n",
        "    page.add_to_retrieve_output = add_to_retrieve_output\n",
        "    image_path = TextField(label=\"Image File or Folder Path\", value=retrieve_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))\n",
        "    add_to_prompts = Checkbox(label=\"Add to Prompts\", value=retrieve_prefs['add_to_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'add_to_prompts'))\n",
        "    display_full_metadata = Checkbox(label=\"Display Full Metadata\", value=retrieve_prefs['display_full_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_full_metadata'))\n",
        "    display_image = Checkbox(label=\"Display Image\", value=retrieve_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))\n",
        "    retrieve_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(retrieve_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üì∞  Retrieve Dream Prompts from Image Metadata\", \"Give it images made here and gives you all parameters used to recreate it. Either upload png file(s) or paste path to image or folder or config.json to revive your dreams..\"),\n",
        "        image_path,\n",
        "        add_to_prompts,\n",
        "        display_full_metadata,\n",
        "        display_image,\n",
        "        ElevatedButton(content=Text(\"üò¥  Retrieve Dream\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_retrieve(page)),\n",
        "        retrieve_output,\n",
        "        clear_button,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "initfolder_prefs = {\n",
        "    'prompt_string': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_folder': '',\n",
        "    'include_strength': True,\n",
        "    'image_strength': 0.5,\n",
        "}\n",
        "def buildInitFolder(page):\n",
        "    def changed(e, pref=None):\n",
        "        if pref is not None:\n",
        "          initfolder_prefs[pref] = e.control.value\n",
        "    def add_to_initfolder_output(o):\n",
        "      initfolder_output.controls.append(o)\n",
        "      initfolder_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      initfolder_output.controls = []\n",
        "      initfolder_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def pick_init(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to use as Init Folder, Right Click and Copy Path, then Paste here\")\n",
        "    def toggle_strength(e):\n",
        "      changed(e,'include_strength')\n",
        "      strength_row.visible = e.control.value\n",
        "      strength_row.update()\n",
        "    page.add_to_initfolder_output = add_to_initfolder_output\n",
        "    prompt_string = TextField(label=\"Prompt Text\", value=initfolder_prefs['prompt_string'], col={'md': 9}, on_change=lambda e:changed(e,'prompt_string'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=initfolder_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "\n",
        "    init_folder = TextField(label=\"Init Image Folder Path\", value=initfolder_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    include_strength = Checkbox(label=\"Include Strength\", value=initfolder_prefs['include_strength'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)\n",
        "    image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}\", round=2, value=float(initfolder_prefs['image_strength']), expand=True)\n",
        "    strength_row = Row([Text(\"Image Strength:\"), image_strength])\n",
        "    strength_row.visible = initfolder_prefs['include_strength']\n",
        "    initfolder_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(initfolder_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìÇ Generate Prompts from Folder as Init Images\", \"Provide a Folder with a collection of images that you want to automatically add to prompts list with init_image overides...\"),\n",
        "        init_folder,\n",
        "        ResponsiveRow([prompt_string, negative_prompt]),\n",
        "        include_strength,\n",
        "        strength_row,\n",
        "        ElevatedButton(content=Text(\"‚ûï  Add to Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_initfolder(page)),\n",
        "        initfolder_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "init_video_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_folder': '',\n",
        "    'include_strength': False,\n",
        "    'image_strength': 0.5,\n",
        "    'max_size': 1024,\n",
        "    'file_prefix': 'frame-',\n",
        "    'video_file': '',\n",
        "    'fps': 15,\n",
        "    'start_time': 0.0,\n",
        "    'end_time': 0.0,\n",
        "    'batch_folder_name': '',\n",
        "    'show_images': False,\n",
        "}\n",
        "def buildInitVideo(page):\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            init_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            init_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            init_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_init_video_output(o):\n",
        "      init_video_output.controls.append(o)\n",
        "      init_video_output.update()\n",
        "      if clear_button.visible == False:\n",
        "          clear_button.visible = True\n",
        "          clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      init_video_output.controls = []\n",
        "      init_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          init_video_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          init_video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        video_file.value = fname\n",
        "        video_file.update()\n",
        "        init_video_prefs['video_file'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_video(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"avi\", \"mp4\", \"mov\"], dialog_title=\"Pick Video File\")\n",
        "    def toggle_strength(e):\n",
        "      changed(e,'include_strength')\n",
        "      strength_row.visible = e.control.value\n",
        "      strength_row.update()\n",
        "    page.add_to_init_video_output = add_to_init_video_output\n",
        "    prompt = TextField(label=\"Prompt Text\", value=init_video_prefs['prompt'], col={'md': 9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=init_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=init_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Image Filename Prefix\", value=init_video_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    video_file = TextField(label=\"Video File\", value=init_video_prefs['video_file'], on_change=lambda e:changed(e,'video_file'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))\n",
        "    #init_folder = TextField(label=\"Init Image Folder Path\", value=init_video_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    fps = TextField(label=\"Frames Per Seccond\", value=init_video_prefs['fps'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'fps', ptype=\"int\"))\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=init_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=init_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=init_video_prefs, key='max_size')\n",
        "    show_images = Checkbox(label=\"Show Extracted Images\", value=init_video_prefs['show_images'], tooltip=\"Fills up screen with all frames, you probably don't need to.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_images'))\n",
        "    include_strength = Checkbox(label=\"Include Strength   \", value=init_video_prefs['include_strength'], tooltip=\"Otherwise defaults to setting in Image Parameters\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)\n",
        "    image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}\", round=2, value=float(init_video_prefs['image_strength']), expand=True)\n",
        "    strength_row = Row([Text(\"Image Strength:\"), image_strength])\n",
        "    strength_row.visible = init_video_prefs['include_strength']\n",
        "    init_video_output = Column([])\n",
        "    page.init_video_output = init_video_output\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Prompts List\", size=18), on_click=page.clear_prompts_list), ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(init_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé• Generate Prompts from Video File Frames\", \"Provide a short video clip to automatically add sequence to prompts list with init_image overides...\"),\n",
        "        video_file, #init_folder,\n",
        "        Row([fps, start_time, end_time]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([include_strength, show_images]),\n",
        "        strength_row,\n",
        "        max_row,\n",
        "        ElevatedButton(content=Text(\"‚è©  Frames to Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_init_video(page)),\n",
        "        init_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "image2text_prefs = {\n",
        "    'method': 'Fuyu-8B',\n",
        "    'mode': 'Best',\n",
        "    'fuyu_mode': 'Detailed Caption',\n",
        "    'gemini_mode': 'Detailed Caption',\n",
        "    'request_mode': 'Caption',\n",
        "    'slow_workers': True,\n",
        "    'trusted_workers': False,\n",
        "    'question': '',\n",
        "    'folder_path': '',\n",
        "    'image_path': '',\n",
        "    'max_size': 768,\n",
        "    'save_csv': False,\n",
        "    'images': [],\n",
        "    'use_AIHorde': False,\n",
        "}\n",
        "\n",
        "def buildImage2Text(page):\n",
        "    global prefs, image2text_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            image2text_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            image2text_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            image2text_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_image2text_output(o):\n",
        "      page.image2text_output.controls.append(o)\n",
        "      page.image2text_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image2text_output.controls = []\n",
        "      page.image2text_output.update()\n",
        "      save_dir = os.path.join(root_dir, 'image2text')\n",
        "      if os.path.exists(save_dir):\n",
        "        for f in os.listdir(save_dir):\n",
        "            os.remove(os.path.join(save_dir, f))\n",
        "        os.rmdir(save_dir)\n",
        "      page.image2text_file_list.controls = []\n",
        "      page.image2text_file_list.update()\n",
        "      image2text_list_buttons.visible = False\n",
        "      image2text_list_buttons.update()\n",
        "    def i2t_help(e):\n",
        "      def close_i2t_dlg(e):\n",
        "        nonlocal i2t_help_dlg\n",
        "        i2t_help_dlg.open = False\n",
        "        page.update()\n",
        "      i2t_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Image2Text CLIP Interrogator\"), content=Column([\n",
        "          Text(\"You have 4 methods to get a caption prompt from you input image. Using BLIP Interrogation Mode with Best, Classic or Fast will use the older BLIP models that work great but takes longer to run. The latest is Google Gemini Pro Vision API is very impressive and fast, using your Bard API key for free. The Fuyu-8B is a much faster captioning technique using Transformers, and can give Detailed Coco-style prompts or simpler captioning. You can also use AIHorde to process the caption interrogation in the cloud using Stable Horde services without using your GPU.\"),\n",
        "          Text(\"Fuyu-8B by AdeptAI Labs is a small version of the multimodal model. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy. It‚Äôs designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images. It‚Äôs fast - we can get responses for large images in less than 100 milliseconds.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  Okay then... \", on_click=close_i2t_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = i2t_help_dlg\n",
        "      i2t_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        save_dir = os.path.join(root_dir, 'image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        image2text_prefs['folder_path'] = save_dir\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          fpath = os.path.join(save_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "        original_img = PILImage.open(fname)\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        #shutil.move(fname, fpath)\n",
        "        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        page.image2text_file_list.update()\n",
        "        image2text_prefs['images'].append(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                #uf.append(FilePickerUploadFile(f.name, upload_url=f.path))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.image2text_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.image2text_file_list.controls[i]\n",
        "              page.image2text_file_list.update()\n",
        "              continue\n",
        "        if f in image2text_prefs['images']:\n",
        "          image2text_prefs['images'].remove(f)\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.image2text_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.image2text_file_list.controls.clear()\n",
        "        page.image2text_file_list.update()\n",
        "        image2text_prefs['images'].clear()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        alert_msg(e.page, \"Image Details\", content=Image(src=img), sound=False)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        image2text_prefs['folder_path'] = save_dir\n",
        "        if image_path.value.startswith('http'):\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        page.image2text_file_list.update()\n",
        "        image2text_prefs['images'].append(fpath)\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    page.image2text_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_image2text(p):\n",
        "      page.image2text_list.controls.append(ListTile(title=Text(p, max_lines=10, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.image2text_list.update()\n",
        "      image2text_list_buttons.visible = True\n",
        "      image2text_list_buttons.update()\n",
        "    page.add_to_image2text = add_to_image2text\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.image2text_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def toggle_AIHorde(e):\n",
        "      use = e.control.value\n",
        "      changed(e,'use_AIHorde')\n",
        "      AIHorde_row.height=None if use else 0\n",
        "      AIHorde_row.update()\n",
        "      mode.visible = not use\n",
        "      mode.update()\n",
        "      request_mode.visible = use\n",
        "      request_mode.update()\n",
        "    def change_method(e):\n",
        "      method = e.control.value\n",
        "      changed(e,'method')\n",
        "      AIHorde_row.visible = method==\"AIHorde Crowdsourced\"\n",
        "      AIHorde_row.update()\n",
        "      mode.visible = method==\"BLIP-Interrogation\"\n",
        "      mode.update()\n",
        "      request_mode.visible = method==\"AIHorde Crowdsourced\"\n",
        "      request_mode.update()\n",
        "      fuyu_mode.visible = method==\"Fuyu-8B\"\n",
        "      fuyu_mode.update()\n",
        "      gemini_mode.visible = method==\"Google Gemini Pro\"\n",
        "      gemini_mode.update()\n",
        "      question_prompt.visible = (method==\"Fuyu-8B\" and image2text_prefs['fuyu_mode']==\"Question\") or (method==\"Google Gemini Pro\" and image2text_prefs['gemini_mode']==\"Question\")\n",
        "      question_prompt.update()\n",
        "    def change_fuyu(e):\n",
        "      fuyu = e.control.value\n",
        "      changed(e,'fuyu_mode')\n",
        "      question_prompt.visible = fuyu==\"Question\"\n",
        "      question_prompt.update()\n",
        "    def change_gemini(e):\n",
        "      gemini = e.control.value\n",
        "      changed(e,'gemini_mode')\n",
        "      question_prompt.visible = gemini==\"Question\"\n",
        "      question_prompt.update()\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image2text_list.controls = []\n",
        "      page.image2text_list.update()\n",
        "      prompts = []\n",
        "      image2text_list_buttons.visible = False\n",
        "      image2text_list_buttons.update()\n",
        "    image2text_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\"), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.image2text_list.controls) < 1:\n",
        "      image2text_list_buttons.visible = False\n",
        "\n",
        "    method = Dropdown(label=\"Captioning Method\", width=250, options=[dropdown.Option(\"Fuyu-8B\"), dropdown.Option(\"Google Gemini Pro\"), dropdown.Option(\"BLIP-Interrogation\"), dropdown.Option(\"AIHorde Crowdsourced\")], value=image2text_prefs['method'], on_change=change_method)\n",
        "    #use_AIHorde = Switcher(label=\"Use AIHorde Crowdsourced Interrogator\", value=image2text_prefs['use_AIHorde'], on_change=toggle_AIHorde)\n",
        "    mode = Dropdown(label=\"Interrogation Mode\", width=200, options=[dropdown.Option(\"Best\"), dropdown.Option(\"Classic\"), dropdown.Option(\"Fast\")], value=image2text_prefs['mode'], visible=image2text_prefs['method']==\"BLIP-Interrogation\", on_change=lambda e: changed(e, 'mode'))\n",
        "    request_mode = Dropdown(label=\"Request Mode\", width=200, options=[dropdown.Option(\"Caption\"), dropdown.Option(\"Interrogation\"), dropdown.Option(\"Full Prompt\")], value=image2text_prefs['request_mode'], visible=image2text_prefs['method']==\"AIHorde Crowdsourced\", on_change=lambda e: changed(e, 'request_mode'))\n",
        "    fuyu_mode = Dropdown(label=\"Fuyu Request Mode\", width=200, options=[dropdown.Option(\"Detailed Caption\"), dropdown.Option(\"Simple Caption\"), dropdown.Option(\"Question\")], value=image2text_prefs['fuyu_mode'], visible=image2text_prefs['method']==\"Fuyu-8B\", on_change=change_fuyu)\n",
        "    gemini_mode = Dropdown(label=\"Gemini Request Mode\", width=200, options=[dropdown.Option(\"Detailed Caption\"), dropdown.Option(\"Poetic Caption\"), dropdown.Option(\"Artistic Caption\"), dropdown.Option(\"Technical Caption\"), dropdown.Option(\"Simple Caption\"), dropdown.Option(\"Question\")], value=image2text_prefs['gemini_mode'], visible=image2text_prefs['method']==\"Google Gemini Pro\", on_change=change_gemini)\n",
        "    slow_workers = Checkbox(label=\"Allow Slow Workers\", tooltip=\"\", value=image2text_prefs['slow_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'slow_workers'))\n",
        "    trusted_workers = Checkbox(label=\"Only Trusted Workers\", tooltip=\"\", value=image2text_prefs['trusted_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trusted_workers'))\n",
        "    AIHorde_row = Container(content=Row([slow_workers, trusted_workers]), visible=image2text_prefs['method']==\"AIHorde Crowdsourced\", animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    question_prompt = TextField(label=\"Ask Question about Image\", value=image2text_prefs['question'], visible=image2text_prefs['method']==\"Fuyu-8B\" and image2text_prefs['fuyu_mode']==\"Question\", expand=True, on_change=lambda e:changed(e,'question'))\n",
        "    save_csv = Checkbox(label=\"Save CSV file of Prompts\", tooltip=\"\", value=image2text_prefs['save_csv'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_csv'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=32, multiple=16, suffix=\"px\", pref=image2text_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.image2text_file_list = Column([], tight=True, spacing=0)\n",
        "    page.image2text_output = Column([])\n",
        "    #clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    #clear_button.visible = len(page.image2text_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Image2Text CLIP-Interrogator\", subtitle=\"Create text prompts by describing input images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image2Text Interrogator\", on_click=i2t_help)]),\n",
        "        #mode,\n",
        "        Row([method, fuyu_mode, mode, request_mode, gemini_mode, question_prompt, AIHorde_row]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.image2text_file_list,\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Get Prompts from Images\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image2text(page)),\n",
        "        page.image2text_list,\n",
        "        image2text_list_buttons,\n",
        "        page.image2text_output,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "BLIP2_image2text_prefs = {\n",
        "    'folder_path': '',\n",
        "    'image_path': '',\n",
        "    'max_size': 768,\n",
        "    'num_captions': 1,\n",
        "    'question_prompt': '',\n",
        "    #'model_name': 'blip2_t5',\n",
        "    'model_type': 'pretrain_flant5xxl', #pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl\n",
        "    'images': [],\n",
        "}\n",
        "\n",
        "def buildBLIP2Image2Text(page):\n",
        "    global prefs, BLIP2_image2text_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            BLIP2_image2text_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            BLIP2_image2text_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            BLIP2_image2text_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_BLIP2_image2text_output(o):\n",
        "      page.BLIP2_image2text_output.controls.append(o)\n",
        "      page.BLIP2_image2text_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.BLIP2_image2text_output.controls = []\n",
        "      page.BLIP2_image2text_output.update()\n",
        "      save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "      if os.path.exists(save_dir):\n",
        "        for f in os.listdir(save_dir):\n",
        "            os.remove(os.path.join(save_dir, f))\n",
        "        os.rmdir(save_dir)\n",
        "      page.BLIP2_image2text_file_list.controls = []\n",
        "      page.BLIP2_image2text_file_list.update()\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    def BLIP2_i2t_help(e):\n",
        "      def close_BLIP2_i2t_dlg(e):\n",
        "        nonlocal BLIP2_i2t_help_dlg\n",
        "        BLIP2_i2t_help_dlg.open = False\n",
        "        page.update()\n",
        "      BLIP2_i2t_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with BLIP2 Image2Text Interrogator\"), content=Column([\n",
        "          Text(\"Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Model\", weight=FontWeight.BOLD),\n",
        "          Text(\"BLIP-2 is a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòë  Alright, got it... \", on_click=close_BLIP2_i2t_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = BLIP2_i2t_help_dlg\n",
        "      BLIP2_i2t_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        BLIP2_image2text_prefs['folder_path'] = save_dir\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          fpath = os.path.join(save_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "        original_img = PILImage.open(fname)\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        #shutil.move(fname, fpath)\n",
        "        page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "        page.BLIP2_image2text_file_list.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        BLIP2_image2text_prefs['folder_path'] = save_dir\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.BLIP2_image2text_file_list.update()\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.BLIP2_image2text_file_list.update()\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "              page.BLIP2_image2text_file_list.update()\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    page.BLIP2_image2text_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_BLIP2_image2text(p):\n",
        "      page.BLIP2_image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.BLIP2_image2text_list.update()\n",
        "      BLIP2_image2text_list_buttons.visible = True\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    page.add_to_BLIP2_image2text = add_to_BLIP2_image2text\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.BLIP2_image2text_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.BLIP2_image2text_list.controls = []\n",
        "      page.BLIP2_image2text_list.update()\n",
        "      prompts = []\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    BLIP2_image2text_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\"), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.BLIP2_image2text_list.controls) < 1:\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "    #'pretrain_flant5xxl', pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl\n",
        "    #model_name = Dropdown(label=\"Model Nane\", width=250, options=[dropdown.Option(\"blip2_t5\"), dropdown.Option(\"blip2_opt\"), dropdown.Option(\"img2prompt_vqa\")], value=BLIP2_image2text_prefs['model_name'], on_change=lambda e: changed(e, 'model_name'))\n",
        "    model_type = Dropdown(label=\"Model Type\", width=250, options=[dropdown.Option(\"pretrain_flant5xxl\"), dropdown.Option(\"pretrain_opt2.7b\"), dropdown.Option(\"pretrain_opt6.7b\"), dropdown.Option(\"caption_coco_opt6.7b\"), dropdown.Option(\"caption_coco_flant5xl\"), dropdown.Option(\"base\")], value=BLIP2_image2text_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'))\n",
        "    num_captions = Container(NumberPicker(label=\"Number of Captions: \", min=1, max=10, value=BLIP2_image2text_prefs['num_captions'], on_change=lambda e: changed(e, 'num_captions')))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=BLIP2_image2text_prefs, key='max_size')\n",
        "    question_prompt = TextField(label=\"Ask Question about Image (optional)\", value=BLIP2_image2text_prefs['question_prompt'], on_change=lambda e:changed(e,'question_prompt'))\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Examine\", value=BLIP2_image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=True)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.BLIP2_image2text_file_list = Column([], tight=True, spacing=0)\n",
        "    page.BLIP2_image2text_output = Column([])\n",
        "    #clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    #clear_button.visible = len(page.BLIP2_image2text_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§≥  BLIP2 Image2Text Examiner\", subtitle=\"Create prompts by describing input images... Warning: Uses A LOT of VRAM, may crash session.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image2Text Interrogator\", on_click=BLIP2_i2t_help)]),\n",
        "        Row([model_type, num_captions]),\n",
        "        max_row,\n",
        "        question_prompt,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.BLIP2_image2text_file_list,\n",
        "        page.BLIP2_image2text_list,\n",
        "        BLIP2_image2text_list_buttons,\n",
        "        ElevatedButton(content=Text(\"üì∏  Get Prompts from Images\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_BLIP2_image2text(page)),\n",
        "        page.BLIP2_image2text_output,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "dance_prefs = {\n",
        "    'dance_model': 'maestro-150k',\n",
        "    'installed_model': None,\n",
        "    'inference_steps': 50,\n",
        "    'batch_size': 1,\n",
        "    'seed': 0,\n",
        "    'audio_length_in_s': 4.5,\n",
        "    'community_model': 'Daft Punk',\n",
        "    'custom_model': '',\n",
        "    'train_custom': False,#https://colab.research.google.com/github/Harmonai-org/sample-generator/blob/main/Finetune_Dance_Diffusion.ipynb\n",
        "    'custom_name': '',\n",
        "    'wav_path': '',\n",
        "    'demo_every': 250,#Number of training steps between demos\n",
        "    'checkpoint_every': 500,#Number of training steps between saving model checkpoints\n",
        "    'sample_rate': 48000,#Sample rate to train at\n",
        "    'sample_size': 65536,#Number of audio samples per training sample\n",
        "    'random_crop': True,#If true, the audio samples provided will be randomly cropped to SAMPLE_SIZE samples. Turn off if you want to ensure the training data always starts at the beginning of the audio files (good for things like drum one-shots)\n",
        "    'finetune_batch_size': 2,#Batch size to fine-tune (make it as high as it can go for your GPU)\n",
        "    'accumulate_batches': 4,#Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints\n",
        "    'save_model': False,\n",
        "    'where_to_save_model': \"Public Library\",\n",
        "    'readme_description': '',\n",
        "}\n",
        "community_dance_diffusion_models = [\n",
        "    #{'name': 'LCD Soundsystem', 'download': 'https://drive.google.com/uc?id=1WX8nL4_x49h0OJE5iGrjXJnIJ0yvsTxI', 'ckpt':'lcd-soundsystem-200k.ckpt'}, #https://huggingface.co/Gecktendo/lcd-soundsystem/\n",
        "    {'name': 'Daft Punk', 'download': 'https://drive.google.com/uc?id=1CZjWIcL528zbZa6GrS_triob0hUy6KEs', 'ckpt':'daft-punk-241.5k.ckpt'}, #https://huggingface.co/Gecktendo/daft-punk\n",
        "    {'name': 'Vague Phrases', 'download': 'https://drive.google.com/uc?id=1nUn2qydqU7hlDUT-Skq_Ionte_8-Vdjr', 'ckpt': 'SingingInFepoch=1028-step=195500-pruned.ckpt'},\n",
        "    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/uc?id=1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},\n",
        "    {'name': 'Smash Mouth Vocals', 'download': 'https://drive.google.com/uc?id=1h3fkJnByw3mKpXUiNPWKoYtzmpeg1QEt', 'ckpt':'epoch=773-step=191500.ckpt'},\n",
        "    {'name': 'Dubstep Bass Growls', 'download': 'https://drive.google.com/file/d/104Ni-suQ0-tt2Xe9SbWjTnWFOSkT6O47', 'ckpt':'epoch=1266-step=195000.ckpt'},\n",
        "    {'name': 'Jumango Ambient', 'download': 'https://drive.google.com/file/d/1-gpOee-v7ZGFJtzr76sYTKuIKTQKTCmN', 'ckpt':'jumango-ambient-v1.ckpt'},\n",
        "    {'name': 'Serum Wavetables', 'download': 'https://drive.google.com/file/d/1l0JhA2qTaXtt5pdyv7rW1Ls4wmAFWVD1', 'ckpt':'serumwavetables-49k.ckpt'},\n",
        "    {'name': 'Textured Grooves Club', 'download': 'https://drive.google.com/file/d/13VAGMSPaIGo7FGDAtedaykhcasrFk2Z_', 'ckpt':'TexturedGroovesLargeClub_step_592200.ckpt'},\n",
        "    {'name': 'Abstract Vocal', 'download': 'https://drive.google.com/file/d/1izVPIYgPhpIT8lZtaWIO8gbTnfEL9g_J', 'ckpt':'Singing-step277000-pruned.ckpt'},\n",
        "    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/file/d/1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},\n",
        "    {'name': 'Paul McCartney Vocals', 'download': 'https://drive.google.com/file/d/1-_FtUwLMnMUGLMpvtnE0EDAcUjDlXSeS', 'ckpt':'epoch=1148-step=193000.ckpt'},\n",
        "    {'name': 'Techno Kicks', 'download': 'https://drive.google.com/file/d/1-gR9QFq7ZYHn2ep0gw5IyQ6WzRZJW_tG', 'ckpt':'epoch=1296-step=441500.ckpt'},\n",
        "    {'name': 'Electronic Snare Drums', 'download': 'https://drive.google.com/file/d/1-T-PFtfyc_JUan71Px_FWsxiiuBuNN_1', 'ckpt':'epoch=1078-step=195000.ckpt'},\n",
        "    {'name': 'Electronic Snare Drums+', 'download': 'https://drive.google.com/file/d/1-50R5wwyhNrQSlvaxEqQ8CiRJPYgzGsr', 'ckpt':'epoch=1110-step=195500.ckpt'},\n",
        "    {'name': 'Electronic Kick Drums', 'download': 'https://drive.google.com/file/d/1-46jYgYfz_Jbnu-dNvepWqa7rcSP__zo', 'ckpt':'epoch=1234-step=197500.ckpt'},\n",
        "]\n",
        "dance_pipe = None\n",
        "def buildDanceDiffusion(page):\n",
        "    global dance_pipe, dance_prefs\n",
        "    def changed(e, pref=None, isInt=False):\n",
        "        if pref is not None:\n",
        "          if isInt:\n",
        "            dance_prefs[pref] = int(e.control.value)\n",
        "          else:\n",
        "            dance_prefs[pref] = e.control.value\n",
        "    def dance_help(e):\n",
        "        def close_dance_dlg(e):\n",
        "          nonlocal dance_help_dlg\n",
        "          dance_help_dlg.open = False\n",
        "          page.update()\n",
        "        dance_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Dance Diffusion\"), content=Column([\n",
        "            Text(\"HarmonAI Dance Diffusion\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üëÑ  Sounds Good...I hope \", on_click=close_dance_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dance_help_dlg\n",
        "        dance_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_audio(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.dance_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.dance_file_list.controls[i]\n",
        "              page.dance_file_list.update()\n",
        "              if f in dance_prefs['custom_wavs']:\n",
        "                dance_prefs['custom_wavs'].remove(f)\n",
        "              continue\n",
        "    def delete_all_audios(e):\n",
        "        for fl in page.dance_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.dance_file_list.controls.clear()\n",
        "        page.dance_file_list.update()\n",
        "        dance_prefs['custom_wavs'].clear()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.dance_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Audio\", on_click=delete_audio, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_audios, data=fpath),\n",
        "          ])))\n",
        "        dance_prefs['custom_wavs'].append(fpath)\n",
        "        if update: page.dance_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'dance-audio')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "            shutil.move(fname, fpath)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"wav\", \"WAV\", \"mp3\", \"MP3\"], dialog_title=\"Pick Audio WAV or MP3 Files to Train\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_wav(e):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        if wav_path.value.startswith('http'):\n",
        "            #import requests\n",
        "            from io import BytesIO\n",
        "            #response = requests.get(wav_path.value)\n",
        "            fpath = download_file(wav_path.value)\n",
        "            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "            add_file(fpath)\n",
        "        elif os.path.isfile(wav_path.value):\n",
        "          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "          shutil.copy(wav_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(wav_path.value):\n",
        "          for f in os.listdir(wav_path.value):\n",
        "            file_path = os.path.join(wav_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(wav_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        wav_path.value = \"\"\n",
        "        wav_path.update()\n",
        "    def load_wavs():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_custom(e):\n",
        "        changed(e, 'train_custom')\n",
        "        custom_box.height = None if dance_prefs['train_custom'] else 0\n",
        "        custom_box.update()\n",
        "        custom_audio_name.visible = dance_prefs['train_custom']\n",
        "        custom_audio_name.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = dance_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = dance_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def changed_model(e):\n",
        "      dance_prefs['dance_model'] = e.control.value\n",
        "      if e.control.value == 'Community':\n",
        "        community_dance_diffusion_model.visible = True\n",
        "        community_dance_diffusion_model.update()\n",
        "      else:\n",
        "        if community_dance_diffusion_model.visible:\n",
        "          community_dance_diffusion_model.visible = False\n",
        "          community_dance_diffusion_model.update()\n",
        "      if e.control.value == 'Custom':\n",
        "        custom_model.visible = True\n",
        "        custom_model.update()\n",
        "      else:\n",
        "        if custom_model.visible:\n",
        "          custom_model.visible = False\n",
        "          custom_model.update()\n",
        "    dance_model = Dropdown(label=\"Dance Diffusion Model\", width=250, options=[dropdown.Option(\"maestro-150k\"), dropdown.Option(\"glitch-440k\"), dropdown.Option(\"jmann-small-190k\"), dropdown.Option(\"jmann-large-580k\"), dropdown.Option(\"unlocked-250k\"), dropdown.Option(\"honk-140k\"), dropdown.Option(\"gwf-440k\"), dropdown.Option(\"Community\"), dropdown.Option(\"Custom\")], value=dance_prefs['dance_model'], on_change=changed_model)\n",
        "    community_dance_diffusion_model = Dropdown(label=\"Community Model\", width=250, options=[], value=dance_prefs['community_model'], on_change=lambda e: changed(e, 'community_model'))\n",
        "    page.community_dance_diffusion_model = community_dance_diffusion_model\n",
        "    custom_model = TextField(label=\"Custom Model Path\", value=dance_prefs['custom_model'], on_change=lambda e:changed(e,'custom_model'))\n",
        "    for c in prefs['custom_dance_diffusion_models']:\n",
        "      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))\n",
        "    for c in community_dance_diffusion_models:\n",
        "      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))\n",
        "    if not dance_prefs['dance_model'] == 'Community':\n",
        "      community_dance_diffusion_model.visible = False\n",
        "    if not dance_prefs['dance_model'] == 'Custom':\n",
        "      custom_model.visible = False\n",
        "    inference_row = SliderRow(label=\"Number of Inference Steps\", min=10, max=200, divisions=190, pref=dance_prefs, key='inference_steps')\n",
        "    batch_size = TextField(label=\"Batch Size\", value=dance_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    seed = TextField(label=\"Random Seed\", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', isInt=True), width = 110)\n",
        "    audio_length_in_s = TextField(label=\"Audio Length in Seconds\", value=dance_prefs['audio_length_in_s'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'audio_length_in_s'), width = 190)\n",
        "    number_row = Row([batch_size, seed, audio_length_in_s])\n",
        "    train_custom = Switcher(label=\"Train Custom Audio \", value=dance_prefs['train_custom'], on_change=toggle_custom)\n",
        "    custom_audio_name = TextField(label=\"Custom Audio Name\", value=dance_prefs['custom_name'], on_change=lambda e:changed(e,'custom_name'))\n",
        "    wav_path = TextField(label=\"Audio Files or Folder Path or URL to Train\", value=dance_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_wav_button = ElevatedButton(content=Text(\"Add Audio Files\"), on_click=add_wav)\n",
        "    page.dance_file_list = Column([], tight=True, spacing=0)\n",
        "    sample_rate = Tooltip(message=\"Sample rate to train at\", content=TextField(label=\"Sample Rate\", value=dance_prefs['sample_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_rate', ptype='int'), width = 160))\n",
        "    sample_size = Tooltip(message=\"Number of audio samples per training sample\", content=TextField(label=\"Sample Size\", value=dance_prefs['sample_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_size', ptype='int'), width = 160))\n",
        "    finetune_batch_size = Tooltip(message=\"Batch size to fine-tune (make it as high as it can go for your GPU)\", content=TextField(label=\"Finetune Batch Size\", value=dance_prefs['finetune_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'finetune_batch_size', ptype='int'), width = 160))\n",
        "    accumulate_batches = Tooltip(message=\"Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints\", content=TextField(label=\"Accumulate Batches\", value=dance_prefs['accumulate_batches'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'accumulate_batches', ptype='int'), width = 160))\n",
        "    demo_every = Tooltip(message=\"Number of training steps between demos\", content=TextField(label=\"Demo Every\", value=dance_prefs['demo_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'demo_every', ptype='int'), width = 160))\n",
        "    checkpoint_every = Tooltip(message=\"Number of training steps between saving model checkpoints\", content=TextField(label=\"Checkpoint Every\", value=dance_prefs['checkpoint_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'checkpoint_every', ptype='int'), width = 160))\n",
        "    save_model = Switcher(label=\"Save Model to HuggingFace \", value=dance_prefs['save_model'], on_change=toggle_save, tooltip=\"Requires WRITE access on API Key to Upload Checkpoint\")\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=dance_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=dance_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    where_to_save_model.visible = dance_prefs['save_model']\n",
        "    readme_description.visible = dance_prefs['save_model']\n",
        "    custom_box = Container(Column([\n",
        "        Container(content=None, height=3),\n",
        "        Row([sample_rate, sample_size]),\n",
        "        Row([finetune_batch_size, accumulate_batches]),\n",
        "        Row([demo_every, checkpoint_every]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        Text(\"Provide 3 or more ~10 second clips of Music as mp3 or wav files:\", weight=FontWeight.BOLD),\n",
        "        Row([wav_path, add_wav_button]),\n",
        "        page.dance_file_list,]), padding=padding.only(left=11), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    custom_box.height = None if dance_prefs['train_custom'] else 0\n",
        "    custom_audio_name.visible = dance_prefs['train_custom']\n",
        "    load_wavs()\n",
        "    #seed = TextField(label=\"Seed\", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusdance_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=dance_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.dance_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dance_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëØ Create experimental music or sounds with HarmonAI trained audio models\", \"Tools to train a generative model on arbitrary audio samples...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DanceDiffusion Settings\", on_click=dance_help)]),\n",
        "        Row([dance_model, community_dance_diffusion_model, custom_model]),\n",
        "        inference_row,\n",
        "        number_row,\n",
        "        Row([train_custom, custom_audio_name], vertical_alignment=CrossAxisAlignment.START),\n",
        "        custom_box,\n",
        "        ElevatedButton(content=Text(\"üéµ  Run Dance Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dance_diffusion(page)),\n",
        "        page.dance_output,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "audio_diffusion_prefs = {\n",
        "    'audio_file': '',\n",
        "    'file_name': '',\n",
        "    'audio_model': 'teticio/audio-diffusion-ddim-256',\n",
        "    'scheduler': 'DDIM', #DDPM\n",
        "    'steps': 50, #number of de-noising steps (defaults to 50 for DDIM, 1000 for DDPM)\n",
        "    'start_step': 0, #step to start from\n",
        "    'slice': 0, #slice number of audio to convert\n",
        "    'eta': 0.2, #parameter between 0 and 1 used with DDIM scheduler\n",
        "    'mask_start_secs': 0.0, #number of seconds of audio to mask (not generate) at start\n",
        "    'mask_end_secs': 0.0, #number of seconds of audio to mask (not generate) at end\n",
        "    'seed': 0,\n",
        "    'audio_name': '',\n",
        "    'wav_path': '',\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ad-',\n",
        "    'loaded_model': '',\n",
        "}\n",
        "\n",
        "def buildAudioDiffusion(page):\n",
        "    global prefs, audio_diffusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              audio_diffusion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              audio_diffusion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              audio_diffusion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_audio_diffusion_output(o):\n",
        "        page.audio_diffusion_output.controls.append(o)\n",
        "        page.audio_diffusion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.audio_diffusion_output.controls = []\n",
        "        page.audio_diffusion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def audio_diffusion_help(e):\n",
        "        def close_audio_diffusion_dlg(e):\n",
        "          nonlocal audio_diffusion_help_dlg\n",
        "          audio_diffusion_help_dlg.open = False\n",
        "          page.update()\n",
        "        audio_diffusion_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-Diffusion\"), content=Column([\n",
        "            Text(\"Audio Diffusion leverages the recent advances in image generation using diffusion models by converting audio samples to and from mel spectrogram images.\"),\n",
        "            Markdown(\"The original codebase of this implementation can be found [here](https://github.com/teticio/audio-diffusion), including training scripts and example notebooks.\\n[Audio Diffusion](https://github.com/teticio/audio-diffusion) by Robert Dargavel Smith.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üîä  Sounds Groovie... \", on_click=close_audio_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = audio_diffusion_help_dlg\n",
        "        audio_diffusion_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              audio_diffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])\n",
        "              audio_diffusion_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            audio_diffusion_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp3\", \"wav\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Input Audio File (optional)\", value=audio_diffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {audio_diffusion_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    steps_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=audio_diffusion_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", round=2, value=float(audio_diffusion_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {audio_diffusion_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"DDIM ETA:\"), eta_value, eta,])\n",
        "    page.etas.append(eta_row)\n",
        "    audio_model = Dropdown(label=\"Audio Model\", width=400, options=[dropdown.Option(\"teticio/audio-diffusion-ddim-256\"), dropdown.Option(\"teticio/audio-diffusion-breaks-256\"), dropdown.Option(\"teticio/audio-diffusion-instrumental-hiphop-256\"), dropdown.Option(\"teticio/latent-audio-diffusion-256\"), dropdown.Option(\"teticio/latent-audio-diffusion-ddim-256\"), dropdown.Option(\"teticio/conditional-latent-audio-diffusion-512\")], value=audio_diffusion_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))\n",
        "    scheduler = Dropdown(label=\"De-noise Scheduler\", width=250, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"DDPM\")], value=audio_diffusion_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))\n",
        "    slice_audio = TextField(label=\"Slice of Audio\", value=audio_diffusion_prefs['slice'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'slice', ptype='int'), width = 130)\n",
        "    start_step = TextField(label=\"Starting Step\", value=audio_diffusion_prefs['start_step'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'start_step', ptype='int'), width = 130)\n",
        "    mask_start_secs = TextField(label=\"Mask Start (s)\", value=audio_diffusion_prefs['mask_start_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_start_secs', ptype='float'), width = 130)\n",
        "    mask_end_secs = TextField(label=\"Mask End (s)\", value=audio_diffusion_prefs['mask_end_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_end_secs', ptype='float'), width = 130)\n",
        "    audio_name = TextField(label=\"Audio File Name\", value=audio_diffusion_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=audio_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=audio_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:  \", min=1, max=5, value=audio_diffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=audio_diffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.audio_diffusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.audio_diffusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé∂  Audio Diffusion Modeling\", \"Converts Audio Samples to and from Mel Spectrogram Images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio Diffusion-TTS Settings\", on_click=audio_diffusion_help)]),\n",
        "        audio_file,\n",
        "        audio_model,\n",
        "        #scheduler,\n",
        "        steps_row,\n",
        "        eta_row,\n",
        "        Row([slice_audio, start_step, mask_start_secs, mask_end_secs]),\n",
        "        Row([batch_size, seed, file_prefix]),\n",
        "        Row([audio_name, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"ü™ó  Run Audio Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_diffusion(page)),\n",
        "        page.audio_diffusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "music_gen_prefs = {\n",
        "    'prompt': '',\n",
        "    'audio_file': '',\n",
        "    'file_name': '',\n",
        "    'audio_model': 'medium',\n",
        "    'duration': 30,\n",
        "    'top_k': 280,\n",
        "    'top_p': 1150,\n",
        "    'temperature': 0.7,\n",
        "    'guidance': 5,\n",
        "    'overlap': 2,\n",
        "    'dimension': 1,\n",
        "    #'recondition': False,\n",
        "    'harmony_only': False,\n",
        "    'use_sampling': True,\n",
        "    'two_step_cfg': False, #If True, performs 2 forward for Classifier Free Guidance, instead of batching together the two. This has some impact on how things are padded but seems to have little impact in practice.\n",
        "    'seed': 0,\n",
        "    'audio_name': '',\n",
        "    'wav_path': '',\n",
        "    'num_samples': 1,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'musicgen-',\n",
        "    'loaded_model': '',\n",
        "}\n",
        "\n",
        "def buildMusicGen(page):\n",
        "    global prefs, music_gen_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              music_gen_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              music_gen_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              music_gen_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_music_gen_output(o):\n",
        "        page.music_gen_output.controls.append(o)\n",
        "        page.music_gen_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.music_gen_output.controls = []\n",
        "        page.music_gen_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def music_gen_help(e):\n",
        "        def close_music_gen_dlg(e):\n",
        "          nonlocal music_gen_help_dlg\n",
        "          music_gen_help_dlg.open = False\n",
        "          page.update()\n",
        "        music_gen_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audiocraft MusicGen\"), content=Column([\n",
        "            Text(\"Meta releases new SOTA text to music model MusicGen. Demonstrated samples are better than existing models including Google's MusicLM. We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen.\"),\n",
        "            Text(\"Small: This model has 300M parameters and can only generate music from text. It is the quickest model, however it may not yield the best results.\\nMedium: This model has 1.5B parameters and can generate music from text as well. It is slower than the little model, but it produces better results.\\nMelody: This 1.5B parameter model can generate music from both text and melody. It is the slowest model, but it produces the best results.\\nLarge: This model has 3.3B parameters and can only generate music from text. It is the slowest model, but it produces the best results.\"),\n",
        "            Markdown(\"[GitHub Code](https://github.com/facebookresearch/audiocraft) | [Paper](https://arxiv.org/abs/2306.05284)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•Å  Let's hear it... \", on_click=close_music_gen_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = music_gen_help_dlg\n",
        "        music_gen_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              music_gen_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])\n",
        "              music_gen_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            music_gen_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp3\", \"wav\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Melody Conditioning Audio File (optional)\", value=music_gen_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "    prompt = TextField(label=\"Prompt to generate a track (genre, theme, etc.)\", value=music_gen_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=720, divisions=718, suffix=\"s\", expand=True, pref=music_gen_prefs, key='duration')\n",
        "    #steps_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=music_gen_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    audio_model = Dropdown(label=\"Audio Model\", width=150, options=[dropdown.Option(\"small\"), dropdown.Option(\"medium\"), dropdown.Option(\"large\")], value=music_gen_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))\n",
        "    guidance = SliderRow(label=\"Classifier Free Guidance\", min=0, max=10, divisions=20, round=1, pref=music_gen_prefs, key='guidance', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\", col={'lg':6})\n",
        "    temperature = SliderRow(label=\"AI Temperature\", min=0, max=1, divisions=10, round=1, pref=music_gen_prefs, key='temperature', tooltip=\"Softmax value used to module the next token probabilities\", col={'lg':6})\n",
        "    top_k = SliderRow(label=\"Top-K Samples\", min=0, max=300, divisions=299, round=0, pref=music_gen_prefs, key='top_k', tooltip=\"Number of highest probability vocabulary tokens to keep for top-k-filtering\", col={'lg':6})\n",
        "    top_p = SliderRow(label=\"Top-P Samples\", min=0, max=1500, divisions=1499, round=0, pref=music_gen_prefs, key='top_p', tooltip=\"Highest probability vocabulary tokens to keep, when set to 0 top_k is used\", col={'lg':6})\n",
        "    #recondition = Checkbox(label=\"Recondition Chunks over 30s\", tooltip=\"Condition next chunks with the first chunk.\", value=music_gen_prefs['recondition'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'recondition'), col={'lg':6})\n",
        "    harmony_only = Checkbox(label=\"Harmony Only\", tooltip=\"Remove Drums?\", value=music_gen_prefs['harmony_only'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'harmony_only'), col={'lg':6})\n",
        "    #harmony_only = gr.Radio(label=\"Harmony Only\",choices=[\"No\", \"Yes\"], value=\"No\", interactive=True, info=\"Remove Drums?\")\n",
        "    overlap = SliderRow(label=\"Overlap\", min=0, max=29, divisions=29, round=0, pref=music_gen_prefs, key='overlap', tooltip=\"Time to resample chunks longer than 30s.\", col={'lg':6})\n",
        "    dimension = SliderRow(label=\"Dimension\", min=-2, max=2, divisions=3, round=0, pref=music_gen_prefs, key='dimension', tooltip=\"Which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)\", col={'lg':6})\n",
        "    #dimension = gr.Slider(minimum=-2, maximum=2, value=2, step=1, label=\"Dimension\", info=\"determines which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)\", interactive=True)\n",
        "    audio_name = TextField(label=\"Audio File Name\", value=music_gen_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=music_gen_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=music_gen_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    num_samples = NumberPicker(label=\"Number of Samples:  \", min=1, max=10, value=music_gen_prefs['num_samples'], on_change=lambda e: changed(e, 'num_samples'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:  \", min=1, max=4, value=music_gen_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=music_gen_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.music_gen_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.music_gen_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü™ó  Meta Audiocraft MusicGen\", \"Simple and Controllable Music Generation with Audio tokenization model...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with MusicGen Settings\", on_click=music_gen_help)]),\n",
        "        prompt,\n",
        "        audio_file,\n",
        "        Row([audio_model, duration_row]),\n",
        "        ResponsiveRow([guidance, temperature]),\n",
        "        ResponsiveRow([top_k, top_p]),\n",
        "        ResponsiveRow([dimension, overlap]),\n",
        "        Row([num_samples, seed, file_prefix]),\n",
        "        Row([audio_name, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"üé∑  Run MusicGen\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_gen(page)),\n",
        "        page.music_gen_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "dreamfusion_prefs = {\n",
        "    'prompt_text': '',\n",
        "    'training_iters': 5000,\n",
        "    'learning_rate': 0.001,\n",
        "    'training_nerf_resolution': 64,\n",
        "    'seed': 0,\n",
        "    'lambda_entropy': 0.0001,\n",
        "    'max_steps': 512,\n",
        "    'checkpoint': 'latest',\n",
        "    'workspace': 'trial',\n",
        "}\n",
        "\n",
        "def buildDreamFusion(page):\n",
        "    global prefs, dreamfusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            dreamfusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            dreamfusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            dreamfusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_dreamfusion_output(o):\n",
        "      page.dreamfusion_output.controls.append(o)\n",
        "      page.dreamfusion_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.dreamfusion_output.controls = []\n",
        "      page.dreamfusion_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DreamFusion\"), content=Column([\n",
        "          Text(\"It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know.\"),\n",
        "          Text('It takes about 0.7s per training step, so the default 5000 training steps take around 1 hour to finish. A larger Training_iters usually leads to better results.'),\n",
        "          Text('If CUDA OOM, try to decrease Max_steps and Training_nerf_resolution.'),\n",
        "          Text('If the NeRF fails to learn anything (empty scene, only background), try to decrease Lambda_entropy which regularizes the learned opacity.')\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòä  So Exciting... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=dreamfusion_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))\n",
        "    training_iters = TextField(label=\"Training Iterations\", value=dreamfusion_prefs['training_iters'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_iters', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=dreamfusion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    training_nerf_resolution = TextField(label=\"Training NERF Res\", value=dreamfusion_prefs['training_nerf_resolution'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_nerf_resolution', ptype='int'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=dreamfusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    max_steps = TextField(label=\"Max Steps\", value=dreamfusion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    workspace = TextField(label=\"Workspace Folder\", value=dreamfusion_prefs['workspace'], on_change=lambda e:changed(e,'workspace'))\n",
        "    page.dreamfusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dreamfusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üóø  Create experimental DreamFusion 3D Model and Video\", \"Provide a prompt to render a model. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DreamFusion Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        Row([training_iters,learning_rate, lambda_entropy]),\n",
        "        Row([seed, training_nerf_resolution, max_steps]),\n",
        "        Row([workspace]),\n",
        "        ElevatedButton(content=Text(\"üî®  Run DreamFusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreamfusion(page)),\n",
        "        page.dreamfusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "point_e_prefs = {\n",
        "    'prompt_text': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'\n",
        "    'upsample': False,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'seed': 0,\n",
        "    'max_steps': 512,\n",
        "}\n",
        "\n",
        "def buildPoint_E(page):\n",
        "    global prefs, point_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            point_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            point_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            point_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_point_e_output(o):\n",
        "      page.point_e_output.controls.append(o)\n",
        "      page.point_e_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.point_e_output.controls = []\n",
        "      page.point_e_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with OpenAI Point-E\"), content=Column([\n",
        "          Markdown(\"This is an interface for running the [official codebase](https://github.com/openai/point-e) for point cloud diffusion models and SDF regression models described in [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751). These models were trained and released by OpenAI. Following [Model Cards for Model Reporting (Mitchell et al.)](https://arxiv.org/abs/1810.03993), we're providing some information about how the models were trained and evaluated.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"The Point-E models are trained for use as point cloud diffusion models and SDF regression models. Our image-conditional models are often capable of producing coherent 3D point clouds, given a single rendering of a 3D object. However, the models sometimes fail to do so, either producing incorrect geometry where the rendering is occluded, or producing geometry that is inconsistent with visible parts of the rendering. The resulting point clouds are relatively low-resolution, and are often noisy and contain defects such as outliers or cracks. Our text-conditional model is sometimes capable of producing 3D point clouds which can be recognized as the provided text description, especially when the text description is simple. However, we find that this model fails to generalize to complex prompts or unusual objects.\"),\n",
        "          Text(\"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models for you to use.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"‚òùÔ∏è  Good Points... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          point_e_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          point_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        point_e_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=point_e_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))\n",
        "    init_image = TextField(label=\"Sample Image (instead of prompt)\", value=point_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    base_model = Dropdown(label=\"Base Model\", width=250, options=[dropdown.Option(\"base40M-imagevec\"), dropdown.Option(\"base40M-textvec\"), dropdown.Option(\"base40M\"), dropdown.Option(\"base300M\"), dropdown.Option(\"base1B\")], value=point_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))\n",
        "    batch_folder_name = TextField(label=\"3D Model Folder Name\", value=point_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=point_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=5, value=point_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=point_e_prefs, key='guidance_scale')\n",
        "    #seed = TextField(label=\"Seed\", value=point_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    page.point_e_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.point_e_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëÜ  Point-E 3D Point Clouds\", \"Provide a Prompt or Image to render from a CLIP ViT-L/14 diffusion model...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Point-E Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        init_image,\n",
        "        base_model,\n",
        "        Row([batch_folder_name, batch_size]),\n",
        "        guidance,\n",
        "        ElevatedButton(content=Text(\"üêû  Run Point-E\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_point_e(page)),\n",
        "        page.point_e_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "shap_e_prefs = {\n",
        "    'prompt_text': ' with solid white background',\n",
        "    'init_image': '',\n",
        "    'init_images': [],\n",
        "    'guidance_scale': 5.0,\n",
        "    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'\n",
        "    'render_mode': 'NeRF', #STF\n",
        "    'use_karras': True,\n",
        "    'karras_steps': 50,\n",
        "    'size': 256,\n",
        "    'save_frames': False,\n",
        "    'use_original': False,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    #'seed': 0,\n",
        "    #'max_steps': 512,\n",
        "}\n",
        "\n",
        "def buildShap_E(page):\n",
        "    global prefs, shap_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            shap_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            shap_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            shap_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.shap_e_output.controls = []\n",
        "      page.shap_e_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with OpenAI Shap-E\"), content=Column([\n",
        "          Text(\"We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.\"),\n",
        "          Markdown(\"[GitHub Page](https://github.com/openai/shap-e) - [Read the Paper](https://arxiv.org/pdf/2305.02463.pdf)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üö¥  Shaping up... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          shap_e_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          shap_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        shap_e_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def toggle_original(e):\n",
        "      shap_e_prefs['use_original'] = e.control.value\n",
        "      original_row.height=None if shap_e_prefs['use_original'] else 0\n",
        "      original_row.update()\n",
        "    def run_shap(e):\n",
        "      if shap_e_prefs['use_original']:\n",
        "        run_shap_e(page)\n",
        "      else:\n",
        "        run_shap_e2(page)\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=shap_e_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))\n",
        "    init_image = TextField(label=\"Sample Image (optional, instead of prompt)\", value=shap_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    #TODO: Add Multi-Image List\n",
        "    #base_model = Dropdown(label=\"Base Model\", width=250, options=[dropdown.Option(\"base40M-imagevec\"), dropdown.Option(\"base40M-textvec\"), dropdown.Option(\"base40M\"), dropdown.Option(\"base300M\"), dropdown.Option(\"base1B\")], value=shap_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))\n",
        "    render_mode = Dropdown(label=\"Render Mode\", width=150, options=[dropdown.Option(\"NeRF\"), dropdown.Option(\"STF\")], value=shap_e_prefs['render_mode'], on_change=lambda e: changed(e, 'render_mode'))\n",
        "    size = SliderRow(label=\"Size of Render\", min=32, max=512, divisions=15, multiple=32, tooltip=\"Higher values take longer to render.\", suffix=\"px\", pref=shap_e_prefs, key='size')\n",
        "    karras_steps = SliderRow(label=\"Karras Steps\", min=1, max=100, divisions=99, round=0, pref=shap_e_prefs, key='karras_steps')\n",
        "    batch_folder_name = TextField(label=\"3D Model Folder Name\", value=shap_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=shap_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=5, value=shap_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=shap_e_prefs, key='guidance_scale')\n",
        "    use_original = Switcher(label=\"Use Original Shap-E Method\", value=shap_e_prefs['use_original'], tooltip=\"By default uses latest HuggingFace Diffusers Pipeline.\", on_change=toggle_original)\n",
        "    save_frames = Checkbox(label=\"Save Preview Frames\", tooltip=\"Saves PNG Sequence of camera rotation, same as animated gif preview.\", value=shap_e_prefs['save_frames'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_frames'))\n",
        "    original_row = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height = None if shap_e_prefs['use_original'] else 0, padding=padding.only(top=4), content=Row([render_mode, save_frames]))\n",
        "    #seed = TextField(label=\"Seed\", value=shap_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    page.shap_e_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.shap_e_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßä  Shap-E 3D Mesh\", \"Provide a Prompt or Image to Generate Conditional 3D PLY Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Shap-E Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        init_image,\n",
        "        Row([use_original, original_row]),\n",
        "        guidance,\n",
        "        karras_steps,\n",
        "        size,\n",
        "        Row([batch_folder_name, batch_size]),\n",
        "        ElevatedButton(content=Text(\"ü™Ä  Run Shap-E\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=run_shap),\n",
        "        page.shap_e_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "zoe_depth_prefs = {\n",
        "    'init_image': '',\n",
        "    'keep_edges': False,\n",
        "    'colorize': False,\n",
        "    'pano_360': False,\n",
        "    'zoe_model': 'ZoeD_N', #ZoeD_K, ZoeD_NK\n",
        "    'loaded_model': '',\n",
        "    'max_size': 1024,\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "def buildZoeDepth(page):\n",
        "    global zoe_depth_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            zoe_depth_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            zoe_depth_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            zoe_depth_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def zoe_depth_help(e):\n",
        "      def close_zoe_depth_dlg(e):\n",
        "        nonlocal zoe_depth_help_dlg\n",
        "        zoe_depth_help_dlg.open = False\n",
        "        page.update()\n",
        "      zoe_depth_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Zoe Depth\"), content=Column([\n",
        "          Text(\"ZoeDepth is a deep learning model for metric depth estimation from a single image.\"),\n",
        "          Text(\"Give it any of your favorite images and create a 3D glb file from the depth map to import into your 3D Modeling program with texture.... Simple as that, no prompt needed.\"),\n",
        "          Markdown(\"[Paper](https://arxiv.org/abs/2302.12288) | [GitHub](https://github.com/isl-org/ZoeDepth) | [HuggingFace Space](https://huggingface.co/spaces/shariqfarooq/ZoeDepth)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßä  The depths we go... \", on_click=close_zoe_depth_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = zoe_depth_help_dlg\n",
        "      zoe_depth_help_dlg.open = True\n",
        "      page.update()\n",
        "    init_image = FileInput(label=\"Initial Image\", pref=zoe_depth_prefs, key='init_image', page=page)\n",
        "    zoe_model = Dropdown(label=\"ZoeDepth Model\", width=150, options=[dropdown.Option(\"ZoeD_N\"), dropdown.Option(\"ZoeD_K\"), dropdown.Option(\"ZoeD_NK\")], value=zoe_depth_prefs['zoe_model'], on_change=lambda e:changed(e,'zoe_model'))\n",
        "    keep_edges = Switcher(label=\"Keep Occlusion Edges\", value=zoe_depth_prefs['keep_edges'], on_change=lambda e:changed(e,'keep_edges'))\n",
        "    pano_360 = Switcher(label=\"Input 360 Panoramic\", value=zoe_depth_prefs['pano_360'], on_change=lambda e:changed(e,'pano_360'))\n",
        "    colorize = Switcher(label=\"Show Colorized Depth\", value=zoe_depth_prefs['colorize'], on_change=lambda e:changed(e,'colorize'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, expand=True, multiple=16, suffix=\"px\", pref=zoe_depth_prefs, key='max_size')\n",
        "    page.zoe_depth_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.zoe_depth_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü•ê  ZoeDepth 3D Depth Model from Init Image\", \"Zero-shot Transfer by Combining Relative and Metric Depth...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Zoe Depth Settings\", on_click=zoe_depth_help)]),\n",
        "        init_image,\n",
        "        Row([keep_edges, pano_360, colorize]),\n",
        "        Row([zoe_model, max_row]),\n",
        "        ElevatedButton(content=Text(\"‚úä  Get Zoe Depth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_zoe_depth(page)),\n",
        "        page.zoe_depth_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "marigold_depth_prefs = {\n",
        "    'init_image': '',\n",
        "    'denoising_steps': 10,\n",
        "    'ensemble_size': 10,\n",
        "    'match_input_res': True,\n",
        "    'color_map': 'Spectral', #\n",
        "    'processing_res': 768,\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "def buildMarigoldDepth(page):\n",
        "    global marigold_depth_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            marigold_depth_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            marigold_depth_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            marigold_depth_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def marigold_depth_help(e):\n",
        "      def close_marigold_depth_dlg(e):\n",
        "        nonlocal marigold_depth_help_dlg\n",
        "        marigold_depth_help_dlg.open = False\n",
        "        page.update()\n",
        "      marigold_depth_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Marigold Depth\"), content=Column([\n",
        "          Text(\"Marigold is a universal monocular depth estimator that delivers accurate and sharp predictions in the wild. Based on Stable Diffusion, it is trained exclusively with synthetic depth data and excels in zero-shot adaptation to real-world imagery. This pipeline is an official implementation of the inference process. This depth estimation pipeline processes a single input image through multiple diffusion denoising stages to estimate depth maps. These maps are subsequently merged to produce the final output.\"),\n",
        "          Markdown(\"[Project Page](https://marigoldmonodepth.github.io) | [Paper](https://arxiv.org/abs/2312.02145) | [GitHub](https://github.com/prs-eth/marigold) | [HuggingFace Space](https://huggingface.co/spaces/toshas/marigold)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßä  The depths we go... \", on_click=close_marigold_depth_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = marigold_depth_help_dlg\n",
        "      marigold_depth_help_dlg.open = True\n",
        "      page.update()\n",
        "    init_image = FileInput(label=\"Initial Image\", pref=marigold_depth_prefs, key='init_image', page=page)\n",
        "    color_map = Dropdown(label=\"Colormap\", width=150, options=[dropdown.Option(c) for c in ['Spectral', 'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'coolwarm', 'bwr', 'seismic']], value=marigold_depth_prefs['color_map'], on_change=lambda e:changed(e,'color_map'))\n",
        "    match_input_res = Switcher(label=\"Match Input Resolution\", value=marigold_depth_prefs['match_input_res'], on_change=lambda e:changed(e,'match_input_res'), tooltip=\"Resize depth prediction to match input resolution.\")\n",
        "    denoising_steps = SliderRow(label=\"Number of Denoising Steps\", min=1, max=50, divisions=49, pref=marigold_depth_prefs, key='denoising_steps', tooltip=\"Number of denoising steps of each inference pass.\")\n",
        "    ensemble_size = SliderRow(label=\"Ensemble Size\", min=1, max=50, divisions=49, pref=marigold_depth_prefs, key='ensemble_size', tooltip=\"Number of inference passes in the ensemble.\")\n",
        "    #pano_360 = Switcher(label=\"Input 360 Panoramic\", value=marigold_depth_prefs['pano_360'], on_change=lambda e:changed(e,'pano_360'))\n",
        "    #colorize = Switcher(label=\"Show Colorized Depth\", value=marigold_depth_prefs['colorize'], on_change=lambda e:changed(e,'colorize'))\n",
        "    processing_res = SliderRow(label=\"Processing Resolution\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=marigold_depth_prefs, key='processing_res')\n",
        "    page.marigold_depth_output = Column([])\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü™∑  Marigold Depth Estimation\", \"Monocular depth estimator that delivers accurate & sharp predictions in the wild... Based on SD.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Marigold Depth Settings\", on_click=marigold_depth_help)]),\n",
        "        init_image,\n",
        "        processing_res,\n",
        "        denoising_steps,\n",
        "        ensemble_size,\n",
        "        #Row([match_input_res, pano_360, colorize]),\n",
        "        Row([color_map, match_input_res]),\n",
        "        ElevatedButton(content=Text(\"üå∫  Get Marigold Depth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_marigold_depth(page)),\n",
        "        page.marigold_depth_output,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "instant_ngp_prefs = {\n",
        "    'train_steps': 2000, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'sharpen': 0.0,\n",
        "    'exposure': 0.0,\n",
        "    'vr_mode': False,\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 768,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildInstantNGP(page):\n",
        "    global prefs, instant_ngp_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              instant_ngp_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              instant_ngp_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              instant_ngp_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_instant_ngp_output(o):\n",
        "        page.instant_ngp_output.controls.append(o)\n",
        "        page.instant_ngp_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.instant_ngp_output.controls = []\n",
        "        page.instant_ngp_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def instant_ngp_help(e):\n",
        "        def close_instant_ngp_dlg(e):\n",
        "          nonlocal instant_ngp_help_dlg\n",
        "          instant_ngp_help_dlg.open = False\n",
        "          page.update()\n",
        "        instant_ngp_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Instant-NGP\"), content=Column([\n",
        "            Text(\"Ever wanted to train a NeRF model of a fox in under 5 seconds? Or fly around a scene captured from photos of a factory robot? Of course you have! Here you will find an implementation of four neural graphics primitives, being neural radiance fields (NeRF), signed distance functions (SDFs), neural images, and neural volumes. In each case, we train and render a MLP with multiresolution hash input encoding using the tiny-cuda-nn framework.\"),\n",
        "            Text(\"We demonstrate near-instant training of neural graphics primitives on a single GPU for multiple tasks. In gigapixel image we represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. NeRF [Mildenhall et al. 2020] uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. Lastly, neural volume learns a denoised radiance and density field directly from a volumetric path tracer. In all tasks, our encoding and its efficient implementation provide clear benefits: instant training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance.\"),\n",
        "            Text(\"Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations. A small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920x1080.\"),\n",
        "            Markdown(\"[Project page](https://nvlabs.github.io/instant-ngp) / [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) / [Video](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4) / [Presentation](https://tom94.net/data/publications/mueller22instant/mueller22instant-gtc.mp4) / [Real-Time Live](https://tom94.net/data/publications/mueller22instant/mueller22instant-rtl.mp4) / [BibTeX](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üå†  Like Magic... \", on_click=close_instant_ngp_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = instant_ngp_help_dlg\n",
        "        instant_ngp_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.instant_ngp_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.instant_ngp_file_list.controls[i]\n",
        "              page.instant_ngp_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.instant_ngp_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.instant_ngp_file_list.controls.clear()\n",
        "        page.instant_ngp_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        alert_msg(e.page, \"Image Details\", content=Image(src=img), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.instant_ngp_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        if update: page.instant_ngp_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_ngp')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web:\n",
        "            os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\", 'obj', 'stl', 'nvdb'], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_ngp')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):\n",
        "              add_file(existing, update=False)\n",
        "    #instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=instant_ngp_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=instant_ngp_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'))\n",
        "    train_steps = Tooltip(message=\"Total number of training steps to perform.  More the better, even around 35000..\", content=TextField(label=\"Max Training Steps\", value=instant_ngp_prefs['train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_steps', ptype='int'), width = 160))\n",
        "    #save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=instant_ngp_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    #save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switcher(label=\"Save Model to HuggingFace    \", value=instant_ngp_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    #where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=instant_ngp_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=instant_ngp_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    #readme_description = TextField(label=\"Extra README Description\", value=instant_ngp_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=64, suffix=\"px\", pref=instant_ngp_prefs, key='resolution')\n",
        "\n",
        "    sharpen = Row([Text(\" Shapen Images:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, value=instant_ngp_prefs['sharpen'], on_change=lambda e: changed(e, 'sharpen'))], col={'lg':6})\n",
        "    exposure = Row([Text(\" Image Exposure:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, round=1, expand=True, value=instant_ngp_prefs['exposure'], on_change=lambda e: changed(e, 'exposure'))], col={'lg':6})\n",
        "    vr_mode = Checkbox(label=\"Output VR Mode\", tooltip=\"Render to a VR headset\", value=instant_ngp_prefs['vr_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'vr_mode'))\n",
        "    image_path = TextField(label=\"Image Files or Folder Path or URL to Train\", value=instant_ngp_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.instant_ngp_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #where_to_save_model.visible = instant_ngp_prefs['save_model']\n",
        "    #readme_description.visible = instant_ngp_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusinstant_ngp_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=instant_ngp_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.instant_ngp_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.instant_ngp_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéë  Instant Neural Graphics Primitives by NVidia\", \"Convert series of images into 3D Models with Multiresolution Hash Encoding...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instant NGP Settings\", on_click=instant_ngp_help)]),\n",
        "        Row([name_of_your_model]),\n",
        "        Row([train_steps, vr_mode]),\n",
        "        ResponsiveRow([sharpen, exposure]),\n",
        "        #Row([save_model, where_to_save_model]),\n",
        "        #readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Text(\"The scene to load. Can be full path to the training data, multiple image angles, NeRF dataset, a *.obj/*.stl mesh for training a SDF, images, or a *.nvdb volume.\"),\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.instant_ngp_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üéá  Run Instant-NGP\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instant_ngp(page))]),\n",
        "        page.instant_ngp_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "luma_vid_to_3d_prefs = {\n",
        "    'init_video': '',\n",
        "    'title': '',\n",
        "    'luma_api_key': '',\n",
        "    'camera_type': 'Normal Perspective',\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "def buildLuma(page):\n",
        "    global luma_vid_to_3d_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            luma_vid_to_3d_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            luma_vid_to_3d_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            luma_vid_to_3d_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def changed_pref(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "        status['changed_parameters'] = True\n",
        "    def luma_vid_to_3d_help(e):\n",
        "      def close_luma_vid_to_3d_dlg(e):\n",
        "        nonlocal luma_vid_to_3d_help_dlg\n",
        "        luma_vid_to_3d_help_dlg.open = False\n",
        "        page.update()\n",
        "      luma_vid_to_3d_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with LumaLabs API\"), content=Column([\n",
        "          Text(\"Luma's NeRF and meshing models are available through their API, giving you access to the world's best 3D modeling and reconstruction capabilities. At a dollar a scene or object. Today it costs anywhere from $60-$1500 and 2-10wk, and rounds of back and forth to have 3D models created. At a dollar a model, and around 30 min of compute now we can imagine 3D models for entire inventories for e-commerce, and every previz scene for VFX. The API expects video walkthroughs of objects or scenes, looking outside in, from 2-3 levels. The output is an interactive 3D scene that can be embedded directly, coarse textured models to build interactions on in traditional 3D pipelines, and pre-rendered 360 images and videos.\"),\n",
        "          Markdown(\"[Luma-API](https://lumalabs.ai/luma-api) | [Capture Practices](https://docs.lumalabs.ai/MCrGAEukR4orR9) | [Client Docs](https://documenter.getpostman.com/view/24305418/2s93CRMCas)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üåö  Who needs 3D scanner? \", on_click=close_luma_vid_to_3d_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = luma_vid_to_3d_help_dlg\n",
        "      luma_vid_to_3d_help_dlg.open = True\n",
        "      page.update()\n",
        "    init_video = FileInput(label=\"Video Walk-thru or Walk-around\", pref=luma_vid_to_3d_prefs, key='init_video', ftype=\"video\", page=page)\n",
        "    camera_type = Dropdown(label=\"Camera Type\", width=200, options=[dropdown.Option(\"Normal Perspective\"), dropdown.Option(\"Fisheye Lens\"), dropdown.Option(\"Equirectangular 360\")], value=luma_vid_to_3d_prefs['camera_type'], on_change=lambda e:changed(e,'camera_type'))\n",
        "    title = TextField(label=\"Project Title\", value=luma_vid_to_3d_prefs['title'], on_change=lambda e:changed(e,'title'))\n",
        "    luma_api_key = TextField(label=\"LumaLabs.ai API Key\", value=prefs['luma_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e,'luma_api_key'))\n",
        "    api_instructions = Markdown(\"Sign-up and get your LumaLabs.ai API Key here [https://lumalabs.ai/dashboard/api](https://lumalabs.ai/dashboard/api), you should get 10 models free.\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=luma_vid_to_3d_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    page.luma_vid_to_3d_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.luma_vid_to_3d_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåî  LumaLabs Video-to-3D API\", \"Costs $1 per Model, takes ~30min, but well worth it for these NeRF and meshing models in their cloud...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LumaLabs API Settings\", on_click=luma_vid_to_3d_help)]),\n",
        "        init_video,\n",
        "        title,\n",
        "        Divider(thickness=2, height=4),\n",
        "        api_instructions,\n",
        "        luma_api_key,\n",
        "        Row([camera_type, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"üåú  Get Luma Vid-to-3D\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_luma_vid_to_3d(page)),\n",
        "        page.luma_vid_to_3d_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "repaint_prefs = {\n",
        "    'original_image': '',\n",
        "    'mask_image': '',\n",
        "    'num_inference_steps': 500,\n",
        "    'eta': 0.0,\n",
        "    'jump_length': 10,\n",
        "    'jump_n_sample': 10,\n",
        "    'seed': 0,\n",
        "    'file_name': '',\n",
        "    'max_size': 1024,\n",
        "    'invert_mask': False,\n",
        "}\n",
        "def buildRepainter(page):\n",
        "    global repaint_prefs, prefs, pipe_repaint\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            repaint_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            repaint_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            repaint_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_repaint_output(o):\n",
        "      page.repaint_output.controls.append(o)\n",
        "      page.repaint_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.repaint_output.controls = []\n",
        "      page.repaint_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def repaint_help(e):\n",
        "      def close_repaint_dlg(e):\n",
        "        nonlocal repaint_help_dlg\n",
        "        repaint_help_dlg.open = False\n",
        "        page.update()\n",
        "      repaint_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Repainter\"), content=Column([\n",
        "          Text(\"It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  Okay then... \", on_click=close_repaint_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = repaint_help_dlg\n",
        "      repaint_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          repaint_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          repaint_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"original\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          repaint_prefs['original_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          repaint_prefs['mask_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"original\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {repaint_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=repaint_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=repaint_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=repaint_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    jump_length = TextField(label=\"Jump Length\", width=130, tooltip=\"The number of steps taken forward in time before going backward in time for a single jump\", value=repaint_prefs['jump_length'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_length', ptype='int'))\n",
        "    jump_n_sample = TextField(label=\"Jump # of Sample\", width=130, tooltip=\"The number of times we will make forward time jump for a given chosen time sample.\", value=repaint_prefs['jump_n_sample'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_n_sample', ptype='int'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(repaint_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #num_inference_steps = TextField(label=\"Inference Steps\", value=str(repaint_prefs['num_inference_steps']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=10, max=3000, divisions=2990, pref=repaint_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(repaint_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", round=2, value=float(repaint_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {repaint_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=repaint_prefs, key='max_size')\n",
        "    page.repaint_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.repaint_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üíÖ  Repaint masked areas of an image\", \"Fills in areas of picture with what it thinks it should be, without a prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Repainter Settings\", on_click=repaint_help)]),\n",
        "        Row([original_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([jump_length, jump_n_sample, seed]),\n",
        "        ElevatedButton(content=Text(\"üñåÔ∏è  Run Repainter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_repainter(page)),\n",
        "        page.repaint_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "image_variation_prefs = {\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'eta': 0.4,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'file_name': '',\n",
        "    'max_size': 1024,\n",
        "    'width': 960,\n",
        "    'height': 512,\n",
        "}\n",
        "def buildImageVariation(page):\n",
        "    global image_variation_prefs, prefs, pipe_image_variation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            image_variation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            image_variation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            image_variation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_image_variation_output(o):\n",
        "      page.image_variation_output.controls.append(o)\n",
        "      page.image_variation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_image_variation_output = add_to_image_variation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image_variation_output.controls = []\n",
        "      page.image_variation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def image_variation_help(e):\n",
        "      def close_image_variation_dlg(e):\n",
        "        nonlocal image_variation_help_dlg\n",
        "        image_variation_help_dlg.open = False\n",
        "        page.update()\n",
        "      image_variation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Image Variations\"), content=Column([\n",
        "          Text(\"Give it any of your favorite images and create variations of it.... Simple as that, no prompt needed.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§ó  Sounds Fun... \", on_click=close_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = image_variation_help_dlg\n",
        "      image_variation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        image_variation_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=image_variation_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=image_variation_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", round=2, value=float(image_variation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=image_variation_prefs, key='max_size')\n",
        "    page.image_variation_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.image_variation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü™©  Image Variations of any Init Image\", \"Creates a new version of your picture, without a prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=image_variation_help)]),\n",
        "        init_image,\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed]),\n",
        "        ElevatedButton(content=Text(\"üñçÔ∏è  Get Image Variation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image_variation(page)),\n",
        "        page.image_variation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "background_remover_prefs = {\n",
        "    'init_image': '',\n",
        "    'threshold': 100,\n",
        "    'file_name': '',\n",
        "    'max_size': 1024,\n",
        "    'transparent_png': False,\n",
        "    'save_mask': False,\n",
        "    'output_name': '',\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "def buildBackgroundRemover(page):\n",
        "    global background_remover_prefs, prefs, pipe_background_remover\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            background_remover_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            background_remover_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            background_remover_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_background_remover_output(o):\n",
        "      page.background_remover_output.controls.append(o)\n",
        "      page.background_remover_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_background_remover_output = add_to_background_remover_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.background_remover_output.controls = []\n",
        "      page.background_remover_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def background_remover_help(e):\n",
        "      def close_background_remover_dlg(e):\n",
        "        nonlocal background_remover_help_dlg\n",
        "        background_remover_help_dlg.open = False\n",
        "        page.update()\n",
        "      background_remover_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Background Remover\"), content=Column([\n",
        "          Text(\"Give it any of your favorite images and it finds the main subject within the threshold value and gives you a cleaned up version back.... Simple as that, very useful to reuse as init image in another pipeline without needing to edit in Photoshop first..\"),\n",
        "          Markdown(\"[MODNet GitHub](https://github.com/Mazhar004/MODNet-BGRemover) | [HuggingFace Space](https://huggingface.co/spaces/nateraw/background-remover)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò∏  Quite Convenient... \", on_click=close_background_remover_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = background_remover_help_dlg\n",
        "      background_remover_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          background_remover_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          background_remover_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        background_remover_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=background_remover_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    threshold = SliderRow(label=\"Mask Cutoff Threshold\", min=0, max=250, divisions=250, round=0, pref=background_remover_prefs, key='threshold')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=background_remover_prefs, key='max_size')\n",
        "    save_mask = Switcher(label=\"Save B&W Mask\", value=background_remover_prefs['save_mask'], tooltip=\"Gives you a Mask File you can reuse for Inpainting.\", on_change=lambda e:changed(e,'save_mask'))\n",
        "    output_name = TextField(label=\"Output File Name\", value=background_remover_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=background_remover_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    page.background_remover_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.background_remover_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üñº  MODNet Background Remover\", \"A deep learning approach to clear the background of most images to isolate subject...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Background Remover Settings\", on_click=background_remover_help)]),\n",
        "        init_image,\n",
        "        threshold,\n",
        "        max_row,\n",
        "        save_mask,\n",
        "        Row([output_name, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"üèò  Get Background Remover\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_background_remover(page)),\n",
        "        page.background_remover_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "blip_diffusion_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"source_subject_category\": '',\n",
        "    \"target_subject_category\": '',\n",
        "    \"steps\":50,\n",
        "    #\"ddim_eta\":0.05,\n",
        "    \"width\": 768,\n",
        "    \"height\":768,\n",
        "    \"guidance_scale\":7.5,\n",
        "    \"init_image\": '',\n",
        "    \"control_image\": '',\n",
        "    \"strength\": 1.0,\n",
        "    \"prompt_reps\": 20,\n",
        "    \"controlnet_type\": 'None',\n",
        "    \"use_controlnet_canny\": False,\n",
        "    \"seed\": 0,\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"blip-\",\n",
        "    \"num_images\": 1,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildBLIPDiffusion(page):\n",
        "    global prefs, blip_diffusion_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            blip_diffusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            blip_diffusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            blip_diffusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def blip_diffusion_help(e):\n",
        "      def close_blip_diffusion_dlg(e):\n",
        "        nonlocal blip_diffusion_help_dlg\n",
        "        blip_diffusion_help_dlg.open = False\n",
        "        page.update()\n",
        "      blip_diffusion_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with BLIP Diffusion Pipeline\"), content=Column([\n",
        "          Markdown(\"Blip Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üíñ  Lovely... \", on_click=close_blip_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = blip_diffusion_help_dlg\n",
        "      blip_diffusion_help_dlg.open = True\n",
        "      page.update()\n",
        "\n",
        "    def toggle_controlnet_image(e):\n",
        "        blip_diffusion_prefs['use_controlnet_canny'] = e.control.value\n",
        "        control_image_container.height=None if blip_diffusion_prefs['use_controlnet_canny'] else 0\n",
        "        control_image_container.update()\n",
        "    def change_controlnet_type(e):\n",
        "        blip_diffusion_prefs['controlnet_type'] = e.control.value\n",
        "        control_image_container.height=None if blip_diffusion_prefs['controlnet_type'] != \"None\" else 0\n",
        "        control_image_container.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        blip_diffusion_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=blip_diffusion_prefs['prompt'], multiline=True, filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=blip_diffusion_prefs['negative_prompt'], multiline=True, filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    source_subject_category = TextField(label=\"Source Subject Category List\", value=blip_diffusion_prefs['source_subject_category'], multiline=True, col={'md':6}, on_change=lambda e:changed(e,'source_subject_category'))\n",
        "    target_subject_category = TextField(label=\"Target Subject Category List\", value=blip_diffusion_prefs['target_subject_category'], multiline=True, col={'md':6}, on_change=lambda e:changed(e,'target_subject_category'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=blip_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=blip_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    steps = TextField(label=\"Number of Steps\", value=blip_diffusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=blip_diffusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=blip_diffusion_prefs, key='steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=blip_diffusion_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=blip_diffusion_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=blip_diffusion_prefs, key='height')\n",
        "    init_image = FileInput(label=\"Reference Image\", pref=blip_diffusion_prefs, key='init_image', page=page)\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #use_controlnet_image = Switcher(label=\"Use ControlNet Canny\", value=blip_diffusion_prefs['use_controlnet_canny'], on_change=toggle_controlnet_image)\n",
        "    controlnet_type = Dropdown(label=\"ControlNet Image Layer\", width=177, options=[dropdown.Option(\"None\"), dropdown.Option(\"Canny Edge\"), dropdown.Option(\"HED\")], value=blip_diffusion_prefs['controlnet_type'], on_change=change_controlnet_type)\n",
        "    control_image = FileInput(label=\"ControlNet Image\", pref=blip_diffusion_prefs, key='control_image', page=page)\n",
        "    control_image_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, expand=True, alignment = alignment.top_left, height = None if blip_diffusion_prefs['use_controlnet_canny'] else 0, padding=padding.only(top=4), content=Column([control_image]))\n",
        "    strength_slider = SliderRow(label=\"Prompt Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=blip_diffusion_prefs, key='strength', col={'md':6}, tooltip=\"Specifies the number of times the prompt is repeated along with prompt_reps.\")\n",
        "    prompt_reps = SliderRow(label=\"Prompt Repetitions\", min=0, max=50, divisions=50, pref=blip_diffusion_prefs, key='prompt_reps', col={'md':6}, tooltip=\"The number of times the prompt is repeated along with prompt_strength to amplify the prompt.\")\n",
        "    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(blip_diffusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=blip_diffusion_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=blip_diffusion_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=blip_diffusion_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=blip_diffusion_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_blip_diffusion = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_blip_diffusion.height = None if status['installed_ESRGAN'] else 0\n",
        "    ESRGAN_settings.height = None if blip_diffusion_prefs['apply_ESRGAN_upscale'] else 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"ü¶æ   Run BLIP-Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.blip_diffusion_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üì°  BLIP-Diffusion by Salesforce\", \"Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with BLIP Diffusion Settings\", on_click=blip_diffusion_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            ResponsiveRow([source_subject_category, target_subject_category]),\n",
        "            #img_block,\n",
        "            image_pickers,\n",
        "            ResponsiveRow([strength_slider, prompt_reps]),\n",
        "            Row([controlnet_type, control_image_container]),\n",
        "            #ResponsiveRow([prior_steps, prior_guidance_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            #Row([batch_folder_name, file_prefix]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_blip_diffusion,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.blip_diffusion_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "anytext_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": 'low-res, bad anatomy, cropped, worst quality, low quality, watermark, unreadable text, messy words, distorted text, disorganized writing',\n",
        "    \"a_prompt\": 'best quality, extremely detailed,4k, HD, supper legible text, clear text edges, clear strokes, neat writing, no watermarks',\n",
        "    \"file_prefix\": \"anytext-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 768,\n",
        "    \"height\":768,\n",
        "    \"guidance_scale\": 9.0,\n",
        "    'num_inference_steps': 20,\n",
        "    \"eta\": 0.0,\n",
        "    \"seed\": 0,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'init_image_strength': 0.8,\n",
        "    'font_ttf': '',\n",
        "    'sort_priority': \"‚Üï\",# \"‚Üî\"\n",
        "    'revise_pos': True,\n",
        "    \"batch_folder_name\": '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildAnyText(page):\n",
        "    global prefs, anytext_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            anytext_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            anytext_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            anytext_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def anytext_help(e):\n",
        "      def close_anytext_dlg(e):\n",
        "        nonlocal anytext_help_dlg\n",
        "        anytext_help_dlg.open = False\n",
        "        page.update()\n",
        "      anytext_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with AnyText Pipeline\"), content=Column([\n",
        "          Text(\"AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. The drawing of text positions is crucial to the quality of the resulting image, please do not draw too casually or too small. The number of positions should match the number of text lines, and the size of each position should be matched as closely as possible to the length or width of the corresponding text line. When generating multiple lines, each position is matched with the text line according to a certain rule.\"),\n",
        "          Text('Example Prompt: Photo of caramel macchiato coffee on the table, top-down perspective, with \"Any\" \"Text\" written on it'),\n",
        "          Markdown(\"[GitHub](https://github.com/tyxsspa/AnyText) | [Paper](https://arxiv.org/abs/2311.03054) | [HuggingFace Space](https://huggingface.co/spaces/modelscope/AnyText) | [ModelScope](https://modelscope.cn/models/damo/cv_anytext_text_generation_editing/summary)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"Contributors include Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie, and ModelScope Developers.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üî†  Finally, words...\", on_click=close_anytext_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = anytext_help_dlg\n",
        "      anytext_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        anytext_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label='Prompt with Text in \"Double\" \"Quotes\"', value=anytext_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    a_prompt = TextField(label=\"Additional Prompt Text\", value=anytext_prefs['a_prompt'], multiline=True, col={'md':9}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=anytext_prefs['negative_prompt'], multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = FileInput(label=\"Initial Image (optional)\", pref=anytext_prefs, key='init_image', page=page, col={'md':6})\n",
        "    mask_image = FileInput(label=\"Text Area Mask (optional)\", pref=anytext_prefs, key='mask_image', page=page, col={'md':6})\n",
        "    init_image_strength = SliderRow(label=\"Init-Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=anytext_prefs, key='init_image_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    font_ttf = FileInput(label=\"Font .ttf File (optional)\", pref=anytext_prefs, key='font_ttf', page=page, ftype=\"font\", col={'md':6})\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=anytext_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=anytext_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=8, step=1, value=anytext_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=1, max=50, divisions=49, pref=anytext_prefs, key='num_inference_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=anytext_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=12, multiple=64, suffix=\"px\", pref=anytext_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=12, multiple=64, suffix=\"px\", pref=anytext_prefs, key='height')\n",
        "    eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=10, round=1, pref=anytext_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    #anytext_model = Dropdown(label=\"AnyText Model\", width=220, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"anytext-256\"), dropdown.Option(\"anytext-512\")], value=anytext_prefs['anytext_model'], on_change=changed_model)\n",
        "    #anytext_custom_model = TextField(label=\"Custom AnyText Model (URL or Path)\", value=anytext_prefs['custom_model'], expand=True, visible=anytext_prefs['anytext_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    def change_sort(e):\n",
        "        anytext_prefs['sort_priority'] = e.data.split('\"')[1]\n",
        "    sort_priority = ft.SegmentedButton(on_change=change_sort, selected={anytext_prefs['sort_priority']}, allow_multiple_selection=False,\n",
        "        segments=[\n",
        "            ft.Segment(value=\"‚Üï\", label=ft.Text(\"Vertical ‚Üï\"), icon=ft.Icon(ft.icons.SWAP_VERTICAL_CIRCLE)),\n",
        "            ft.Segment(value=\"‚Üî\", label=ft.Text(\"Horizontal ‚Üî\"), icon=ft.Icon(ft.icons.SWAP_HORIZONTAL_CIRCLE)),\n",
        "        ], tooltip=\"When generating multiple lines, each position is matched with the text line according to a certain rule. This determines whether to prioritize sorting from top to bottom or from left to right.\"\n",
        "    )\n",
        "    revise_pos = Checkbox(label=\"Revise Position\", value=anytext_prefs['revise_pos'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'revise_pos'), tooltip=\"Uses the bounding box of the rendered text as the revised position. However, it is occasionally found that the creativity of the generated text is slightly lower using this method.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(anytext_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=anytext_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=anytext_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=anytext_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=anytext_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_anytext = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_anytext.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not anytext_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"‚úçÔ∏è   Run AnyText\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üî§  AnyText\", \"Multilingual Visual Text Generation and Text Editing...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with AnyText Settings\", on_click=anytext_help)]),\n",
        "            prompt,\n",
        "            ResponsiveRow([a_prompt, negative_prompt]),\n",
        "            ResponsiveRow([init_image, mask_image]),\n",
        "            init_image_strength,\n",
        "            font_ttf,\n",
        "            Row([Text(\"Sort Priority: \"), sort_priority, revise_pos]),\n",
        "            steps,\n",
        "            guidance, eta,\n",
        "            width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            #Row([anytext_model, anytext_custom_model]),\n",
        "            #Row([cpu_offload, cpu_only]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_anytext,\n",
        "            parameters_row,\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "ip_adapter_models = [\n",
        "    {'name': 'SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter_sd15.bin'},\n",
        "    {'name': 'Plus SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-plus_sd15.bin'},\n",
        "    {'name': 'Plus Face SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-plus-face_sd15.bin'},\n",
        "    {'name': 'Full Face SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-full-face_sd15.bin'},\n",
        "    {'name': 'Light SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter_sd15_light.bin'},\n",
        "]\n",
        "ip_adapter_SDXL_models = [\n",
        "    {'name': 'SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sdxl.bin'},\n",
        "    {'name': 'Plus SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter-plus_sdxl_vit-h.bin'},\n",
        "    {'name': 'Plus Face SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter-plus-face_sdxl_vit-h.bin'},\n",
        "    {'name': 'SDXL ViT-H', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sdxl_vit-h.bin'},\n",
        "    #{'name': 'Light SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sd15_light.bin'},\n",
        "]\n",
        "ip_adapter_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"sd-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\":4.5,\n",
        "    'num_inference_steps': 8,\n",
        "    \"seed\": 0,\n",
        "    'ip_adapter_image':'',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'init_image': '',\n",
        "    'init_image_strength': 0.8,\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'use_SDXL': False,\n",
        "    \"cpu_offload\": False,\n",
        "    \"cpu_only\": False,\n",
        "    \"ip_adapter_model\": \"SD v1.5\",\n",
        "    \"ip_adapter_SDXL_model\": \"SDXL\",\n",
        "    \"custom_model\": \"\",\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildIP_Adapter(page):\n",
        "    global prefs, ip_adapter_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            ip_adapter_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            ip_adapter_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            ip_adapter_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def ip_adapter_help(e):\n",
        "      def close_ip_adapter_dlg(e):\n",
        "        nonlocal ip_adapter_help_dlg\n",
        "        ip_adapter_help_dlg.open = False\n",
        "        page.update()\n",
        "      ip_adapter_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with IP-Adapter Pipeline\"), content=Column([\n",
        "          Text(\"IP-Adapter is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model. This adapter works by decoupling the cross-attention layers of the image and text features. All the other model components are frozen and only the embedded image features in the UNet are trained. As a result, IP-Adapter files are typically only ~100MBs.\"),\n",
        "          Text(\"IP-Adapter works with most of our pipelines, including Stable Diffusion, Stable Diffusion XL (SDXL), ControlNet, T2I-Adapter, AnimateDiff. And you can use any custom models finetuned from the same base models. It also works with LCM-Lora out of box.\"),\n",
        "          Markdown(\"[Project](https://ip-adapter.github.io/) | [Paper](https://arxiv.org/abs/2308.06721) | [GitHub Code](https://github.com/tencent-ailab/IP-Adapter) | [Checkpoint Models](https://huggingface.co/h94/IP-Adapter)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"IP-Adapter was contributed by [okotaku](https://github.com/okotaku), Tencent, okotaku, sayakpaul, yiyixuxu, Patrick von Platen and Steven Liu ..\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üèÉ  Adapt This...\", on_click=close_ip_adapter_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = ip_adapter_help_dlg\n",
        "      ip_adapter_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        ip_adapter_prefs['ip_adapter_model'] = e.control.value\n",
        "        ip_adapter_custom_model.visible = e.control.value == \"Custom\"\n",
        "        ip_adapter_custom_model.update()\n",
        "    def toggle_SDXL(e):\n",
        "        ip_adapter_prefs['use_SDXL'] = e.control.value\n",
        "        ip_adapter_model.visible = not ip_adapter_prefs['use_SDXL']\n",
        "        ip_adapter_SDXL_model.visible = ip_adapter_prefs['use_SDXL']\n",
        "        ip_adapter_model.update()\n",
        "        ip_adapter_SDXL_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        ip_adapter_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text (optional)\", value=ip_adapter_prefs['prompt'], filled=True, hint_text=\"Leave blank for Image Variation\", multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=ip_adapter_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=ip_adapter_prefs, key='ip_adapter_image', expand=True, page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=ip_adapter_prefs, key='ip_adapter_strength', expand=True, col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    init_image = FileInput(label=\"Init Image (optional)\", pref=ip_adapter_prefs, key='init_image', page=page, col={'md':6})\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=ip_adapter_prefs, key='mask_image', page=page, col={'md':6})\n",
        "    init_image_strength = SliderRow(label=\"Init-Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=ip_adapter_prefs, key='init_image_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=ip_adapter_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=ip_adapter_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=ip_adapter_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=40, divisions=40, pref=ip_adapter_prefs, key='num_inference_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=ip_adapter_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=ip_adapter_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=ip_adapter_prefs, key='height')\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL\", value=ip_adapter_prefs['use_SDXL'], on_change=toggle_SDXL, tooltip=\"SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.\")\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=ip_adapter_prefs['ip_adapter_model'], visible=not ip_adapter_prefs['use_SDXL'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=ip_adapter_prefs['ip_adapter_SDXL_model'], visible=ip_adapter_prefs['use_SDXL'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))\n",
        "    for m in ip_adapter_SDXL_models:\n",
        "        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_custom_model = TextField(label=\"Custom IP_Adapter Model (URL or Path)\", value=ip_adapter_prefs['custom_model'], expand=True, visible=ip_adapter_prefs['ip_adapter_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=ip_adapter_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    cpu_only = Switcher(label=\"CPU Only (not yet)\", value=ip_adapter_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip=\"If you don't have a good GPU, can run entirely on CPU\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(ip_adapter_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=ip_adapter_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=ip_adapter_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=ip_adapter_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=ip_adapter_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_ip_adapter = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_ip_adapter.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not ip_adapter_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"ü™û   Run IP-Adapter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.ip_adapter_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"ü¶ä  IP-Adapter\", \"Image Prompting capabilities to Transfer Subject with or without Prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with IP_Adapter Settings\", on_click=ip_adapter_help)]),\n",
        "            Row([ip_adapter_model, ip_adapter_SDXL_model, ip_adapter_image]),\n",
        "            Row([use_SDXL, ip_adapter_strength]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            ResponsiveRow([init_image, mask_image]),\n",
        "            init_image_strength,\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_ip_adapter,\n",
        "            parameters_row,\n",
        "            page.ip_adapter_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "reference_prefs = {\n",
        "    'ref_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'seed': 0,\n",
        "    'eta': 0.0,\n",
        "    'attention_auto_machine_weight': 1.0,\n",
        "    'gn_auto_machine_weight': 1.0,\n",
        "    'style_fidelity': 0.5,\n",
        "    'reference_attn': True,\n",
        "    'reference_adain': True,\n",
        "    'batch_size': 1,\n",
        "    'num_images': 1,\n",
        "    'max_size': 1024,\n",
        "    'width': 960,\n",
        "    'height': 768,\n",
        "    'last_model': '',\n",
        "    'use_SDXL': False,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildReference(page):\n",
        "    global reference_prefs, prefs, pipe_reference\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            reference_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            reference_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            reference_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_reference_output(o):\n",
        "      page.Reference.controls.append(o)\n",
        "      page.Reference.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_reference_output = add_to_reference_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      for i, c in enumerate(page.Reference.controls):\n",
        "        if i == 0: continue\n",
        "        else: del page.Reference.controls[i]\n",
        "      page.Reference.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def reference_help(e):\n",
        "      def close_reference_dlg(e):\n",
        "        nonlocal reference_help_dlg\n",
        "        reference_help_dlg.open = False\n",
        "        page.update()\n",
        "      reference_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Reference-Only\"), content=Column([\n",
        "          Text(\"This reference-only ControlNet can directly link the attention layers of your SD to any independent images, so that your SD will read arbitary images for reference. This preprocessor does not require any control models. It can guide the diffusion directly using images as references. This method is similar to inpaint-based reference but it does not make your image disordered.\"),\n",
        "          Text(\"Note that this method is as 'non-opinioned' as possible. It only contains very basic connection codes, without any personal preferences, to connect the attention layers with your reference images. However, even if we tried best to not include any opinioned codes, we still need to write some subjective implementations to deal with weighting, cfg-scale, etc - tech report is on the way.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßô  Powerful... \", on_click=close_reference_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = reference_help_dlg\n",
        "      reference_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          reference_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          reference_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        ref_image.value = fname\n",
        "        ref_image.update()\n",
        "        reference_prefs['ref_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    prompt = TextField(label=\"Prompt Text\", value=reference_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=reference_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        reference_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    ref_image = TextField(label=\"Reference Image\", value=reference_prefs['ref_image'], on_change=lambda e:changed(e,'ref_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL Reference Pipeline\", value=reference_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip=\"Otherwise use standard Model Checkpoint set in Installation.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(reference_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=reference_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=reference_prefs, key='guidance_scale')\n",
        "    eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=20, round=1, pref=reference_prefs, key='eta', tooltip=\"\", visible=False)\n",
        "    page.etas.append(eta)\n",
        "    style_fidelity = SliderRow(label=\"Style Fidelity\", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='style_fidelity', tooltip=\"Style fidelity of ref_uncond_xt. If style_fidelity=1.0, control more important, elif style_fidelity=0.0, prompt more important, else balanced.\")\n",
        "    attention_auto_machine_weight = SliderRow(label=\"Attention Auto Machine Weight\", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='attention_auto_machine_weight', tooltip=\"Weight of using reference query for self attention's context. If attention_auto_machine_weight=1.0, use reference query for all self attention's context.\")\n",
        "    gn_auto_machine_weight = SliderRow(label=\"Gn Auto Machine Weight\", min=0.0, max=2.0, divisions=20, round=1, pref=reference_prefs, key='gn_auto_machine_weight', tooltip=\"Weight of using reference adain. If gn_auto_machine_weight=2.0, use all reference adain plugins.\")\n",
        "    #reference_attn = Checkbox(label=\"Reference Attention\", value=reference_prefs['reference_attn'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_attn'), tooltip=\"Whether to use reference query for self attention's context.\")\n",
        "    #reference_adain = Checkbox(label=\"Reference Adain\", value=reference_prefs['reference_adain'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_adain'), tooltip=\"Whether to use reference Adain (Adaptive Instance Normalization).\")\n",
        "    reference_attn = Switcher(label=\"Reference Attention \", value=reference_prefs['reference_attn'], on_change=lambda e:changed(e,'reference_attn'), tooltip=\"Whether to use reference query for self attention's context.\")\n",
        "    reference_adain = Switcher(label=\"Reference Adain\", value=reference_prefs['reference_adain'], on_change=lambda e:changed(e,'reference_adain'), tooltip=\"Whether to use reference Adain (Adaptive Instance Normalization).\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=reference_prefs, key='max_size')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=reference_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=reference_prefs, key='height')\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=8, value=reference_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    num_images = NumberPicker(label=\"Number of Iterations: \", min=1, max=12, value=reference_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=reference_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=reference_prefs['apply_ESRGAN_upscale'], on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=reference_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=reference_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_reference = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_reference.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not reference_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.reference_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.reference_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé©  Reference-Only Image with Prompt\", \"ControlNet Pipeline for Transfering Ref Subject to new images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Reference Settings\", on_click=reference_help)]),\n",
        "        ref_image,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        use_SDXL,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta,\n",
        "        style_fidelity,\n",
        "        ResponsiveRow([attention_auto_machine_weight, gn_auto_machine_weight]),\n",
        "        Row([reference_attn, reference_adain]),\n",
        "        max_row,\n",
        "        width_slider,\n",
        "        height_slider,\n",
        "        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),\n",
        "        page.ESRGAN_block_reference,\n",
        "        Row([ElevatedButton(content=Text(\"üíó  Make Reference\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page, from_list=True))]),\n",
        "        page.reference_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_qr_prefs = {\n",
        "    'init_image': '',\n",
        "    'ref_image': '',\n",
        "    'qr_content': '',\n",
        "    'border_thickness': 5,\n",
        "    'use_image': False,\n",
        "    'selected_mode': 'link',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'ugly, disfigured, low quality, blurry, nsfw',\n",
        "    'guidance_scale': 8.0,\n",
        "    'conditioning_scale': 1.8,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'strength': 0.9,\n",
        "    'num_inference_steps': 50,\n",
        "    'controlnet_version': \"Stable Diffusion 2.1\",\n",
        "    'seed': 0,\n",
        "    'batch_size': 1,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    'last_model': '',\n",
        "    'last_controlnet_model': '',\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_model': 'SD v1.5',\n",
        "    'ip_adapter_SDXL_model': 'SDXL',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildControlNetQR(page):\n",
        "    global controlnet_qr_prefs, prefs, pipe_controlnet_qr\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_qr_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_qr_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_qr_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_qr_output(o):\n",
        "      page.ControlNetQR.controls.append(o)\n",
        "      page.ControlNetQR.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_controlnet_qr_output = add_to_controlnet_qr_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      for i, c in enumerate(page.ControlNetQR.controls):\n",
        "        if i == 0: continue\n",
        "        else: del page.ControlNetQR.controls[i]\n",
        "      page.ControlNetQR.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_qr_help(e):\n",
        "      def close_controlnet_qr_dlg(e):\n",
        "        nonlocal controlnet_qr_help_dlg\n",
        "        controlnet_qr_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_qr_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with ControlNet-QRCode\"), content=Column([\n",
        "          Text(\"These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape.\"),\n",
        "          Text(\"The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version. You'll also get great results with QR Code Monster and QR Pattern, with or without SDXL. Experiment...\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•É  Scantastic... \", on_click=close_controlnet_qr_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_qr_help_dlg\n",
        "      controlnet_qr_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "                fname = os.path.join(root_dir, e.file_name)\n",
        "                controlnet_qr_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "                fname = e.file_name\n",
        "                controlnet_qr_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                controlnet_qr_prefs['init_image'] = fname\n",
        "            elif pick_type == \"ref\":\n",
        "                ref_image.value = fname\n",
        "                ref_image.update()\n",
        "                controlnet_qr_prefs['ref_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_ref(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"ref\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick QR Code Image File\")\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_qr_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_qr_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_qr_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def toggle_image(e):\n",
        "        controlnet_qr_prefs['use_image'] = e.control.value\n",
        "        qr_generator.height = None if not e.control.value else 0\n",
        "        qr_generator.update()\n",
        "        qr_content.visible = not e.control.value\n",
        "        qr_content.update()\n",
        "        ref_image.visible = e.control.value\n",
        "        ref_image.update()\n",
        "    def change_mode(e):\n",
        "        controlnet_qr_prefs['selected_mode'] = e.data\n",
        "        image_mode = e.data.split('\"')[1] == \"image\"\n",
        "        qr_generator.height = None if not image_mode else 0\n",
        "        qr_generator.update()\n",
        "        qr_content.visible = not image_mode\n",
        "        qr_content.update()\n",
        "        ref_image.visible = image_mode\n",
        "        ref_image.update()\n",
        "    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={controlnet_qr_prefs['selected_mode']}, allow_multiple_selection=False,\n",
        "        segments=[\n",
        "            ft.Segment(value=\"link\", label=ft.Text(\"URL Text\"), icon=ft.Icon(ft.icons.LINK)),\n",
        "            ft.Segment(value=\"image\", label=ft.Text(\"QR Image\"), icon=ft.Icon(ft.icons.QR_CODE)),\n",
        "        ],\n",
        "    )\n",
        "    qr_content = TextField(label=\"QR Code Content (URL or whatever text)\", value=controlnet_qr_prefs['qr_content'], expand=True, visible=not controlnet_qr_prefs['use_image'], on_change=lambda e:changed(e,'qr_content'))\n",
        "    init_image = TextField(label=\"Initial Image (optional)\", value=controlnet_qr_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=64, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    ref_image = TextField(label=\"ControlNet QR Code Image\", value=controlnet_qr_prefs['ref_image'], expand=True, visible=controlnet_qr_prefs['use_image'], on_change=lambda e:changed(e,'ref_image'), height=64, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_ref))\n",
        "    use_image = Switcher(label=\"Use QR Image Instead\", value=controlnet_qr_prefs['use_image'], tooltip=\"Provide your own QR Code made elsewhere, or generate here.\", on_change=toggle_image)\n",
        "    #use_image = Switcher(label=\"Use QR Image Instead\", value=controlnet_qr_prefs['use_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_image)\n",
        "    border_thickness = SliderRow(label=\"Border Thickness\", min=0, max=20, divisions=20, round=0, pref=controlnet_qr_prefs, key='border_thickness', tooltip=\"The border is equal to the thickness of 5 tiny black boxes around QR.\")\n",
        "    qr_generator = Container(content=Column([border_thickness], alignment=MainAxisAlignment.START), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    qr_generator.height = None if not controlnet_qr_prefs['use_image'] else 0\n",
        "    controlnet_version = Dropdown(label=\"ControlNet QRCode Version\", width=250, options=[dropdown.Option(\"Stable Diffusion 2.1\"), dropdown.Option(\"Stable Diffusion 1.5\"), dropdown.Option(\"QR Code Monster v2 1.5\"), dropdown.Option(\"QR Pattern v2 1.5\"), dropdown.Option(\"SDXL QR Code Monster v1\"), dropdown.Option(\"SDXL QR Pattern\"), dropdown.Option(\"SDXL QR Pattern LLLite\")], value=controlnet_qr_prefs['controlnet_version'], on_change=lambda e: changed(e, 'controlnet_version'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_qr_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=controlnet_qr_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=controlnet_qr_prefs, key='guidance_scale')\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0.0, max=5.0, divisions=50, round=1, pref=controlnet_qr_prefs, key='conditioning_scale', tooltip=\"Strength of the ControlNet Mask.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    strength = SliderRow(label=\"Init Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_qr_prefs, key='strength', tooltip=\"How strong the Initial Image should be over the ControlNet. Higher value give less influence.\")\n",
        "    max_size = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_qr_prefs, key='max_size')\n",
        "    def toggle_ip_adapter(e):\n",
        "        controlnet_qr_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_model.visible = e.control.value\n",
        "        ip_adapter_model.update()\n",
        "        ip_adapter_SDXL_model.visible = e.control.value\n",
        "        ip_adapter_SDXL_model.update()\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=controlnet_qr_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip=\"Uses both image and text to condition the image generation process.\")\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=controlnet_qr_prefs['ip_adapter_model'], visible=controlnet_qr_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=controlnet_qr_prefs['ip_adapter_SDXL_model'], visible=controlnet_qr_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_SDXL_models:\n",
        "        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=controlnet_qr_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_qr_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_qr_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_qr_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    num_images = NumberPicker(label=\"Number of Iterations: \", min=1, max=12, value=controlnet_qr_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_qr_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_qr_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_qr_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_qr_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_qr = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_qr.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not controlnet_qr_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.controlnet_qr_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_qr_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîó  ControlNet QRCode Art Generator\", \"ControlNet Img2Img for Inpainting QR Code with Prompt and/or Init Image...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNetQR Settings\", on_click=controlnet_qr_help)]),\n",
        "        Row([selected_mode, qr_content, ref_image]),\n",
        "        qr_generator,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([controlnet_version, init_image]),\n",
        "        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        ip_adapter_container,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        conditioning_scale,\n",
        "        strength,\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        max_size,\n",
        "        #batch_size, \n",
        "        ResponsiveRow([Row([num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),\n",
        "        page.ESRGAN_block_controlnet_qr,\n",
        "        Row([ElevatedButton(content=Text(\"üßë‚ÄçüíªÔ∏è  Make ControlNet QR\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page, from_list=True), tooltip=\"Runs from global Prompt Queue using Image Parameters for Prompt, Neg Prompt, Steps, Guidance, Init Image, Strength and Seed.\")]),\n",
        "        page.controlnet_qr_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_segment_prefs = {\n",
        "    'ref_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'low quality, low resolution, render, oversaturated, low contrast',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'seed': 0,\n",
        "    'batch_size': 1,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    'width': 960,\n",
        "    'height': 768,\n",
        "    'last_model': '',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildControlNetSegmentAnything(page):\n",
        "    global controlnet_segment_prefs, prefs, pipe_controlnet_segment\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_segment_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_segment_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_segment_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_segment_output(o):\n",
        "      page.ControlNetSegmentAnything.controls.append(o)\n",
        "      page.ControlNetSegmentAnything.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_controlnet_segment_output = add_to_controlnet_segment_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      for i, c in enumerate(page.ControlNetSegmentAnything.controls):\n",
        "        if i == 0: continue\n",
        "        else: del page.ControlNetSegmentAnything.controls[i]\n",
        "      page.ControlNetSegmentAnything.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_segment_help(e):\n",
        "      def close_controlnet_segment_dlg(e):\n",
        "        nonlocal controlnet_segment_help_dlg\n",
        "        controlnet_segment_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_segment_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with ControlNet Segment-Anything\"), content=Column([\n",
        "          Text(\"This model is based on the ControlNet Model, which allow us to generate Images using some sort of condition image. For this model, we selected the segmentation maps produced by Meta‚Äôs new segmentation model called Segment Anything Model as the condition image. We then trained the model to generate images based on the structure of the segmentation maps and the text prompts given.\"),\n",
        "          Text(\"For the training, we generated a segmented dataset based on the COYO-700M dataset. The dataset provided us with the images, and the text prompts. For the segmented images, we used Segment Anything Model. We then created 8k samples to train our model on, which isn‚Äôt a lot, but as a team, we have been very busy with many other responsibilities and time constraints, which made it challenging to dedicate a lot of time to generating a larger dataset. Despite the constraints we faced, we have still managed to achieve some nice results üôå\"),\n",
        "          Markdown(\"[Project Page](https://segment-anything.com) | [Model Card](https://huggingface.co/mfidabel/controlnet-segment-anything) | [Segment-Anything Code](https://github.com/facebookresearch/segment-anything) | [HuggingFace Demo](https://huggingface.co/spaces/mfidabel/controlnet-segment-anything)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üß©  Puzzle the Pieces... \", on_click=close_controlnet_segment_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_segment_help_dlg\n",
        "      controlnet_segment_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_segment_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_segment_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        ref_image.value = fname\n",
        "        ref_image.update()\n",
        "        controlnet_segment_prefs['ref_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_segment_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_segment_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_segment_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    ref_image = TextField(label=\"Initial Image to Segment\", value=controlnet_segment_prefs['ref_image'], on_change=lambda e:changed(e,'ref_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_segment_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=controlnet_segment_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=controlnet_segment_prefs, key='guidance_scale')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_segment_prefs, key='max_size')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_segment_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_segment_prefs, key='height')\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_segment_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    num_images = NumberPicker(label=\"Number of Iterations: \", min=1, max=12, value=controlnet_segment_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_segment_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_segment_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_segment_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_segment_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_segment = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_segment.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not controlnet_segment_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.controlnet_segment_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_segment_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü•∏  ControlNet on Meta's Segment-Anything\", \"Upload an Image, Segment it with Segment Anything, write a prompt, and generate images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNet Segment Anything Settings\", on_click=controlnet_segment_help)]),\n",
        "        ref_image,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        max_row,\n",
        "        #width_slider,\n",
        "        #height_slider,\n",
        "        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),\n",
        "        page.ESRGAN_block_controlnet_segment,\n",
        "        Row([ElevatedButton(content=Text(\"üëπ  Make ControlNet Segments\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page, from_list=True))]),\n",
        "        page.controlnet_segment_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "EDICT_prefs = {\n",
        "    'target_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'base_prompt': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'strength': 0.8,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'max_size': 512,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildEDICT(page):\n",
        "    global EDICT_prefs, prefs, pipe_EDICT\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            EDICT_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            EDICT_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            EDICT_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_EDICT_output(o):\n",
        "      page.EDICT_output.controls.append(o)\n",
        "      page.EDICT_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_EDICT_output = add_to_EDICT_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.EDICT_output.controls = []\n",
        "      page.EDICT_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def EDICT_help(e):\n",
        "      def close_EDICT_dlg(e):\n",
        "        nonlocal EDICT_help_dlg\n",
        "        EDICT_help_dlg.open = False\n",
        "        page.update()\n",
        "      EDICT_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with EDICT Edit\"), content=Column([\n",
        "          Text(\"EDICT: Exact Diffusion Inversion via Coupled Transformations\"),\n",
        "          Text(\"Using the iterative denoising diffusion principle, denoising diffusion models (DDMs) trained with web-scale data can generate highly realistic images conditioned on input text, layouts, and scene graphs. After image generation, the next important application of DDMs being explored by the research community is that of image editing. Models such as DALL-E-2 and Stable Diffusion [24] can perform inpainting, allowing users to edit images through manual annotation. Methods such as SDEdit have demonstrated that both synthetic and real images can be edited using stroke or composite guidance via DDMs. However, the goal of a holistic image editing tool that can edit any real/artificial image using purely text has still not been achieved, until now.\"),\n",
        "          Markdown(\"[Read Arxiv Paper](https://arxiv.org/pdf/2211.12446.pdf)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•¥  Not Complicated... \", on_click=close_EDICT_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = EDICT_help_dlg\n",
        "      EDICT_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        EDICT_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          EDICT_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          EDICT_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        EDICT_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    base_prompt = TextField(label=\"Base Prompt Text (describe init image)\", value=EDICT_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))\n",
        "    target_prompt = TextField(label=\"Target Prompt Text\", value=EDICT_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=EDICT_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = TextField(label=\"Initial Image to Edit (crops square)\", value=EDICT_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(EDICT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=EDICT_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=EDICT_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    strength = SliderRow(label=\"Strength\", min=0, max=1, divisions=20, round=2, pref=EDICT_prefs, key='strength')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=EDICT_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=EDICT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=EDICT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=EDICT_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=EDICT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_EDICT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_EDICT.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.EDICT_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.EDICT_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§π  EDICT Image Editing\", \"Diffusion pipeline for text-guided image editing... Exact Diffusion Inversion via Coupled Transformations.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=EDICT_help)]),\n",
        "        init_image,\n",
        "        base_prompt,\n",
        "        ResponsiveRow([target_prompt, negative_prompt]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        strength,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=EDICT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_EDICT,\n",
        "        ElevatedButton(content=Text(\"üßù  Run EDICT Edit\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_EDICT(page)),\n",
        "        page.EDICT_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "DiffEdit_prefs = {\n",
        "    'target_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'source_prompt': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'strength': 0.8,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'max_size': 768,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDiffEdit(page):\n",
        "    global DiffEdit_prefs, prefs, pipe_DiffEdit\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            DiffEdit_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            DiffEdit_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            DiffEdit_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_DiffEdit_output(o):\n",
        "      page.DiffEdit_output.controls.append(o)\n",
        "      page.DiffEdit_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_DiffEdit_output = add_to_DiffEdit_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.DiffEdit_output.controls = []\n",
        "      page.DiffEdit_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def DiffEdit_help(e):\n",
        "      def close_DiffEdit_dlg(e):\n",
        "        nonlocal DiffEdit_help_dlg\n",
        "        DiffEdit_help_dlg.open = False\n",
        "        page.update()\n",
        "      DiffEdit_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DiffEdit\"), content=Column([\n",
        "          Text(\"Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.\"),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"[Read Arxiv Paper](https://arxiv.org/abs/2210.11427) - [Blog Post with Demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html) - [GitHub](https://github.com/Xiang-cd/DiffEdit-stable-diffusion/)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòá  Not Difficult... \", on_click=close_DiffEdit_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = DiffEdit_help_dlg\n",
        "      DiffEdit_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        DiffEdit_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          DiffEdit_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          DiffEdit_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        DiffEdit_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    source_prompt = TextField(label=\"Source Prompt Text (blank to auto-caption)\", value=DiffEdit_prefs['source_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'source_prompt'))\n",
        "    target_prompt = TextField(label=\"Target Prompt Text (describe edits)\", value=DiffEdit_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=DiffEdit_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = TextField(label=\"Initial Image to Edit (crops square)\", value=DiffEdit_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(DiffEdit_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=DiffEdit_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=DiffEdit_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    strength = SliderRow(label=\"Strength\", min=0, max=1, divisions=20, round=2, pref=DiffEdit_prefs, key='strength')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=DiffEdit_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=DiffEdit_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=DiffEdit_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=DiffEdit_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=DiffEdit_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiffEdit = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiffEdit.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.DiffEdit_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.DiffEdit_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üçí  DiffEdit Image Editing\", \"Zero-shot Diffusion-based Semantic Image Editing with Mask Guidance...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=DiffEdit_help)]),\n",
        "        init_image,\n",
        "        source_prompt,\n",
        "        ResponsiveRow([target_prompt, negative_prompt]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        strength,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=DiffEdit_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_DiffEdit,\n",
        "        ElevatedButton(content=Text(\"üòÉ  Run DiffEdit\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiffEdit(page)),\n",
        "        page.DiffEdit_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "null_text_prefs = {\n",
        "    'target_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'base_prompt': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'num_inner_steps': 10,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'max_size': 1024,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildNull_Text(page):\n",
        "    global null_text_prefs, prefs, pipe_null_text\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            null_text_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            null_text_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            null_text_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def null_text_help(e):\n",
        "      def close_null_text_dlg(e):\n",
        "        nonlocal null_text_help_dlg\n",
        "        null_text_help_dlg.open = False\n",
        "        page.update()\n",
        "      null_text_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Null-Text Inversion\"), content=Column([\n",
        "          Text(\"This pipeline provides null-text inversion for editing real images. It enables null-text optimization, and DDIM reconstruction via w, w/o null-text optimization. No prompt-to-prompt code is implemented as there is a Prompt2PromptPipeline.\"),\n",
        "          Markdown(\"[Read Arxiv Paper](https://arxiv.org/pdf/2208.01626.pdf) | [Null-Text Inversion](https://github.com/google/prompt-to-prompt/) | [Junsheng Luan](https://github.com/Junsheng121)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Credits go to Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel and HuggingFace team.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üôÉ  Edit Away... \", on_click=close_null_text_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = null_text_help_dlg\n",
        "      null_text_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        null_text_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    base_prompt = TextField(label=\"Inverted Prompt Text (describe init-image)\", value=null_text_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))\n",
        "    target_prompt = TextField(label=\"Target Edited Prompt Text\", value=null_text_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=null_text_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = FileInput(label=\"Initial Image to Edit\", pref=null_text_prefs, key='init_image', page=page)\n",
        "    #init_image = TextField(label=\"Initial Image to Edit\", value=null_text_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(null_text_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=null_text_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=null_text_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    num_inner_steps = SliderRow(label=\"Number of Inner Steps\", min=1, max=100, divisions=99, pref=null_text_prefs, key='num_inner_steps', tooltip=\"The number of steps to Invert init-image in the Null Optimization.\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=null_text_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=null_text_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=null_text_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=null_text_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=null_text_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_null_text = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_null_text.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üòë  Null-Text Inversion Image Editing\", \"Editing Real Images using Guided Diffusion Models... Exact Diffusion Inversion via Coupled Transformations. Prompt-to-prompt image editing with cross attention control.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=null_text_help)]),\n",
        "        init_image,\n",
        "        base_prompt,\n",
        "        ResponsiveRow([target_prompt, negative_prompt]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inner_steps,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        #strength,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=null_text_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_null_text,\n",
        "        ElevatedButton(content=Text(\"‚≠ï  Run Null-Text Inversion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_null_text(page)),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_prefs = {\n",
        "    'prompt': '',\n",
        "    'batch_folder_name': '',\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'prior_num_inference_steps': 25,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'use_StableUnCLIP_pipeline': False,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP(page):\n",
        "    global unCLIP_prefs, prefs, pipe_unCLIP\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_output(o):\n",
        "      page.unCLIP_output.controls.append(o)\n",
        "      page.unCLIP_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_output = add_to_unCLIP_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_output.controls = []\n",
        "      page.unCLIP_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_help(e):\n",
        "      def close_unCLIP_dlg(e):\n",
        "        nonlocal unCLIP_help_dlg\n",
        "        unCLIP_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Pipeline\"), content=Column([\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Tricky... \", on_click=close_unCLIP_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_help_dlg\n",
        "      unCLIP_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def toggle_unCLIP(e):\n",
        "        unCLIP_prefs['use_StableUnCLIP_pipeline'] = e.control.value\n",
        "        if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "            decoder_num_inference_row.visible = False\n",
        "            super_res_num_inference_row.visible = False\n",
        "            decoder_guidance.visible = False\n",
        "            decoder_num_inference_row.update()\n",
        "            super_res_num_inference_row.update()\n",
        "            decoder_guidance.update()\n",
        "        else:\n",
        "            decoder_num_inference_row.visible = True\n",
        "            super_res_num_inference_row.visible = True\n",
        "            decoder_guidance.visible = True\n",
        "            decoder_guidance.update()\n",
        "            super_res_num_inference_row.update()\n",
        "            decoder_num_inference_row.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=unCLIP_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    prior_num_inference_row = SliderRow(label=\"Number of Prior Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='prior_num_inference_steps', tooltip=\"The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    prior_guidance = SliderRow(label=\"Prior Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='prior_guidance_scale')\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    use_StableUnCLIP_pipeline = Tooltip(message=\"Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)\", content=Switcher(label=\"Use Stable UnCLIP Pipeline Instead\", value=unCLIP_prefs['use_StableUnCLIP_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_unCLIP))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(unCLIP_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=unCLIP_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=unCLIP_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåê  unCLIP Text-to-Image Generator\", \"Hierarchical Text-Conditional Image Generation with CLIP Latents.  Similar results to DALL-E 2...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Settings\", on_click=unCLIP_help)]),\n",
        "        prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,\n",
        "        prior_guidance, decoder_guidance,\n",
        "        #eta_row, max_row,\n",
        "        use_StableUnCLIP_pipeline,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP,\n",
        "        Row([ElevatedButton(content=Text(\"üñáÔ∏è   Get unCLIP Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page, from_list=True))]),\n",
        "        page.unCLIP_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )),\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_image_variation_prefs = {\n",
        "    'init_image': '',\n",
        "    'file_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_ImageVariation(page):\n",
        "    global unCLIP_image_variation_prefs, prefs, pipe_unCLIP_image_variation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_image_variation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_image_variation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_image_variation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_image_variation_output(o):\n",
        "      page.unCLIP_image_variation_output.controls.append(o)\n",
        "      page.unCLIP_image_variation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_image_variation_output = add_to_unCLIP_image_variation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_image_variation_output.controls = []\n",
        "      page.unCLIP_image_variation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_image_variation_help(e):\n",
        "      def close_unCLIP_image_variation_dlg(e):\n",
        "        nonlocal unCLIP_image_variation_help_dlg\n",
        "        unCLIP_image_variation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_image_variation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Image Variation Pipeline\"), content=Column([\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP Image Variation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üêá  We'll see... \", on_click=close_unCLIP_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_image_variation_help_dlg\n",
        "      unCLIP_image_variation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        unCLIP_image_variation_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=unCLIP_image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    #prompt = TextField(label=\"Prompt Text\", value=unCLIP_image_variation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_variation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_image_variation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_image_variation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=unCLIP_image_variation_prefs, key='max_size')\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=unCLIP_image_variation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_image_variation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", round=1, value=unCLIP_image_variation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_image_variation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_variation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_variation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_image_variation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_image_variation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéÜ  unCLIP Image Variation Generator\", \"Generate Variations from an input image using unCLIP...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Image Variation Settings\", on_click=unCLIP_image_variation_help)]),\n",
        "        init_image,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        decoder_num_inference_row, super_res_num_inference_row,\n",
        "        decoder_guidance,\n",
        "        #eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_image_variation,\n",
        "        Row([ElevatedButton(content=Text(\"ü¶Ñ   Get unCLIP Image Variation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page, from_list=True))]),\n",
        "        page.unCLIP_image_variation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )),\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_interpolation_prefs = {\n",
        "    'prompt': '',\n",
        "    'end_prompt': '',\n",
        "    'steps': 5,\n",
        "    'batch_folder_name': '',\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'prior_num_inference_steps': 25,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_Interpolation(page):\n",
        "    global unCLIP_interpolation_prefs, prefs, pipe_unCLIP_interpolation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_interpolation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_interpolation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_interpolation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_interpolation_output(o):\n",
        "      page.unCLIP_interpolation_output.controls.append(o)\n",
        "      page.unCLIP_interpolation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_interpolation_output = add_to_unCLIP_interpolation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_interpolation_output.controls = []\n",
        "      page.unCLIP_interpolation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_interpolation_help(e):\n",
        "      def close_unCLIP_interpolation_dlg(e):\n",
        "        nonlocal unCLIP_interpolation_help_dlg\n",
        "        unCLIP_interpolation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_interpolation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Text Interpolation Pipeline\"), content=Column([\n",
        "          Text(\"This Diffusion Pipeline takes two prompts and interpolates between the two input prompts using spherical interpolation ( slerp ). The input prompts are converted to text embeddings by the pipeline's text_encoder and the interpolation is done on the resulting text_embeddings over the number of steps specified. Defaults to 5 steps.\"),\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Transformative... \", on_click=close_unCLIP_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_interpolation_help_dlg\n",
        "      unCLIP_interpolation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    prompt = TextField(label=\"Start Prompt Text\", value=unCLIP_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    end_prompt = TextField(label=\"End Prompt Text\", value=unCLIP_interpolation_prefs['end_prompt'], on_change=lambda e:changed(e,'end_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    interpolation_steps_row = SliderRow(label=\"Number of Interpolation Step\", min=1, max=50, divisions=49, pref=unCLIP_interpolation_prefs, key='steps', tooltip=\"The number of steps over which to interpolate from start_prompt to end_prompt.\")\n",
        "    prior_num_inference_row = SliderRow(label=\"Number of Prior Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='prior_num_inference_steps', tooltip=\"The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    prior_guidance = SliderRow(label=\"Prior Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='prior_guidance_scale')\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #use_StableunCLIP_interpolation_pipeline = Tooltip(message=\"Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)\", content=Switcher(label=\"Use Stable UnCLIP Pipeline Instead\", value=unCLIP_interpolation_prefs['use_StableunCLIP_interpolation_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_StableunCLIP_interpolation_pipeline')))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_interpolation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(unCLIP_interpolation_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=unCLIP_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_interpolation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", round=1, value=unCLIP_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_interpolation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_interpolation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_interpolation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåå  unCLIP Text Interpolation Generator\", \"Takes two prompts and interpolates between the two input prompts using spherical interpolation...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Settings\", on_click=unCLIP_interpolation_help)]),\n",
        "        prompt,\n",
        "        end_prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        interpolation_steps_row,\n",
        "        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,\n",
        "        prior_guidance, decoder_guidance,\n",
        "        #eta_row, max_row,\n",
        "        #use_StableunCLIP_interpolation_pipeline,\n",
        "        #NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),\n",
        "        Row([seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_interpolation,\n",
        "        Row([ElevatedButton(content=Text(\"üéÜ   Get unCLIP Interpolations\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page))]),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page, from_list=True))]),\n",
        "        page.unCLIP_interpolation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )),\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_image_interpolation_prefs = {\n",
        "    'init_image': '',\n",
        "    'end_image': '',\n",
        "    'file_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    'interpolation_steps': 6,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_ImageInterpolation(page):\n",
        "    global unCLIP_image_interpolation_prefs, prefs, pipe_unCLIP_image_interpolation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_image_interpolation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_image_interpolation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_image_interpolation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_image_interpolation_output(o):\n",
        "      page.unCLIP_image_interpolation_output.controls.append(o)\n",
        "      page.unCLIP_image_interpolation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_image_interpolation_output = add_to_unCLIP_image_interpolation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_image_interpolation_output.controls = []\n",
        "      page.unCLIP_image_interpolation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_image_interpolation_help(e):\n",
        "      def close_unCLIP_image_interpolation_dlg(e):\n",
        "        nonlocal unCLIP_image_interpolation_help_dlg\n",
        "        unCLIP_image_interpolation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_image_interpolation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Image Interpolation Pipeline\"), content=Column([\n",
        "          Text(\"This Diffusion Pipeline takes two images or an image_embeddings tensor of size 2 and interpolates between their embeddings using spherical interpolation ( slerp ). The input images/image_embeddings are converted to image embeddings by the pipeline's image_encoder and the interpolation is done on the resulting image_embeddings over the number of steps specified.\"),\n",
        "          #Text(\"\"),\n",
        "          #Markdown(\"The unCLIP Image Interpolation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü¶ø  Transformers Activate... \", on_click=close_unCLIP_image_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_image_interpolation_help_dlg\n",
        "      unCLIP_image_interpolation_help_dlg.open = True\n",
        "      page.update()\n",
        "    pick_type = \"\"\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"init\":\n",
        "            init_image.value = fname\n",
        "            init_image.update()\n",
        "            unCLIP_image_interpolation_prefs['init_image'] = fname\n",
        "        elif pick_type == \"end\":\n",
        "            end_image.value = fname\n",
        "            end_image.update()\n",
        "            unCLIP_image_interpolation_prefs['end_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_end(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"end\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Ending Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=unCLIP_image_interpolation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    end_image = TextField(label=\"Ending Image\", value=unCLIP_image_interpolation_prefs['end_image'], on_change=lambda e:changed(e,'end_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_end))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    #prompt = TextField(label=\"Prompt Text\", value=unCLIP_image_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_image_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    interpolation_steps = SliderRow(label=\"Interpolation Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='interpolation_steps', tooltip=\"The number of interpolation images to generate.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_interpolation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_image_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_image_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_image_interpolation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=unCLIP_image_interpolation_prefs, key='max_size')\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_image_interpolation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", round=1, value=unCLIP_image_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_image_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_interpolation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_image_interpolation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_image_interpolation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§ñ  unCLIP Image Interpolation Generator\", \"Pass two images and produces in-betweens while interpolating between their image-embeddings...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Image Interpolation Settings\", on_click=unCLIP_image_interpolation_help)]),\n",
        "        init_image, end_image,\n",
        "        interpolation_steps,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        decoder_num_inference_row, super_res_num_inference_row,\n",
        "        decoder_guidance,\n",
        "        #eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_image_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_image_interpolation,\n",
        "        Row([ElevatedButton(content=Text(\"ü¶æ   Get unCLIP Image Interpolation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page, from_list=True))\n",
        "             ]),\n",
        "\n",
        "      ]\n",
        "    )), page.unCLIP_image_interpolation_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "magic_mix_prefs = {\n",
        "    'init_image': '',\n",
        "    'prompt': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'mix_factor': 0.5,\n",
        "    'kmin': 0.3,\n",
        "    'kmax': 0.6,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 1024,\n",
        "    'scheduler_mode': 'DDIM',\n",
        "    'scheduler_last': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildMagicMix(page):\n",
        "    global magic_mix_prefs, prefs, pipe_magic_mix\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            magic_mix_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            magic_mix_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            magic_mix_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_magic_mix_output(o):\n",
        "      page.MagicMix.controls.append(o)\n",
        "      page.MagicMix.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_magic_mix_output = add_to_magic_mix_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      for i, c in enumerate(page.MagicMix.controls):\n",
        "        if i == 0: continue\n",
        "        else: del page.MagicMix.controls[i]\n",
        "      page.MagicMix.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def magic_mix_help(e):\n",
        "      def close_magic_mix_dlg(e):\n",
        "        nonlocal magic_mix_help_dlg\n",
        "        magic_mix_help_dlg.open = False\n",
        "        page.update()\n",
        "      magic_mix_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with MagicMix\"), content=Column([\n",
        "          Text(\"Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -- > corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßô  Sounds like magic... \", on_click=close_magic_mix_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = magic_mix_help_dlg\n",
        "      magic_mix_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          magic_mix_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          magic_mix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        magic_mix_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    prompt = TextField(label=\"Prompt Text\", value=magic_mix_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        magic_mix_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    init_image = TextField(label=\"Initial Image\", value=magic_mix_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(magic_mix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    scheduler_mode = Dropdown(label=\"Scheduler/Sampler Mode\", hint_text=\"They're very similar, with minor differences in the noise\", width=200,\n",
        "            options=[\n",
        "                dropdown.Option(\"DDIM\"),\n",
        "                dropdown.Option(\"LMS Discrete\"),\n",
        "                dropdown.Option(\"PNDM\"),\n",
        "            ], value=magic_mix_prefs['scheduler_mode'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_mode'),\n",
        "        )\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=magic_mix_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=magic_mix_prefs, key='guidance_scale')\n",
        "    mix_factor_row = SliderRow(label=\"Mix Factor\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='mix_factor', tooltip=\"Interpolation constant used in the layout generation phase. The greater the value of `mix_factor`, the greater the influence of the prompt on the layout generation process.\")\n",
        "    kmin_row = SliderRow(label=\"k-Min\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmin', tooltip=\"A higher value of kmin results in more steps for content generation process. Determine the range for the layout and content generation process.\")\n",
        "    kmax_row = SliderRow(label=\"k-Max\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmax', tooltip=\"A higher value of kmax results in loss of more information about the layout of the original image. Determine the range for the layout and content generation process.\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=magic_mix_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=magic_mix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=magic_mix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=magic_mix_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=magic_mix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_magic_mix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_magic_mix.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not magic_mix_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "\n",
        "    page.magic_mix_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.magic_mix_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßö  MagicMix Init Image with Prompt\", \"Diffusion Pipeline for semantic mixing of an image and a text prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with MagicMix Settings\", on_click=magic_mix_help)]),\n",
        "        init_image,\n",
        "        prompt,\n",
        "        scheduler_mode,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        mix_factor_row,\n",
        "        ResponsiveRow([kmin_row, kmax_row]),\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=magic_mix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_magic_mix,\n",
        "        Row([ElevatedButton(content=Text(\"ü™Ñ  Make MagicMix\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page)),\n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page, from_list=True))]),\n",
        "        page.magic_mix_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "paint_by_example_prefs = {\n",
        "    'original_image': '',\n",
        "    'mask_image': '',\n",
        "    'example_image': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 7.5,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildPaintByExample(page):\n",
        "    global paint_by_example_prefs, prefs, pipe_paint_by_example\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            paint_by_example_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            paint_by_example_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            paint_by_example_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_paint_by_example_output(o):\n",
        "      page.paint_by_example_output.controls.append(o)\n",
        "      page.paint_by_example_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.paint_by_example_output.controls = []\n",
        "      page.paint_by_example_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def paint_by_example_help(e):\n",
        "      def close_paint_by_example_dlg(e):\n",
        "        nonlocal paint_by_example_help_dlg\n",
        "        paint_by_example_help_dlg.open = False\n",
        "        page.update()\n",
        "      paint_by_example_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Paint-by-Example\"), content=Column([\n",
        "          Text(\"Language-guided image editing has achieved great success recently. In this pipeline, we use exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.  Credit goes to https://github.com/Fantasy-Studio/Paint-by-Example\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò∏  Sweetness... \", on_click=close_paint_by_example_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = paint_by_example_help_dlg\n",
        "      paint_by_example_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          paint_by_example_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          paint_by_example_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"original\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          paint_by_example_prefs['original_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          paint_by_example_prefs['mask_image'] = fname\n",
        "        elif pick_type == \"example\":\n",
        "          example_image.value = fname\n",
        "          example_image.update()\n",
        "          paint_by_example_prefs['example_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"original\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def pick_example(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"example\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Example Style Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        paint_by_example_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {paint_by_example_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=paint_by_example_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=paint_by_example_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=paint_by_example_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=paint_by_example_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    example_image = TextField(label=\"Example Style Image\", value=paint_by_example_prefs['example_image'], on_change=lambda e:changed(e,'example_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_example))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(paint_by_example_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=paint_by_example_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=paint_by_example_prefs, key='guidance_scale')\n",
        "    #eta = TextField(label=\"ETA\", value=str(paint_by_example_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(paint_by_example_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {paint_by_example_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=paint_by_example_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=paint_by_example_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=paint_by_example_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=paint_by_example_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=paint_by_example_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_paint_by_example = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_paint_by_example.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.paint_by_example_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.paint_by_example_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü¶Å  Paint-by-Example\", \"Image-guided Inpainting using an Example Image to Transfer Subject to Masked area...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Paint-by-Example Settings\", on_click=paint_by_example_help)]),\n",
        "        ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        example_image,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=paint_by_example_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_paint_by_example,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        ElevatedButton(content=Text(\"üêæ  Run Paint-by-Example\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_paint_by_example(page)),\n",
        "        page.paint_by_example_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "instruct_pix2pix_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 7.5,\n",
        "    'image_guidance_scale': 1.5,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'num_images': 1,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'control_v': 'v1.1',\n",
        "    'use_SDXL': False,\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_model': 'SD v1.5',\n",
        "    'ip_adapter_SDXL_model': 'SDXL',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildInstructPix2Pix(page):\n",
        "    global instruct_pix2pix_prefs, prefs, pipe_instruct_pix2pix\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            instruct_pix2pix_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            instruct_pix2pix_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            instruct_pix2pix_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_instruct_pix2pix_output(o):\n",
        "      page.instruct_pix2pix_output.controls.append(o)\n",
        "      page.instruct_pix2pix_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.instruct_pix2pix_output.controls = []\n",
        "      page.instruct_pix2pix_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def instruct_pix2pix_help(e):\n",
        "      def close_instruct_pix2pix_dlg(e):\n",
        "        nonlocal instruct_pix2pix_help_dlg\n",
        "        instruct_pix2pix_help_dlg.open = False\n",
        "        page.update()\n",
        "      instruct_pix2pix_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Instruct-Pix2Pix\"), content=Column([\n",
        "          Text(\"A method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòé  Fun2Fun... \", on_click=close_instruct_pix2pix_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = instruct_pix2pix_help_dlg\n",
        "      instruct_pix2pix_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          instruct_pix2pix_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          instruct_pix2pix_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        instruct_pix2pix_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {instruct_pix2pix_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def toggle_init_video(e):\n",
        "        changed(e, 'use_init_video')\n",
        "        show = e.control.value\n",
        "        original_image.visible = not show\n",
        "        original_image.update()\n",
        "        init_video.visible = show\n",
        "        init_video.update()\n",
        "        vid_params.height = None if show else 0\n",
        "        vid_params.update()\n",
        "        run_prompt_list.visible = not show\n",
        "        run_prompt_list.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=instruct_pix2pix_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Editing Instructions Prompt Text\", value=instruct_pix2pix_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=instruct_pix2pix_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(instruct_pix2pix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switcher(label=\"Use Init Video\", value=instruct_pix2pix_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=instruct_pix2pix_prefs['init_video'], expand=True, visible=instruct_pix2pix_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=instruct_pix2pix_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=instruct_pix2pix_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=instruct_pix2pix_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if instruct_pix2pix_prefs['use_init_video'] else 0)\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL Instruct Pix2Pix Pipeline\", value=instruct_pix2pix_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip=\"Otherwise use standard 1.5/2.1 Model Checkpoint.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=instruct_pix2pix_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=instruct_pix2pix_prefs, key='guidance_scale')\n",
        "    image_guidance = SliderRow(label=\"Image Guidance Scale\", min=0, max=200, divisions=400, round=1, pref=instruct_pix2pix_prefs, key='image_guidance_scale', tooltip=\"Image guidance scale is to push the generated image towards the inital image `image`. Higher image guidance scale encourages to generate images that are closely linked to the source image `image`, usually at the expense of lower image quality.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(instruct_pix2pix_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(instruct_pix2pix_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {instruct_pix2pix_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=instruct_pix2pix_prefs, key='max_size')\n",
        "    def toggle_ip_adapter(e):\n",
        "        instruct_pix2pix_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_model.visible = e.control.value\n",
        "        ip_adapter_model.update()\n",
        "        ip_adapter_SDXL_model.visible = e.control.value\n",
        "        ip_adapter_SDXL_model.update()\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=instruct_pix2pix_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip=\"Uses both image and text to condition the image generation process.\")\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=instruct_pix2pix_prefs['ip_adapter_model'], visible=instruct_pix2pix_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=instruct_pix2pix_prefs['ip_adapter_SDXL_model'], visible=instruct_pix2pix_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_SDXL_models:\n",
        "        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=instruct_pix2pix_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=instruct_pix2pix_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if instruct_pix2pix_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=instruct_pix2pix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=instruct_pix2pix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=instruct_pix2pix_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=instruct_pix2pix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_instruct_pix2pix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_instruct_pix2pix.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.instruct_pix2pix_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.instruct_pix2pix_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üèúÔ∏è  Instruct-Pix2Pix\", \"Text-Based Image Editing - Follow Image Editing Instructions...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=instruct_pix2pix_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        Row([original_image, init_video, use_init_video]),\n",
        "        vid_params,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        image_guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        use_SDXL,#, ip_adapter_SDXL_model\n",
        "        Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        ip_adapter_container,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=instruct_pix2pix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_instruct_pix2pix,\n",
        "        Row([ElevatedButton(content=Text(\"üèñÔ∏è  Run Instruct Pix2Pix\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page)),\n",
        "             run_prompt_list]),\n",
        "        page.instruct_pix2pix_output,\n",
        "        clear_button,\n",
        "      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'lowres, text, watermark, cropped, low quality',\n",
        "    'control_task': 'Scribble',\n",
        "    'conditioning_scale': 1.0,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'multi_controlnets': [],\n",
        "    'batch_size': 1,\n",
        "    'max_size': 768,\n",
        "    'low_threshold': 100, #1-255\n",
        "    'high_threshold': 200, #1-255\n",
        "    'steps': 50, #100\n",
        "    'guidance_scale': 9, #30\n",
        "    'seed': 0,\n",
        "    'eta': 0,\n",
        "    'show_processed_image': False,\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_model': 'SD v1.5',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'use_image2image': False,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'file_prefix': 'controlnet-',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildControlNet(page):\n",
        "    global controlnet_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_output(o):\n",
        "      page.controlnet_output.controls.append(o)\n",
        "      page.controlnet_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_output.controls = []\n",
        "      page.controlnet_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_help(e):\n",
        "      def close_controlnet_dlg(e):\n",
        "        nonlocal controlnet_help_dlg\n",
        "        controlnet_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ControlNet\"), content=Column([\n",
        "          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" one learns your condition. The \"locked\" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The \"zero convolution\" is 1√ó1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),\n",
        "          Markdown(\"This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Scribble - A hand-drawn monochrome image with white outlines on a black background.\"),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"OpenPose - A OpenPose bone image.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"HED - A monochrome image with white soft edges on a black background.\"),\n",
        "          Text(\"M-LSD - A monochrome image composed only of white straight lines on a black background.\"),\n",
        "          Text(\"Normal Map - A normal mapped image.\"),\n",
        "          Text(\"Segmented - An ADE20K's semantic segmentation protocol image.\"),\n",
        "          Text(\"LineArt - An image with line art, usually black lines on a white background.\"),\n",
        "          Text(\"Shuffle - An image with shuffled patches or regions.\"),\n",
        "          Text(\"Brightness - An image based on brightness of init.\"),\n",
        "          Text(\"Mediapipe Face - An image based on face position & expression.\"),\n",
        "          Text(\"Instruct Pix2Pix - Trained with pixel to pixel instruction.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçÑ  Too much control... \", on_click=close_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_help_dlg\n",
        "      controlnet_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          controlnet_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          controlnet_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        task = e.control.value\n",
        "        show = task.startswith(\"Video\")# or task == \"Video OpenPose\"\n",
        "        update = controlnet_prefs['use_init_video'] != show\n",
        "        changed(e,'control_task')\n",
        "        threshold.height = None if controlnet_prefs['control_task'] == \"Canny Map Edge\" or controlnet_prefs['control_task'] == \"Video Canny Edge\" else 0\n",
        "        threshold.update()\n",
        "        if update:\n",
        "            original_image.visible = not show\n",
        "            original_image.update()\n",
        "            init_video.visible = show\n",
        "            init_video.update()\n",
        "            vid_params.height = None if show else 0\n",
        "            vid_params.update()\n",
        "            conditioning_scale.visible = not show\n",
        "            conditioning_scale.update()\n",
        "            add_layer_btn.visible = not show\n",
        "            add_layer_btn.update()\n",
        "            multi_layers.visible = not show\n",
        "            multi_layers.update()\n",
        "            run_prompt_list.visible = not show\n",
        "            run_prompt_list.update()\n",
        "            controlnet_prefs['use_init_video'] = show\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {controlnet_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def add_layer(e):\n",
        "        layer = {'control_task': controlnet_prefs['control_task'], 'original_image': controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'use_init_video': False}\n",
        "        if controlnet_prefs['control_task'] == \"Video Canny Edge\" or controlnet_prefs['control_task'] == \"Video OpenPose\":\n",
        "          layer['use_init_video'] = True\n",
        "          layer['init_video'] = controlnet_prefs['init_video']\n",
        "          layer['fps'] = controlnet_prefs['fps']\n",
        "          layer['start_time'] = controlnet_prefs['start_time']\n",
        "          layer['end_time'] = controlnet_prefs['end_time']\n",
        "          controlnet_prefs['init_video'] = \"\"\n",
        "          init_video.value = \"\"\n",
        "          original_image.update()\n",
        "        controlnet_prefs['multi_controlnets'].append(layer)\n",
        "        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f\"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}\")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Control Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "          ]), data=layer))\n",
        "        multi_layers.update()\n",
        "        controlnet_prefs['original_image'] = \"\"\n",
        "        original_image.value = \"\"\n",
        "        original_image.update()\n",
        "    def delete_layer(e):\n",
        "        controlnet_prefs['multi_controlnets'].remove(e.control.data)\n",
        "        for c in multi_layers.controls:\n",
        "          if c.data['original_image'] == e.control.data['original_image']:\n",
        "             multi_layers.controls.remove(c)\n",
        "             break\n",
        "        multi_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        controlnet_prefs['multi_controlnets'].clear()\n",
        "        multi_layers.controls.clear()\n",
        "        multi_layers.update()\n",
        "    def toggle_img2img(e):\n",
        "        controlnet_prefs['use_image2image'] = e.control.value\n",
        "        img2img_row.height = None if e.control.value else 0\n",
        "        img2img_row.update()\n",
        "    def toggle_ip_adapter(e):\n",
        "        controlnet_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_model.visible = e.control.value\n",
        "        ip_adapter_model.update()\n",
        "    original_image = TextField(label=\"Original Drawing\", value=controlnet_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    #a_prompt  = TextField(label=\"Added Prompt Text\", value=controlnet_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=200, options=[dropdown.Option(\"Scribble\"), dropdown.Option(\"Canny Map Edge\"), dropdown.Option(\"OpenPose\"), dropdown.Option(\"Depth\"), dropdown.Option(\"HED\"), dropdown.Option(\"M-LSD\"), dropdown.Option(\"Normal Map\"), dropdown.Option(\"Segmentation\"), dropdown.Option(\"LineArt\"), dropdown.Option(\"Shuffle\"), dropdown.Option(\"Instruct Pix2Pix\"), dropdown.Option(\"Brightness\"), dropdown.Option(\"Mediapipe Face\"), dropdown.Option(\"Video Canny Edge\"), dropdown.Option(\"Video OpenPose\")], value=controlnet_prefs['control_task'], on_change=change_task)\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=controlnet_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    #add_layer_btn = IconButton(icons.ADD, tooltip=\"Add Multi-ControlNet Layer\", on_click=add_layer)\n",
        "    add_layer_btn = ft.FilledButton(\"‚ûï Add Layer\", width=140, on_click=add_layer)\n",
        "    multi_layers = Column([], spacing=0)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switcher(label=\"Use Init Video\", value=controlnet_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=controlnet_prefs['init_video'], expand=True, visible=controlnet_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)\n",
        "    num_inference_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=controlnet_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=30, divisions=60, round=1, pref=controlnet_prefs, key='guidance_scale')\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_prefs, key='low_threshold', col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_prefs, key='high_threshold', col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    threshold.height = None if controlnet_prefs['control_task'] == \"Canny Map Edge\" else 0\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(controlnet_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {controlnet_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_prefs, key='max_size')\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=controlnet_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=controlnet_prefs['ip_adapter_model'], visible=controlnet_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=controlnet_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    use_image2image = Switcher(label=\"Use Image2Image or Inpainting\", value=controlnet_prefs['use_image2image'], on_change=toggle_img2img)\n",
        "    init_image = FileInput(label=\"Init Image\", pref=controlnet_prefs, key='init_image', expand=True, page=page)\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=controlnet_prefs, key='mask_image', expand=True, page=page)\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=controlnet_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=controlnet_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=controlnet_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    show_processed_image = Checkbox(label=\"Show Pre-Processed Image\", value=controlnet_prefs['show_processed_image'], tooltip=\"Displays the Init-Image after being process by Canny, Depth, etc.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üï∏Ô∏è  ControlNet Image+Text-to-Image+Video-to-Video\", \"Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNet Settings\", on_click=controlnet_help)]),\n",
        "        Row([control_task, original_image, init_video, add_layer_btn]),\n",
        "        conditioning_scale,\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        multi_layers,\n",
        "        vid_params,\n",
        "        Divider(thickness=2, height=4),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        threshold,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        use_image2image,\n",
        "        img2img_row,\n",
        "        Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        ip_adapter_container,\n",
        "        show_processed_image,\n",
        "        Row([NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üè∏  Run ControlNet\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page)),\n",
        "             run_prompt_list]),\n",
        "        page.controlnet_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_xl_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'lowres, text, watermark, cropped, low quality',\n",
        "    'control_task': 'Canny Map Edge',\n",
        "    'conditioning_scale': 1.0,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'multi_controlnets': [],\n",
        "    'batch_size': 1,\n",
        "    'max_size': 1024,\n",
        "    'low_threshold': 100, #1-255\n",
        "    'high_threshold': 200, #1-255\n",
        "    'steps': 50, #100\n",
        "    'guidance_scale': 7.5, #30\n",
        "    'seed': 0,\n",
        "    'eta': 0,\n",
        "    'show_processed_image': False,\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_SDXL_model': 'SDXL',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'use_image2image': False,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'file_prefix': 'controlnet-',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildControlNetXL(page):\n",
        "    global controlnet_xl_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_xl_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_xl_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_xl_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_xl_output(o):\n",
        "      page.controlnet_xl_output.controls.append(o)\n",
        "      page.controlnet_xl_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_xl_output.controls = []\n",
        "      page.controlnet_xl_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_xl_help(e):\n",
        "      def close_controlnet_xl_dlg(e):\n",
        "        nonlocal controlnet_xl_help_dlg\n",
        "        controlnet_xl_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_xl_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ControlNet SDXL\"), content=Column([\n",
        "          Text('ControlNet-XL is a neural network structure to control diffusion models by adding extra conditions, using the latest StableDiffusion XL models. It copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" one learns your condition. The \"locked\" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The \"zero convolution\" is 1√ó1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNetXL will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),\n",
        "          Markdown(\"This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          #Text(\"Scribble - A hand-drawn monochrome image with white outlines on a black background.\"),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"OpenPose - A OpenPose bone image.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"Softedge - A monochrome image with white soft edges on a black background.\"),\n",
        "          #Text(\"M-LSD - A monochrome image composed only of white straight lines on a black background.\"),\n",
        "          #Text(\"Normal Map - A normal mapped image.\"),\n",
        "          Text(\"Segmented - An ADE20K's semantic segmentation protocol image.\"),\n",
        "          Text(\"LineArt - An image with line art, usually black lines on a white background.\"),\n",
        "          #Text(\"Shuffle - An image with shuffled patches or regions.\"),\n",
        "          #Text(\"Brightness - An image based on brightness of init.\"),\n",
        "          #Text(\"Instruct Pix2Pix - Trained with pixel to pixel instruction.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçÑ  Too much control... \", on_click=close_controlnet_xl_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_xl_help_dlg\n",
        "      controlnet_xl_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_xl_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_xl_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          controlnet_xl_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          controlnet_xl_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_xl_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        task = e.control.value\n",
        "        show = task.startswith(\"Video\")# or task == \"Video OpenPose\"\n",
        "        update = controlnet_xl_prefs['use_init_video'] != show\n",
        "        changed(e,'control_task')\n",
        "        threshold.height = None if \"Canny\" in controlnet_xl_prefs['control_task'] else 0\n",
        "        threshold.update()\n",
        "        if update:\n",
        "            original_image.visible = not show\n",
        "            original_image.update()\n",
        "            init_video.visible = show\n",
        "            init_video.update()\n",
        "            vid_params.height = None if show else 0\n",
        "            vid_params.update()\n",
        "            conditioning_scale.visible = not show\n",
        "            conditioning_scale.update()\n",
        "            add_layer_btn.visible = not show\n",
        "            add_layer_btn.update()\n",
        "            multi_layers.visible = not show\n",
        "            multi_layers.update()\n",
        "            run_prompt_list.visible = not show\n",
        "            run_prompt_list.update()\n",
        "            controlnet_xl_prefs['use_init_video'] = show\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {controlnet_xl_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def add_layer(e):\n",
        "        layer = {'control_task': controlnet_xl_prefs['control_task'], 'original_image': controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'use_init_video': False}\n",
        "        if controlnet_xl_prefs['control_task'] == \"Video Canny Edge\" or controlnet_xl_prefs['control_task'] == \"Video OpenPose\":\n",
        "          layer['use_init_video'] = True\n",
        "          layer['init_video'] = controlnet_xl_prefs['init_video']\n",
        "          layer['fps'] = controlnet_xl_prefs['fps']\n",
        "          layer['start_time'] = controlnet_xl_prefs['start_time']\n",
        "          layer['end_time'] = controlnet_xl_prefs['end_time']\n",
        "          controlnet_xl_prefs['init_video'] = \"\"\n",
        "          init_video.value = \"\"\n",
        "          original_image.update()\n",
        "        controlnet_xl_prefs['multi_controlnets'].append(layer)\n",
        "        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f\"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}\")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Control Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "          ]), data=layer))\n",
        "        multi_layers.update()\n",
        "        controlnet_xl_prefs['original_image'] = \"\"\n",
        "        original_image.value = \"\"\n",
        "        original_image.update()\n",
        "    def delete_layer(e):\n",
        "        controlnet_xl_prefs['multi_controlnets'].remove(e.control.data)\n",
        "        for c in multi_layers.controls:\n",
        "          if c.data['original_image'] == e.control.data['original_image']:\n",
        "             multi_layers.controls.remove(c)\n",
        "             break\n",
        "        multi_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        controlnet_xl_prefs['multi_controlnets'].clear()\n",
        "        multi_layers.controls.clear()\n",
        "        multi_layers.update()\n",
        "    def toggle_img2img(e):\n",
        "        controlnet_xl_prefs['use_image2image'] = e.control.value\n",
        "        img2img_row.height = None if e.control.value else 0\n",
        "        img2img_row.update()\n",
        "    def toggle_ip_adapter(e):\n",
        "        controlnet_xl_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_SDXL_model.visible = e.control.value\n",
        "        ip_adapter_SDXL_model.update()\n",
        "    original_image = TextField(label=\"Original Drawing\", value=controlnet_xl_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_xl_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    #a_prompt  = TextField(label=\"Added Prompt Text\", value=controlnet_xl_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_xl_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    control_task = Dropdown(label=\"ControlNet-SDXL Task\", width=210, options=[dropdown.Option(\"Canny Map Edge\"), dropdown.Option(\"Canny Map Edge mid\"), dropdown.Option(\"Canny Map Edge small\"), dropdown.Option(\"Depth\"), dropdown.Option(\"Depth mid\"), dropdown.Option(\"Depth small\"), dropdown.Option(\"Segmentation\"), dropdown.Option(\"LineArt\"), dropdown.Option(\"Softedge\"), dropdown.Option(\"OpenPose\")], value=controlnet_xl_prefs['control_task'], on_change=change_task)\n",
        "    #, dropdown.Option(\"Scribble\"), dropdown.Option(\"HED\"), dropdown.Option(\"M-LSD\"), dropdown.Option(\"Normal Map\"), dropdown.Option(\"Shuffle\"), dropdown.Option(\"Instruct Pix2Pix\"), dropdown.Option(\"Brightness\"), dropdown.Option(\"Video Canny Edge\"), dropdown.Option(\"Video OpenPose\")\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=controlnet_xl_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    #add_layer_btn = IconButton(icons.ADD, tooltip=\"Add Multi-ControlNetXL Layer\", on_click=add_layer)\n",
        "    add_layer_btn = ft.FilledButton(\"‚ûï Add Layer\", width=140, on_click=add_layer)\n",
        "    multi_layers = Column([], spacing=0)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switcher(label=\"Use Init Video\", value=controlnet_xl_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=controlnet_xl_prefs['init_video'], expand=True, visible=controlnet_xl_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_xl_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_xl_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_xl_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_xl_prefs['use_init_video'] else 0)\n",
        "\n",
        "    num_inference_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=controlnet_xl_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=30, divisions=60, round=1, pref=controlnet_xl_prefs, key='guidance_scale')\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='low_threshold', col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='high_threshold', col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    threshold.height = None if controlnet_xl_prefs['control_task'] == \"Canny Map Edge\" else 0\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(controlnet_xl_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {controlnet_xl_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_xl_prefs, key='max_size')\n",
        "    \n",
        "    use_image2image = Switcher(label=\"Use Image2Image or Inpainting\", value=controlnet_xl_prefs['use_image2image'], on_change=toggle_img2img)\n",
        "    init_image = FileInput(label=\"Init Image\", pref=controlnet_xl_prefs, key='init_image', expand=True, page=page)\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=controlnet_xl_prefs, key='mask_image', expand=True, page=page)\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=controlnet_xl_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=controlnet_xl_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_xl_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=controlnet_xl_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)\n",
        "    ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=controlnet_xl_prefs['ip_adapter_SDXL_model'], visible=controlnet_xl_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))\n",
        "    for m in ip_adapter_SDXL_models:\n",
        "        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=controlnet_xl_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_xl_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_xl_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=controlnet_xl_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    show_processed_image = Checkbox(label=\"Show Pre-Processed Image\", value=controlnet_xl_prefs['show_processed_image'], tooltip=\"Displays the Init-Image after being process by Canny, Depth, etc.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_xl_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_xl_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_xl_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_xl_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_xl_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üï∑  ControlNet SDXL Image+Text-to-Image\", \"Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNetXL Settings\", on_click=controlnet_xl_help)]),\n",
        "        Row([control_task, original_image, init_video, add_layer_btn]),\n",
        "        conditioning_scale,\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        multi_layers,\n",
        "        vid_params,\n",
        "        Divider(thickness=2, height=4),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        threshold,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        use_image2image,\n",
        "        img2img_row,\n",
        "        Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        ip_adapter_container,\n",
        "        show_processed_image,\n",
        "        Row([NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_xl_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üõÉ  Run ControlNet-XL\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page)),\n",
        "             run_prompt_list]),\n",
        "        page.controlnet_xl_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_xs_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'lowres, text, watermark, cropped, low quality',\n",
        "    'use_SDXL': True,\n",
        "    'cpu_offload': True,\n",
        "    'control_task': 'Canny Map Edge',\n",
        "    'conditioning_scale': 1.0,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'multi_controlnets': [],\n",
        "    'batch_size': 1,\n",
        "    'max_size': 1024,\n",
        "    'low_threshold': 100, #1-255\n",
        "    'high_threshold': 200, #1-255\n",
        "    'steps': 50, #100\n",
        "    'guidance_scale': 7.5, #30\n",
        "    'seed': 0,\n",
        "    'eta': 0,\n",
        "    'show_processed_image': False,\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_SDXL_model': 'SDXL',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'use_image2image': False,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'file_prefix': 'controlnet-',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildControlNetXS(page):\n",
        "    global controlnet_xs_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_xs_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_xs_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_xs_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_xs_output(o):\n",
        "      page.controlnet_xs_output.controls.append(o)\n",
        "      page.controlnet_xs_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_xs_output.controls = []\n",
        "      page.controlnet_xs_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_xs_help(e):\n",
        "      def close_controlnet_xs_dlg(e):\n",
        "        nonlocal controlnet_xs_help_dlg\n",
        "        controlnet_xs_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_xs_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ControlNet XS\"), content=Column([\n",
        "          Markdown(\"ControlNet-XS was introduced in [ControlNet-XS](https://vislearn.github.io/ControlNet-XS/) by Denis Zavadski and Carsten Rother. It is based on the observation that the control model in the [original ControlNet](https://huggingface.co/papers/2302.05543) can be made much smaller and still produce good results. Like the original ControlNet model, you can provide an additional control image to condition and control Stable Diffusion generation. For example, if you provide a depth map, the ControlNet model generates an image that'll preserve the spatial information from the depth map. It is a more flexible and accurate way to control the image generation process. ControlNet-XS generates images with comparable quality to a regular ControlNet, but it is 20-25% faster ([see benchmark](https://github.com/UmerHA/controlnet-xs-benchmark/blob/main/Speed%20Benchmark.ipynb)) and uses ~45% less memory.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text('With increasing computing capabilities, current model architectures appear to follow the trend of simply upscaling all components without validating the necessity for doing so. In this project we investigate the size and architectural design of ControlNet [Zhang et al., 2023] for controlling the image generation process with stable diffusion-based models. We show that a new architecture with as little as 1% of the parameters of the base model achieves state-of-the art results, considerably better than ControlNet in terms of FID score. Hence we call it ControlNet-XS. We provide the code for controlling StableDiffusion-XL [Podell et al., 2023] (Model B, 48M Parameters) and StableDiffusion 2.1 [Rombach et al. 2022] (Model B, 14M Parameters), all under openrail license.'),\n",
        "          Markdown(\"[XS Project Page](https://vislearn.github.io/ControlNet-XS/)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçÑ  Too much control... \", on_click=close_controlnet_xs_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_xs_help_dlg\n",
        "      controlnet_xs_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_xs_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_xs_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          controlnet_xs_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          controlnet_xs_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_xs_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        task = e.control.value\n",
        "        show = task.startswith(\"Video\")# or task == \"Video OpenPose\"\n",
        "        update = controlnet_xs_prefs['use_init_video'] != show\n",
        "        changed(e,'control_task')\n",
        "        threshold.height = None if \"Canny\" in controlnet_xs_prefs['control_task'] else 0\n",
        "        threshold.update()\n",
        "        if update:\n",
        "            original_image.visible = not show\n",
        "            original_image.update()\n",
        "            init_video.visible = show\n",
        "            init_video.update()\n",
        "            vid_params.height = None if show else 0\n",
        "            vid_params.update()\n",
        "            conditioning_scale.visible = not show\n",
        "            conditioning_scale.update()\n",
        "            add_layer_btn.visible = not show\n",
        "            add_layer_btn.update()\n",
        "            multi_layers.visible = not show\n",
        "            multi_layers.update()\n",
        "            run_prompt_list.visible = not show\n",
        "            run_prompt_list.update()\n",
        "            controlnet_xs_prefs['use_init_video'] = show\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {controlnet_xs_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def add_layer(e):\n",
        "        layer = {'control_task': controlnet_xs_prefs['control_task'], 'original_image': controlnet_xs_prefs['original_image'], 'conditioning_scale': controlnet_xs_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xs_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xs_prefs['control_guidance_end'], 'use_init_video': False}\n",
        "        if controlnet_xs_prefs['control_task'] == \"Video Canny Edge\" or controlnet_xs_prefs['control_task'] == \"Video OpenPose\":\n",
        "          layer['use_init_video'] = True\n",
        "          layer['init_video'] = controlnet_xs_prefs['init_video']\n",
        "          layer['fps'] = controlnet_xs_prefs['fps']\n",
        "          layer['start_time'] = controlnet_xs_prefs['start_time']\n",
        "          layer['end_time'] = controlnet_xs_prefs['end_time']\n",
        "          controlnet_xs_prefs['init_video'] = \"\"\n",
        "          init_video.value = \"\"\n",
        "          original_image.update()\n",
        "        controlnet_xs_prefs['multi_controlnets'].append(layer)\n",
        "        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f\"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}\")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Control Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "          ]), data=layer))\n",
        "        multi_layers.update()\n",
        "        controlnet_xs_prefs['original_image'] = \"\"\n",
        "        original_image.value = \"\"\n",
        "        original_image.update()\n",
        "    def delete_layer(e):\n",
        "        controlnet_xs_prefs['multi_controlnets'].remove(e.control.data)\n",
        "        for c in multi_layers.controls:\n",
        "          if c.data['original_image'] == e.control.data['original_image']:\n",
        "             multi_layers.controls.remove(c)\n",
        "             break\n",
        "        multi_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        controlnet_xs_prefs['multi_controlnets'].clear()\n",
        "        multi_layers.controls.clear()\n",
        "        multi_layers.update()\n",
        "    def toggle_img2img(e):\n",
        "        controlnet_xs_prefs['use_image2image'] = e.control.value\n",
        "        img2img_row.height = None if e.control.value else 0\n",
        "        img2img_row.update()\n",
        "    def toggle_ip_adapter(e):\n",
        "        controlnet_xs_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_SDXL_model.visible = e.control.value\n",
        "        ip_adapter_SDXL_model.update()\n",
        "    original_image = TextField(label=\"Original Drawing\", value=controlnet_xs_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_xs_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL\", value=controlnet_xs_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip=\"SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.\")\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=controlnet_xs_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    #a_prompt  = TextField(label=\"Added Prompt Text\", value=controlnet_xs_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_xs_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=210, options=[dropdown.Option(\"Canny Map Edge\"), dropdown.Option(\"Depth\")], value=controlnet_xs_prefs['control_task'], on_change=change_task)\n",
        "    #, dropdown.Option(\"Scribble\"), dropdown.Option(\"HED\"), dropdown.Option(\"M-LSD\"), dropdown.Option(\"Normal Map\"), dropdown.Option(\"Shuffle\"), dropdown.Option(\"Instruct Pix2Pix\"), dropdown.Option(\"Brightness\"), dropdown.Option(\"Video Canny Edge\"), dropdown.Option(\"Video OpenPose\")\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=controlnet_xs_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xs_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xs_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    #add_layer_btn = IconButton(icons.ADD, tooltip=\"Add Multi-ControlNetXS Layer\", on_click=add_layer)\n",
        "    add_layer_btn = ft.FilledButton(\"‚ûï Add Layer\", width=140, on_click=add_layer)\n",
        "    multi_layers = Column([], spacing=0)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_xs_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switcher(label=\"Use Init Video\", value=controlnet_xs_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=controlnet_xs_prefs['init_video'], expand=True, visible=controlnet_xs_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_xs_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_xs_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_xs_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_xs_prefs['use_init_video'] else 0)\n",
        "\n",
        "    num_inference_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=controlnet_xs_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=30, divisions=60, round=1, pref=controlnet_xs_prefs, key='guidance_scale')\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_xs_prefs, key='low_threshold', col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_xs_prefs, key='high_threshold', col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    threshold.height = None if controlnet_xs_prefs['control_task'] == \"Canny Map Edge\" else 0\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(controlnet_xs_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {controlnet_xs_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_xs_prefs, key='max_size')\n",
        "    \n",
        "    use_image2image = Switcher(label=\"Use Image2Image or Inpainting\", value=controlnet_xs_prefs['use_image2image'], on_change=toggle_img2img)\n",
        "    init_image = FileInput(label=\"Init Image\", pref=controlnet_xs_prefs, key='init_image', expand=True, page=page)\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=controlnet_xs_prefs, key='mask_image', expand=True, page=page)\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=controlnet_xs_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=controlnet_xs_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_xs_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=controlnet_xs_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)\n",
        "    ip_adapter_SDXL_model = Dropdown(label=\"IP-Adapter SDXL Model\", width=220, options=[], value=controlnet_xs_prefs['ip_adapter_SDXL_model'], visible=controlnet_xs_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))\n",
        "    for m in ip_adapter_SDXL_models:\n",
        "        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=controlnet_xs_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_xs_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_xs_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=controlnet_xs_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    show_processed_image = Checkbox(label=\"Show Pre-Processed Image\", value=controlnet_xs_prefs['show_processed_image'], tooltip=\"Displays the Init-Image after being process by Canny, Depth, etc.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_xs_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_xs_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_xs_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_xs_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_xs_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_xs_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xs(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üï∏  ControlNet-XS Image+Text-to-Image\", \"Faster & Smaller Controlnet Image Conditioning Text-to-Image Diffusion Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNetXS Settings\", on_click=controlnet_xs_help)]),\n",
        "        Row([control_task, original_image, init_video, add_layer_btn]),\n",
        "        conditioning_scale,\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        multi_layers,\n",
        "        vid_params,\n",
        "        Divider(thickness=2, height=4),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        threshold,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([use_SDXL, cpu_offload]),\n",
        "        #use_image2image,\n",
        "        #img2img_row,\n",
        "        #Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "        #ip_adapter_container,\n",
        "        show_processed_image,\n",
        "        Row([NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_xs_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"‚õπÔ∏è  Run ControlNet-XS\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xs(page)),\n",
        "             run_prompt_list]),\n",
        "        page.controlnet_xs_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "controlnet_video2video_prefs = {\n",
        "    'init_video': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'control_task': 'Canny21',\n",
        "    'controlnet_strength': 1.0,\n",
        "    'init_image_strength': 0.5,\n",
        "    'feedthrough_strength': 0.0,\n",
        "    'motion_alpha': 0.1,\n",
        "    'motion_sigma': 0.3,\n",
        "    'color_fix': 'Lab', #'none', 'rgb', 'hsv', 'lab'\n",
        "    'color_amount': 0.0,\n",
        "    'max_size': 768,\n",
        "    'low_threshold': 100, #1-255 canny\n",
        "    'high_threshold': 200, #1-255\n",
        "    'mlsd_score_thr': 0.1, #mlsd line detector v threshold\n",
        "    'mlsd_dist_thr': 0.1, #mlsd line detector d threshold\n",
        "    'steps': 25,\n",
        "    'prompt_strength': 7.5,\n",
        "    'start_time': 0.0,\n",
        "    'end_time': 0.0,\n",
        "    'duration': 0.0,\n",
        "    'max_dimension': 832,\n",
        "    'min_dimension': 512,\n",
        "    'round_dims_to': 64,\n",
        "    'no_audio': False,\n",
        "    'skip_dumped_frames': False,\n",
        "    'save_frames': False,\n",
        "    'show_console': True,\n",
        "    'fix_orientation': True,\n",
        "    'file_prefix': 'controlnet-',\n",
        "    'output_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildControlNet_Video2Video(page):\n",
        "    global controlnet_video2video_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_video2video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_video2video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_video2video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_video2video_output.controls = []\n",
        "      page.controlnet_video2video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_video2video_help(e):\n",
        "      def close_controlnet_video2video_dlg(e):\n",
        "        nonlocal controlnet_video2video_help_dlg\n",
        "        controlnet_video2video_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_video2video_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ControlNet Video2Video\"), content=Column([\n",
        "          Text(\"Can apply Stable Diffusion to a video, while maintaining frame-to-frame consistency. It is based on the Stable Diffusion img2img model, but adds a motion estimator and motion compensator to maintain consistency between frames. It will process each input frame with some preprocessing (motion transfer/compensation of the output feedback), followed by a detector and diffusion models in a pipeline configured by the ControlNet Type option.\"),\n",
        "          Text(\"Feedback strength, set with init-image-strength controls frame-to-frame consistency, by changing how much the motion-compensated previous output frame is fed into the next frame's diffusion pipeline in place of initial latent noise, a la img2img latent diffusion (citation needed). Values around 0.3 to 0.5 and sometimes much higher (closer to 1.0, the maximum which means no noise is added and no denoising steps will be run).\"),\n",
        "          Markdown(\"This is an interface for running the [ControlNet Video codebase](https://github.com/un1tz3r0/controlnetvideo) by [Victor Condino](un1tz3r0@gmail.com).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" one learns your condition. The \"locked\" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The \"zero convolution\" is 1√ó1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),\n",
        "          Text(\"Aesthetic - Uses image features extracted using a Canny edge detector trained on a large aesthetic dataset.\"),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"OpenPose - A OpenPose bone image.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"HED - A monochrome image with white soft edges on a black background.\"),\n",
        "          Text(\"M-LSD - A monochrome image composed only of white straight lines on a black background.\"),\n",
        "          Text(\"Normal Map - A normal mapped image.\"),\n",
        "          Text(\"LineArt - An image with line art, usually black lines on a white background.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò∏  Could get crazy... \", on_click=close_controlnet_video2video_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_video2video_help_dlg\n",
        "      controlnet_video2video_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_video2video_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_video2video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          controlnet_video2video_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_video2video_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        changed(e,'control_task')\n",
        "        canny_threshold.height = None if controlnet_video2video_prefs['control_task'] == \"Canny\" or controlnet_video2video_prefs['control_task'] == \"Canny21\" else 0\n",
        "        canny_threshold.update()\n",
        "        mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == \"MLSD\" else 0\n",
        "        mlsd_threshold.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_video2video_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_video2video_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=150, options=[dropdown.Option(\"Aesthetic\"), dropdown.Option(\"Lineart21\"), dropdown.Option(\"HED\"), dropdown.Option(\"HED21\"), dropdown.Option(\"HED\"), dropdown.Option(\"Canny\"), dropdown.Option(\"Canny21\"), dropdown.Option(\"OpenPose\"), dropdown.Option(\"OpenPose21\"), dropdown.Option(\"Depth\"), dropdown.Option(\"Depth21\"), dropdown.Option(\"Normal\"), dropdown.Option(\"MLSD\")], value=controlnet_video2video_prefs['control_task'], on_change=change_task)\n",
        "    #conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=controlnet_video2video_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_video2video_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    controlnet_strength = SliderRow(label=\"ControlNet Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='controlnet_strength', tooltip=\"How much influence the controlnet annotator's output is used to guide the denoising process.\")\n",
        "    init_image_strength = SliderRow(label=\"Init-Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='init_image_strength', tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    feedthrough_strength = SliderRow(label=\"Feedthrough Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='feedthrough_strength', tooltip=\"The ratio of input to motion compensated prior output to feed through to the next frame.\")\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=controlnet_video2video_prefs['init_video'], expand=True, on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    #fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_video2video_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_video2video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_video2video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    duration = TextField(label=\"Duration (0 for all)\", value=controlnet_video2video_prefs['duration'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'duration', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([Row([start_time, end_time, duration])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=5))\n",
        "    num_inference_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=controlnet_video2video_prefs, key='steps', tooltip=\"Number of inference steps, depends on the scheduler, trades off speed for quality.\")\n",
        "    prompt_strength = SliderRow(label=\"Prompt Strength\", min=0, max=30, divisions=60, round=1, pref=controlnet_video2video_prefs, key='prompt_strength', tooltip=\"How much influence the prompt has on the output. Guidance Scale.\")\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    canny_threshold.height = None if controlnet_video2video_prefs['control_task'] == \"Canny Map Edge\" else 0\n",
        "    mlsd_score_thr = SliderRow(label=\"MLSD Score Threshold\", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_score_thr', expand=True, col={'lg':6})\n",
        "    mlsd_dist_thr = SliderRow(label=\"MLSD Dist Threshold\", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_dist_thr', expand=True, col={'lg':6})\n",
        "    mlsd_threshold = Container(ResponsiveRow([mlsd_score_thr, mlsd_dist_thr]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == \"MLSD\" else 0\n",
        "    motion_alpha = SliderRow(label=\"Motion Alpha\", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_alpha', expand=True, col={'lg':6}, tooltip=\"Smooth the motion vectors over time, 0.0 is no smoothing, 1.0 is maximum smoothing.\")\n",
        "    motion_sigma = SliderRow(label=\"Motion Sigma\", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_sigma', expand=True, col={'lg':6}, tooltip=\"Smooth the motion estimate spatially, 0.0 is no smoothing, used as sigma for gaussian blur.\")\n",
        "    max_dimension = SliderRow(label=\"Max Dimension\", min=256, max=1280, divisions=32, multiple=32, suffix=\"px\", pref=controlnet_video2video_prefs, key='max_dimension', expand=True, col={'lg':6})\n",
        "    min_dimension = SliderRow(label=\"Mix Dimension\", min=256, max=1280, divisions=32, multiple=32, suffix=\"px\", pref=controlnet_video2video_prefs, key='min_dimension', expand=True, col={'lg':6})\n",
        "    color_fix = Dropdown(label=\"Color Fix\", width=150, options=[dropdown.Option(\"None\"), dropdown.Option(\"RGB\"), dropdown.Option(\"HSV\"), dropdown.Option(\"Lab\")], value=controlnet_video2video_prefs['color_fix'], on_change=lambda e:changed(e,'color_fix'), tooltip=\"Prevent color from drifting due to feedback and model bias by fixing the histogram to the first frame. Specify colorspace for histogram matching\")\n",
        "    color_amount = SliderRow(label=\"Color Amount\", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='color_amount', expand=True, tooltip=\"Blend between the original color and the color matched version.\")\n",
        "    #max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_video2video_prefs, key='max_size')\n",
        "    no_audio = Switcher(label=\"No Audio\", value=controlnet_video2video_prefs['no_audio'], tooltip=\"Don't include audio in the output video, even if the input video has audio\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'no_audio'))\n",
        "    skip_dumped_frames = Switcher(label=\"Skip Dumped Frames\", value=controlnet_video2video_prefs['skip_dumped_frames'], tooltip=\"Read dumped frames from a previous run instead of processing the input video.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'skip_dumped_frames'))\n",
        "    save_frames = Switcher(label=\"Save all Frames\", value=controlnet_video2video_prefs['save_frames'], tooltip=\"Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))\n",
        "    fix_orientation = Switcher(label=\"Fix Orientation\", value=controlnet_video2video_prefs['fix_orientation'], tooltip=\"Resize videos shot in portrait mode on some devices to fix incorrect aspect ratio bug.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'fix_orientation'))\n",
        "    show_console = Switcher(label=\"Show Console Output\", value=controlnet_video2video_prefs['show_console'], tooltip=\"Outputs the progress run log in the console window. Gets messy, but useful.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_console'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=controlnet_video2video_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    output_name = TextField(label=\"Output Name\", value=controlnet_video2video_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_video2video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_video2video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_video2video_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_video2video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_video2video_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_video2video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§™  ControlNet Video2Video\", \"Apply Stable Diffusion to a video, while maintaining frame-to-frame consistency with motion estimator & compensator...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNet Vid2Vid Settings\", on_click=controlnet_video2video_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([control_task, init_video]),\n",
        "        canny_threshold,\n",
        "        mlsd_threshold,\n",
        "        vid_params,\n",
        "        controlnet_strength,\n",
        "        init_image_strength,\n",
        "        feedthrough_strength,\n",
        "        prompt_strength,\n",
        "        num_inference_row,\n",
        "        Row([color_fix, color_amount]),\n",
        "        ResponsiveRow([motion_alpha, motion_sigma]),\n",
        "        ResponsiveRow([max_dimension, min_dimension]),\n",
        "        Row([no_audio, skip_dumped_frames, save_frames, fix_orientation, show_console]),\n",
        "        Row([output_name, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üçÉ  Run ControlNet Vid2Vid\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_video2video(page))]),\n",
        "        page.controlnet_video2video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "deepfloyd_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 10,\n",
        "    'image_strength': 0.7,\n",
        "    'superres_num_inference_steps': 50,\n",
        "    'superres_guidance_scale': 4,\n",
        "    'upscale_num_inference_steps': 75,\n",
        "    'upscale_guidance_scale': 9,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'model_size': 'L (900M)',\n",
        "    'apply_watermark': True,\n",
        "    'low_memory': True,\n",
        "    'keep_pipelines': False,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'IF-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDeepFloyd(page):\n",
        "    global deepfloyd_prefs, prefs, pipe_deepfloyd\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            deepfloyd_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            deepfloyd_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            deepfloyd_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_deepfloyd_output(o):\n",
        "      page.deepfloyd_output.controls.append(o)\n",
        "      page.deepfloyd_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.deepfloyd_output.controls = []\n",
        "      page.deepfloyd_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def deepfloyd_help(e):\n",
        "      def close_deepfloyd_dlg(e):\n",
        "        nonlocal deepfloyd_help_dlg\n",
        "        deepfloyd_help_dlg.open = False\n",
        "        page.update()\n",
        "      deepfloyd_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DeepFloyd-IF\"), content=Column([\n",
        "          Markdown(\"**You must accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) before using.**\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text('DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. Built by the team behind ruDALL-E (the Russian-language version of OpenAI\\'s DALL-E algorithm), inspired by Google\\'s \"Imagen\", and backed by the company behind Stable Diffusion, DeepFloyd\\'s IF outperforms all of those algorithms. DeepFloyd IF is particularly good at understanding complex prompts and relationships between objects. It is also very good at inserting legible text into images - even more so than Stable Diffusion XL. It can even understand prompts in multiple languages. IF, or \"Intelligent Fiction\", is a text2image generator that is designed to create text and captions in the images in response to a prompt. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:'),\n",
        "          Markdown(\"\"\"* Stage 1: a base model that generates 64x64 px image based on text prompt,\n",
        "* Stage 2: a 64x64 px => 256x256 px super-resolution model, and a\n",
        "* Stage 3: a 256x256 px => 1024x1024 px super-resolution model Stage 1 and Stage 2 utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. Stage 3 is Stability's x4 Upscaling model. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòå  Let's go Deep... \", on_click=close_deepfloyd_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = deepfloyd_help_dlg\n",
        "      deepfloyd_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          deepfloyd_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          deepfloyd_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          deepfloyd_prefs['init_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          deepfloyd_prefs['mask_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Mask Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        deepfloyd_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {deepfloyd_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    init_image = FileInput(label=\"Original Image (optional)\", pref=deepfloyd_prefs, key='init_image', expand=True, page=page)\n",
        "    #init_image = TextField(label=\"Original Image (optional)\", value=deepfloyd_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=deepfloyd_prefs, key='mask_image', expand=1, page=page)\n",
        "    #mask_image = TextField(label=\"Mask Image (optional)\", value=deepfloyd_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=deepfloyd_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=deepfloyd_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "\n",
        "    prompt = TextField(label=\"Prompt Text\", value=deepfloyd_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=deepfloyd_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(deepfloyd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='guidance_scale')\n",
        "\n",
        "    superres_num_inference_row = SliderRow(label=\"Super Res Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='superres_num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    superres_guidance = SliderRow(label=\"Super Res Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='superres_guidance_scale')\n",
        "    upscale_num_inference_row = SliderRow(label=\"Upscale Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='upscale_num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    upscale_guidance = SliderRow(label=\"Upscale Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='upscale_guidance_scale')\n",
        "    image_strength = SliderRow(label=\"Image Strength\", min=0, max=1, divisions=20, round=2, pref=deepfloyd_prefs, key='image_strength', tooltip=\"Conceptually, indicates how much to transform the reference `image`. Denoising steps depends on the amount of noise initially added.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(deepfloyd_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label=\"{value}\", value=float(deepfloyd_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {deepfloyd_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    #page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=deepfloyd_prefs, key='max_size', tooltip=\"Resizes your Init and Mask Image to save memory.\")\n",
        "    apply_watermark = Tooltip(message=\"Under the license, you are legally required to include the watermark on bottom right corner.\", content=Switcher(label=\"Apply IF Watermark\", value=deepfloyd_prefs['apply_watermark'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'apply_watermark')))\n",
        "    low_memory = Tooltip(message=\"Needed for < 16GB VRAM to run. If you have more power, disable for faster runs.\", content=Switcher(label=\"Lower Memory\", value=deepfloyd_prefs['low_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'low_memory')))\n",
        "    keep_pipelines = Tooltip(message=\"Keeps all 3 Pipelines loaded persistantly. Faster reuse, but needs a lot of VRAM.\", content=Switcher(label=\"Keep Pipelines Loaded\", value=deepfloyd_prefs['keep_pipelines'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'keep_pipelines')))\n",
        "    model_size = Dropdown(label=\"Model Size\", hint_text=\"Amount of Parameters trained on. Depends on your available memory.\", width=240, options=[dropdown.Option(\"XL (4.3B)\"), dropdown.Option(\"L (900M)\"), dropdown.Option(\"M (400M)\")], value=deepfloyd_prefs['model_size'], autofocus=False, on_change=lambda e:changed(e, 'model_size'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=deepfloyd_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=deepfloyd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=deepfloyd_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=deepfloyd_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=deepfloyd_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deepfloyd = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deepfloyd.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.deepfloyd_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.deepfloyd_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåà  DeepFloyd IF (under construction, may not work)\", \"A new AI image generator that achieves state-of-the-art results on numerous image-generation tasks...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with IF-DeepFloyd Settings\", on_click=deepfloyd_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        image_strength,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        superres_num_inference_row,\n",
        "        superres_guidance,\n",
        "        upscale_num_inference_row,\n",
        "        upscale_guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([apply_watermark, low_memory, keep_pipelines, model_size]),\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=deepfloyd_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_deepfloyd,\n",
        "        Row([ElevatedButton(content=Text(\"üéà  Run DeepFloyd\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page)),\n",
        "             run_prompt_list]),\n",
        "        page.deepfloyd_output,\n",
        "        clear_button,\n",
        "      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "amused_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"amused-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\": 10.0,\n",
        "    'num_inference_steps': 12,\n",
        "    \"seed\": 0,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'init_image_strength': 0.8,\n",
        "    \"cpu_offload\": False,\n",
        "    \"cpu_only\": False,\n",
        "    \"amused_model\": \"amused-256\",\n",
        "    \"custom_model\": \"\",\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildAmused(page):\n",
        "    global prefs, amused_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            amused_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            amused_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            amused_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def amused_help(e):\n",
        "      def close_amused_dlg(e):\n",
        "        nonlocal amused_help_dlg\n",
        "        amused_help_dlg.open = False\n",
        "        page.update()\n",
        "      amused_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Amused Pipeline\"), content=Column([\n",
        "          Text(\"Amused is a lightweight text to image model based off of the muse architecture. Amused is particularly useful in applications that require a lightweight and fast model such as generating many images quickly at once. Amused is a vqvae token based transformer that can generate an image in fewer forward passes than many diffusion models. In contrast with muse, it uses the smaller text encoder CLIP-L/14 instead of t5-xxl. Due to its small parameter count and few forward pass generation process, amused can generate many images quickly. This benefit is seen particularly at larger batch sizes.\"),\n",
        "          Text(\"Muse is a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality, etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing.\"),\n",
        "          Markdown(\"[Project](https://muse-model.github.io) | [Paper](https://huggingface.co/papers/2401.01808) | [HuggingFace Model](https://huggingface.co/amused/amused-256)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"Contributors include Suraj Patil, William Berman, Patrick von Platen, Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li and Dilip Krishnan.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üé¢  Very Amusing...\", on_click=close_amused_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = amused_help_dlg\n",
        "      amused_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        amused_prefs['amused_model'] = e.control.value\n",
        "        amused_custom_model.visible = e.control.value == \"Custom\"\n",
        "        amused_custom_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        amused_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=amused_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=amused_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = FileInput(label=\"Init Image (optional)\", pref=amused_prefs, key='init_image', page=page, col={'md':6})\n",
        "    mask_image = FileInput(label=\"Mask Image (optional)\", pref=amused_prefs, key='mask_image', page=page, col={'md':6})\n",
        "    init_image_strength = SliderRow(label=\"Init-Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=amused_prefs, key='init_image_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=amused_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=amused_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=20, step=1, value=amused_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=40, divisions=40, pref=amused_prefs, key='num_inference_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=amused_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=amused_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=amused_prefs, key='height')\n",
        "    amused_model = Dropdown(label=\"Amused Model\", width=220, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"amused-256\"), dropdown.Option(\"amused-512\")], value=amused_prefs['amused_model'], on_change=changed_model)\n",
        "    amused_custom_model = TextField(label=\"Custom Amused Model (URL or Path)\", value=amused_prefs['custom_model'], expand=True, visible=amused_prefs['amused_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=amused_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    #cpu_only = Switcher(label=\"CPU Only (not yet)\", value=amused_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip=\"If you don't have a good GPU, can run entirely on CPU\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(amused_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=amused_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=amused_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=amused_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=amused_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_amused = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_amused.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not amused_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üé†   Run aMUSEd\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.amused_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üé°  Amused Open-MUSE\", \"Lightweight and Fast vqVAE Masked Generative Transformer Model to make many images quickly at once...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Amused Settings\", on_click=amused_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            ResponsiveRow([init_image, mask_image]),\n",
        "            init_image_strength,\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            Row([amused_model, amused_custom_model]),\n",
        "            #Row([cpu_offload, cpu_only]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_amused,\n",
        "            parameters_row,\n",
        "            page.amused_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "wuerstchen_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"wuerstchen-\",\n",
        "    \"num_images\": 1,\n",
        "    \"steps\":12,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\":4,\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'prior_steps': 60,\n",
        "    \"seed\": 0,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildWuerstchen(page):\n",
        "    global prefs, wuerstchen_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            wuerstchen_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            wuerstchen_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            wuerstchen_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def wuerstchen_help(e):\n",
        "      def close_wuerstchen_dlg(e):\n",
        "        nonlocal wuerstchen_help_dlg\n",
        "        wuerstchen_help_dlg.open = False\n",
        "        page.update()\n",
        "      wuerstchen_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with W√ºrstchen Pipeline\"), content=Column([\n",
        "          Text(\"W√ºrstchen: Efficient Pretraining of Text-to-Image Models is by Pablo Pernias, Dominic Rampas, and Marc Aubreville.\"),\n",
        "          Text(\"We introduce W√ºrstchen, a novel technique for text-to-image synthesis that unites competitive performance with unprecedented cost-effectiveness and ease of training on constrained hardware. Building on recent advancements in machine learning, our approach, which utilizes latent diffusion strategies at strong latent image compression rates, significantly reduces the computational burden, typically associated with state-of-the-art models, while preserving, if not enhancing, the quality of generated images. Wuerstchen achieves notable speed improvements at inference time, thereby rendering real-time applications more viable. One of the key advantages of our method lies in its modest training requirements of only 9,200 GPU hours, slashing the usual costs significantly without compromising the end performance. In a comparison against the state-of-the-art, we found the approach to yield strong competitiveness. This paper opens the door to a new line of research that prioritizes both performance and computational accessibility, hence democratizing the use of sophisticated AI technologies. Through Wuerstchen, we demonstrate a compelling stride forward in the realm of text-to-image synthesis, offering an innovative path to explore in future research.\"),\n",
        "          Markdown(\"[Paper](https://huggingface.co/papers/2306.00637) | [Original GitHub](https://github.com/dome272/Wuerstchen)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üêó  Some Br√§twurst? \", on_click=close_wuerstchen_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = wuerstchen_help_dlg\n",
        "      wuerstchen_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        wuerstchen_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=wuerstchen_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=wuerstchen_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=wuerstchen_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=wuerstchen_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    steps = TextField(label=\"Number of Steps\", value=wuerstchen_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=wuerstchen_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=wuerstchen_prefs, key='steps')\n",
        "    prior_guidance_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=20, round=1, expand=True, pref=wuerstchen_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, expand=True, pref=wuerstchen_prefs, key='prior_steps', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=wuerstchen_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=wuerstchen_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=wuerstchen_prefs, key='height')\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(wuerstchen_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=wuerstchen_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=wuerstchen_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=wuerstchen_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=wuerstchen_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_wuerstchen = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_wuerstchen.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not wuerstchen_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üê∑   Run W√ºrstchen\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.wuerstchen_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üå≠  W√ºrstchen\", \"Text-to-Image Synthesis uniting competitive performance, cost-effectiveness and ease of training on constrained hardware.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with W√ºrstchen Settings\", on_click=wuerstchen_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            ResponsiveRow([prior_steps, prior_guidance_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_wuerstchen,\n",
        "            parameters_row,\n",
        "            page.wuerstchen_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "pixart_alpha_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"pixart-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\":4.5,\n",
        "    'num_inference_steps': 30,\n",
        "    \"seed\": 0,\n",
        "    \"clean_caption\": True,\n",
        "    \"resolution_binning\": True,\n",
        "    #\"mask_feature\": True,\n",
        "    \"cpu_offload\": True,\n",
        "    \"use_8bit\": False,\n",
        "    \"pixart_model\": \"PixArt-XL-2-1024-MS\",\n",
        "    \"custom_model\": \"\",\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildPixArtAlpha(page):\n",
        "    global prefs, pixart_alpha_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            pixart_alpha_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            pixart_alpha_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            pixart_alpha_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pixart_alpha_help(e):\n",
        "      def close_pixart_alpha_dlg(e):\n",
        "        nonlocal pixart_alpha_help_dlg\n",
        "        pixart_alpha_help_dlg.open = False\n",
        "        page.update()\n",
        "      pixart_alpha_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with PixArt-Œ± Pipeline\"), content=Column([\n",
        "          Text(\"The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-Œ±, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-Œ±'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-Œ± only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly $300,000 ($26,000 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-Œ± excels in image quality, artistry, and semantic control. We hope PIXART-Œ± will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.\"),\n",
        "          Text(\"It uses a Transformer backbone (instead of a UNet) for denoising. As such it has a similar architecture as DiT. It was trained using text conditions computed from T5. This aspect makes the pipeline better at following complex text prompts with intricate details. It is good at producing high-resolution images at different aspect ratios. It rivals the quality of state-of-the-art text-to-image generation systems (as of this writing) such as Stable Diffusion XL, Imagen, and DALL-E 2, while being more efficient than them.\"),\n",
        "          Markdown(\"[Paper](https://huggingface.co/papers/2310.00426) | [PixArt-alpha GitHub](https://github.com/PixArt-alpha/PixArt-alpha) | [PixArt-alpha Checkpoints](https://huggingface.co/PixArt-alpha) | [Recomended Sizes](https://github.com/PixArt-alpha/PixArt-alpha/blob/master/diffusion/data/datasets/utils.py)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üêó  Pix that Art \", on_click=close_pixart_alpha_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = pixart_alpha_help_dlg\n",
        "      pixart_alpha_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        pixart_alpha_prefs['pixart_model'] = e.control.value\n",
        "        pixart_custom_model.visible = e.control.value == \"Custom\"\n",
        "        pixart_custom_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        pixart_alpha_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=pixart_alpha_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=pixart_alpha_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=pixart_alpha_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=pixart_alpha_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_inference_steps = TextField(label=\"Number of Steps\", value=pixart_alpha_prefs['num_inference_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=pixart_alpha_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=pixart_alpha_prefs, key='num_inference_steps')\n",
        "    #prior_guidance_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=20, round=1, expand=True, pref=pixart_alpha_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=pixart_alpha_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=pixart_alpha_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=pixart_alpha_prefs, key='height')\n",
        "    pixart_model = Dropdown(label=\"PixArt-Œ± Model\", width=230, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"PixArt-XL-2-1024-MS\"), dropdown.Option(\"PixArt-XL-2-512x512\"), dropdown.Option(\"PixArt-LCM-XL-2-1024-MS\")], value=pixart_alpha_prefs['pixart_model'], on_change=changed_model)\n",
        "    pixart_custom_model = TextField(label=\"Custom PixArt-Œ± Model (URL or Path)\", value=pixart_alpha_prefs['custom_model'], expand=True, visible=pixart_alpha_prefs['pixart_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    clean_caption = Switcher(label=\"Clean Caption\", value=pixart_alpha_prefs['clean_caption'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'clean_caption'), tooltip=\"Whether or not to clean the caption before creating embeddings.\")\n",
        "    resolution_binning = Switcher(label=\"Resolution Binning\", value=pixart_alpha_prefs['resolution_binning'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'resolution_binning'), tooltip=\"The requested height and width are first mapped to the closest resolutions using `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to the requested resolution. Useful for generating non-square images.\")\n",
        "    #mask_feature = Switcher(label=\"Feature Mask\", value=pixart_alpha_prefs['mask_feature'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'mask_feature'), tooltip=\"If enabled, the text embeddings will be masked.\")\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=pixart_alpha_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    use_8bit = Switcher(label=\"Use 8-bit Precision\", value=pixart_alpha_prefs['use_8bit'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_8bit'), tooltip=\"Runs with under 8GB VRAM by loading the text encoder in 8-bit numerical precision. Reduces quality & loads slower.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(pixart_alpha_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=pixart_alpha_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=pixart_alpha_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=pixart_alpha_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=pixart_alpha_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_pixart_alpha = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_pixart_alpha.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not pixart_alpha_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üèÇ   Run PixArt-Œ±\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.pixart_alpha_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üßö  PixArt-Œ±lpha\", \"Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis... Note: Uses a lot of RAM & Space, may run out.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with PixArt-Œ± Settings\", on_click=pixart_alpha_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            #ResponsiveRow([num_inference_steps]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            Row([pixart_model, pixart_custom_model]),\n",
        "            Row([clean_caption, resolution_binning, cpu_offload, use_8bit]),\n",
        "            #Can't get wrap to work!! Container(Row([Container(clean_caption), Container(resolution_binning), Container(cpu_offload), Container(use_8bit)], wrap=True, expand=True, width=page.width, alignment=ft.MainAxisAlignment.START), width=800),#], expand=True),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_pixart_alpha,\n",
        "            parameters_row,\n",
        "            page.pixart_alpha_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "  \n",
        "lmd_plus_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"lmd-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'gligen_scheduled_sampling_beta': 0.4,\n",
        "    'AI_engine': 'ChatGPT-3.5 Turbo',\n",
        "    \"temperature\": 0.7,\n",
        "    \"seed\": 0,\n",
        "    'init_image': '',\n",
        "    \"cpu_offload\": False,\n",
        "    \"lmd_plus_model\": \"longlian/lmd_plus\",\n",
        "    \"custom_model\": \"\",\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildLMD_Plus(page):\n",
        "    global prefs, lmd_plus_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            lmd_plus_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            lmd_plus_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            lmd_plus_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def lmd_plus_help(e):\n",
        "      def close_lmd_plus_dlg(e):\n",
        "        nonlocal lmd_plus_help_dlg\n",
        "        lmd_plus_help_dlg.open = False\n",
        "        page.update()\n",
        "      lmd_plus_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with LMD+ Pipeline\"), content=Column([\n",
        "          Text(\"LMD+ greatly improves the prompt understanding ability of text-to-image generation models by introducing an LLM as a front-end prompt parser and layout planner. It improves spatial reasoning, the understanding of negation, attribute binding, generative numeracy, etc. in a unified manner without explicitly aiming for each. LMD is completely training-free (i.e., uses SD model off-the-shelf). LMD+ takes in additional adapters for better control. This is a reproduction of LMD+ model used in our work.\"),\n",
        "          Text(\"We provide a parser that parses LLM outputs to the layouts. You can obtain the prompt to input to the LLM for layout generation\"),\n",
        "          Markdown(\"[Paper](https://arxiv.org/pdf/2305.13655.pdf) | [Project Page](https://llm-grounded-diffusion.github.io/) | [GitHub](https://github.com/TonyLianLong/LLM-groundedDiffusion)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üêó  Understandable \", on_click=close_lmd_plus_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = lmd_plus_help_dlg\n",
        "      lmd_plus_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        lmd_plus_prefs['lmd_plus_model'] = e.control.value\n",
        "        lmd_plus_custom_model.visible = e.control.value == \"Custom\"\n",
        "        lmd_plus_custom_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        lmd_plus_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=lmd_plus_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=lmd_plus_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=lmd_plus_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=lmd_plus_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_inference_steps = TextField(label=\"Number of Steps\", value=lmd_plus_prefs['num_inference_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=lmd_plus_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=lmd_plus_prefs, key='num_inference_steps')\n",
        "    #prior_guidance_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=20, round=1, expand=True, pref=lmd_plus_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=lmd_plus_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=lmd_plus_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=lmd_plus_prefs, key='height')\n",
        "    lmd_plus_model = Dropdown(label=\"LMD+ Model\", width=250, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"longlian/lmd_plus\")], value=lmd_plus_prefs['lmd_plus_model'], on_change=changed_model)\n",
        "    lmd_plus_custom_model = TextField(label=\"Custom LMD_Plus Model (URL or Path)\", value=lmd_plus_prefs['custom_model'], expand=True, visible=lmd_plus_prefs['lmd_plus_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"OpenAI GPT-4\"), dropdown.Option(\"GPT-4 Turbo\"), dropdown.Option(\"Google Gemini\")], value=lmd_plus_prefs['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    temperature = SliderRow(label=\"AI Temperature\", min=0, max=1, divisions=10, round=1, expand=True, pref=lmd_plus_prefs, key='temperature', tooltip=\"Softmax value used to module the next token probabilities\", col={'lg':6})\n",
        "    gligen_scheduled_sampling_beta = SliderRow(label=\"Gligen Scheduled Sampling Beta\", min=0, max=1, divisions=10, round=1, pref=lmd_plus_prefs, key='gligen_scheduled_sampling_beta', tooltip=\"Scheduled Sampling factor from GLIGEN: Open-Set Grounded Text-to-Image Generation. Scheduled Sampling factor is only varied for scheduled sampling during inference for improved quality and controllability.\", col={'lg':6})\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=lmd_plus_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(lmd_plus_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=lmd_plus_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=lmd_plus_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=lmd_plus_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=lmd_plus_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lmd_plus = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lmd_plus.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not lmd_plus_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"ü¶î   Run LMD+\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.lmd_plus_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üêÜ  LMD+ LLM-grounded Diffusion\", \"Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LMD_Plus Settings\", on_click=lmd_plus_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            #ResponsiveRow([num_inference_steps]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            gligen_scheduled_sampling_beta,\n",
        "            Row([AI_engine, temperature]),\n",
        "            Row([lmd_plus_model, lmd_plus_custom_model, cpu_offload]),\n",
        "            #Row([clean_caption, mask_feature, cpu_offload]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_lmd_plus,\n",
        "            parameters_row,\n",
        "            page.lmd_plus_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "lcm_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"lcm-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\":4.5,\n",
        "    'num_inference_steps': 8,\n",
        "    \"seed\": 0,\n",
        "    'init_image': '',\n",
        "    'init_image_strength': 0.8,\n",
        "    \"cpu_offload\": False,\n",
        "    \"cpu_only\": False,\n",
        "    \"lcm_model\": \"LCM_Dreamshaper_v7\",\n",
        "    \"custom_model\": \"\",\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_model': 'SD v1.5',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildLCM(page):\n",
        "    global prefs, lcm_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            lcm_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            lcm_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            lcm_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def lcm_help(e):\n",
        "      def close_lcm_dlg(e):\n",
        "        nonlocal lcm_help_dlg\n",
        "        lcm_help_dlg.open = False\n",
        "        page.update()\n",
        "      lcm_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with LCM Pipeline\"), content=Column([\n",
        "          Text(\"Latent Diffusion Models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference.\"),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"[Project](https://latent-consistency-models.github.io/) | [Paper](https://arxiv.org/pdf/2310.04378.pdf) | [SimianLuo/LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7) | [Checkpoint](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üèÉ  How many steps?\", on_click=close_lcm_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = lcm_help_dlg\n",
        "      lcm_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        lcm_prefs['lcm_model'] = e.control.value\n",
        "        lcm_custom_model.visible = e.control.value == \"Custom\"\n",
        "        lcm_custom_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        lcm_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=lcm_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=lcm_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = FileInput(label=\"Init Image (optional)\", pref=lcm_prefs, key='init_image', page=page, col={'md':6})\n",
        "    init_image_strength = SliderRow(label=\"Init-Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=lcm_prefs, key='init_image_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=lcm_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=lcm_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=lcm_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=40, divisions=40, pref=lcm_prefs, key='num_inference_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=lcm_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=lcm_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=lcm_prefs, key='height')\n",
        "    def toggle_ip_adapter(e):\n",
        "        lcm_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_model.visible = e.control.value\n",
        "        ip_adapter_model.update()\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=lcm_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip=\"Uses both image and text to condition the image generation process.\")\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=lcm_prefs['ip_adapter_model'], visible=lcm_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=lcm_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=lcm_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if lcm_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "    lcm_model = Dropdown(label=\"LCM Model\", width=220, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"LCM_Dreamshaper_v7\"), dropdown.Option(\"LCM_Dreamshaper_v8\")], value=lcm_prefs['lcm_model'], on_change=changed_model)\n",
        "    lcm_custom_model = TextField(label=\"Custom LCM Model (URL or Path)\", value=lcm_prefs['custom_model'], expand=True, visible=lcm_prefs['lcm_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=lcm_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    cpu_only = Switcher(label=\"CPU Only (not yet)\", value=lcm_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip=\"If you don't have a good GPU, can run entirely on CPU\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(lcm_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=lcm_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=lcm_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=lcm_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=lcm_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lcm = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lcm.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not lcm_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üèé   Run LCM\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.lcm_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üíª  Latent Consistency Model (LCM)\", \"Synthesizing High-Resolution Images with Few-Step Inference.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LCM Settings\", on_click=lcm_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            ResponsiveRow([init_image, init_image_strength]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            Row([lcm_model, lcm_custom_model]),\n",
        "            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "            ip_adapter_container,\n",
        "            Row([cpu_offload, cpu_only]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_lcm,\n",
        "            parameters_row,\n",
        "            page.lcm_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "lcm_interpolation_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"lcm-\",\n",
        "    \"num_images\": 1,\n",
        "    \"mixes\": [],\n",
        "    \"num_interpolation_steps\": 60,\n",
        "    \"steps\":8,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":8,\n",
        "    'process_batch_size': 4,\n",
        "    \"embedding_interpolation_type\": \"lerp\",\n",
        "    \"latent_interpolation_type\": \"slerp\",\n",
        "    \"save_video\": False,\n",
        "    \"interpolate_video\": True,\n",
        "    \"source_fps\": 8,\n",
        "    \"target_fps\": 24,\n",
        "    \"seed\": 0,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildLCMInterpolation(page):\n",
        "    global prefs, lcm_interpolation_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            lcm_interpolation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            lcm_interpolation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            lcm_interpolation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        lcm_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def lcm_interpolation_help(e):\n",
        "      def close_lcm_interpolation_dlg(e):\n",
        "        nonlocal lcm_interpolation_help_dlg\n",
        "        lcm_interpolation_help_dlg.open = False\n",
        "        page.update()\n",
        "      lcm_interpolation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with LCM Interpolation Pipeline\"), content=Column([\n",
        "          Text(\"This pipeline extends the Latent Consistency Pipeline to allow for interpolation of the latent space between multiple prompts. It is similar to the Stable Diffusion Interpolate and unCLIP Interpolate community pipelines.\"),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üî≤  The Space Between... \", on_click=close_lcm_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = lcm_interpolation_help_dlg\n",
        "      lcm_interpolation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def add_prompt(e):\n",
        "        if not bool(lcm_interpolation_prefs['prompt']): return\n",
        "        layer = {'prompt': lcm_interpolation_prefs['prompt']}\n",
        "        lcm_interpolation_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD)], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Text Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Text Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        lcm_interpolation_prefs['prompt'] = \"\"\n",
        "        prompt.value = \"\"\n",
        "        prompt.update()\n",
        "    def delete_layer(e):\n",
        "        lcm_interpolation_prefs['mixes'].remove(e.control.data)\n",
        "        for c in fuse_layers.controls:\n",
        "          if c.data['prompt'] == e.control.data['prompt']:\n",
        "              fuse_layers.controls.remove(c)\n",
        "              break\n",
        "        fuse_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        lcm_interpolation_prefs['mixes'].clear()\n",
        "        fuse_layers.controls.clear()\n",
        "        fuse_layers.update()\n",
        "    def move_down(e):\n",
        "        idx = lcm_interpolation_prefs['mixes'].index(e.control.data)\n",
        "        if idx < (len(lcm_interpolation_prefs['mixes']) - 1):\n",
        "          d = lcm_interpolation_prefs['mixes'].pop(idx)\n",
        "          lcm_interpolation_prefs['mixes'].insert(idx+1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx+1, dr)\n",
        "          fuse_layers.update()\n",
        "    def move_up(e):\n",
        "        idx = lcm_interpolation_prefs['mixes'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = lcm_interpolation_prefs['mixes'].pop(idx)\n",
        "          lcm_interpolation_prefs['mixes'].insert(idx-1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx-1, dr)\n",
        "          fuse_layers.update()\n",
        "    def edit_layer(e):\n",
        "        data = e.control.data\n",
        "        prompt_value = data[\"prompt\"]\n",
        "        image_value = \"\"\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_layer(e):\n",
        "            layer = None\n",
        "            for l in lcm_interpolation_prefs['mixes']:\n",
        "                if data[\"prompt\"] == l[\"prompt\"]:\n",
        "                    layer = l\n",
        "                    layer['prompt'] = prompt_text.value\n",
        "                    break\n",
        "            for c in fuse_layers.controls:\n",
        "                if 'prompt' not in data: continue\n",
        "                if c.data['prompt'] == data['prompt']:\n",
        "                    c.title.controls[0].value = layer['prompt']\n",
        "                    c.update()\n",
        "                    break\n",
        "            layer['prompt'] = prompt_text.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        prompt_text = TextField(label=\"Interpolation Prompt Text\", value=prompt_value, multiline=True)\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit Interpolation Prompt\"), content=Container(Column([prompt_text], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window_width) - 100)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Layer \", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def toggle_video(e):\n",
        "        lcm_interpolation_prefs['save_video'] = e.control.value\n",
        "        video_container.visible = lcm_interpolation_prefs['save_video']\n",
        "        video_container.update()\n",
        "    add_prompt_btn = ft.FilledButton(\"‚ûï Add Prompt\", width=150, on_click=add_prompt)\n",
        "    prompt = TextField(label=\"Interpolation Prompt Text\", value=lcm_interpolation_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))\n",
        "    prompt_row = Row([prompt, add_prompt_btn])\n",
        "    fuse_layers = Column([], spacing=0)\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=lcm_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=lcm_interpolation_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    num_interpolation_steps = SliderRow(label=\"Interpolation Steps\", min=0, max=200, divisions=200, pref=lcm_interpolation_prefs, key='num_interpolation_steps', tooltip=\"Number of Image Frames between each Prompt to Interpolate.\")\n",
        "    process_batch_size = SliderRow(label=\"Process Batch Size\", min=0, max=20, divisions=20, pref=lcm_interpolation_prefs, key='process_batch_size', tooltip=\"The batch size to use for processing the images. This is useful when generating a large number of images and you want to avoid running out of memory.\")\n",
        "    steps = SliderRow(label=\"Number of Inference Steps\", min=0, max=40, divisions=40, pref=lcm_interpolation_prefs, key='steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=lcm_interpolation_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=lcm_interpolation_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=lcm_interpolation_prefs, key='height')\n",
        "    embedding_interpolation_type = Dropdown(label=\"Embedding Interpolation Type\", tooltip=\"The type of interpolation to use for interpolating between text embeddings.\", width=220, options=[dropdown.Option(\"lerp\"), dropdown.Option(\"slerp\")], value=lcm_interpolation_prefs['embedding_interpolation_type'], autofocus=False, on_change=lambda e:changed(e, 'embedding_interpolation_type'))\n",
        "    latent_interpolation_type = Dropdown(label=\"Latent Interpolation Type\", tooltip=\"The type of interpolation to use for interpolating between latents.\", width=220, options=[dropdown.Option(\"lerp\"), dropdown.Option(\"slerp\")], value=lcm_interpolation_prefs['latent_interpolation_type'], autofocus=False, on_change=lambda e:changed(e, 'latent_interpolation_type'))\n",
        "    save_video = Switcher(label=\"Save Video\", value=lcm_interpolation_prefs['save_video'], on_change=toggle_video)\n",
        "    interpolate_vid = Switcher(label=\"Interpolate\", value=lcm_interpolation_prefs['interpolate_video'], tooltip=\"Use Google FiLM Interpolation to transition between frames.\", on_change=lambda e:changed(e,'interpolate_video'))\n",
        "    source_fps = SliderRow(label=\"Source FPS\", min=0, max=30, suffix=\"fps\", divisions=30, expand=1, pref=lcm_interpolation_prefs, key='source_fps')\n",
        "    target_fps = SliderRow(label=\"Target FPS\", min=0, max=30, suffix=\"fps\", divisions=30, expand=1, pref=lcm_interpolation_prefs, key='target_fps')\n",
        "    video_container = Container(content=Row([interpolate_vid, source_fps, target_fps], expand=True), expand=True, visible=lcm_interpolation_prefs['save_video'])\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(lcm_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=lcm_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=lcm_interpolation_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=lcm_interpolation_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=lcm_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lcm_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_lcm_interpolation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not lcm_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üß∂   Run LCM Interpolation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm_interpolation(page))\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.lcm_interpolation_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üë™  LCM Interpolation\", \"Transition the Latent Consistancy between multiple text prompts... Good fast results in only 4 Steps!\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LCM Settings\", on_click=lcm_interpolation_help)]),\n",
        "            prompt_row,\n",
        "            fuse_layers,\n",
        "            Divider(height=5, thickness=4),\n",
        "            num_interpolation_steps,\n",
        "            process_batch_size,\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider,\n",
        "            Row([embedding_interpolation_type, latent_interpolation_type]),\n",
        "            Row([save_video, video_container]),\n",
        "            ResponsiveRow([Row([seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_lcm_interpolation,\n",
        "            parameters_row,\n",
        "            page.lcm_interpolation_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "ldm3d_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"ldm3d-\",\n",
        "    \"num_images\": 1,\n",
        "    \"width\": 1024,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":5.0,\n",
        "    'num_inference_steps': 50,\n",
        "    \"seed\": 0,\n",
        "    \"cpu_offload\": True,\n",
        "    \"use_upscale\": False,\n",
        "    \"ldm3d_model\": \"Intel/ldm3d-4c\",\n",
        "    \"custom_model\": \"\",\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_model': 'SD v1.5',\n",
        "    'ip_adapter_strength': 0.8,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildLDM3D(page):\n",
        "    global prefs, ldm3d_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            ldm3d_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            ldm3d_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            ldm3d_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def ldm3d_help(e):\n",
        "      def close_ldm3d_dlg(e):\n",
        "        nonlocal ldm3d_help_dlg\n",
        "        ldm3d_help_dlg.open = False\n",
        "        page.update()\n",
        "      ldm3d_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with LDM3D Pipeline\"), content=Column([\n",
        "          Text(\"A Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences.\"),\n",
        "          Markdown(\"You can use this model to generate RGB and depth map given a text prompt. A short video summarizing the approach can be found at [this url](https://t.ly/tdi2) and a VR demo can be found [here](https://www.youtube.com/watch?v=3hbUo-hwAs0). A demo is also accessible on [Spaces](https://huggingface.co/spaces/Intel/ldm3d)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"The LDM3D model was proposed in [LDM3D: Latent Diffusion Model for 3D](https://arxiv.org/abs/2305.10853) by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"[Paper](https://huggingface.co/papers/2305.10853) | [Video](https://t.ly/tdi2) | [Intel/ldm3d-4c Model](https://huggingface.co/Intel/ldm3d-4c) | [Checkpoint](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üï≥  Going Deep\", on_click=close_ldm3d_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = ldm3d_help_dlg\n",
        "      ldm3d_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_model(e):\n",
        "        ldm3d_prefs['ldm3d_model'] = e.control.value\n",
        "        ldm3d_custom_model.visible = e.control.value == \"Custom\"\n",
        "        ldm3d_custom_model.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        ldm3d_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=ldm3d_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=ldm3d_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=ldm3d_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=ldm3d_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=ldm3d_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=ldm3d_prefs, key='num_inference_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=ldm3d_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=ldm3d_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=2048, divisions=15, multiple=128, suffix=\"px\", pref=ldm3d_prefs, key='height')\n",
        "    def toggle_ip_adapter(e):\n",
        "        ldm3d_prefs['use_ip_adapter'] = e.control.value\n",
        "        ip_adapter_container.height = None if e.control.value else 0\n",
        "        ip_adapter_container.update()\n",
        "        ip_adapter_model.visible = e.control.value\n",
        "        ip_adapter_model.update()\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Reference Image\", value=ldm3d_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip=\"Uses both image and text to condition the image generation process.\")\n",
        "    ip_adapter_model = Dropdown(label=\"IP-Adapter SD Model\", width=220, options=[], value=ldm3d_prefs['ip_adapter_model'], visible=ldm3d_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))\n",
        "    for m in ip_adapter_models:\n",
        "        ip_adapter_model.options.append(dropdown.Option(m['name']))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=ldm3d_prefs, key='ip_adapter_image', page=page)\n",
        "    ip_adapter_strength = SliderRow(label=\"IP-Adapter Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=ldm3d_prefs, key='ip_adapter_strength', col={'md':6}, tooltip=\"The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.\")\n",
        "    ip_adapter_container = Container(Column([ip_adapter_image, ip_adapter_strength]), height = None if ldm3d_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "\n",
        "    ldm3d_model = Dropdown(label=\"LDM3D Model\", width=220, options=[dropdown.Option(\"Custom\"), dropdown.Option(\"Intel/ldm3d-4c\"), dropdown.Option(\"Intel/ldm3d-pano\"), dropdown.Option(\"Intel/ldm3d\")], value=ldm3d_prefs['ldm3d_model'], on_change=changed_model)\n",
        "    ldm3d_custom_model = TextField(label=\"Custom LDM3D Model (URL or Path)\", value=ldm3d_prefs['custom_model'], expand=True, visible=ldm3d_prefs['ldm3d_model']==\"Custom\", on_change=lambda e:changed(e,'custom_model'))\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=ldm3d_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    use_upscale = Switcher(label=\"Use SR Upscale\", value=ldm3d_prefs['use_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_upscale'), tooltip=\"Uses ldm3d-sr to double the size of rgb & depth.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(ldm3d_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=ldm3d_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=ldm3d_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=ldm3d_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=ldm3d_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_ldm3d = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_ldm3d.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not ldm3d_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üèé   Run LDM3D\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.ldm3d_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üçã  Latent Diffusion Model for 3D (LDM3D)\", \"Generate RGB Images and 3D Depth Maps given a text prompt... Made with Intel.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LDM3D Settings\", on_click=ldm3d_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            Row([ldm3d_model, ldm3d_custom_model]),\n",
        "            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),\n",
        "            ip_adapter_container,\n",
        "            Row([cpu_offload, use_upscale]),\n",
        "            page.ESRGAN_block_ldm3d,\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            parameters_row,\n",
        "            page.ldm3d_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "text_to_video_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 256,\n",
        "    'height': 256,\n",
        "    'num_frames': 16,\n",
        "    'model': 'damo-vilab/text-to-video-ms-1.7b',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "\n",
        "def buildTextToVideo(page):\n",
        "    global text_to_video_prefs, prefs, pipe_text_to_video, editing_prompt\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            text_to_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            text_to_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            text_to_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.text_to_video_output.controls = []\n",
        "      page.text_to_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def text_to_video_help(e):\n",
        "      def close_text_to_video_dlg(e):\n",
        "        nonlocal text_to_video_help_dlg\n",
        "        text_to_video_help_dlg.open = False\n",
        "        page.update()\n",
        "      text_to_video_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Text-To-Video\"), content=Column([\n",
        "          Text(\"Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt.\"),\n",
        "          Markdown(\"\"\"From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):\n",
        "*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*\n",
        "Resources:\n",
        "* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)\n",
        "* [GitHub repository](https://github.com/modelscope/modelscope/)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  What'll be next... \", on_click=close_text_to_video_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = text_to_video_help_dlg\n",
        "      text_to_video_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        text_to_video_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=text_to_video_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=text_to_video_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=text_to_video_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=text_to_video_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=text_to_video_prefs, key='guidance_scale')\n",
        "    eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=text_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switcher(label=\"Lower Memory Mode\", value=text_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    model = Dropdown(label=\"Video Model\", hint_text=\"\", expand=True, options=[dropdown.Option(\"damo-vilab/text-to-video-ms-1.7b\"), dropdown.Option(\"modelscope-damo-text2video-synthesis\"), dropdown.Option(\"modelscope-damo-text2video-pruned-weights\"), dropdown.Option(\"cerspense/zeroscope_v2_XL\"), dropdown.Option(\"cerspense/zeroscope_v2_576w\")], value=text_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=text_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(text_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=text_to_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=text_to_video_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=text_to_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.text_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.text_to_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé•  Text-To-Video Synthesis\", \"Modelscope's Text-to-video-synthesis Model to Animate Diffusion\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Text-to-Video Settings\", on_click=text_to_video_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([NumberPicker(label=\"Number of Frames: \", min=1, max=8, value=text_to_video_prefs['num_frames'], tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),\n",
        "        Row([export_to_video, lower_memory, model]),\n",
        "        num_frames,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_slider,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_text_to_video,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üìπ  Run Text-To-Video\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page, from_list=True))\n",
        "        ]),\n",
        "        page.text_to_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "text_to_video_zero_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 1024,\n",
        "    'height': 1024,\n",
        "    'num_frames': 8,\n",
        "    'motion_field_strength_x': 12,\n",
        "    'motion_field_strength_y': 12,\n",
        "    't0': 42,\n",
        "    't1': 47,\n",
        "    'input_video': '',\n",
        "    'prep_type': 'Zero Shot',\n",
        "    'use_SDXL': False,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "\n",
        "def buildTextToVideoZero(page):\n",
        "    global text_to_video_zero_prefs, prefs, pipe_text_to_video_zero\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            text_to_video_zero_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            text_to_video_zero_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            text_to_video_zero_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.text_to_video_zero_output.controls = []\n",
        "      page.text_to_video_zero_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def text_to_video_zero_help(e):\n",
        "      def close_text_to_video_zero_dlg(e):\n",
        "        nonlocal text_to_video_zero_help_dlg\n",
        "        text_to_video_zero_help_dlg.open = False\n",
        "        page.update()\n",
        "      text_to_video_zero_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Text-To-Video Zero\"), content=Column([\n",
        "          Text(\"Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task of zero-shot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object.\"),\n",
        "          Text(\"Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data.\"),\n",
        "          Markdown(\"\"\"* [Project Page](https://text2video-zero.github.io/)\n",
        "* [Paper](https://arxiv.org/abs/2303.13439)\n",
        "* [Original Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  Let's go crazy... \", on_click=close_text_to_video_zero_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = text_to_video_zero_help_dlg\n",
        "      text_to_video_zero_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        text_to_video_zero_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_steps(e):\n",
        "        steps = e.control.value\n",
        "        t0.set_max(steps - 1)\n",
        "        t0.set_max(steps - 1)\n",
        "        t0.set_divisions(steps - 1)\n",
        "        if text_to_video_zero_prefs['t0'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t0'] = steps - 1\n",
        "            t0.set_value(text_to_video_zero_prefs['t0'])\n",
        "        t1.set_min(text_to_video_zero_prefs['t0'] + 1)\n",
        "        t1.set_max(steps - 1)\n",
        "        if text_to_video_zero_prefs['t1'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t1'] = steps - 1\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        if text_to_video_zero_prefs['t1'] < t1.min:\n",
        "            text_to_video_zero_prefs['t1'] = t1.min\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        t1.set_divisions(t1.max - t1.min)\n",
        "        t0.update_slider()\n",
        "        t1.update_slider()\n",
        "    def change_t0(e):\n",
        "        t0_value = e.control.value\n",
        "        steps = num_inference_row.value\n",
        "        t1.set_min(t0_value + 1)\n",
        "        if text_to_video_zero_prefs['t1'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t1'] = steps - 1\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        if text_to_video_zero_prefs['t1'] < t1.min:\n",
        "            text_to_video_zero_prefs['t1'] = t1.min\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        t1.set_divisions(t1.max - t1.min)\n",
        "        t1.update_slider()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=text_to_video_zero_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=text_to_video_zero_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=text_to_video_zero_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=text_to_video_zero_prefs, key='num_inference_steps', on_change=change_steps, tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=text_to_video_zero_prefs, key='guidance_scale')\n",
        "    eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_zero_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    motion_field_strength_x = SliderRow(label=\"Motion Field Strength X\", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_x', tooltip=\"Strength of motion in generated video along x-axis\")\n",
        "    motion_field_strength_y = SliderRow(label=\"Motion Field Strength Y\", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_y', tooltip=\"Strength of motion in generated video along y-axis\")\n",
        "    t0 = SliderRow(label=\"Timestep t0\", min=0, max=50, divisions=50, pref=text_to_video_zero_prefs, key='t0', on_change=change_t0, tooltip=\"Should be in the range [0, num_inference_steps - 1]\")\n",
        "    t1 = SliderRow(label=\"Timestep t1\", min=43, max=50, divisions=7, pref=text_to_video_zero_prefs, key='t1', tooltip=\"Should be in the range [t0 + 1, num_inference_steps - 1]\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_zero_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_zero_prefs, key='height')\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL\", value=text_to_video_zero_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip=\"SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Model.\")\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=text_to_video_zero_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    #lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switcher(label=\"Lower Memory Mode\", value=text_to_video_zero_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=text_to_video_zero_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(text_to_video_zero_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=text_to_video_zero_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=text_to_video_zero_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=text_to_video_zero_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video_zero = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video_zero.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.text_to_video_zero_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.text_to_video_zero_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé•  Text-To-Video Zero\", \"Text-to-Image Diffusion Models for Zero-Shot Video Generators\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Text-To-Video Zero Settings\", on_click=text_to_video_zero_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([NumberPicker(label=\"Number of Frames: \", min=1, max=8, value=text_to_video_zero_prefs['num_frames'], tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),\n",
        "        Row([export_to_video, use_SDXL]),\n",
        "        num_frames,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_slider,\n",
        "        motion_field_strength_x, motion_field_strength_y,\n",
        "        t0, t1,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_text_to_video_zero,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üìπ  Run Text2Video-Zero\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page, from_list=True))\n",
        "        ]),\n",
        "        page.text_to_video_zero_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "video_to_video_prefs = {\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 15.0,\n",
        "    'strength': 0.6,\n",
        "    'export_to_video': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 1024,\n",
        "    'width': 256,\n",
        "    'height': 256,\n",
        "    'num_frames': 16,\n",
        "    'model': 'cerspense/zeroscope_v2_576w',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "\n",
        "def buildVideoToVideo(page):\n",
        "    global video_to_video_prefs, prefs, pipe_video_to_video, editing_prompt\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            video_to_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            video_to_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            video_to_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.video_to_video_output.controls = []\n",
        "      page.video_to_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def video_to_video_help(e):\n",
        "      def close_video_to_video_dlg(e):\n",
        "        nonlocal video_to_video_help_dlg\n",
        "        video_to_video_help_dlg.open = False\n",
        "        page.update()\n",
        "      video_to_video_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Video-To-Video\"), content=Column([\n",
        "          Text(\"Cerspense Zeroscope are watermark-free model and have been trained on images that are 1024x576. Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt.\"),\n",
        "          Markdown(\"\"\"From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):\n",
        "*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*\n",
        "Resources:\n",
        "* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)\n",
        "* [GitHub repository](https://github.com/modelscope/modelscope/)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  What'll be next... \", on_click=close_video_to_video_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = video_to_video_help_dlg\n",
        "      video_to_video_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          video_to_video_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          video_to_video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_video.value = fname\n",
        "        init_video.update()\n",
        "        video_to_video_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_video(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"avi\", \"mp4\", \"mov\"], dialog_title=\"Pick Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        video_to_video_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Video Prompt Text\", value=video_to_video_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=video_to_video_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_video = TextField(label=\"Init Video File\", value=video_to_video_prefs['init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=video_to_video_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)\n",
        "    #num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=video_to_video_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=video_to_video_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=video_to_video_prefs, key='guidance_scale')\n",
        "    strength = SliderRow(label=\"Init Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=video_to_video_prefs, key='strength', tooltip=\"Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.\")\n",
        "    eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=video_to_video_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=video_to_video_prefs, key='max_size')\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=video_to_video_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=video_to_video_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=video_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switcher(label=\"Lower Memory Mode\", value=video_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    model = Dropdown(label=\"Video Model\", hint_text=\"\", expand=True, options=[dropdown.Option(\"damo-vilab/text-to-video-ms-1.7b\"), dropdown.Option(\"modelscope-damo-text2video-synthesis\"), dropdown.Option(\"modelscope-damo-text2video-pruned-weights\"), dropdown.Option(\"cerspense/zeroscope_v2_XL\"), dropdown.Option(\"cerspense/zeroscope_v2_576w\")], value=video_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=video_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(video_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=video_to_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=video_to_video_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=video_to_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_video_to_video = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_video_to_video.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.video_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.video_to_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìΩ  Video-To-Video Synthesis\", \"Note: Uses more than 16GB VRAM, may crash session. Video-to-video-synthesis Model to Reanimate Video Clips\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=video_to_video_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        init_video,\n",
        "        fps,\n",
        "        Row([start_time, end_time]),\n",
        "        #Row([NumberPicker(label=\"Number of Frames: \", min=1, max=8, value=video_to_video_prefs['num_frames'], tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),\n",
        "        Row([export_to_video, lower_memory, model]),\n",
        "        #num_frames,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        strength,\n",
        "        eta_slider,\n",
        "        max_row,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_video_to_video,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üé¶  Run Video-To-Video\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page, from_list=True))\n",
        "        ]),\n",
        "        page.video_to_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_temporalnet_prefs = {\n",
        "    'init_video': '',\n",
        "    'init_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 7.0,\n",
        "    'temporalnet_strength': 0.6,\n",
        "    'canny_strength': 0.7,\n",
        "    'export_to_video': True,\n",
        "    'save_frames': False,\n",
        "    'save_canny': False,\n",
        "    \"lower_memory\": True,\n",
        "    'seed': 0,\n",
        "    'max_size': 1024,\n",
        "    'low_threshold': 25,\n",
        "    'high_threshold': 200,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildTemporalNet_XL(page):\n",
        "    global controlnet_temporalnet_prefs, prefs, pipe_controlnet, editing_prompt\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_temporalnet_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_temporalnet_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_temporalnet_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_temporalnet_output.controls = []\n",
        "      page.controlnet_temporalnet_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_temporalnet_help(e):\n",
        "      def close_controlnet_temporalnet_dlg(e):\n",
        "        nonlocal controlnet_temporalnet_help_dlg\n",
        "        controlnet_temporalnet_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_temporalnet_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Controlnet TemporalNet XL\"), content=Column([\n",
        "          Text(\"This is TemporalNet-XL, it is a re-train of the controlnet TemporalNet1 with Stable Diffusion XL. This does not use the control mechanism of TemporalNet2 as it would require some additional work to adapt the diffusers pipeline to work with a 6-channel input. While it does not eliminate all flickering, it significantly reduces it, particularly at higher denoise levels. For optimal results, it is recommended to use TemporalNet in combination with other methods.\"),\n",
        "          Markdown(\"Credit goes to Ciara Rowles - [Huggingface Model](https://huggingface.co/CiaraRowles/controlnet-temporalnet-sdxl-1.0)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  Temporal Anomaly... \", on_click=close_controlnet_temporalnet_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_temporalnet_help_dlg\n",
        "      controlnet_temporalnet_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_temporalnet_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Video Prompt Text\", value=controlnet_temporalnet_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_temporalnet_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_video = FileInput(label=\"Init Video Clip\", pref=controlnet_temporalnet_prefs, key='init_video', ftype=\"video\", page=page)\n",
        "    init_image = FileInput(label=\"Frame 0 Init Image (optional)\", pref=controlnet_temporalnet_prefs, key='init_image', ftype=\"image\", page=page)\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=controlnet_temporalnet_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=controlnet_temporalnet_prefs, key='guidance_scale')\n",
        "    #strength = SliderRow(label=\"Init Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_temporalnet_prefs, key='strength', tooltip=\"Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_temporalnet_prefs, key='max_size')\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_temporalnet_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_temporalnet_prefs, key='height')\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_temporalnet_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_temporalnet_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    temporalnet_strength = SliderRow(label=\"TemporalNet Strength\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_temporalnet_prefs, key='temporalnet_strength', col={'lg':6}, tooltip=\"\")\n",
        "    canny_strength = SliderRow(label=\"Canny Strength\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_temporalnet_prefs, key='canny_strength', col={'lg':6}, tooltip=\"\")\n",
        "    lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switcher(label=\"Lower Memory Mode\", value=controlnet_temporalnet_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    save_frames = Tooltip(message=\"Save Frames\", content=Switcher(label=\"Save Frames\", value=controlnet_temporalnet_prefs['save_frames'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames')))\n",
        "    save_canny = Tooltip(message=\"Save Canny\", content=Switcher(label=\"Save Canny Frames\", value=controlnet_temporalnet_prefs['save_canny'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_canny')))\n",
        "    #model = Dropdown(label=\"Video Model\", hint_text=\"\", expand=True, options=[dropdown.Option(\"damo-vilab/text-to-video-ms-1.7b\"), dropdown.Option(\"modelscope-damo-text2video-synthesis\"), dropdown.Option(\"modelscope-damo-text2video-pruned-weights\"), dropdown.Option(\"cerspense/zeroscope_v2_XL\"), dropdown.Option(\"cerspense/zeroscope_v2_576w\")], value=controlnet_temporalnet_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_temporalnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_temporalnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=controlnet_temporalnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_temporalnet_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_temporalnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_temporalnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet_temporalnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_temporalnet_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_temporalnet_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚åõ  Controlnet TemporalNet-XL\", \"Video2Video ControlNet model designed to enhance the temporal consistency of video frames...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with TemporalNet-XL Settings\", on_click=controlnet_temporalnet_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        init_video,\n",
        "        init_image,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        ResponsiveRow([temporalnet_strength, canny_strength]),\n",
        "        canny_threshold,\n",
        "        max_row,\n",
        "        Row([lower_memory, save_frames, save_canny]),\n",
        "        page.ESRGAN_block_controlnet_temporalnet,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([ElevatedButton(content=Text(\"‚è≤Ô∏è  Run TemporalNet-XL\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_temporalnet(page)),]),\n",
        "        page.controlnet_temporalnet_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "infinite_zoom_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'frame': '0',\n",
        "    'guidance_scale': 6.0,\n",
        "    'num_inference_steps': 40,\n",
        "    'editing_prompts': [],\n",
        "    'animation_prompts': {},\n",
        "    'init_image': '',\n",
        "    'fps': 30,\n",
        "    'mask_width': 128,\n",
        "    'num_outpainting_steps': 20,\n",
        "    'frame_dupe_amount': 15,\n",
        "    'num_interpol_frames': 30,\n",
        "    'inpainting_model': 'stabilityai/stable-diffusion-2-inpainting',\n",
        "    'use_SDXL': False,\n",
        "    'max_size': 512,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    'seed': 0,\n",
        "    'save_frames': True,\n",
        "    'save_gif': True,\n",
        "    'save_video': True,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 1.5,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildInfiniteZoom(page):\n",
        "    global infinite_zoom_prefs, prefs, pipe_infinite_zoom, editing_prompt\n",
        "    editing_prompt = {'prompt':'', 'negative_prompt':'', 'seed':0}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            infinite_zoom_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            infinite_zoom_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            infinite_zoom_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_infinite_zoom_output(o):\n",
        "      page.infinite_zoom_output.controls.append(o)\n",
        "      page.infinite_zoom_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.infinite_zoom_output.controls = []\n",
        "      page.infinite_zoom_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def infinite_zoom_help(e):\n",
        "      def close_infinite_zoom_dlg(e):\n",
        "        nonlocal infinite_zoom_help_dlg\n",
        "        infinite_zoom_help_dlg.open = False\n",
        "        page.update()\n",
        "      infinite_zoom_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Infinite-Zoom\"), content=Column([\n",
        "          Text('We shrink the init image from the previous block and outpaint its outer frame using the same concept (e.g. prompt, negative prompt, inference steps) but with a different seed. To generate an \"inifinte zoom\" video this is repeated num_outpainting_steps times and then rendered in reversed order. To keep the outpainted part coherent and full of new content its width has to be relatively large (e.g. mask_width = 128 pixels if resolution is 512*512). This on the other hand means that the generated video would be too fast and aestetically unpleasant. To slow down and smoothen the video we generate num_interpol_frames additional images between outpainted images using simple \"interpolation\".'),\n",
        "          Text('Notes: Length of the video is proportional to num_outpainting_steps * num_interpol_frames. The time to generate the video is proportional to num_outpainting_steps. On a T4 GPU it takes about ~7 minutes to generate the video of width = 512, num_inference_steps = 20, num_outpainting_steps = 100. With fps = 24 and num_interpol_frames = 24 the video will be about 1:40 minutes long.'),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"[Github Project](https://github.com/v8hid/infinite-zoom-stable-diffusion) | [Google Colab](https://colab.research.google.com/github/v8hid/infinite-zoom-stable-diffusion/blob/main/smooth_infinite_zoom.ipynb)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üïµ  It's Endlessness... \", on_click=close_infinite_zoom_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = infinite_zoom_help_dlg\n",
        "      infinite_zoom_help_dlg.open = True\n",
        "      page.update()\n",
        "    \n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        infinite_zoom_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def infinite_zoom_tile(animate_prompt):\n",
        "        params = []\n",
        "        for k, v in animate_prompt.items():\n",
        "            if k == 'prompt': continue\n",
        "            params.append(f'{to_title(k)}: {v}')\n",
        "        sub = ', '.join(params)\n",
        "        return ListTile(title=Text(animate_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=animate_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Animation Prompt\", on_click=lambda e: edit_infinite_zoom(animate_prompt), data=animate_prompt),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Animation Prompt\", on_click=lambda e: del_infinite_zoom(animate_prompt), data=animate_prompt)]), on_click=lambda e: edit_infinite_zoom(animate_prompt))\n",
        "    def edit_infinite_zoom(edit=None):\n",
        "        infinite_zoom_prompt = edit if bool(edit) else editing_prompt.copy()\n",
        "        edit_prompt = edit['prompt'] if bool(edit) else infinite_zoom_prefs['prompt']\n",
        "        if not bool(edit):\n",
        "            infinite_zoom_prompt['prompt'] = infinite_zoom_prefs['prompt']\n",
        "            infinite_zoom_prompt['negative_prompt'] = infinite_zoom_prefs['negative_prompt']\n",
        "            infinite_zoom_prompt['seed'] = infinite_zoom_prefs['seed']\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def changed_p(e, pref=None):\n",
        "            if pref is not None:\n",
        "                infinite_zoom_prompt[pref] = e.control.value\n",
        "        def save_infinite_zoom_prompt(e):\n",
        "            if edit == None:\n",
        "                infinite_zoom_prefs['editing_prompts'].append(infinite_zoom_prompt)\n",
        "                page.infinite_zoom_prompts.controls.append(infinite_zoom_tile(infinite_zoom_prompt))\n",
        "                page.infinite_zoom_prompts.update()\n",
        "            else:\n",
        "                for s in infinite_zoom_prefs['editing_prompts']:\n",
        "                    if s['prompt'] == edit_prompt:\n",
        "                        s = infinite_zoom_prompt\n",
        "                        break\n",
        "                for t in page.infinite_zoom_prompts.controls:\n",
        "                    #print(f\"{t.data['prompt']} == {edit_prompt} - {t.title.value}\")\n",
        "                    if t.title.value == edit_prompt: #t.data['prompt']\n",
        "                        params = []\n",
        "                        for k, v in infinite_zoom_prompt.items():\n",
        "                            if k == 'prompt': continue\n",
        "                            params.append(f'{to_title(k)}: {v}')\n",
        "                        sub = ', '.join(params)\n",
        "                        t.title = Text(infinite_zoom_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)\n",
        "                        t.subtitle = Text(sub)\n",
        "                        t.data = infinite_zoom_prompt\n",
        "                        t.update()\n",
        "                        break\n",
        "                dlg_edit.open = False\n",
        "                e.control.update()\n",
        "                page.update()\n",
        "        infinite_zoom_editing_prompt = TextField(label=\"InfiniteZoom Prompt Modifier\", value=infinite_zoom_prompt['prompt'], autofocus=True, on_change=lambda e:changed_p(e,'prompt'))\n",
        "        infinite_zoom_negative_prompt = TextField(label=\"Negative Prompt\", value=infinite_zoom_prompt['negative_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'negative_prompt'))\n",
        "        infinite_zoom_seed = TextField(label=\"Seed\", width=90, value=str(infinite_zoom_prompt['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed_p(e,'seed'), col={'md':1})\n",
        "        if edit != None:\n",
        "            dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è {'Edit' if bool(edit) else 'Add'} Animated Prompt\"), content=Container(Column([\n",
        "                infinite_zoom_editing_prompt,\n",
        "                infinite_zoom_negative_prompt, infinite_zoom_seed,\n",
        "            ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_infinite_zoom_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "            page.dialog = dlg_edit\n",
        "            dlg_edit.open = True\n",
        "            page.update()\n",
        "        else:\n",
        "            save_infinite_zoom_prompt(None)\n",
        "    def del_infinite_zoom(edit=None):\n",
        "        for s in infinite_zoom_prefs['editing_prompts']:\n",
        "            if s['prompt'] == edit['prompt']:\n",
        "                infinite_zoom_prefs['editing_prompts'].remove(s)\n",
        "                break\n",
        "        for t in page.infinite_zoom_prompts.controls:\n",
        "            if t.data['prompt'] == edit['prompt']:\n",
        "                page.infinite_zoom_prompts.controls.remove(t)\n",
        "                break\n",
        "        page.infinite_zoom_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def edit_prompt(e):\n",
        "      nonlocal animation_prompts\n",
        "      f = int(e.control.data)\n",
        "      edit_prompt = infinite_zoom_prefs['animation_prompts'][str(f)]\n",
        "      def close_dlg(e):\n",
        "        dlg_edit.open = False\n",
        "        page.update()\n",
        "      def save_prompt(e):\n",
        "        fr = int(editing_frame.value)\n",
        "        pline = f'{fr}: \"{editing_prompt.value}\"'\n",
        "        for p in animation_prompts.controls:\n",
        "          if p.data == f:\n",
        "            p.title.value = pline\n",
        "            p.data = fr\n",
        "            for button in p.trailing.items:\n",
        "              button.data = fr\n",
        "            break\n",
        "        infinite_zoom_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()\n",
        "        if f != int(editing_frame.value):\n",
        "          del infinite_zoom_prefs['animation_prompts'][str(f)]\n",
        "          sorted_dict = {}\n",
        "          for key in sorted(infinite_zoom_prefs['animation_prompts'].keys()):\n",
        "              sorted_dict[key] = infinite_zoom_prefs['animation_prompts'][key]\n",
        "          infinite_zoom_prefs['animation_prompts'] = sorted_dict\n",
        "          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "        animation_prompts.update()\n",
        "        close_dlg(e)\n",
        "      editing_frame = TextField(label=\"Frame\", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "      editing_prompt = TextField(label=\"Keyframe Prompt Animation\", expand=True, multiline=True, value=edit_prompt, autofocus=True)\n",
        "      dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è Edit Prompt Keyframe\"), content=Container(Column([\n",
        "          Row([editing_frame, editing_prompt])\n",
        "      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_edit\n",
        "      dlg_edit.open = True\n",
        "      page.update()\n",
        "    def del_prompt(e):\n",
        "      f = e.control.data\n",
        "      for i, p in enumerate(animation_prompts.controls):\n",
        "        if p.data == f:\n",
        "          del animation_prompts.controls[i]\n",
        "          break\n",
        "      animation_prompts.update()\n",
        "      del infinite_zoom_prefs['animation_prompts'][str(f)]\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompts(e):\n",
        "      animation_prompts.controls.clear()\n",
        "      animation_prompts.update()\n",
        "      infinite_zoom_prefs['animation_prompts'] = {}\n",
        "      clear_prompt(e)\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompt(e):\n",
        "      prompt.value = \"\"\n",
        "      prompt.update()\n",
        "    def copy_prompt(e):\n",
        "      p = infinite_zoom_prefs['animation_prompts'][str(e.control.data)]\n",
        "      page.set_clipboard(p)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã  Prompt Text copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "    def add_prompt(e, f=None, p=None, sound=True):\n",
        "      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return\n",
        "      if f == None: f = int(frame.value)\n",
        "      if p == None: p = prompt.value.strip()\n",
        "      pline = f'{f}: \"{p}\"'\n",
        "      if str(f) in infinite_zoom_prefs['animation_prompts']:\n",
        "        for i, pr in enumerate(animation_prompts.controls):\n",
        "          if pr.data == f:\n",
        "            pr.title.value = pline\n",
        "      else:\n",
        "        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Animation Prompt\", on_click=edit_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.COPY, text=\"Copy Prompt Text\", on_click=copy_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete Animation Prompt\", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Prompts\", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))\n",
        "      infinite_zoom_prefs['animation_prompts'][str(f)] = p\n",
        "      sorted_dict = {}\n",
        "      for key in sorted(infinite_zoom_prefs['animation_prompts'].keys()):\n",
        "          sorted_dict[key] = infinite_zoom_prefs['animation_prompts'][key]\n",
        "      infinite_zoom_prefs['animation_prompts'] = sorted_dict\n",
        "      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "      animation_prompts.update()\n",
        "      if prefs['enable_sounds'] and sound: page.snd_drop.play()\n",
        "\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=infinite_zoom_prefs['prompt'], filled=True, expand=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=infinite_zoom_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=76, value=str(infinite_zoom_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'), col={'md':1})\n",
        "    frame = TextField(label=\"Frame\", width=76, value=\"0\", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "    add_prompt_keyframe = ft.FilledButton(\"‚ûï  Add Keyframe\", on_click=add_prompt)\n",
        "    num_interpol_frames = SliderRow(label=\"Interpolation Frames\", min=1, max=150, divisions=149, pref=infinite_zoom_prefs, key='num_interpol_frames', tooltip=\"Number of images to be interpolated between each outpainting step.\")\n",
        "    num_inference_row = SliderRow(label=\"Inference Steps\", min=1, max=150, divisions=149, pref=infinite_zoom_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=infinite_zoom_prefs, key='guidance_scale')\n",
        "    mask_width = SliderRow(label=\"Mask Width\", min=1, max=256, divisions=255, suffix=\"px\", pref=infinite_zoom_prefs, key='mask_width', col={'md': 6}, tooltip=\"Width of the border in pixels to be outpainted during each step. Make sure: mask_width < image resolution / 2\")\n",
        "    num_outpainting_steps = SliderRow(label=\"Outpainting Steps\", min=1, max=100, divisions=99, pref=infinite_zoom_prefs, key='num_outpainting_steps', col={'md': 6}, tooltip=\"Number of outpainting images between each interpolation.\")\n",
        "    frame_dupe_amount = SliderRow(label=\"Frame Dupe Amount\", min=1, max=90, divisions=89, pref=infinite_zoom_prefs, key='frame_dupe_amount', tooltip=\"Duplicates the first and last frames, use to add a delay before animation based on playback fps (15 = 0.5 seconds @ 30fps)\")\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=60, divisions=59, suffix='fps', pref=infinite_zoom_prefs, key='fps', tooltip=\"The FPS to save video clip.\")\n",
        "    #max_size = SliderRow(label=\"Max Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=infinite_zoom_prefs, key='max_size')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=infinite_zoom_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=infinite_zoom_prefs, key='height')\n",
        "    use_SDXL = Switcher(label=\"Use Stable Diffusion XL\", value=infinite_zoom_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip=\"SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.\")\n",
        "    inpainting_model = Dropdown(label=\"Inpainting Model\", width=386, options=[dropdown.Option(m) for m in [\"stabilityai/stable-diffusion-2-inpainting\", \"runwayml/stable-diffusion-inpainting\", \"ImNoOne/f222-inpainting-diffusers\", \"parlance/dreamlike-diffusion-1.0-inpainting\", \"ghunkins/stable-diffusion-liberty-inpainting\", \"piyushaaryan011/realistic-vision-inpainting\"]], value=infinite_zoom_prefs['inpainting_model'], on_change=lambda e: changed(e, 'inpainting_model'))\n",
        "    save_frames = Switcher(label=\"Save Frames\", value=infinite_zoom_prefs['save_frames'], on_change=lambda e:changed(e,'save_frames'))\n",
        "    save_gif = Switcher(label=\"Save Animated GIF\", value=infinite_zoom_prefs['save_gif'], on_change=lambda e:changed(e,'save_gif'))\n",
        "    save_video = Switcher(label=\"Save Video\", value=infinite_zoom_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))\n",
        "    init_image = FileInput(label=\"Initial Image (optional)\", pref=infinite_zoom_prefs, key='init_image', page=page)\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name (required)\", value=infinite_zoom_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=infinite_zoom_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=infinite_zoom_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=infinite_zoom_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_infinite_zoom = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_infinite_zoom.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.infinite_zoom_prompts = Column([], spacing=0)\n",
        "    page.infinite_zoom_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.infinite_zoom_output.controls) > 0\n",
        "    animation_prompts = Column([], spacing=0)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîç  Infinite Zoom Text-to-Video\", \"Animate your Keyframe Prompts with an Endless Zooming Effect...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with InfiniteZoom Settings\", on_click=infinite_zoom_help)]),\n",
        "        Row([frame, prompt, add_prompt_keyframe]),\n",
        "        animation_prompts,\n",
        "        Divider(thickness=2, height=4),\n",
        "        negative_prompt,\n",
        "        init_image,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        #max_size,\n",
        "        width_slider, height_slider,\n",
        "        mask_width,\n",
        "        num_outpainting_steps,\n",
        "        num_interpol_frames,\n",
        "        fps,\n",
        "        frame_dupe_amount,\n",
        "        Divider(thickness=4, height=4),\n",
        "        Row([inpainting_model, use_SDXL]),\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([save_frames, save_gif, save_video]),\n",
        "        page.ESRGAN_block_infinite_zoom,\n",
        "        Row([ElevatedButton(content=Text(\"üí´  Run Infinite Zoom\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_infinite_zoom(page)),]),\n",
        "        page.infinite_zoom_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "potat1_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, watermark, copyright, blurry, low resolution, blur, low quality',\n",
        "    'num_inference_steps': 25,\n",
        "    'guidance_scale': 23.0,\n",
        "    'fps': 24,\n",
        "    'num_frames': 16,\n",
        "    'export_to_video': False,\n",
        "    'remove_watermark': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 1024,\n",
        "    'height': 576,\n",
        "    'init_video': '',\n",
        "    'init_weight': 0.5,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildPotat1(page):\n",
        "    global potat1_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            potat1_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            potat1_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            potat1_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.potat1_output.controls = []\n",
        "      page.potat1_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def potat1_help(e):\n",
        "      def close_potat1_dlg(e):\n",
        "        nonlocal potat1_help_dlg\n",
        "        potat1_help_dlg.open = False\n",
        "        page.update()\n",
        "      potat1_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Potat1 Text-To-Video\"), content=Column([\n",
        "          Text(\"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The text-to-video generation diffusion model consists of three sub-networks: text feature extraction, text feature-to-video latent space diffusion model, and video latent space to video visual space. The overall model parameters are about 1.7 billion. Support English input. The diffusion model adopts the Unet3D structure, and realizes the function of video generation through the iterative denoising process from the pure Gaussian noise video. Trained with https://lambdalabs.com ‚ù§ 1xA100 (40GB) 2197 clips, 68388 tagged frames ( salesforce/blip2-opt-6.7b-coco ) train_steps: 10000\"),\n",
        "          Markdown(\"[Huggingface Website](https://huggingface.co/camenduru/potat1) | [GitHub repository](https://github.com/camenduru/text-to-video-synthesis-colab)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üç†  Spuds up... \", on_click=close_potat1_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = potat1_help_dlg\n",
        "      potat1_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        potat1_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=potat1_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=potat1_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=potat1_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=potat1_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=potat1_prefs, key='guidance_scale')\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=potat1_prefs, key='fps')\n",
        "    #eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=potat1_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=potat1_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=potat1_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=potat1_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=potat1_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=potat1_prefs, key='height')\n",
        "    batch_folder_name = TextField(label=\"Video Folder Name\", value=potat1_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(potat1_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=potat1_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=potat1_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=potat1_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_potat1 = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_potat1.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.potat1_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.potat1_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü•î  Potat1Ô∏è‚É£ Text-To-Video Synthesis\", \"CamenDuru's Open-Source 1024x576 Text-To-Video Model \", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Potat1 Settings\", on_click=potat1_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([export_to_video, lower_memory]),\n",
        "        num_frames,\n",
        "        fps,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        #eta_slider,\n",
        "        width_slider, height_slider,\n",
        "        #page.ESRGAN_block_potat1,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üçü  Run Potat1\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_potat1(page)),\n",
        "        ]),\n",
        "        page.potat1_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "stable_animation_prefs = {\n",
        "    #'animation_prompt': '{\\n0:\"\",\\n}',\n",
        "    'animation_prompts': {},\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    'negative_prompt_weight': -1.0,\n",
        "    #'prompt': '',\n",
        "    #'frame': 0,\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'seed': 0,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    'max_frames': 300,\n",
        "    'steps_curve': \"0:(30)\",\n",
        "    'model': 'stable-diffusion-v1-5',\n",
        "    'style_preset': \"None\",\n",
        "    'sampler': 'K_dpmpp_2m',\n",
        "    'clip_guidance': \"None\",\n",
        "    'steps_strength_adj': True,\n",
        "    'interpolate_prompts': False,\n",
        "    'locked_seed': False,\n",
        "    'noise_add_curve': \"0:(0.02)\",\n",
        "    'noise_scale_curve': \"0:(0.99)\",\n",
        "    'strength_curve': \"0:(0.65)\",\n",
        "    'diffusion_cadence_curve': \"0:(1.0)\",\n",
        "    'cadence_interp': 'Mix',\n",
        "    'cadence_spans': False,\n",
        "    'inpaint_border': False,\n",
        "    'border': \"replicate\",\n",
        "    'use_inpainting_model': False,\n",
        "    'fps': 12,\n",
        "    'mask_min_value': \"0:(0.25)\",\n",
        "    'mask_binarization_thr': 0.5,\n",
        "    #Colour & Depth Parameters\n",
        "    'color_coherence': 'LAB',\n",
        "    'brightness_curve': \"0:(1.0)\",\n",
        "    'contrast_curve': \"0:(1.0)\",\n",
        "    'hue_curve': \"0:(0.0)\",\n",
        "    'saturation_curve': \"0:(1.0)\",\n",
        "    'lightness_curve': \"0:(0.0)\",\n",
        "    'color_match_animate': True,\n",
        "    'depth_model_weight': 0.3,\n",
        "    'near_plane': 200,\n",
        "    'far_plane': 10000,\n",
        "    'fov_curve': \"0:(25)\", #-180\n",
        "    'depth_blur_curve': \"0:(0.0)\", #-7\n",
        "    'depth_warp_curve': \"0:(1.0)\", #-1\n",
        "    #2D & 3D Parameters\n",
        "    'translation_x': \"0:(0)\",\n",
        "    'translation_y': \"0:(0)\",\n",
        "    'translation_z': \"0:(0)\",\n",
        "    'angle': \"0:(0)\",\n",
        "    'zoom': \"0:(1)\",\n",
        "    'rotation_x': \"0:(0)\",\n",
        "    'rotation_y': \"0:(0)\",\n",
        "    'rotation_z': \"0:(0)\",\n",
        "    'camera_type': \"Perspective\",#Orthographic\n",
        "    'render_mode': \"Mesh\",#Pointcloud\n",
        "    'animation_mode': \"3D warp\",\n",
        "    'mask_power': 0.3, #-4\n",
        "    #Input Parameters\n",
        "    'init_image': \"\",\n",
        "    'init_sizing': \"Stretch\",\n",
        "    'mask_image': \"\",\n",
        "    'mask_invert': False,\n",
        "    'video_init_path': \"\",\n",
        "    'video_flow_warp': True,\n",
        "    'video_mix_in_curve': \"0:(0.02)\",\n",
        "    'extract_nth_frame': 1,\n",
        "    'video_init_fps': 12,\n",
        "    'resume': False,\n",
        "    'resume_from': '-1',\n",
        "    #Post-Processor Parameters\n",
        "    'output_fps': 24,\n",
        "    'frame_interpolation_mode': \"Rife\", #film, none\n",
        "    'frame_interpolation_factor': \"2\", #4, 8\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "stable_animation_prefs_default = stable_animation_prefs.copy()\n",
        "\n",
        "def buildStableAnimation(page):\n",
        "    global stable_animation_prefs, prefs, pipe_stable_animation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            stable_animation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            stable_animation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            stable_animation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.stable_animation_output.controls = []\n",
        "      page.stable_animation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def stable_animation_help(e):\n",
        "      def close_stable_animation_dlg(e):\n",
        "        nonlocal stable_animation_help_dlg\n",
        "        stable_animation_help_dlg.open = False\n",
        "        page.update()\n",
        "      stable_animation_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Stable Animation\"), content=Column([\n",
        "          Text(\"With our Animation SDK, artists and developers have the ability to use the Stable Diffusion family of models to generate stunning animations. Create animations purely from prompts, start with an initial image, or drive an animation from a source video. \"),\n",
        "          Text(\"Artists have the ability to use all of our available inference models to generate animations. We currently support text-to-animation, image-to-animation, and video-to-animation. To see animated previews of how these parameters affect the resulting animation, please check out our Animation Handbook.\"),\n",
        "          Markdown(\" [Project Page](https://platform.stability.ai/docs/features/animation) | [Colab Gradio](https://colab.research.google.com/github/Stability-AI/stability-sdk/blob/animation/nbs/animation_gradio.ipynb) | [Animation Handbook](https://docs.google.com/document/d/1iHcAu_5rG11guGFie8sXBXPGuM4yKzqdd13MJ_1LU8U/edit?usp=sharing)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üí∏  Worth it... \", on_click=close_stable_animation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = stable_animation_help_dlg\n",
        "      stable_animation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        stable_animation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          stable_animation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          stable_animation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          stable_animation_prefs['init_image'] = fname\n",
        "        if pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          stable_animation_prefs['mask_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          stable_animation_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Mask Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def save_preset(e):\n",
        "      def copy_preset(pl):\n",
        "        nonlocal text_list, enter_text\n",
        "        page.set_clipboard(enter_text.value)\n",
        "        page.snack_bar = SnackBar(content=Text(f\"üìã   Animation Preset copied to clipboard...\"))\n",
        "        page.snack_bar.open = True\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_copy.open = False\n",
        "          page.update()\n",
        "      stable_animation_prefs['animation_prompts'] = {str(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}\n",
        "      text_list = json.dumps(stable_animation_prefs, indent = 4)\n",
        "      enter_text = TextField(label=\"Stable Animation Preset JSON\", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)\n",
        "      dlg_copy = AlertDialog(modal=False, title=Text(\"üìù   Stable Animation as JSON\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll=\"none\"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Copy Preset JSON to Clipboard\", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_preset(text_list))], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_copy\n",
        "      dlg_copy.open = True\n",
        "      page.update()\n",
        "    def toggle_resume(e):\n",
        "      stable_animation_prefs['resume'] = e.control.value\n",
        "      resume_from.visible = stable_animation_prefs['resume']\n",
        "      resume_from.update()\n",
        "    def paste_preset(e):\n",
        "      def save_preset(e):\n",
        "        try:\n",
        "          preset_json = json.loads(enter_text.value.strip())\n",
        "        except UnicodeDecodeError:\n",
        "          close_dlg(e)\n",
        "          alert_msg(\"Error Parsing JSON Data...\")\n",
        "          return\n",
        "        load_preset(preset_json)\n",
        "        close_dlg(e)\n",
        "      def paste_clipboard(e):\n",
        "          enter_text.value = page.get_clipboard()\n",
        "          enter_text.update()\n",
        "      def close_dlg(e):\n",
        "          dlg_paste.open = False\n",
        "          page.update()\n",
        "      enter_text = TextField(label=\"Enter Stable Animation Preset JSON\", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)\n",
        "      dlg_paste = AlertDialog(modal=False, title=Text(\"üìù  Paste Saved Preset JSON\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll=\"none\"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), TextButton(content=Text(\"üìã  Paste from Clipboard\", size=18), on_click=paste_clipboard), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Load Preset JSON Values \", size=19, weight=FontWeight.BOLD), on_click=save_preset)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_paste\n",
        "      dlg_paste.open = True\n",
        "      page.update()\n",
        "    def load_preset(d):\n",
        "      global stable_animation_prefs\n",
        "      stable_animation_prefs = d.copy()\n",
        "      stable_animation_prefs['animation_prompts'] = {}\n",
        "      #animation_prompt.value = d['animation_prompt']\n",
        "      animation_prompts.controls.clear()\n",
        "      prompts = d['animation_prompts'].copy()\n",
        "      for k, v in prompts.items():\n",
        "        add_prompt(d, int(k), v, False)\n",
        "      negative_prompt.value = d['negative_prompt']\n",
        "      negative_prompt_weight.value = d['negative_prompt_weight']\n",
        "      max_frames.value = d['max_frames']\n",
        "      fps.set_value(d['fps'])\n",
        "      num_inference_row.set_value(d['num_inference_steps'])\n",
        "      guidance.set_value(d['guidance_scale'])\n",
        "      model_checkpoint.value = d['model']\n",
        "      generation_sampler.value = d['sampler']\n",
        "      style_preset.value = d['style_preset']\n",
        "      clip_guidance.value = d['clip_guidance']\n",
        "      steps_strength_adj.value = d['steps_strength_adj']\n",
        "      interpolate_prompts.value = d['interpolate_prompts']\n",
        "      locked_seed.value = d['locked_seed']\n",
        "      noise_add_curve.value = d['noise_add_curve']\n",
        "      noise_scale_curve.value = d['noise_scale_curve']\n",
        "      strength_curve.value = d['strength_curve']\n",
        "      diffusion_cadence_curve.value = d['diffusion_cadence_curve']\n",
        "      cadence_interp.value = d['cadence_interp']\n",
        "      cadence_spans.value = d['cadence_spans']\n",
        "      inpaint_border.value = d['inpaint_border']\n",
        "      border.value = d['border']\n",
        "      use_inpainting_model.value = d['use_inpainting_model']\n",
        "      mask_min_value.value = d['mask_min_value']\n",
        "      mask_binarization_thr.set_value(d['mask_binarization_thr'])\n",
        "      color_coherence.value = d['color_coherence']\n",
        "      brightness_curve.value = d['brightness_curve']\n",
        "      contrast_curve.value = d['contrast_curve']\n",
        "      hue_curve.value = d['hue_curve']\n",
        "      saturation_curve.value = d['saturation_curve']\n",
        "      lightness_curve.value = d['lightness_curve']\n",
        "      color_match_animate.value = d['color_match_animate']\n",
        "      depth_model_weight.set_value(d['depth_model_weight'])\n",
        "      fov_curve.value = d['fov_curve']\n",
        "      depth_blur_curve.value = d['depth_blur_curve']\n",
        "      depth_warp_curve.value = d['depth_warp_curve']\n",
        "      translation_x.value = d['translation_x']\n",
        "      translation_y.value = d['translation_y']\n",
        "      translation_z.value = d['translation_z']\n",
        "      angle.value = d['angle']\n",
        "      zoom.value = d['zoom']\n",
        "      rotation_x.value = d['rotation_x']\n",
        "      rotation_y.value = d['rotation_y']\n",
        "      rotation_z.value = d['rotation_z']\n",
        "      camera_type.value = d['camera_type']\n",
        "      render_mode.value = d['render_mode']\n",
        "      mask_power.set_value(d['mask_power'])\n",
        "      init_image.value = d['init_image']\n",
        "      init_sizing.value = d['init_sizing']\n",
        "      mask_image.value = d['mask_image']\n",
        "      mask_invert.value = d['mask_invert']\n",
        "      video_mix_in_curve.value = d['video_mix_in_curve']\n",
        "      animation_mode.value = d['animation_mode']\n",
        "      init_video.value = d['video_init_path']\n",
        "      video_mix_in_curve.value = d['video_mix_in_curve']\n",
        "      video_flow_warp.value = d['video_flow_warp']\n",
        "      extract_nth_frame.value = d['extract_nth_frame']\n",
        "      video_init_fps.value = d['video_init_fps']\n",
        "      output_fps.value = d['output_fps']\n",
        "      resume.value = d['resume']\n",
        "      resume_from.value = d['resume_from']\n",
        "      frame_interpolation_mode.value = d['frame_interpolation_mode']\n",
        "      frame_interpolation_factor.value = d['frame_interpolation_factor']\n",
        "      width_slider.set_value(d['width'])\n",
        "      height_slider.set_value(d['height'])\n",
        "      seed.value = d['seed']\n",
        "      batch_folder_name.value = d['batch_folder_name']\n",
        "      #.set_value(d[''])\n",
        "      page.update()\n",
        "    def load_default(e):\n",
        "      global stable_animation_prefs_default\n",
        "      load_preset(stable_animation_prefs_default)\n",
        "    def edit_prompt(e):\n",
        "      nonlocal animation_prompts\n",
        "      f = int(e.control.data)\n",
        "      edit_prompt = stable_animation_prefs['animation_prompts'][str(f)]\n",
        "      def close_dlg(e):\n",
        "        dlg_edit.open = False\n",
        "        page.update()\n",
        "      def save_prompt(e):\n",
        "        fr = int(editing_frame.value)\n",
        "        pline = f'{fr}: \"{editing_prompt.value}\"'\n",
        "        for p in animation_prompts.controls:\n",
        "          if p.data == f:\n",
        "            p.title.value = pline\n",
        "            p.data = fr\n",
        "            for button in p.trailing.items:\n",
        "              button.data = fr\n",
        "            break\n",
        "        stable_animation_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()\n",
        "        if f != int(editing_frame.value):\n",
        "          del stable_animation_prefs['animation_prompts'][str(f)]\n",
        "          sorted_dict = {}\n",
        "          for key in sorted(stable_animation_prefs['animation_prompts'].keys()):\n",
        "              sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]\n",
        "          stable_animation_prefs['animation_prompts'] = sorted_dict\n",
        "          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "        animation_prompts.update()\n",
        "        close_dlg(e)\n",
        "      editing_frame = TextField(label=\"Frame\", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "      editing_prompt = TextField(label=\"Keyframe Prompt Animation\", expand=True, multiline=True, value=edit_prompt, autofocus=True)\n",
        "      dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è Edit Prompt Keyframe\"), content=Container(Column([\n",
        "          Row([editing_frame, editing_prompt])\n",
        "      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_edit\n",
        "      dlg_edit.open = True\n",
        "      page.update()\n",
        "    def del_prompt(e):\n",
        "      f = e.control.data\n",
        "      for i, p in enumerate(animation_prompts.controls):\n",
        "        if p.data == f:\n",
        "          del animation_prompts.controls[i]\n",
        "          break\n",
        "      animation_prompts.update()\n",
        "      del stable_animation_prefs['animation_prompts'][str(f)]\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompts(e):\n",
        "      animation_prompts.controls.clear()\n",
        "      animation_prompts.update()\n",
        "      stable_animation_prefs['animation_prompts'] = {}\n",
        "      clear_prompt(e)\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompt(e):\n",
        "      prompt.value = \"\"\n",
        "      prompt.update()\n",
        "    def copy_prompt(e):\n",
        "      p = stable_animation_prefs['animation_prompts'][str(e.control.data)]\n",
        "      page.set_clipboard(p)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã  Prompt Text copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "    def add_prompt(e, f=None, p=None, sound=True):\n",
        "      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return\n",
        "      if f == None: f = int(frame.value)\n",
        "      if p == None: p = prompt.value.strip()\n",
        "      pline = f'{f}: \"{p}\"'\n",
        "      if f in stable_animation_prefs['animation_prompts']:\n",
        "        for i, p in enumerate(animation_prompts.controls):\n",
        "          if p.data == f:\n",
        "            p.title.value = pline\n",
        "      else:\n",
        "        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Animation Prompt\", on_click=edit_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.COPY, text=\"Copy Prompt Text\", on_click=copy_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete Animation Prompt\", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Prompts\", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))\n",
        "      stable_animation_prefs['animation_prompts'][str(f)] = p\n",
        "      #stable_animation_prefs['animation_prompts'] = {int(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}\n",
        "      #stable_animation_prefs['animation_prompts'] = sorted(stable_animation_prefs['animation_prompts'].keys())\n",
        "      #stable_animation_prefs['animation_prompts'] = {i: stable_animation_prefs['animation_prompts'][i] for i in list(stable_animation_prefs['animation_prompts'].keys()).sort()}\n",
        "      sorted_dict = {}\n",
        "      for key in sorted(stable_animation_prefs['animation_prompts'].keys()):\n",
        "          sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]\n",
        "      stable_animation_prefs['animation_prompts'] = sorted_dict\n",
        "      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "      animation_prompts.update()\n",
        "      if prefs['enable_sounds'] and sound: page.snd_drop.play()\n",
        "    copy_preset_button = IconButton(icons.COPY_ALL, tooltip=\"Save Animation Presets as JSON\", on_click=save_preset)\n",
        "    paste_preset_button = IconButton(icons.CONTENT_PASTE, tooltip=\"Load Animation Presets as JSON\", on_click=paste_preset)\n",
        "    default_preset_button = IconButton(icons.REFRESH, tooltip=\"Reset Animation Presets to Default\", on_click=load_default) #, suffix=IconButton(icons.CLEAR, on_click=clear_prompt), width=50, height=50\n",
        "    prompt = TextField(label=\"Keyframe Prompt Text\", value=\"\", filled=True, expand=True, multiline=True)\n",
        "    frame = TextField(label=\"Frame\", width=76, value=\"0\", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "    negative_prompt_weight = TextField(label=\"Negative Weight\", width=120, value=stable_animation_prefs['negative_prompt_weight'], keyboard_type=KeyboardType.NUMBER, tooltip=\"\", on_change=lambda e:changed(e,'negative_prompt_weight'))\n",
        "    add_prompt_keyframe = ft.FilledButton(\"‚ûï  Add Keyframe\", on_click=add_prompt)\n",
        "    #animation_prompt = TextField(label=\"Animation Prompt Text\", value=stable_animation_prefs['animation_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'animation_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=stable_animation_prefs['negative_prompt'], col={'md':3}, expand=True, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    max_frames  = TextField(label=\"Max Number of Frames\", width=200, hint_text=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", value=stable_animation_prefs['max_frames'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'max_frames', ptype='int'))\n",
        "    #max_frames = SliderRow(label=\"Max Number of Frames\", min=1, max=300, divisions=299, pref=stable_animation_prefs, key='max_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', expand=True, pref=stable_animation_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=stable_animation_prefs, key='num_inference_steps', on_change=lambda e:changed(e,'num_inference_steps'), tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=stable_animation_prefs, key='guidance_scale')\n",
        "    #eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    model_checkpoint = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=270, options=[dropdown.Option(\"stable-diffusion-xl-beta-v2-2-2\"), dropdown.Option(\"stable-diffusion-768-v2-1\"), dropdown.Option(\"stable-diffusion-512-v2-1\"), dropdown.Option(\"stable-diffusion-768-v2-0\"), dropdown.Option(\"stable-diffusion-512-v2-0\"), dropdown.Option(\"stable-diffusion-v1-5\"), dropdown.Option(\"stable-diffusion-v1\"), dropdown.Option(\"stable-inpainting-512-v2-0\"), dropdown.Option(\"stable-inpainting-v1-0\")], value=stable_animation_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=230, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"PLMS\"), dropdown.Option(\"K_euler\"), dropdown.Option(\"K_euler_ancestral\"), dropdown.Option(\"K_heun\"), dropdown.Option(\"K_dpmpp_2m\"), dropdown.Option(\"K_dpm_2_ancestral\"), dropdown.Option(\"K_lms\"), dropdown.Option(\"K_dpmpp_2s_ancestral\"), dropdown.Option(\"K_dpm_2\")], value=stable_animation_prefs['sampler'], autofocus=False, on_change=lambda e:changed(e, 'sampler'))\n",
        "    #DDIM, PLMS, K_euler, K_euler_ancestral, K_heun, K_dpm_2, K_dpm_2_ancestral, K_lms, K_dpmpp_2m, K_dpmpp_2s_ancestral\n",
        "    style_preset = Dropdown(label=\"Style Preset\", hint_text=\"\", width=240, options=[dropdown.Option(\"None\"), dropdown.Option(\"3d-model\"), dropdown.Option(\"analog-film\"), dropdown.Option(\"anime\"), dropdown.Option(\"cinematic\"), dropdown.Option(\"comic-book\"), dropdown.Option(\"digital-art\"), dropdown.Option(\"enhance fantasy-art\"), dropdown.Option(\"isometric\"), dropdown.Option(\"line-art\"), dropdown.Option(\"low-poly\"), dropdown.Option(\"modeling-compound\"), dropdown.Option(\"neon-punk\"), dropdown.Option(\"origami\"), dropdown.Option(\"photographic\"), dropdown.Option(\"pixel-art\")], value=stable_animation_prefs['style_preset'], autofocus=False, on_change=lambda e:changed(e, 'style_preset'))\n",
        "    clip_guidance = Dropdown(label=\"Clip Guidance\", hint_text=\"\", width=240, options=[dropdown.Option(\"None\"), dropdown.Option(\"Simple\"), dropdown.Option(\"FastBlue\"), dropdown.Option(\"FastGreen\")], value=stable_animation_prefs['clip_guidance'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance'))\n",
        "    steps_strength_adj = Checkbox(label=\"Steps Strength Adjustment\", tooltip=\"Adjusts number of diffusion steps based on current previous frame strength value.\", value=stable_animation_prefs['steps_strength_adj'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'steps_strength_adj'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})\n",
        "    interpolate_prompts = Checkbox(label=\"Interpolate Prompts\", tooltip=\"Smoothly interpolate prompts between keyframes.\", value=stable_animation_prefs['interpolate_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'interpolate_prompts'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})\n",
        "    locked_seed = Checkbox(label=\"Locked Seed\", tooltip=\"Keep the same seed for all frames.\", value=stable_animation_prefs['locked_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'locked_seed'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})\n",
        "    noise_add_curve  = TextField(label=\"Noise Add Curve\", value=stable_animation_prefs['noise_add_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_add_curve'))\n",
        "    noise_scale_curve  = TextField(label=\"Noise Scale Curve\", value=stable_animation_prefs['noise_scale_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_scale_curve'))\n",
        "    strength_curve  = TextField(label=\"Strength Curve\", value=stable_animation_prefs['strength_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'strength_curve'))\n",
        "    diffusion_cadence_curve  = TextField(label=\"Diffusion Cadence Curve\", hint_text=\"One greater than the number of frames between diffusion operations. A cadence of 1 performs diffusion on each frame. Values greater than one will generate frames using interpolation methods.\", value=stable_animation_prefs['diffusion_cadence_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'diffusion_cadence_curve'))\n",
        "    cadence_interp = Dropdown(label=\"Cadence Interpolation\", hint_text=\"\", width=200, options=[dropdown.Option(\"Mix\"), dropdown.Option(\"RIFE\"), dropdown.Option(\"VAE-LERP\"), dropdown.Option(\"VAE-SLERP\")], value=stable_animation_prefs['cadence_interp'], autofocus=False, on_change=lambda e:changed(e, 'cadence_interp'))\n",
        "    cadence_spans = Checkbox(label=\"Cadence Spans\", tooltip=\"Experimental diffusion cadence mode for better outpainting\", value=stable_animation_prefs['cadence_spans'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'cadence_spans'))\n",
        "    inpaint_border = Checkbox(label=\"Inpaint Border\", tooltip=\"Use inpainting on top of border regions for 2D and 3D warp modes.\", value=stable_animation_prefs['inpaint_border'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'inpaint_border'))\n",
        "    border = Dropdown(label=\"Border\", hint_text=\"Method that will be used to fill empty regions, e.g. after a rotation transform.\", width=200, options=[dropdown.Option(\"reflect\"), dropdown.Option(\"replicate\"), dropdown.Option(\"wrap\"), dropdown.Option(\"zero\"), dropdown.Option(\"prefill\")], value=stable_animation_prefs['border'], autofocus=False, on_change=lambda e:changed(e, 'border'))\n",
        "    use_inpainting_model = Checkbox(label=\"Use Inpainting Model\", tooltip=\"\", value=stable_animation_prefs['use_inpainting_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'use_inpainting_model'))\n",
        "    mask_min_value  = TextField(label=\"Mask Minimum\", value=stable_animation_prefs['mask_min_value'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'mask_min_value'))\n",
        "    mask_binarization_thr = SliderRow(label=\"Mask Binarization Threshold\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='mask_binarization_thr', expand=True, tooltip=\"Grayscale mask values lower than this value will be set to 0, values that are higher ‚Äî to 1.\")\n",
        "    color_coherence = Dropdown(label=\"Color Coherance\", hint_text=\"Color space that will be used for inter-frame color adjustments.\", width=350, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, options=[dropdown.Option(\"None\"), dropdown.Option(\"HSV\"), dropdown.Option(\"LAB\"), dropdown.Option(\"RGB\")], value=stable_animation_prefs['color_coherence'], autofocus=False, on_change=lambda e:changed(e, 'color_coherence'))\n",
        "    brightness_curve  = TextField(label=\"Brightness Curve\", value=stable_animation_prefs['brightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'brightness_curve'))\n",
        "    contrast_curve  = TextField(label=\"Contrast Curve\", value=stable_animation_prefs['contrast_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'contrast_curve'))\n",
        "    hue_curve  = TextField(label=\"Hue Curve\", value=stable_animation_prefs['hue_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'hue_curve'))\n",
        "    saturation_curve  = TextField(label=\"Saturation Curve\", value=stable_animation_prefs['saturation_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'saturation_curve'))\n",
        "    lightness_curve  = TextField(label=\"Lightness Curve\", value=stable_animation_prefs['lightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'lightness_curve'))\n",
        "    color_match_animate = Checkbox(label=\"Color Match Animate\", tooltip=\"Animate color match between key frames.\", value=stable_animation_prefs['color_match_animate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'color_match_animate'))\n",
        "    depth_model_weight = SliderRow(label=\"Depth Model Weight\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='depth_model_weight', expand=True, tooltip=\"Blend factor between AdaBins and MiDaS depth models.\")\n",
        "    fov_curve  = TextField(label=\"FOV Curve\", hint_text=\"FOV angle of camera volume in degrees. Max 180.\", value=stable_animation_prefs['fov_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'fov_curve'))\n",
        "    depth_blur_curve  = TextField(label=\"Depth Blur Curve\", hint_text=\"\", value=stable_animation_prefs['depth_blur_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_blur_curve'))\n",
        "    depth_warp_curve  = TextField(label=\"Depth Warp Curve\", hint_text=\"\", value=stable_animation_prefs['depth_warp_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_warp_curve'))\n",
        "    translation_x  = TextField(label=\"Translation X\", hint_text=\"\", value=stable_animation_prefs['translation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_x'))\n",
        "    translation_y  = TextField(label=\"Translation Y\", hint_text=\"\", value=stable_animation_prefs['translation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_y'))\n",
        "    translation_z  = TextField(label=\"Translation Z\", hint_text=\"\", value=stable_animation_prefs['translation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_z'))\n",
        "    angle  = TextField(label=\"Angle\", hint_text=\"\", value=stable_animation_prefs['angle'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'angle'))\n",
        "    zoom  = TextField(label=\"Zoom\", hint_text=\"\", value=stable_animation_prefs['zoom'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'zoom'))\n",
        "    rotation_x  = TextField(label=\"Rotation X\", hint_text=\"\", value=stable_animation_prefs['rotation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_x'))\n",
        "    rotation_y  = TextField(label=\"Rotation Y\", hint_text=\"\", value=stable_animation_prefs['rotation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_y'))\n",
        "    rotation_z  = TextField(label=\"Rotation Z\", hint_text=\"\", value=stable_animation_prefs['rotation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_z'))\n",
        "    camera_type = Dropdown(label=\"Camera Type\", hint_text=\"\", width=200, options=[dropdown.Option(\"Perspective\"), dropdown.Option(\"Orthographic\")], value=stable_animation_prefs['camera_type'], autofocus=False, on_change=lambda e:changed(e, 'camera_type'))\n",
        "    render_mode = Dropdown(label=\"Render Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"Mesh\"), dropdown.Option(\"Pointcloud\")], value=stable_animation_prefs['render_mode'], autofocus=False, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e, 'render_mode'))\n",
        "    mask_power = SliderRow(label=\"Mesh Power\", min=0, max=4.0, divisions=80, round=1, expand=True, pref=stable_animation_prefs, key='mask_power', tooltip=\"\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=stable_animation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    init_sizing = Dropdown(label=\"Init Sizing\", hint_text=\"\", width=200, options=[dropdown.Option(\"Cover\"), dropdown.Option(\"Stretch\"), dropdown.Option(\"Resize-Canvas\")], value=stable_animation_prefs['init_sizing'], autofocus=False, on_change=lambda e:changed(e, 'init_sizing'))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=stable_animation_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_mask))\n",
        "    mask_invert = Checkbox(label=\"Invert Mask\", tooltip=\"White in mask marks areas to change by default.\", value=stable_animation_prefs['mask_invert'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mask_invert'))\n",
        "    video_mix_in_curve  = TextField(label=\"Video Mixin Curve\", hint_text=\"\", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))\n",
        "    animation_mode = Dropdown(label=\"Animation Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"2D\"), dropdown.Option(\"3D warp\"), dropdown.Option(\"3D render\"), dropdown.Option(\"Video Input\")], value=stable_animation_prefs['animation_mode'], autofocus=False, on_change=lambda e:changed(e, 'animation_mode'))\n",
        "    init_video = TextField(label=\"Init Video File\", value=stable_animation_prefs['video_init_path'], on_change=lambda e:changed(e,'video_init_path'), expand=True, height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))\n",
        "    video_mix_in_curve  = TextField(label=\"Video Mixin Curve\", hint_text=\"\", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))\n",
        "    video_flow_warp = Checkbox(label=\"Video Flow Warp\", tooltip=\"Whether or not to transfer the optical flow from the video to the generated animation as a warp effect.\", value=stable_animation_prefs['video_flow_warp'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'video_flow_warp'))\n",
        "    extract_nth_frame  = TextField(label=\"Extract Nth Frame\", width=200, hint_text=\"Only use every Nth frame of the video\", value=stable_animation_prefs['extract_nth_frame'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'extract_nth_frame', ptype='int'))\n",
        "    video_init_fps  = TextField(label=\"Init Video FPS\", width=200, hint_text=\"\", value=stable_animation_prefs['video_init_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'video_init_fps', ptype='int'))\n",
        "    output_fps  = TextField(label=\"Output FPS\", width=200, hint_text=\"Frame rate to use when generating video output.\", value=stable_animation_prefs['output_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'output_fps', ptype='int'))\n",
        "    frame_interpolation_mode = Dropdown(label=\"Frame Interpolation Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"Rife\"), dropdown.Option(\"Film\"), dropdown.Option(\"None\")], value=stable_animation_prefs['frame_interpolation_mode'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_mode'))\n",
        "    frame_interpolation_factor = Dropdown(label=\"Frame Interpolation Factor\", hint_text=\"\", width=200, options=[dropdown.Option(\"2\"), dropdown.Option(\"4\"), dropdown.Option(\"8\")], value=stable_animation_prefs['frame_interpolation_factor'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_factor'))\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=stable_animation_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=stable_animation_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=stable_animation_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    #lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switcher(label=\"Lower Memory Mode\", value=stable_animation_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(stable_animation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    resume = Switcher(label=\"Resume Previous Animation\", value=stable_animation_prefs['resume'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume)\n",
        "    resume_from = TextField(label=\"Resume from Frame\", width=150, value=str(stable_animation_prefs['resume_from']), visible=stable_animation_prefs['resume'], keyboard_type=KeyboardType.NUMBER, tooltip=\"-1 resumes from last frame\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    batch_folder_name = TextField(label=\"Animation Folder Name\", value=stable_animation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=stable_animation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=stable_animation_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=stable_animation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_stable_animation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_stable_animation.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.stable_animation_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.stable_animation_output.controls) > 0\n",
        "    animation_prompts = Column([], spacing=0)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü•è  Stable Animation SDK\", \"Use Stability.ai API Credits for Advanced Video Generation, similar to Deforum & Disco Diffusion.\", actions=[copy_preset_button, paste_preset_button, default_preset_button, IconButton(icon=icons.HELP, tooltip=\"Help with Stable Animation Settings\", on_click=stable_animation_help)]),\n",
        "        #ResponsiveRow([animation_prompt, negative_prompt], vertical_alignment=CrossAxisAlignment.START),\n",
        "        Row([frame, prompt, add_prompt_keyframe]),\n",
        "        animation_prompts,\n",
        "        Divider(thickness=2, height=4),\n",
        "        Row([negative_prompt, negative_prompt_weight]),\n",
        "        Row([max_frames, fps]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        Row([model_checkpoint, generation_sampler, style_preset, clip_guidance], wrap=True),\n",
        "        ResponsiveRow([steps_strength_adj, interpolate_prompts, locked_seed]),\n",
        "        ResponsiveRow([noise_add_curve, noise_scale_curve, strength_curve, diffusion_cadence_curve]),\n",
        "        Row([cadence_interp, cadence_spans]),\n",
        "        Row([border, inpaint_border, use_inpainting_model]),\n",
        "        Row([mask_min_value, mask_binarization_thr]),\n",
        "        ResponsiveRow([color_coherence, brightness_curve, contrast_curve, hue_curve, saturation_curve, lightness_curve]),\n",
        "        Row([color_match_animate, depth_model_weight]),\n",
        "        ResponsiveRow([fov_curve, depth_blur_curve, depth_warp_curve]),\n",
        "        ResponsiveRow([translation_x, translation_y, translation_z, zoom]),\n",
        "        ResponsiveRow([rotation_x, rotation_y, rotation_z, angle]),\n",
        "        Row([camera_type, render_mode, mask_power]),\n",
        "        Row([init_image, init_sizing]),\n",
        "        Row([mask_image, mask_invert]),\n",
        "        Row([init_video, animation_mode]),\n",
        "        Row([video_flow_warp, video_mix_in_curve]),\n",
        "        Row([extract_nth_frame, video_init_fps, output_fps, Row([resume, resume_from])], wrap=True),\n",
        "        Row([export_to_video, frame_interpolation_mode, frame_interpolation_factor]),\n",
        "        width_slider, height_slider,\n",
        "        page.ESRGAN_block_stable_animation,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üßå  Run Stable Animation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page, from_list=True))\n",
        "        ]),\n",
        "        page.stable_animation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "svd_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_image': '',\n",
        "    'svd_model': 'SVD-img2vid-XT',\n",
        "    'num_inference_steps': 25,\n",
        "    'min_guidance_scale': 1.0,\n",
        "    'max_guidance_scale': 3.0,\n",
        "    'fps': 8,\n",
        "    'target_fps': 30,\n",
        "    'motion_bucket_id': 127,#180\n",
        "    'num_frames': 25,\n",
        "    'decode_chunk_size': 10,\n",
        "    'noise_aug_strength': 0.02,\n",
        "    'export_to_video': True,\n",
        "    \"interpolate_video\": True,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'width': 1024,\n",
        "    'height': 576,\n",
        "    'cpu_offload': not prefs['higher_vram_mode'],\n",
        "    'num_videos': 1,\n",
        "    'resume_frame': False,\n",
        "    'continue_times': 1,\n",
        "    'file_prefix': 'svd-',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildSVD(page):\n",
        "    global svd_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            svd_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            svd_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            svd_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def svd_help(e):\n",
        "      def close_svd_dlg(e):\n",
        "        nonlocal svd_help_dlg\n",
        "        svd_help_dlg.open = False\n",
        "        page.update()\n",
        "      svd_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with SVD Image-To-Video\"), content=Column([\n",
        "          Text(\"Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. The SVD checkpoint is trained to generate 14 frames and the SVD-XT checkpoint is further finetuned to generate 25 frames. (SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. This model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size. We also finetune the widely used f8-decoder for temporal consistency. Developed and funded by Stability AI.\"),\n",
        "          Text(\"The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism. The model may generate videos without motion, or very slow camera pans. The model cannot be controlled through text. The model cannot render legible text. Faces and people in general may not be generated properly. The autoencoding part of the model is lossy.\"),\n",
        "          Markdown(\"[Huggingface Model](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid) | [GitHub repository](https://github.com/Stability-AI/generative-models) | [Paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üÜí  Extra cool... \", on_click=close_svd_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = svd_help_dlg\n",
        "      svd_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        svd_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def toggle_resume(e):\n",
        "        svd_prefs['resume_frame'] = e.control.value\n",
        "        resume_container.visible = svd_prefs['resume_frame']\n",
        "        resume_container.update()\n",
        "    #prompt = TextField(label=\"Animation Prompt Text\", value=svd_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    #negative_prompt  = TextField(label=\"Negative Prompt Text\", value=svd_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = FileInput(label=\"Init Image\", pref=svd_prefs, key='init_image', filled=True, page=page)\n",
        "    svd_model = Dropdown(label=\"SVD Model\", width=200, options=[dropdown.Option(\"SVD-img2vid-XT\"), dropdown.Option(\"SVD-img2vid\")], value=svd_prefs['svd_model'], on_change=lambda e: changed(e, 'svd_model'))\n",
        "    #num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=svd_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=svd_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    min_guidance = SliderRow(label=\"Min Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=svd_prefs, key='min_guidance_scale', col={'sm':6}, tooltip=\"Used for the classifier free guidance with first frame.\")\n",
        "    max_guidance = SliderRow(label=\"Max Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=svd_prefs, key='max_guidance_scale', col={'sm':6}, tooltip=\"Used for the classifier free guidance with last frame.\")\n",
        "    motion_bucket_id = SliderRow(label=\"Motion Bucket ID\", min=1, max=300, divisions=299, pref=svd_prefs, key='motion_bucket_id', tooltip=\"Increasing the motion bucket id will increase the motion of the generated video.\")\n",
        "    decode_chunk_size = SliderRow(label=\"Decode Chunk Size\", min=1, max=25, divisions=24, pref=svd_prefs, key='decode_chunk_size', tooltip=\"The number of frames to decode at a time. The higher the chunk size, the higher the temporal consistency between frames, but also the higher the memory consumption. By default, the decoder will decode all frames at once for maximal quality. Reduce `decode_chunk_size` to reduce memory usage.\")\n",
        "    noise_aug_strength = SliderRow(label=\"Noise Augmented Strength\", min=0.0, max=0.1, divisions=20, round=3, pref=svd_prefs, key='noise_aug_strength', tooltip=\"The amount of noise added to the init image, the higher it is the less the video will look like the init image. Increase it for more motion.\")\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=svd_prefs, key='fps', tooltip=\"The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=svd_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=svd_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=svd_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    max_size = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=16, multiple=64, suffix=\"px\", pref=svd_prefs, key='max_size')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=svd_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=svd_prefs, key='height')\n",
        "    interpolate_video = Switcher(label=\"Interpolate Video\", value=svd_prefs['interpolate_video'], tooltip=\"Use Google FiLM Interpolation to transition between frames.\", on_change=lambda e:changed(e,'interpolate_video'))\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=svd_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.\")\n",
        "    num_videos = NumberPicker(label=\"Number of Videos: \", min=1, max=8, value=svd_prefs['num_videos'], on_change=lambda e: changed(e, 'num_videos'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=svd_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    batch_folder_name = TextField(label=\"Video Folder Name\", value=svd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    resume_frame = Switcher(label=\"Resume from Last Frame\", value=svd_prefs['resume_frame'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume, tooltip=\"Continues generating frames in chunks for videos longer than 14 or 25 frame limit.\")\n",
        "    resume_container = Container(content=NumberPicker(label=\"Times to Continue: \", min=1, max=10, value=svd_prefs['continue_times'], on_change=lambda e: changed(e, 'continue_times')), visible=svd_prefs['resume_frame'])\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(svd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=svd_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=svd_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=svd_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=svd_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_svd = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_svd.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not svd_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üçÉ  Stable Video Diffusion Image-To-Video Synthesis\", \"Generate high resolution (576x1024) 2-4 second videos conditioned on the input image...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with SVD Settings\", on_click=svd_help)]),\n",
        "        #ResponsiveRow([prompt, negative_prompt]),\n",
        "        init_image,\n",
        "        decode_chunk_size,\n",
        "        motion_bucket_id,\n",
        "        noise_aug_strength,\n",
        "        #num_frames,\n",
        "        fps,\n",
        "        Row([resume_frame, resume_container]),\n",
        "        num_inference_row,\n",
        "        ResponsiveRow([min_guidance, max_guidance]),\n",
        "        max_size,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_svd,\n",
        "        Row([svd_model, interpolate_video, cpu_offload]),\n",
        "        Row([num_videos, seed, batch_folder_name, file_prefix]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"ü™≠  Run SVD\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_svd(page)),\n",
        "        ]),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "roop_prefs = {\n",
        "    'source_image': '',\n",
        "    'target_image': '',\n",
        "    'frame_processor': 'face_swapper',\n",
        "    'keep_fps': True,\n",
        "    'keep_audio': True,\n",
        "    'keep_frames': True,\n",
        "    'many_faces': False,\n",
        "    'video_encoder': 'libx264',\n",
        "    'video_quality': 18,\n",
        "    'output_name': '',\n",
        "    'max_size': 768,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildROOP(page):\n",
        "    global roop_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            roop_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            roop_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            roop_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_roop_output(o):\n",
        "      page.roop_output.controls.append(o)\n",
        "      page.roop_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.roop_output.controls = []\n",
        "      page.roop_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def roop_help(e):\n",
        "      def close_roop_dlg(e):\n",
        "        nonlocal roop_help_dlg\n",
        "        roop_help_dlg.open = False\n",
        "        page.update()\n",
        "      roop_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ROOP Face Swap\"), content=Column([\n",
        "          Text(\"Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on Start. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named <video_title> where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That's it.\"),\n",
        "          Text(\"Roop is such a powerful tool that can be used for many purposes, so it‚Äôs crucial to know the potential risks of using Roop. Roop Deepfake is an experimental project that aims to make deep fake technology more accessible and easy to use. It uses a library called insightface and some models to detect and replace faces. You can also use GPU acceleration to speed up the process.\"),\n",
        "          Text(\"Disclaimer: This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.\"),\n",
        "          Markdown(\" [GitHub Page](https://github.com/s0md3v/roop) | [HuggingFace Space](https://huggingface.co/spaces/zhsso/roop) | [Colab](https://colab.research.google.com/drive/1uX5k33KNXprOeu_P9iov1byLOd4XGo1i#scrollTo=aN1XeEX_tsra)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üë∂  Promise not to abuse... \", on_click=close_roop_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = roop_help_dlg\n",
        "      roop_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          roop_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          roop_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"source\":\n",
        "          source_image.value = fname\n",
        "          source_image.update()\n",
        "          roop_prefs['source_image'] = fname\n",
        "        elif pick_type == \"target\":\n",
        "          target_image.value = fname\n",
        "          target_image.update()\n",
        "          roop_prefs['target_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_source(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"source\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Source Image File\")\n",
        "    def pick_target(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"target\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\", \"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Target Video or Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        roop_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    source_image = TextField(label=\"Source Image of Face\", value=roop_prefs['source_image'], on_change=lambda e:changed(e,'source_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_source))\n",
        "    target_image = TextField(label=\"Target Video or Image\", value=roop_prefs['target_image'], on_change=lambda e:changed(e,'target_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_target))\n",
        "    keep_fps = Switcher(label=\"Keep FPS\", value=roop_prefs['keep_fps'], on_change=lambda e:changed(e,'keep_fps'))\n",
        "    keep_audio = Switcher(label=\"Keep Audio\", value=roop_prefs['keep_audio'], on_change=lambda e:changed(e,'keep_audio'))\n",
        "    keep_frames = Switcher(label=\"Keep Frames\", value=roop_prefs['keep_frames'], on_change=lambda e:changed(e,'keep_frames'))\n",
        "    many_faces = Switcher(label=\"Many Faces\", value=roop_prefs['many_faces'], on_change=lambda e:changed(e,'many_faces'))\n",
        "    frame_processor = Dropdown(label=\"Frame Processor\", width=160, options=[dropdown.Option(\"face_swapper\"), dropdown.Option(\"face_enhancer\")], value=roop_prefs['frame_processor'], on_change=lambda e: changed(e, 'frame_processor'))\n",
        "    video_encoder = Dropdown(label=\"Video Encoder\", width=160, options=[dropdown.Option(\"libx264\"), dropdown.Option(\"libx265\"), dropdown.Option(\"libvpx-vp9\")], value=roop_prefs['video_encoder'], on_change=lambda e: changed(e, 'video_encoder'))\n",
        "    video_quality = SliderRow(label=\"Video Quality\", min=0, max=50, divisions=49, pref=roop_prefs, key='video_quality')\n",
        "    max_row = SliderRow(label=\"Max Image Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=roop_prefs, key='max_size')\n",
        "    output_name = TextField(label=\"Output File Name\", value=roop_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=roop_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    num_images = NumberPicker(label=\"Number of Images: \", min=1, max=8, value=roop_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=roop_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=roop_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=roop_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_roop = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_roop.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.roop_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.roop_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé≠  ROOP Face Swapper\", \"Take a Video or Image and Replace the Face in it with a face of your choice, no dataset, no training needed...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ROOP\", on_click=roop_help)]),\n",
        "        #ResponsiveRow([Row([source_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        source_image,\n",
        "        target_image,\n",
        "        ResponsiveRow([Column([keep_fps, keep_frames, frame_processor], col={'md':6, 'lg':4, 'xl':3}), Column([keep_audio, many_faces, video_encoder], col={'md':6, 'lg':4, 'xl':3})]),\n",
        "        video_quality,\n",
        "        max_row,\n",
        "        Row([output_name, batch_folder_name]),\n",
        "        page.ESRGAN_block_roop,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        ElevatedButton(content=Text(\"üò∑  Run ROOP Swap\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_roop(page)),\n",
        "        page.roop_output,\n",
        "        #clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "video_retalking_prefs = {\n",
        "    'input_audio': '',\n",
        "    'target_video': '',\n",
        "    'exp_img': 'neutral',\n",
        "    'exp_image': '',\n",
        "    'up_face': 'original',\n",
        "    'fps': 25,\n",
        "    'img_size': 384,\n",
        "    'nosmooth': False,\n",
        "    'output_name': '',\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "def buildVideoReTalking(page):\n",
        "    global video_retalking_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            video_retalking_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            video_retalking_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            video_retalking_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_video_retalking_output(o):\n",
        "      page.video_retalking_output.controls.append(o)\n",
        "      page.video_retalking_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.video_retalking_output.controls = []\n",
        "      page.video_retalking_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def toggle_expression(e):\n",
        "      video_retalking_prefs['exp_img'] = e.control.value\n",
        "      exprerssion.visible = video_retalking_prefs['exp_img'] == \"Image\"\n",
        "      exprerssion.update()\n",
        "    def video_retalking_help(e):\n",
        "      def close_video_retalking_dlg(e):\n",
        "        nonlocal video_retalking_help_dlg\n",
        "        video_retalking_help_dlg.open = False\n",
        "        page.update()\n",
        "      video_retalking_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Video ReTalking\"), content=Column([\n",
        "          Text(\"We present VideoReTalking, a new system to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion. Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism. Given a talking-head video, we first modify the expression of each frame according to the same expression template using the expression editing network, resulting in a video with the canonical expression. This video, together with the given audio, is then fed into the lip-sync network to generate a lip-syncing video. Finally, we improve the photo-realism of the synthesized faces through an identity-aware face enhancement network and post-processing. We use learning-based approaches for all three steps and all our modules can be tackled in a sequential pipeline without any user intervention.\"),\n",
        "          Text(\"Credit goes to Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang and Xidian University, Tencent AI Lab, Tsinghua University\"),\n",
        "          Markdown(\"[Paper](https://arxiv.org/abs/2211.14758) | [GitHub Page](https://github.com/OpenTalker/video-retalking) | [Project Page](https://opentalker.github.io/video-retalking/) | [Colab](https://colab.research.google.com/github/vinthony/video-retalking/blob/main/quick_demo.ipynb)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üí¨  Let's Lip Sync... \", on_click=close_video_retalking_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = video_retalking_help_dlg\n",
        "      video_retalking_help_dlg.open = True\n",
        "      page.update()\n",
        "    target_video = FileInput(label=\"Target Video with Face\", pref=video_retalking_prefs, key='target_video', ftype=\"video\", page=page)\n",
        "    input_audio = FileInput(label=\"Input Audio with Dialog\", pref=video_retalking_prefs, key='input_audio', ftype=\"audio\", page=page)\n",
        "    output_name = TextField(label=\"Output File Name\", value=video_retalking_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    up_face = Dropdown(label=\"Up Face\", hint_text=\"\", width=200, options=[dropdown.Option(\"original\"), dropdown.Option(\"surprise\"), dropdown.Option(\"angry\")], value=video_retalking_prefs['up_face'], autofocus=False, on_change=lambda e:changed(e, 'up_face'))\n",
        "    exp_img = Dropdown(label=\"Expression\", hint_text=\"\", width=200, options=[dropdown.Option(\"neutral\"), dropdown.Option(\"smile\"), dropdown.Option(\"Image\")], value=video_retalking_prefs['exp_img'], autofocus=False, on_change=toggle_expression)\n",
        "    exp_image = FileInput(label=\"Facial Expression Image\", pref=video_retalking_prefs, key='exp_image', ftype=\"image\", expand=False, page=page)\n",
        "    exprerssion = Container(content=exp_image, expand=True, visible=False)\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=video_retalking_prefs, key='fps', tooltip=\"The FPS to save target video clip.\", col={'lg':6})\n",
        "    img_size = SliderRow(label=\"Max Image Size\", min=256, max=1024, divisions=48, multiple=16, suffix=\"px\", pref=video_retalking_prefs, key='img_size', col={'lg':6})\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=video_retalking_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    page.video_retalking_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.video_retalking_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëÑ  Video ReTalking\", \"Audio-based Lip Synchronization for Talking Head Video Editing in the Wild...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with VideoReTalking\", on_click=video_retalking_help)]),\n",
        "        #ResponsiveRow([Row([input_audio, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        target_video,\n",
        "        input_audio,\n",
        "        Row([up_face, exp_img, exprerssion]),\n",
        "        ResponsiveRow([fps, img_size]),\n",
        "        Row([output_name, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"üó£  Run Video-ReTalking\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_retalking(page)),\n",
        "        page.video_retalking_output,\n",
        "        #clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "style_crafter_prefs = {\n",
        "    'init_video': '',\n",
        "    'init_image': '',\n",
        "    'style_images': [],\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 7.0,\n",
        "    'eta': 1.0,\n",
        "    'style_strength': 1.0,\n",
        "    'selected_mode': 'video',\n",
        "    'export_to_video': True,\n",
        "    'save_frames': False,\n",
        "    \"output_video\": True,\n",
        "    'seed': 0,\n",
        "    'width': 512,\n",
        "    'height': 320,\n",
        "    'max_size': 1024,\n",
        "    'n_samples': 1,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildStyleCrafter(page):\n",
        "    global style_crafter_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            style_crafter_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            style_crafter_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            style_crafter_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def style_crafter_help(e):\n",
        "      def close_style_crafter_dlg(e):\n",
        "        nonlocal style_crafter_help_dlg\n",
        "        style_crafter_help_dlg.open = False\n",
        "        page.update()\n",
        "      style_crafter_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Style Crafter\"), content=Column([\n",
        "          Text(\"We present StyleCrafter, a generic method that enhances pre-trained T2V models with style control, supporting Style-Guided Text-to-Image Generation and Style-Guided Text-to-Video Generation. Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. styleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors.\"),\n",
        "          Text(\"We propose a method to equip pre-trained Text-to-Video (T2V) models with a style adapter, allowing for the generation of stylized videos based on both a text prompt and a style reference image. The overview diagram is illustrated as the following figure. In this framework, the textual description dictates the video content, while the style image governs the visual style, ensuring a disentangled control over the video generation process. Given the limited availability of stylized videos, we employ a two-stage training strategy. Initially, we utilize an image dataset abundant in artistic styles to learn reference-based style modulation. Subsequently, adaptation finetuning on a mixed dataset of style images and realistic videos is conducted to improve the temporal quality of the generated videos.\"),\n",
        "          Markdown(\"Credits go to [GongyeLiu](https://github.com/GongyeLiu), [Menghan Xia](https://menghanxia.github.io/), [Yong Zhang](https://yzhang2016.github.io), [Haoxin Chen](https://scholar.google.com/citations?user=6UPJSvwAAAAJ&hl=zh-CN&oi=ao), [Jinbo Xing](https://doubiiu.github.io/), [Xintao Wang](https://xinntao.github.io/), [Yujiu Yang](https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=zh-CN&oi=ao), [Ying Shan](https://scholar.google.com/citations?hl=en&user=4oXBp9UAAAAJ&view_op=list_works&sortby=pubdate)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Markdown(\"[Project Page](https://gongyeliu.github.io/StyleCrafter.github.io/) | [Paper](https://arxiv.org/abs/2312.00330) | [GitHub Code](https://github.com/GongyeLiu/StyleCrafter) | [HuggingFace Space](https://huggingface.co/spaces/liuhuohuo/StyleCrafter)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üë∏  So Pretty... \", on_click=close_style_crafter_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = style_crafter_help_dlg\n",
        "      style_crafter_help_dlg.open = True\n",
        "      page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.style_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.style_file_list.controls[i]\n",
        "              page.style_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.style_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.style_file_list.controls.clear()\n",
        "        page.style_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        #TODO: Get file size & resolution\n",
        "        alert_msg(e.page, \"Image Details\", content=Column([Text(img), Img(src=img, gapless_playback=True)]), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.style_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        if update: page.style_file_list.update()\n",
        "    def add_image(e):\n",
        "        save_dir = uploads_dir\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if init_image.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(init_image.value)\n",
        "          fpath = os.path.join(save_dir, init_image.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, style_crafter_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(init_image.value):\n",
        "          fpath = os.path.join(save_dir, init_image.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(init_image.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, style_crafter_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(init_image.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(init_image.value):\n",
        "          for f in os.listdir(init_image.value):\n",
        "            file_path = os.path.join(init_image.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, style_crafter_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(init_image.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          #else:\n",
        "          #  pick_path(e)\n",
        "          return\n",
        "        init_image.value = \"\"\n",
        "        init_image.update()\n",
        "        \n",
        "    prompt = TextField(label=\"Stylized Video Prompt Text\", value=style_crafter_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=style_crafter_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    #init_video = FileInput(label=\"Init Video Clip\", pref=style_crafter_prefs, key='init_video', ftype=\"video\", page=page)\n",
        "    init_image = FileInput(label=\"Input Style Image\", pref=style_crafter_prefs, key='init_image', expand=1, ftype=\"image\", page=page)\n",
        "    add_image_button = ft.FilledButton(content=Text(\"‚ûï  Add Image\"), on_click=add_image)\n",
        "    page.style_file_list = Column([], tight=True, spacing=0)\n",
        "\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=style_crafter_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=style_crafter_prefs, key='guidance_scale')\n",
        "    #strength = SliderRow(label=\"Init Image Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=style_crafter_prefs, key='strength', tooltip=\"Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.\")\n",
        "    #max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=style_crafter_prefs, key='max_size')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=32, multiple=32, suffix=\"px\", pref=style_crafter_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=32, multiple=32, suffix=\"px\", pref=style_crafter_prefs, key='height')\n",
        "    style_strength = SliderRow(label=\"Style Strength\", min=0.0, max=1.0, divisions=10, round=1, pref=style_crafter_prefs, key='style_strength', col={'lg':6}, tooltip=\"\")\n",
        "    output_video = Tooltip(message=\"Otherwise will Save Image with style\", content=Switcher(label=\"Output Animated Video\", value=style_crafter_prefs['output_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'output_video')))\n",
        "    interpolate_vid = Switcher(label=\"Interpolate Video\", value=style_crafter_prefs['export_to_video'], tooltip=\"Use Google FiLM Interpolation to transition between frames.\", on_change=lambda e:changed(e,'export_to_video'))\n",
        "    save_frames = Tooltip(message=\"Save Frames\", content=Switcher(label=\"Save Frames\", value=style_crafter_prefs['save_frames'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames')))\n",
        "    def change_mode(e):\n",
        "        style_crafter_prefs['selected_mode'] = e.data\n",
        "        mode = e.data.split('\"')[1].title()\n",
        "        prompt.label = f\"Stylized {mode} Prompt Text\"\n",
        "        prompt.update()\n",
        "        interpolate_vid.visible = mode == \"Video\"\n",
        "        save_frames.visible = mode == \"Video\"\n",
        "        interpolate_vid.update()\n",
        "        save_frames.update()\n",
        "    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={style_crafter_prefs['selected_mode']}, allow_multiple_selection=False,\n",
        "        segments=[\n",
        "            ft.Segment(value=\"video\", label=ft.Text(\"Video\"), icon=ft.Icon(ft.icons.VIDEO_CAMERA_BACK)),\n",
        "            ft.Segment(value=\"image\", label=ft.Text(\"Image\"), icon=ft.Icon(ft.icons.IMAGE)),\n",
        "        ],\n",
        "    )\n",
        "    #model = Dropdown(label=\"Video Model\", hint_text=\"\", expand=True, options=[dropdown.Option(\"damo-vilab/text-to-video-ms-1.7b\"), dropdown.Option(\"modelscope-damo-text2video-synthesis\"), dropdown.Option(\"modelscope-damo-text2video-pruned-weights\"), dropdown.Option(\"cerspense/zeroscope_v2_XL\"), dropdown.Option(\"cerspense/zeroscope_v2_576w\")], value=style_crafter_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=style_crafter_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(style_crafter_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëó  StyleCrafter Text-to-Video-or-Image\", \"Enhancing Stylized Video or Image Generation with Style Adapter...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with StyleCrafter Settings\", on_click=style_crafter_help)]),\n",
        "        #ResponsiveRow([prompt, negative_prompt]),\n",
        "        prompt,\n",
        "        #init_video,\n",
        "        Row([init_image, add_image_button]),\n",
        "        page.style_file_list,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        style_strength,\n",
        "        #max_row,\n",
        "        width_slider, height_slider,\n",
        "        Row([selected_mode, interpolate_vid, save_frames]),\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([ElevatedButton(content=Text(\"üö§   Run StyleCrafter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_style_crafter(page)),]),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "rave_prefs = {\n",
        "    'init_video': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'control_task': 'SoftEdge HED',\n",
        "    'control_tasks': [],\n",
        "    'conditioning_scale': 1.8,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'controlnet_strength': 0.9,\n",
        "    'num_inference_steps': 50,\n",
        "    'num_inversion_steps': 50,\n",
        "    'max_size': 512,\n",
        "    'give_control_inversion': True,\n",
        "    'is_ddim_inversion': False,\n",
        "    'is_shuffle': False,\n",
        "    'save_frames': True,\n",
        "    'seed': 0,\n",
        "    'batch_size': 1,\n",
        "    'batch_size_vae': 1,\n",
        "    'file_prefix': 'rave-',\n",
        "    'output_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildRAVE(page):\n",
        "    global rave_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            rave_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            rave_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            rave_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def rave_help(e):\n",
        "      def close_rave_dlg(e):\n",
        "        nonlocal rave_help_dlg\n",
        "        rave_help_dlg.open = False\n",
        "        page.update()\n",
        "      rave_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with RAVE\"), content=Column([\n",
        "          Text(\"RAVE is a zero-shot, lightweight, and fast framework for text-guided video editing, supporting videos of any length utilizing text-to-image pretrained diffusion models.\"),\n",
        "          Text('Recent advancements in diffusion-based models have demonstrated significant success in generating images from text. However, video editing models have not yet reached the same level of visual quality and user control. To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training. RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure. It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods. It is also efficient in terms of memory requirements, allowing it to handle longer videos. RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations. In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats. Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods.'),\n",
        "          Markdown(\"Credits go to [Ozgur Kara](https://karaozgur.com/), [Bariscan Kurtkaya](https://bariscankurtkaya.github.io/), [Hidir Yesiltepe](https://sites.google.com/view/hidir-yesiltepe), [James M. Rehg](https://scholar.google.com/citations?hl=en&user=8kA3eDwAAAAJ), [Pinar Yanardag](https://scholar.google.com/citations?user=qzczdd8AAAAJ&hl=en), Georgia Institute of Technology, KUIS AI Center, University of Illinois Urbana-Champaign, Virginia Tech\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          #Text(\"\"),\n",
        "          Text(\"LineArt - An image with line art, usually black lines on a white background.\"),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"SoftEdge HED - A monochrome image with white soft edges on a black background.\"),\n",
        "          Markdown(\"[GitHub Code](https://github.com/rehg-lab/RAVE) | [arXiv](https://arxiv.org/abs/2312.04524) | [Paper](https://rave-video.github.io/static/pdfs/RAVE.pdf) | [Project Page](https://rave-video.github.io/)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üí∫  Real Smooth... \", on_click=close_rave_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = rave_help_dlg\n",
        "      rave_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        rave_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=rave_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=rave_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    #a_prompt = TextField(label=\"Additional Prompt Text\", value=rave_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=220, options=[dropdown.Option(t) for t in ['LineArt Realistic', 'LineArt Coarse', \"LineArt Standard\", \"LineArt Anime\", \"LineArt Anime Denoise\", \"SoftEdge HED\", \"SoftEdge HEDsafe\", \"SoftEdge PIDInet\", \"SoftEdge PIDsafe\", \"Canny\", \"Depth Leres\", \"Depth Leres++\", \"Depth Midas\", \"Depth Zoe\"]], value=rave_prefs['control_task'], on_change=lambda e:changed(e,'control_task'))\n",
        "    #conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=rave_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `rave_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0.0, max=5.0, divisions=50, round=1, pref=rave_prefs, key='conditioning_scale', tooltip=\"Strength of the ControlNet Mask.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=rave_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=rave_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "\n",
        "    controlnet_strength = SliderRow(label=\"ControlNet Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=rave_prefs, key='controlnet_strength', tooltip=\"How much influence the controlnet annotator's output is used to guide the denoising process.\")\n",
        "    init_video = FileInput(label=\"Init Video Clip\", pref=rave_prefs, key='init_video', ftype=\"video\", expand=True, page=page)\n",
        "    #fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=rave_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    num_inference_steps = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=rave_prefs, key='num_inference_steps', tooltip=\"Steps during the sampling process.\")\n",
        "    num_inversion_steps = SliderRow(label=\"Number of Inversion Steps\", min=1, max=100, divisions=99, pref=rave_prefs, key='num_inversion_steps', tooltip=\"Steps during the inversion process.\")\n",
        "    #prompt_strength = SliderRow(label=\"Prompt Strength\", min=0, max=30, divisions=60, round=1, pref=rave_prefs, key='prompt_strength', tooltip=\"How much influence the prompt has on the output. Guidance Scale.\")\n",
        "    max_size = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=48, multiple=16, suffix=\"px\", pref=rave_prefs, key='max_size')\n",
        "    give_control_inversion = Switcher(label=\"Smooth Boundary\", value=rave_prefs['give_control_inversion'], tooltip=\"Give control to the inversion.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'give_control_inversion'))\n",
        "    is_ddim_inversion = Switcher(label=\"DDIM Inversion\", value=rave_prefs['is_ddim_inversion'], tooltip=\"Use ddim for inversion\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'is_ddim_inversion'))\n",
        "    is_shuffle = Switcher(label=\"Shuffling Between Grids\", value=rave_prefs['is_shuffle'], tooltip=\"Apply shuffling between the grids.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'is_shuffle'))\n",
        "    save_frames = Switcher(label=\"Save Frames\", value=rave_prefs['save_frames'], tooltip=\"Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))\n",
        "    seed = TextField(label=\"Seed\", value=rave_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=rave_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    output_name = TextField(label=\"Output Name\", value=rave_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=4, value=rave_prefs['batch_size'], tooltip=\"Batch size of grids (e.g. 4 grids run in parallel)\", on_change=lambda e: changed(e, 'batch_size'))\n",
        "    batch_size_vae = NumberPicker(label=\"VAE Batch Size: \", min=1, max=4, value=rave_prefs['batch_size_vae'], tooltip=\"Batch size for the VAE (e.g. 1 grid runs in parallel for the VAE)\", on_change=lambda e: changed(e, 'batch_size_vae'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=rave_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=rave_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=rave_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=rave_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üê¶‚Äç‚¨õ  RAVE Video-to-Video\", \"Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with RAVE Vid2Vid Settings\", on_click=rave_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([control_task, init_video]),\n",
        "        conditioning_scale,\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        controlnet_strength,\n",
        "        num_inference_steps,\n",
        "        num_inversion_steps,\n",
        "        max_size,\n",
        "        #ResponsiveRow([motion_alpha, motion_sigma]),\n",
        "        #ResponsiveRow([max_dimension, min_dimension]),\n",
        "        Row([batch_size, batch_size_vae, give_control_inversion, is_ddim_inversion, is_shuffle]),\n",
        "        Row([seed, output_name, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üíÄ  Run RAVE on Video\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_rave(page))]),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "tokenflow_prefs = {\n",
        "    'init_video': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'ugly, blurry, low res, unrealistic, unaesthetic',\n",
        "    'inversion_prompt': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'num_inversion_steps': 500,\n",
        "    'guidance_scale': 7.5,\n",
        "    'fps': 30,\n",
        "    'sd_version': '2.1',\n",
        "    'num_frames': 40,\n",
        "    'export_to_video': False,\n",
        "    'seed': 0,\n",
        "    'width': 1024,\n",
        "    'height': 576,\n",
        "    'batch_size': 8,\n",
        "    'selected_mode': 'pnp',\n",
        "    'pnp_attn_t': 0.5,\n",
        "    'pnp_f_t': 0.8,\n",
        "    'start': 0.9,\n",
        "    'use_ddim_noise': True,\n",
        "    'batch_folder_name': '',\n",
        "}\n",
        "\n",
        "def buildTokenFlow(page):\n",
        "    global tokenflow_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            tokenflow_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            tokenflow_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            tokenflow_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def tokenflow_help(e):\n",
        "      def close_tokenflow_dlg(e):\n",
        "        nonlocal tokenflow_help_dlg\n",
        "        tokenflow_help_dlg.open = False\n",
        "        page.update()\n",
        "      tokenflow_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with TokenFlow Video-To-Video\"), content=Column([\n",
        "          Text(\"The generative AI revolution has been recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and dynamics of the input video. Our method is based on our key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.\"),\n",
        "          Text(\"We observe that the level of temporal consistency of a video is tightly related to the temporal consistency of its feature representation, as can be seen in the feature visualization below. The features of a natural video have a shared, temporally consistent representation. When editing the video per frame, this consistency breaks. Our method ensures the same level of feature consistency as in the original video features.\"),\n",
        "          Markdown(\"[Project Page](https://diffusion-tokenflow.github.io) | [GitHub repository](https://github.com/omerbt/TokenFlow) | [Paper](https://diffusion-tokenflow.github.io/TokenFlow_Arxiv.pdf)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Credits go to Geyer, Michal and Bar-Tal, Omer Bar-Tal, Bagon, Shai and Dekel, Tali, Harry Chen and HuggingFace\")\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üåü  Extra Bonus Points... \", on_click=close_tokenflow_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = tokenflow_help_dlg\n",
        "      tokenflow_help_dlg.open = True\n",
        "      page.update()\n",
        "    def change_mode(e):\n",
        "        mode = e.data.split('\"')[1]\n",
        "        tokenflow_prefs['selected_mode'] = mode\n",
        "        pnp_container.visible = mode == \"pnp\"\n",
        "        sdedit_container.visible = mode == \"sdedit\"\n",
        "        pnp_container.update()\n",
        "        sdedit_container.update()\n",
        "        num_inversion_steps.set_value(500 if mode == \"pnp\" else 100)\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=tokenflow_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=tokenflow_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    inversion_prompt = TextField(label=\"Inversion Prompt Describing Video\", value=tokenflow_prefs['inversion_prompt'], multiline=True, on_change=lambda e:changed(e,'inversion_prompt'))\n",
        "    init_video = FileInput(label=\"Initial Video Clip\", pref=tokenflow_prefs, key='init_video', ftype=\"video\", page=page)\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=tokenflow_prefs, key='num_frames', tooltip=\"The number of video frames that are generated from init video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=tokenflow_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    num_inversion_steps = SliderRow(label=\"Number of Inversion Steps\", min=1, max=600, divisions=599, pref=tokenflow_prefs, key='num_inversion_steps', tooltip=\"Steps during the inversion process.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=tokenflow_prefs, key='guidance_scale')\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=tokenflow_prefs, key='fps')\n",
        "    batch_size = SliderRow(label=\"Batch Size\", min=1, max=60, divisions=59, pref=tokenflow_prefs, key='batch_size')\n",
        "    #eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=tokenflow_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=tokenflow_prefs, key='height')\n",
        "    #export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switcher(label=\"Export to Video\", value=tokenflow_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=tokenflow_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=tokenflow_prefs, key='height')\n",
        "    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={tokenflow_prefs['selected_mode']}, allow_multiple_selection=False,\n",
        "        segments=[\n",
        "            ft.Segment(value=\"pnp\", label=ft.Text(\"Plug-and-Play\"), icon=ft.Icon(ft.icons.POWER)),\n",
        "            ft.Segment(value=\"sdedit\", label=ft.Text(\"SDEdit\"), icon=ft.Icon(ft.icons.AUTO_FIX_NORMAL)),\n",
        "        ],\n",
        "    )\n",
        "    pnp_attn_t = SliderRow(label=\"Attention Token\", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='pnp_attn_t', col={'sm':6}, tooltip=\"Attention\")\n",
        "    pnp_f_t = SliderRow(label=\"Frame Token\", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='pnp_f_t', col={'sm':6}, tooltip=\"Frame\")\n",
        "    pnp_container = Container(ResponsiveRow([pnp_attn_t, pnp_f_t]), visible = tokenflow_prefs['selected_mode'] == \"pnp\")\n",
        "    start = SliderRow(label=\"Start Sampling\", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='start', expand=True, tooltip=\"Start sampling from t = start * 1000\")\n",
        "    use_ddim_noise = Switcher(label=\"Use DDIM Noise\", value=tokenflow_prefs['use_ddim_noise'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_ddim_noise'), tooltip=\"Use DDIM noise to noise the images. Better structure preservation.\")\n",
        "    sdedit_container = Container(Row([start, use_ddim_noise]), visible = tokenflow_prefs['selected_mode'] == \"sdedit\")\n",
        "    batch_folder_name = TextField(label=\"Video Folder Name\", value=tokenflow_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(tokenflow_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    sd_version = Dropdown(label=\"Stable Diffision Version\", width=170, options=[dropdown.Option(t) for t in ['1.5', '2.0', '2.1', 'ControlNet', 'depth', 'XL']], value=tokenflow_prefs['sd_version'], on_change=lambda e:changed(e,'sd_version'))\n",
        "    #page.tokenflow_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    #clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    #clear_button.visible = len(page.tokenflow_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåû  TokenFlow Video-To-Video\", \"Consistent Diffusion Features for Consistent Video Editing...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with TokenFlow Settings\", on_click=tokenflow_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        init_video,\n",
        "        inversion_prompt,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([export_to_video, lower_memory]),\n",
        "        num_frames,\n",
        "        fps,\n",
        "        num_inversion_steps,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        batch_size,\n",
        "        Row([Text(\"Diffusion Mode: \"), selected_mode]),\n",
        "        pnp_container,\n",
        "        sdedit_container,\n",
        "        #eta_slider,\n",
        "        #width_slider, height_slider,\n",
        "        #page.ESRGAN_block_tokenflow,\n",
        "        Row([sd_version, seed, batch_folder_name]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üåà  Run TokenFlow\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tokenflow(page)),\n",
        "        ]),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "animate_diff_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'head_prompt': '',\n",
        "    'tail_prompt': '',\n",
        "    'use_prompt_map': False,\n",
        "    'frame': '0',\n",
        "    'steps': 25,\n",
        "    'guidance_scale': 7.5,\n",
        "    'editing_prompts': [],\n",
        "    'animation_prompts': {},\n",
        "    'prompt_map': {},\n",
        "    'dreambooth_lora': 'realisticVisionV40_v20Novae',\n",
        "    'lora_alpha': 0.8,\n",
        "    'custom_lora': '',\n",
        "    'lora_layer': 'Add Detail',\n",
        "    'lora_layer_alpha': 0.8,\n",
        "    'custom_lora_layer': '',\n",
        "    'lora_map': [],\n",
        "    'motion_module': 'mm_sd_v15_v2',\n",
        "    'scheduler': 'k_dpmpp_2m',\n",
        "    'seed': 0,\n",
        "    'video_length': 16,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    'overlap': 12,\n",
        "    'stride': 4,\n",
        "    'context': 16,\n",
        "    'context_schedule': \"Uniform\",\n",
        "    'clip_skip': 1,\n",
        "    'save_frames': True,\n",
        "    'save_gif': True,\n",
        "    'save_video': False,\n",
        "    'is_loop': False,\n",
        "    'compile': prefs['enable_torch_compile'],\n",
        "    'control_task': 'Canny',\n",
        "    'control_frame': '0',\n",
        "    'original_image': '',\n",
        "    'conditioning_scale': 1.0,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'control_scale_list': '0.5,0.4,0.3,0.2,0.1',\n",
        "    'ref_image': '',\n",
        "    'controlnet_image': '',\n",
        "    'controlnet_layers': [],\n",
        "    'controlnet_tile': False, 'controlnet_ip2p': False, 'controlnet_lineart_anime': False, 'controlnet_openpose': False, 'controlnet_softedge': False, 'controlnet_shuffle': False, 'controlnet_depth': False, 'controlnet_canny': False, 'controlnet_inpaint': False, 'controlnet_lineart': False, 'controlnet_mlsd': False, 'controlnet_normalbae': False, 'controlnet_': False, 'controlnet_scribble': False, 'controlnet_seg': False,\n",
        "    'upscale_tile': False, 'upscale_ip2p': False, 'upscale_lineart_anime': False, 'upscale_ip2p': False, 'upscale_ref': False,\n",
        "    'upscale_steps': 20, 'upscale_strength': 0.5, 'upscale_guidance_scale': 10,\n",
        "    'upscale_amount': 1.5,\n",
        "    'use_ip_adapter': False,\n",
        "    'ip_adapter_image': '',\n",
        "    'ip_adapter_frame': '0',\n",
        "    'ip_adapter_layers': {},\n",
        "    'ip_adapter_scale': 0.5,\n",
        "    'ip_adapter_is_plus': True,\n",
        "    'ip_adapter_is_full_face': False,\n",
        "    'ip_adapter_is_plus_face': False,\n",
        "    'ip_adapter_light': False,\n",
        "    'use_img2img': False,\n",
        "    'img2img_image': '',\n",
        "    'img2img_frame': '0',\n",
        "    'img2img_layers': {},\n",
        "    'img2img_strength': 0.7,\n",
        "    'is_simple_composite': False,\n",
        "    'motion_loras': [],\n",
        "    'motion_loras_strength': 0.5,\n",
        "    'apply_lcm_lora': False,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "animate_diff_loras = [\n",
        "    {'name': 'toonyou_beta3', 'file': 'toonyou_beta3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors'},\n",
        "    {'name': 'CounterfeitV30_v30', 'file': 'CounterfeitV30_v30.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/CounterfeitV30_v30.safetensors'},\n",
        "    {'name': 'FilmVelvia2', 'file': 'FilmVelvia2.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/FilmVelvia2.safetensors'},\n",
        "    {'name': 'Pyramid lora_Ghibli_n3', 'file': 'Pyramid%20lora_Ghibli_n3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/Pyramid%20lora_Ghibli_n3.safetensors'},\n",
        "    {'name': 'TUSUN', 'file': 'TUSUN.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/TUSUN.safetensors'},\n",
        "    {'name': 'lyriel_v16', 'file': 'lyriel_v16.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/lyriel_v16.safetensors'},\n",
        "    {'name': 'majicmixRealistic_v5Preview', 'file': 'majicmixRealistic_v5Preview.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/majicmixRealistic_v5Preview.safetensors'},\n",
        "    {'name': 'moonfilm_filmGrain10', 'file': 'moonfilm_filmGrain10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_filmGrain10.safetensors'},\n",
        "    {'name': 'moonfilm_reality20', 'file': 'moonfilm_reality20.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_reality20.safetensors'},\n",
        "    {'name': 'rcnzCartoon3d_v10', 'file': 'rcnzCartoon3d_v10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/rcnzCartoon3d_v10.safetensors'},\n",
        "    {'name': 'realisticVisionV20_v20', 'file': 'realisticVisionV20_v20.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/blob/main/realisticVisionV20_v20.safetensors'},\n",
        "    {'name': 'realisticVisionV40_v20Novae', 'file': 'realisticVisionV40_v20Novae.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/realisticVisionV40_v20Novae.safetensors'},\n",
        "    {'name': 'Mistoon_Anime', 'file': 'mistoonAnime_v20.safetensors', 'path': 'https://huggingface.co/WickedOne/MistoonAnimeV2/blob/main/mistoonAnime_v20.safetensors'},\n",
        "    {'name': 'XXMix_9realistic_v4', 'file': 'xxmix9realistic_v40.safetensors', 'path': 'https://huggingface.co/jzli/XXMix_9realistic-v4/resolve/main/xxmix9realistic_v40.safetensors'},\n",
        "    {'name': 'revAnimated_v122', 'file': 'revAnimated_v122.safetensors', 'path': 'https://huggingface.co/joaov33/revanimated/blob/main/revAnimated_v122.safetensors'},\n",
        "    #{'name': 'Custom', 'file': '', 'path': ''},\n",
        "]\n",
        "animate_diff_lora_layers = [\n",
        "    {'name': 'Add Detail', 'file': 'add_detail.safetensors', 'path': 'https://civitai.com/api/download/models/118311?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Muffet v2', 'file': 'muffet_v2.safetensors', 'path': 'https://civitai.com/api/download/models/36809?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Vector Art', 'file': 'vector_art.safetensors', 'path': 'https://civitai.com/api/download/models/68115?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Detail Enhancer', 'file': 'detail_enhancer.safetensors', 'path': 'https://civitai.com/api/download/models/87153?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Studio Ghibli', 'file': 'studio_ghibli.safetensors', 'path': 'https://civitai.com/api/download/models/7657?type=Model&format=SafeTensor&size=full&fp=fp16'},\n",
        "    {'name': 'Arcane Style', 'file': 'arcane_style.safetensors', 'path': 'https://civitai.com/api/download/models/8339?type=Model&format=SafeTensor&size=full&fp=fp16'},\n",
        "    {'name': 'DC Comics', 'file': 'DC_Comics.safetensors', 'path': 'https://civitai.com/api/download/models/10580?type=Model&format=SafeTensor&size=full&fp=fp16'},\n",
        "    {'name': 'SteampunkAI', 'file': 'SteampunkAI.safetensors', 'path': 'https://civitai.com/api/download/models/24794?type=Model&format=SafeTensor&size=full&fp=fp16'},\n",
        "    {'name': 'Liminal Space', 'file': 'liminal_space.safetensors', 'path': 'https://civitai.com/api/download/models/72282?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Landscapes Mix', 'file': 'landscapes_mix.safetensors', 'path': 'https://civitai.com/api/download/models/8787?type=Model&format=SafeTensor&size=full&fp=fp16'},\n",
        "    {'name': 'Elixir Enhancer', 'file': 'elixir_enhancer.safetensors', 'path': 'https://civitai.com/api/download/models/83081?type=Model&format=SafeTensor'},\n",
        "    {'name': 'Amateur Porn', 'file': 'amateur_porn.safetensors', 'path': 'https://civitai.com/api/download/models/56939?type=Model&format=SafeTensor'},\n",
        "    {'name': 'HD Porn', 'file': 'hd_porn.safetensors', 'path': 'https://civitai.com/api/download/models/54388?type=Model&format=SafeTensor'},\n",
        "    #{'name': '', 'file': '.safetensors', 'path': ''},\n",
        "]\n",
        "animate_diff_motion_modules = [\n",
        "    {'name': 'mm_sd_v14', 'file': 'mm_sd_v14.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v14.ckpt'},\n",
        "    {'name': 'mm_sd_v15', 'file': 'mm_sd_v15.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15.ckpt'},\n",
        "    {'name': 'mm_sd_v15_v2', 'file': 'mm_sd_v15_v2.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15_v2.ckpt'},\n",
        "    {'name': 'mm_sdxl_v10_beta', 'file': 'mm_sdxl_v10_beta.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sdxl_v10_beta.ckpt'},\n",
        "    {'name': 'v3_sd15_mm', 'file': 'v3_sd15_mm.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_mm.ckpt'},\n",
        "    {'name': 'Long_mm_16_64_frames', 'file': 'lt_long_mm_16_64_frames.ckpt', 'path': 'https://huggingface.co/Lightricks/LongAnimateDiff/resolve/main/lt_long_mm_16_64_frames.ckpt'},\n",
        "    {'name': 'Long_mm_32_frames', 'file': 'lt_long_mm_32_frames.ckpt', 'path': 'https://huggingface.co/Lightricks/LongAnimateDiff/resolve/main/lt_long_mm_32_frames.ckpt'},\n",
        "    {'name': 'improved3DMotion', 'file': 'improved3DMotion_improved3DV1.ckpt', 'path': 'https://civitai.com/api/download/models/178017?type=Model&format=PickleTensor'},\n",
        "    {'name': 'TemporalDiff', 'file': 'temporaldiffMotion_v10.ckpt', 'path': 'https://civitai.com/api/download/models/160418?type=Model&format=PickleTensor'},\n",
        "    {'name': 'YoinkoorLab NSFW', 'file': 'yoinkoorlabsNSFWMotion_godmodev20.ckpt', 'path': 'https://civitai.com/api/download/models/177016?type=Model&format=PickleTensor'},\n",
        "    {'name': 'Improved Humans', 'file': 'improvedHumansMotion_refinedHumanMovement.ckpt', 'path': 'https://civitai.com/api/download/models/174464?type=Model&format=PickleTensor'},\n",
        "    {'name': 'ZlikwidDiff', 'file': 'zlikwiddiffV1_v10.ckpt', 'path': 'https://civitai.com/api/download/models/178745?type=Model&format=PickleTensor'},\n",
        "    {'name': 'Viddle-Pix2Pix', 'file': 'viddle-pix2pix-animatediff-v1.ckpt', 'path': 'https://huggingface.co/viddle/viddle-pix2pix-animatediff/resolve/main/viddle-pix2pix-animatediff-v1.ckpt'},\n",
        "]\n",
        "animate_diff_motion_loras = [\n",
        "    {'name': 'Zoom-In', 'file': 'v2_lora_ZoomOut.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_ZoomOut.ckpt'},\n",
        "    {'name': 'Zoom-Out', 'file': 'v2_lora_ZoomIn.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_ZoomIn.ckpt'},\n",
        "    {'name': 'Pan-Left', 'file': 'v2_lora_PanLeft.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_PanLeft.ckpt'},\n",
        "    {'name': 'Pan-Right', 'file': 'v2_lora_PanRight.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_PanRight.ckpt'},\n",
        "    {'name': 'Tilt-Up', 'file': 'v2_lora_TiltUp.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_TiltUp.ckpt'},\n",
        "    {'name': 'Tilt-Down', 'file': 'v2_lora_TiltDown.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_TiltDown.ckpt'},\n",
        "    {'name': 'Clockwise', 'file': 'v2_lora_RollingClockwise.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_RollingClockwise.ckpt'},\n",
        "    {'name': 'Anti-clockwise', 'file': 'v2_lora_RollingAnticlockwise.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_RollingAnticlockwise.ckpt'},\n",
        "    {'name': 'v3-SD1.5-Adapter', 'file': 'v3_sd15_adapter.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_adapter.ckpt'},\n",
        "]\n",
        "\n",
        "def buildAnimateDiff(page):\n",
        "    global animate_diff_prefs, prefs, pipe_animate_diff, editing_prompt\n",
        "    editing_prompt = {'prompt':'', 'negative_prompt':'', 'seed':0}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            animate_diff_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            animate_diff_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            animate_diff_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_animate_diff_output(o):\n",
        "      page.animate_diff_output.controls.append(o)\n",
        "      page.animate_diff_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.animate_diff_output.controls = []\n",
        "      page.animate_diff_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def animate_diff_help(e):\n",
        "      def close_animate_diff_dlg(e):\n",
        "        nonlocal animate_diff_help_dlg\n",
        "        animate_diff_help_dlg.open = False\n",
        "        page.update()\n",
        "      animate_diff_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with AnimateDiff\"), content=Column([\n",
        "          Text(\"With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs.\"),\n",
        "          Text(\"Credits: Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. Also Neggles for refactoring, Camenduru and UI by Alan Bedian\"),\n",
        "          Markdown(\"[Neggles GitHub Code](https://github.com/neggles/animatediff-cli) - [S9roll7 Prompt Travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) - [Original GitHub Code](https://github.com/guoyww/animatediff/) - [Project Page](https://animatediff.github.io/) - [Arxiv Paper](https://arxiv.org/abs/2307.04725)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßû  Make a Wish... \", on_click=close_animate_diff_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = animate_diff_help_dlg\n",
        "      animate_diff_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_lora(e):\n",
        "      animate_diff_prefs['dreambooth_lora'] = e.control.value\n",
        "      custom_lora.visible = e.control.value == \"Custom\"\n",
        "      custom_lora.update()\n",
        "    def changed_lora_layer(e):\n",
        "      animate_diff_prefs['lora_layer'] = e.control.value\n",
        "      custom_lora_layer.visible = e.control.value == \"Custom\"\n",
        "      custom_lora_layer.update()\n",
        "    def add_lora(e):\n",
        "      lora = animate_diff_prefs['lora_layer']\n",
        "      lora_scale = animate_diff_prefs['lora_layer_alpha']\n",
        "      lora_layer = {}\n",
        "      if lora == \"Custom\":\n",
        "        lora_layer = {'name': 'Custom', 'file':'', 'path':animate_diff_prefs['custom_lora_layer'], 'scale': lora_scale}\n",
        "      else:\n",
        "        for l in animate_diff_lora_layers:\n",
        "          if l['name'] == lora:\n",
        "            lora_layer = l.copy()\n",
        "            lora_layer['scale'] = lora_scale\n",
        "        for l in animate_diff_prefs['lora_map']:\n",
        "          if l['name'] == lora:\n",
        "            return\n",
        "      animate_diff_prefs['lora_map'].append(lora_layer)\n",
        "      title = Markdown(f\"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}\")\n",
        "      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "        items=[\n",
        "            PopupMenuItem(icon=icons.DELETE, text=\"Delete LoRA Layer\", on_click=delete_lora_layer, data=lora_layer),\n",
        "            PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_lora_layers, data=lora_layer),\n",
        "        ]), data=lora_layer))\n",
        "      lora_layer_map.update()\n",
        "    def delete_lora_layer(e):\n",
        "        for l in animate_diff_prefs['lora_map']:\n",
        "          if l['name'] == e.control.data['name']:\n",
        "            animate_diff_prefs['lora_map'].remove(l)\n",
        "          #del l #animate_diff_prefs['lora_map'][]\n",
        "        for c in lora_layer_map.controls:\n",
        "          if c.data['name'] == e.control.data['name']:\n",
        "             lora_layer_map.controls.remove(c)\n",
        "             break\n",
        "        lora_layer_map.update()\n",
        "    def delete_all_lora_layers(e):\n",
        "        animate_diff_prefs['lora_map'].clear()\n",
        "        lora_layer_map.controls.clear()\n",
        "        lora_layer_map.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        animate_diff_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def animate_diff_tile(animate_prompt):\n",
        "        params = []\n",
        "        for k, v in animate_prompt.items():\n",
        "            if k == 'prompt': continue\n",
        "            params.append(f'{to_title(k)}: {v}')\n",
        "        sub = ', '.join(params)\n",
        "        return ListTile(title=Text(animate_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=animate_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Animation Prompt\", on_click=lambda e: edit_animate_diff(animate_prompt), data=animate_prompt),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Animation Prompt\", on_click=lambda e: del_animate_diff(animate_prompt), data=animate_prompt)]), on_click=lambda e: edit_animate_diff(animate_prompt))\n",
        "    def edit_animate_diff(edit=None):\n",
        "        animate_diff_prompt = edit if bool(edit) else editing_prompt.copy()\n",
        "        edit_prompt = edit['prompt'] if bool(edit) else animate_diff_prefs['prompt']\n",
        "        #negative_prompt = edit['negative_prompt'] if bool(edit) else animate_diff_prefs['negative_prompt']\n",
        "        if not bool(edit):\n",
        "            animate_diff_prompt['prompt'] = animate_diff_prefs['prompt']\n",
        "            animate_diff_prompt['negative_prompt'] = animate_diff_prefs['negative_prompt']\n",
        "            animate_diff_prompt['seed'] = animate_diff_prefs['seed']\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def changed_p(e, pref=None):\n",
        "            if pref is not None:\n",
        "                animate_diff_prompt[pref] = e.control.value\n",
        "        def save_animate_diff_prompt(e):\n",
        "            if edit == None:\n",
        "                animate_diff_prefs['editing_prompts'].append(animate_diff_prompt)\n",
        "                page.animate_diff_prompts.controls.append(animate_diff_tile(animate_diff_prompt))\n",
        "                page.animate_diff_prompts.update()\n",
        "            else:\n",
        "                for s in animate_diff_prefs['editing_prompts']:\n",
        "                    if s['prompt'] == edit_prompt:\n",
        "                        s = animate_diff_prompt\n",
        "                        break\n",
        "                for t in page.animate_diff_prompts.controls:\n",
        "                    #print(f\"{t.data['prompt']} == {edit_prompt} - {t.title.value}\")\n",
        "                    if t.title.value == edit_prompt: #t.data['prompt']\n",
        "                        params = []\n",
        "                        for k, v in animate_diff_prompt.items():\n",
        "                            if k == 'prompt': continue\n",
        "                            params.append(f'{to_title(k)}: {v}')\n",
        "                        sub = ', '.join(params)\n",
        "                        t.title = Text(animate_diff_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)\n",
        "                        t.subtitle = Text(sub)\n",
        "                        t.data = animate_diff_prompt\n",
        "                        t.update()\n",
        "                        break\n",
        "                dlg_edit.open = False\n",
        "                e.control.update()\n",
        "                page.update()\n",
        "        animate_diff_editing_prompt = TextField(label=\"AnimateDiff Prompt Modifier\", value=animate_diff_prompt['prompt'], autofocus=True, on_change=lambda e:changed_p(e,'prompt'))\n",
        "        animate_diff_negative_prompt = TextField(label=\"Negative Prompt\", value=animate_diff_prompt['negative_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'negative_prompt'))\n",
        "        animate_diff_seed = TextField(label=\"Seed\", width=90, value=str(animate_diff_prompt['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed_p(e,'seed'), col={'md':1})\n",
        "        #reverse_editing_direction = Checkbox(label=\"Reverse Editing Direction\", value=animate_diff_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip=\"Whether the corresponding prompt in `editing_prompt` should be increased or decreased.\")\n",
        "        if edit != None:\n",
        "            dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è {'Edit' if bool(edit) else 'Add'} Animated Prompt\"), content=Container(Column([\n",
        "                animate_diff_editing_prompt,\n",
        "                animate_diff_negative_prompt, animate_diff_seed,\n",
        "            ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_animate_diff_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "            page.dialog = dlg_edit\n",
        "            dlg_edit.open = True\n",
        "            page.update()\n",
        "        else:\n",
        "            save_animate_diff_prompt(None)\n",
        "    def del_animate_diff(edit=None):\n",
        "        for s in animate_diff_prefs['editing_prompts']:\n",
        "            if s['prompt'] == edit['prompt']:\n",
        "                animate_diff_prefs['editing_prompts'].remove(s)\n",
        "                break\n",
        "        for t in page.animate_diff_prompts.controls:\n",
        "            if t.data['prompt'] == edit['prompt']:\n",
        "                page.animate_diff_prompts.controls.remove(t)\n",
        "                break\n",
        "        page.animate_diff_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_animate_diff_prompts(e):\n",
        "        animate_diff_prefs['editing_prompts'].clear()\n",
        "        page.animate_diff_prompts.controls.clear()\n",
        "        page.animate_diff_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def edit_prompt(e):\n",
        "      nonlocal animation_prompts\n",
        "      f = int(e.control.data)\n",
        "      edit_prompt = animate_diff_prefs['animation_prompts'][str(f)]\n",
        "      def close_dlg(e):\n",
        "        dlg_edit.open = False\n",
        "        page.update()\n",
        "      def save_prompt(e):\n",
        "        fr = int(editing_frame.value)\n",
        "        pline = f'{fr}: \"{editing_prompt.value}\"'\n",
        "        for p in animation_prompts.controls:\n",
        "          if p.data == f:\n",
        "            p.title.value = pline\n",
        "            p.data = fr\n",
        "            for button in p.trailing.items:\n",
        "              button.data = fr\n",
        "            break\n",
        "        animate_diff_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()\n",
        "        if f != int(editing_frame.value):\n",
        "          del animate_diff_prefs['animation_prompts'][str(f)]\n",
        "          sorted_dict = {}\n",
        "          for key in sorted(animate_diff_prefs['animation_prompts'].keys()):\n",
        "              sorted_dict[key] = animate_diff_prefs['animation_prompts'][key]\n",
        "          animate_diff_prefs['animation_prompts'] = sorted_dict\n",
        "          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "        animation_prompts.update()\n",
        "        close_dlg(e)\n",
        "      editing_frame = TextField(label=\"Frame\", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "      editing_prompt = TextField(label=\"Keyframe Prompt Animation\", expand=True, multiline=True, value=edit_prompt, autofocus=True)\n",
        "      dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è Edit Prompt Keyframe\"), content=Container(Column([\n",
        "          Row([editing_frame, editing_prompt])\n",
        "      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_edit\n",
        "      dlg_edit.open = True\n",
        "      page.update()\n",
        "    def del_prompt(e):\n",
        "      f = e.control.data\n",
        "      for i, p in enumerate(animation_prompts.controls):\n",
        "        if p.data == f:\n",
        "          del animation_prompts.controls[i]\n",
        "          break\n",
        "      animation_prompts.update()\n",
        "      del animate_diff_prefs['animation_prompts'][str(f)]\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompts(e):\n",
        "      animation_prompts.controls.clear()\n",
        "      animation_prompts.update()\n",
        "      animate_diff_prefs['animation_prompts'] = {}\n",
        "      clear_prompt(e)\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_prompt(e):\n",
        "      prompt.value = \"\"\n",
        "      prompt.update()\n",
        "    def copy_prompt(e):\n",
        "      p = animate_diff_prefs['animation_prompts'][str(e.control.data)]\n",
        "      page.set_clipboard(p)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã  Prompt Text copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "    def add_prompt(e, f=None, p=None, sound=True):\n",
        "      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return\n",
        "      if f == None: f = int(frame.value)\n",
        "      if p == None: p = prompt.value.strip()\n",
        "      pline = f'{f}: \"{p}\"'\n",
        "      if str(f) in animate_diff_prefs['animation_prompts']:\n",
        "        for i, pr in enumerate(animation_prompts.controls):\n",
        "          if pr.data == f:\n",
        "            pr.title.value = pline\n",
        "      else:\n",
        "        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Animation Prompt\", on_click=edit_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.COPY, text=\"Copy Prompt Text\", on_click=copy_prompt, data=f),\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete Animation Prompt\", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Prompts\", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))\n",
        "      animate_diff_prefs['animation_prompts'][str(f)] = p\n",
        "      #animate_diff_prefs['animation_prompts'] = {int(k):v for k,v in animate_diff_prefs['animation_prompts'].items()}\n",
        "      #animate_diff_prefs['animation_prompts'] = sorted(animate_diff_prefs['animation_prompts'].keys())\n",
        "      #animate_diff_prefs['animation_prompts'] = {i: animate_diff_prefs['animation_prompts'][i] for i in list(animate_diff_prefs['animation_prompts'].keys()).sort()}\n",
        "      sorted_dict = {}\n",
        "      for key in sorted(animate_diff_prefs['animation_prompts'].keys()):\n",
        "          sorted_dict[key] = animate_diff_prefs['animation_prompts'][key]\n",
        "      animate_diff_prefs['animation_prompts'] = sorted_dict\n",
        "      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)\n",
        "      animation_prompts.update()\n",
        "      if prefs['enable_sounds'] and sound: page.snd_drop.play()\n",
        "    def add_layer(e):\n",
        "        control_images = {str(int(animate_diff_prefs['control_frame'])): animate_diff_prefs['original_image']}\n",
        "        updating = False\n",
        "        for l in animate_diff_prefs['controlnet_layers']:\n",
        "            if l['control_task'] == animate_diff_prefs['control_task']:\n",
        "                control_images = merge_dict(l['control_images'], control_images)\n",
        "                updating = True\n",
        "        layer = {'control_task': animate_diff_prefs['control_task'], 'control_images': control_images, 'control_frame': animate_diff_prefs['control_frame'], 'original_image': animate_diff_prefs['original_image'], 'control_scale_list': animate_diff_prefs['control_scale_list'], 'conditioning_scale': animate_diff_prefs['conditioning_scale'], 'control_guidance_start': animate_diff_prefs['control_guidance_start'], 'control_guidance_end': animate_diff_prefs['control_guidance_end'], 'use_init_video': False}\n",
        "        images = \"\"\n",
        "        for f, img in control_images.items():\n",
        "            images += f\" - {f}: {img}\"\n",
        "        if updating:\n",
        "            for i, cn in enumerate(animate_diff_prefs['controlnet_layers']):\n",
        "                if cn['control_task'] == animate_diff_prefs['control_task']:\n",
        "                    animate_diff_prefs['controlnet_layers'][i] = layer\n",
        "            for l in multi_layers.controls:\n",
        "                if l.data['control_task'] == animate_diff_prefs['control_task']:\n",
        "                    l.title = Markdown(f\"**{layer['control_task']}** - Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}{images}\")\n",
        "                    #l.title = Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(f\"{images}Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}\")])\n",
        "                    l.update()\n",
        "                    l.data = layer\n",
        "        else:\n",
        "            animate_diff_prefs['controlnet_layers'].append(layer)\n",
        "            title = Markdown(f\"**{layer['control_task']}** - Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}{images}\")\n",
        "            #multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(f\"{images}Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}\")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            multi_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "              items=[\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete Control Layer\", on_click=delete_layer, data=layer),\n",
        "                  PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              ]), data=layer))\n",
        "        multi_layers.update()\n",
        "        #animate_diff_prefs['original_image'] = \"\"\n",
        "        #original_image.value = \"\"\n",
        "        #original_image.update()\n",
        "    def delete_layer(e):\n",
        "        animate_diff_prefs['controlnet_layers'].remove(e.control.data)\n",
        "        for c in multi_layers.controls:\n",
        "          if c.data['original_image'] == e.control.data['original_image']:\n",
        "             multi_layers.controls.remove(c)\n",
        "             break\n",
        "        multi_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        animate_diff_prefs['controlnet_layers'].clear()\n",
        "        multi_layers.controls.clear()\n",
        "        multi_layers.update()\n",
        "    def toggle_ip_adapter(e):\n",
        "      animate_diff_prefs['use_ip_adapter'] = e.control.value\n",
        "      ip_adapter_container.height=None if animate_diff_prefs['use_ip_adapter'] else 0\n",
        "      ip_adapter_container.update()\n",
        "    def add_ip_layer(e):\n",
        "        ip_adapter_image = {str(int(animate_diff_prefs['ip_adapter_frame'])): animate_diff_prefs['ip_adapter_image']}\n",
        "        updating = False\n",
        "        if str(int(animate_diff_prefs['ip_adapter_frame'])) in animate_diff_prefs['ip_adapter_layers']:\n",
        "            updating = True\n",
        "        animate_diff_prefs['ip_adapter_layers'].update(ip_adapter_image)\n",
        "        title = Markdown(f\"**{str(int(animate_diff_prefs['ip_adapter_frame']))}:** {animate_diff_prefs['ip_adapter_image']}\")\n",
        "        if updating:\n",
        "            animate_diff_prefs['ip_adapter_layers'][str(int(animate_diff_prefs['ip_adapter_frame']))] = animate_diff_prefs['ip_adapter_image']\n",
        "            for l in ip_adapter_layers.controls:\n",
        "                if next(iter(l.data.items()))[0] == str(int(animate_diff_prefs['ip_adapter_frame'])):\n",
        "                    l.title = title\n",
        "                    l.update()\n",
        "                    l.data = ip_adapter_image\n",
        "        else:\n",
        "            ip_adapter_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "              items=[\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete IP Frame\", on_click=delete_ip_layer, data=ip_adapter_image),\n",
        "                  PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Frames\", on_click=delete_all_ip_layers, data=ip_adapter_image),\n",
        "              ]), data=ip_adapter_image))\n",
        "        ip_adapter_layers.update()\n",
        "        #animate_diff_prefs['ip_adapter_image'] = \"\"\n",
        "        #ip_adapter_image.value = \"\"\n",
        "        #ip_adapter_image.update()\n",
        "    def delete_ip_layer(e):\n",
        "        del animate_diff_prefs['ip_adapter_layers'][next(iter(e.control.data.items()))[0]]\n",
        "        for c in ip_adapter_layers.controls:\n",
        "          if next(iter(c.data.items()))[0] ==  next(iter(e.control.data.items()))[0]:\n",
        "             ip_adapter_layers.controls.remove(c)\n",
        "             break\n",
        "        ip_adapter_layers.update()\n",
        "    def delete_all_ip_layers(e):\n",
        "        animate_diff_prefs['ip_adapter_layers'].clear()\n",
        "        ip_adapter_layers.controls.clear()\n",
        "        ip_adapter_layers.update()\n",
        "    \n",
        "    def toggle_img2img(e):\n",
        "        animate_diff_prefs['use_img2img'] = e.control.value\n",
        "        img2img_strength.show = animate_diff_prefs['use_img2img']\n",
        "        img2img_container.height=None if animate_diff_prefs['use_img2img'] else 0\n",
        "        img2img_container.update()\n",
        "        img2img_strength.update()\n",
        "    def add_img2img_layer(e):\n",
        "        img2img_image = {str(int(animate_diff_prefs['img2img_frame'])): animate_diff_prefs['img2img_image']}\n",
        "        updating = False\n",
        "        if str(int(animate_diff_prefs['img2img_frame'])) in animate_diff_prefs['img2img_layers']:\n",
        "            updating = True\n",
        "        animate_diff_prefs['img2img_layers'].update(img2img_image)\n",
        "        title = Markdown(f\"**{str(int(animate_diff_prefs['img2img_frame']))}:** {animate_diff_prefs['img2img_image']}\")\n",
        "        if updating:\n",
        "            animate_diff_prefs['img2img_layers'][str(int(animate_diff_prefs['img2img_frame']))] = animate_diff_prefs['img2img_image']\n",
        "            for l in img2img_layers.controls:\n",
        "                if next(iter(l.data.items()))[0] == str(int(animate_diff_prefs['img2img_frame'])):\n",
        "                    l.title = title\n",
        "                    l.update()\n",
        "                    l.data = img2img_image\n",
        "        else:\n",
        "            img2img_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "              items=[\n",
        "                  PopupMenuItem(icon=icons.DELETE, text=\"Delete Img2Img Frame\", on_click=delete_img2img_layer, data=img2img_image),\n",
        "                  PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Frames\", on_click=delete_all_img2img_layers, data=img2img_image),\n",
        "              ]), data=img2img_image))\n",
        "        img2img_layers.update()\n",
        "    def delete_img2img_layer(e):\n",
        "        del animate_diff_prefs['img2img_layers'][next(iter(e.control.data.items()))[0]]\n",
        "        for c in img2img_layers.controls:\n",
        "          if next(iter(c.data.items()))[0] ==  next(iter(e.control.data.items()))[0]:\n",
        "             img2img_layers.controls.remove(c)\n",
        "             break\n",
        "        img2img_layers.update()\n",
        "    def delete_all_img2img_layers(e):\n",
        "        animate_diff_prefs['img2img_layers'].clear()\n",
        "        img2img_layers.controls.clear()\n",
        "        img2img_layers.update()\n",
        "    def changed_motion_lora(e):\n",
        "        on = e.control.value\n",
        "        if e.control.data in animate_diff_prefs['motion_loras']:\n",
        "            animate_diff_prefs['motion_loras'].remove(e.control.data)\n",
        "        else:\n",
        "            animate_diff_prefs['motion_loras'].append(e.control.data)\n",
        "    head_prompt = TextField(label=\"Head Prompt Text\", value=animate_diff_prefs['head_prompt'], filled=False, multiline=True, on_change=lambda e:changed(e,'head_prompt'), col={'md':6})\n",
        "    tail_prompt = TextField(label=\"Tail Prompt Text\", value=animate_diff_prefs['tail_prompt'], filled=False, multiline=True, on_change=lambda e:changed(e,'tail_prompt'), col={'md':6})\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=animate_diff_prefs['prompt'], filled=True, expand=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=animate_diff_prefs['negative_prompt'], expand=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=76, value=str(animate_diff_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'), col={'md':1})\n",
        "    frame = TextField(label=\"Frame\", width=76, value=\"0\", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip=\"\")\n",
        "    add_prompt_keyframe = ft.FilledButton(\"‚ûï  Add Keyframe\", on_click=add_prompt)\n",
        "    video_length = SliderRow(label=\"Video Length\", min=1, max=500, divisions=499, pref=animate_diff_prefs, key='video_length', tooltip=\"The number of frames to animate.\")\n",
        "    num_inference_row = SliderRow(label=\"Inference Steps\", min=1, max=150, divisions=149, pref=animate_diff_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=animate_diff_prefs, key='guidance_scale')\n",
        "    context = SliderRow(label=\"Context Frames to Condition\", min=1, max=24, divisions=23, pref=animate_diff_prefs, key='context', expand=True, col={'md': 6}, tooltip=\"Number of frames to condition on. Drop to 8 on cards with less than 8GB VRAM, can raise it to 20-24 on cards with more. (default: max of <length> or 24)\")\n",
        "    stride = SliderRow(label=\"Max Motion Stride\", min=1, max=8, divisions=7, pref=animate_diff_prefs, key='stride', expand=True, col={'md': 6}, tooltip=\"Max motion stride as a power of 2 (default: 4)\")\n",
        "    clip_skip = SliderRow(label=\"Clip Skip\", min=0, max=4, divisions=4, pref=animate_diff_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip=\"Skips part of the image generation process, leading to slightly different results from the CLIP model.\")\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=animate_diff_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=animate_diff_prefs, key='height')\n",
        "    scheduler = Dropdown(label=\"Scheduler\", options=[dropdown.Option(\"ddim\"), dropdown.Option(\"pndm\"), dropdown.Option(\"lms\"), dropdown.Option(\"euler\"), dropdown.Option(\"euler_a\"), dropdown.Option(\"dpm_2\"), dropdown.Option(\"k_dpm_2\"), dropdown.Option(\"dpm_2_a\"), dropdown.Option(\"k_dpm_2_a\"), dropdown.Option(\"dpmpp_2m\"), dropdown.Option(\"k_dpmpp_2m\"), dropdown.Option(\"unipc\"), dropdown.Option(\"dpmpp_sde\"), dropdown.Option(\"k_dpmpp_sde\"), dropdown.Option(\"dpmpp_2m_sde\"), dropdown.Option(\"k_dpmpp_2m_sde\")], width=176, value=animate_diff_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))\n",
        "    #motion_module = Dropdown(label=\"Motion Module\", options=[dropdown.Option(\"improved3DMotion\"), dropdown.Option(\"mm_sd_v15_v2\"), dropdown.Option(\"mm_sd_v15\"), dropdown.Option(\"mm_sd_v14\")], width=176, value=animate_diff_prefs['motion_module'], on_change=lambda e: changed(e, 'motion_module'))\n",
        "    motion_module = Dropdown(label=\"Motion Module\", options=[], width=220, value=animate_diff_prefs['motion_module'], on_change=lambda e: changed(e, 'motion_module'))\n",
        "    for mm in animate_diff_motion_modules:\n",
        "        motion_module.options.append(dropdown.Option(mm['name']))\n",
        "    dreambooth_lora = Dropdown(label=\"DreamBooth LoRA\", options=[dropdown.Option(\"Custom\")], value=animate_diff_prefs['dreambooth_lora'], on_change=changed_lora)\n",
        "    custom_lora = TextField(label=\"Custom LoRA Safetensor (URL or Path)\", value=animate_diff_prefs['custom_lora'], expand=True, visible=animate_diff_prefs['dreambooth_lora']==\"Custom\", on_change=lambda e:changed(e,'custom_lora'))\n",
        "    for lora in animate_diff_loras:\n",
        "        dreambooth_lora.options.insert(1, dropdown.Option(lora['name']))\n",
        "    lora_alpha = SliderRow(label=\"LoRA Alpha\", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='lora_alpha', tooltip=\"The Weight of the custom LoRA Model to influence diffusion.\")\n",
        "    lora_layer = Dropdown(label=\"LoRA Layer Map\", options=[dropdown.Option(\"Custom\")], value=animate_diff_prefs['lora_layer'], on_change=changed_lora_layer)\n",
        "    custom_lora_layer = TextField(label=\"Custom LoRA Safetensor (URL or Path)\", value=animate_diff_prefs['custom_lora_layer'], expand=True, visible=animate_diff_prefs['lora_layer']==\"Custom\", on_change=lambda e:changed(e,'custom_lora_layer'))\n",
        "    for lora in animate_diff_lora_layers:\n",
        "        lora_layer.options.insert(1, dropdown.Option(lora['name']))\n",
        "    lora_layer_alpha = SliderRow(label=\"LoRA Alpha\", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='lora_layer_alpha', tooltip=\"The Weight of the custom LoRA Model to influence diffusion.\")\n",
        "    add_lora_layer = ft.FilledButton(\"‚ûï  Add LoRA\", on_click=add_lora)\n",
        "    lora_layer_map = Column([], spacing=0)\n",
        "    motion_loras_checkboxes = ResponsiveRow(controls=[Text(\"Motion Module LoRAs:\", col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1.5})], run_spacing=0, vertical_alignment=CrossAxisAlignment.CENTER)\n",
        "    for m in animate_diff_motion_loras:\n",
        "        motion_loras_checkboxes.controls.append(Checkbox(label=m['name'], data=m['name'], value=m['name'] in animate_diff_prefs['motion_loras'], on_change=changed_motion_lora, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1}))\n",
        "    motion_loras_strength = SliderRow(label=\"Motion Module LoRA Strength\", min=0, max=1, divisions=10, round=1, pref=animate_diff_prefs, key='motion_loras_strength', tooltip=\"The Weight of the custom Motion LoRA Module to influence camera.\")\n",
        "    save_frames = Switcher(label=\"Save Frames\", value=animate_diff_prefs['save_frames'], on_change=lambda e:changed(e,'save_frames'))\n",
        "    save_gif = Switcher(label=\"Save Animated GIF\", value=animate_diff_prefs['save_gif'], on_change=lambda e:changed(e,'save_gif'))\n",
        "    save_video = Switcher(label=\"Save Video\", value=animate_diff_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))\n",
        "    is_loop = Switcher(label=\"Loop\", value=animate_diff_prefs['is_loop'], on_change=lambda e:changed(e,'is_loop'))\n",
        "    is_simple_composite = Switcher(label=\"Simple Composite\", value=animate_diff_prefs['is_simple_composite'], on_change=lambda e:changed(e,'is_simple_composite'))\n",
        "    apply_lcm_lora = Switcher(label=\"Apply LCM LoRA\", value=animate_diff_prefs['apply_lcm_lora'], on_change=lambda e:changed(e,'apply_lcm_lora'))\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=215, options=[dropdown.Option(t) for t in ['Canny', 'OpenPose', \"SoftEdge\", \"Shuffle\", \"Depth\", \"Inpaint\", \"LineArt\", \"MLSD\", \"NormalBAE\", \"IP2P\", \"Scribble\", \"Seg\", \"LineArt\", \"LineArt_Anime\", \"Tile\", \"QR_Code_Monster_v1\", \"QR_Code_Monster_v2\", \"Mediapipe_Face\", \"AnimateDiff ControlNet\"]], value=animate_diff_prefs['control_task'], on_change=lambda e:changed(e,'control_task'))\n",
        "    original_image = FileInput(label=\"Original Image or Video Clip\", pref=animate_diff_prefs, key='original_image', ftype=\"picture\", expand=True, page=page)\n",
        "    control_frame = TextField(label=\"Frame\", width=76, value=\"0\", keyboard_type=KeyboardType.NUMBER, tooltip=\"\", on_change=lambda e:changed(e,'control_frame', ptype='int'))\n",
        "    #, dropdown.Option(\"Scribble\"), dropdown.Option(\"HED\"), dropdown.Option(\"M-LSD\"), dropdown.Option(\"Normal Map\"), dropdown.Option(\"Shuffle\"), dropdown.Option(\"Instruct Pix2Pix\"), dropdown.Option(\"Brightness\"), dropdown.Option(\"Video Canny Edge\"), dropdown.Option(\"Video OpenPose\")\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, expand=True, pref=animate_diff_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    ref_image = FileInput(label=\"Reference Image (optional)\", pref=animate_diff_prefs, key='ref_image', page=page)\n",
        "    control_scale_list  = TextField(label=\"Control Scale List\", width=250, value=animate_diff_prefs['control_scale_list'], on_change=lambda e:changed(e,'control_scale_list'))\n",
        "    #add_layer_btn = IconButton(icons.ADD, tooltip=\"Add Multi-ControlNetXL Layer\", on_click=add_layer)\n",
        "    add_layer_btn = ft.FilledButton(\"‚ûï Add Layer\", width=140, on_click=add_layer)\n",
        "    multi_layers = Column([], spacing=0)\n",
        "    ip_adapter_layers = Column([], spacing=0)\n",
        "    use_ip_adapter = Switcher(label=\"Use IP-Adapter Layers\", value=animate_diff_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)\n",
        "    ip_adapter_frame = TextField(label=\"Frame\", width=76, value=\"0\", keyboard_type=KeyboardType.NUMBER, tooltip=\"\", on_change=lambda e:changed(e,'ip_adapter_frame'))\n",
        "    ip_adapter_image = FileInput(label=\"IP-Adapter Image\", pref=animate_diff_prefs, key='ip_adapter_image', expand=True, page=page)\n",
        "    add_frame_btn = ft.FilledButton(\"‚ûï Add Frame\", width=144, on_click=add_ip_layer)\n",
        "    ip_adapter_scale = SliderRow(label=\"IP-Adapter Scale\", min=0.0, max=1.0, divisions=20, round=2, expand=True, pref=animate_diff_prefs, key='ip_adapter_scale', tooltip=\"\")\n",
        "    ip_adapter_is_plus = Checkbox(label=\"IP-Adapter Plus\", value=animate_diff_prefs['ip_adapter_is_plus'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_plus'))\n",
        "    ip_adapter_is_full_face = Checkbox(label=\"IP-Adapter Full Face\", value=animate_diff_prefs['ip_adapter_is_full_face'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_full_face'))\n",
        "    ip_adapter_is_plus_face = Checkbox(label=\"IP-Adapter Plus Face\", value=animate_diff_prefs['ip_adapter_is_plus_face'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_plus_face'))\n",
        "    ip_adapter_light = Checkbox(label=\"IP-Adapter Light\", value=animate_diff_prefs['ip_adapter_light'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_light'))\n",
        "    ip_adapter_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, alignment = alignment.top_left, height = None if animate_diff_prefs['use_ip_adapter'] else 0, padding=padding.only(top=4), content=Column([\n",
        "      Row([ip_adapter_frame, ip_adapter_image, add_frame_btn]),\n",
        "      ip_adapter_layers,\n",
        "      Row([ip_adapter_scale, ip_adapter_is_plus, ip_adapter_is_full_face, ip_adapter_is_plus_face, ip_adapter_light]),\n",
        "      Divider(thickness=2, height=4),\n",
        "    ]))\n",
        "    img2img_layers = Column([], spacing=0)\n",
        "    use_img2img = Switcher(label=\"Use Image2Image Layers\", value=animate_diff_prefs['use_img2img'], on_change=toggle_img2img)\n",
        "    img2img_frame = TextField(label=\"Frame\", width=76, value=\"0\", keyboard_type=KeyboardType.NUMBER, tooltip=\"\", on_change=lambda e:changed(e,'img2img_frame'))\n",
        "    img2img_image = FileInput(label=\"Init Img2Img Image\", pref=animate_diff_prefs, key='img2img_image', expand=True, page=page)\n",
        "    add_img2img_frame_btn = ft.FilledButton(\"‚ûï Add Frame\", width=144, on_click=add_img2img_layer)\n",
        "    img2img_strength = SliderRow(label=\"Denoising Strength\", min=0.0, max=1.0, divisions=20, round=2, expand=True, visible=False, pref=animate_diff_prefs, key='img2img_strength', tooltip=\"The smaller the value, the closer the result will be to the initial image.\")\n",
        "    img2img_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, alignment = alignment.top_left, height = None if animate_diff_prefs['use_img2img'] else 0, padding=padding.only(top=4), content=Column([\n",
        "      Row([img2img_frame, img2img_image, add_img2img_frame_btn]),\n",
        "      img2img_layers,\n",
        "      Divider(thickness=2, height=4),\n",
        "    ]))\n",
        "\n",
        "    upscale_tile = Switcher(label=\"Upscale Tile\", value=animate_diff_prefs['upscale_tile'], on_change=lambda e:changed(e,'upscale_tile'))\n",
        "    upscale_ip2p = Switcher(label=\"Upscale IP2P\", value=animate_diff_prefs['upscale_ip2p'], on_change=lambda e:changed(e,'upscale_ip2p'))\n",
        "    upscale_lineart_anime = Switcher(label=\"Upscale LineArt Anime\", value=animate_diff_prefs['upscale_lineart_anime'], on_change=lambda e:changed(e,'upscale_lineart_anime'))\n",
        "    upscale_steps = SliderRow(label=\"Upscale Steps\", min=1, max=50, divisions=49, pref=animate_diff_prefs, key='upscale_steps', col={'md': 6}, tooltip=\"\")\n",
        "    upscale_strength = SliderRow(label=\"Upscale Strength\", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='upscale_strength', col={'md': 6}, tooltip=\"\")\n",
        "    upscale_guidance_scale = SliderRow(label=\"Upscale Guidance\", min=0, max=20, divisions=40, round=1, pref=animate_diff_prefs, col={'md': 6}, key='upscale_guidance_scale')\n",
        "    upscale_slider = SliderRow(label=\"Upscale Amount\", min=1, max=4, divisions=12, round=2, suffix=\"x\", pref=animate_diff_prefs, col={'md': 6}, key='upscale_amount')\n",
        "    context_schedule = Dropdown(label=\"Context Schedule\", width=136, options=[dropdown.Option(t) for t in ['Uniform', 'Shuffle', \"Composite\"]], value=animate_diff_prefs['context_schedule'], on_change=lambda e:changed(e,'context_schedule'))\n",
        "    \n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name (required)\", value=animate_diff_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    num_videos = NumberPicker(label=\"Number of Videos: \", min=1, max=8, value=animate_diff_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=animate_diff_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=animate_diff_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=animate_diff_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_animate_diff = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_animate_diff.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.animate_diff_prompts = Column([], spacing=0)\n",
        "    page.animate_diff_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.animate_diff_output.controls) > 0\n",
        "    animation_prompts = Column([], spacing=0)\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üë´  AnimateDiff Enhanced Text-to-Video\", \"Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with AnimateDiff Settings\", on_click=animate_diff_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        #ResponsiveRow([prompt, negative_prompt, seed]),\n",
        "        #Row([Text(\"AnimateDiff Prompts\", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),\n",
        "        #            Row([ft.FilledTonalButton(\"Clear Prompts\", on_click=clear_animate_diff_prompts), ft.FilledButton(\"Add Diff Prompt\", on_click=lambda e: edit_animate_diff(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        #page.animate_diff_prompts,\n",
        "        ResponsiveRow([head_prompt, tail_prompt]),\n",
        "        Row([frame, prompt, add_prompt_keyframe]),\n",
        "        animation_prompts,\n",
        "        Divider(thickness=2, height=4),\n",
        "        Row([seed, negative_prompt]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        width_slider, height_slider,\n",
        "        video_length,\n",
        "        ResponsiveRow([context, stride, clip_skip]),\n",
        "        Row([dreambooth_lora, custom_lora]),#, lora_alpha\n",
        "        Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),\n",
        "        lora_layer_map,\n",
        "        Divider(thickness=4, height=4),\n",
        "        Row([control_task, control_frame, original_image, add_layer_btn]),\n",
        "        Row([control_scale_list,\n",
        "        conditioning_scale]),\n",
        "        Row([control_guidance_start, control_guidance_end]),\n",
        "        multi_layers,\n",
        "        Divider(thickness=2, height=4),\n",
        "        use_ip_adapter, \n",
        "        ip_adapter_container,\n",
        "        Row([use_img2img, img2img_strength]),\n",
        "        img2img_container,\n",
        "        ref_image,\n",
        "        Row([upscale_tile, upscale_ip2p, upscale_lineart_anime, apply_lcm_lora]),\n",
        "        ResponsiveRow([upscale_steps, upscale_guidance_scale]),\n",
        "        ResponsiveRow([upscale_strength, upscale_slider]),\n",
        "        #Row([Text(\"Enable Motion Module LoRAs:\"), motion_loras_strength]),\n",
        "        #ResponsiveRow([Container(Text(\"Motion Module LoRAs:\"), col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}), motion_loras_checkboxes]),\n",
        "        motion_loras_checkboxes,\n",
        "        motion_loras_strength,\n",
        "        Row([motion_module, scheduler, context_schedule, batch_folder_name]),\n",
        "        Row([is_loop, save_frames, save_gif, save_video, is_simple_composite]),\n",
        "        page.ESRGAN_block_animate_diff,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üíö  Run AnimateDiff\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page, from_list=True))\n",
        "        ]),\n",
        "        page.animate_diff_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "hotshot_xl_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, watermark, copyright, blurry, low resolution, blur, low quality',\n",
        "    'num_inference_steps': 25,\n",
        "    'guidance_scale': 23.0,\n",
        "    'fps': 24,\n",
        "    'video_length': 16,\n",
        "    'video_duration': 1000,\n",
        "    'export_to_video': False,\n",
        "    'seed': 0,\n",
        "    'width': 608,\n",
        "    'height': 416,\n",
        "    'scheduler': 'EulerAncestralDiscreteScheduler',\n",
        "    'gif': '',\n",
        "    'controlnet_type': 'Canny',\n",
        "    'conditioning_scale': 0.7,\n",
        "    'control_guidance_start': 0.0,\n",
        "    'control_guidance_end': 1.0,\n",
        "    'lora_layer': 'None',\n",
        "    'custom_lora_layer': '',\n",
        "    'init_video': '',\n",
        "    'init_weight': 0.5,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": False,\n",
        "    \"enlarge_scale\": 2,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildHotshotXL(page):\n",
        "    global hotshot_xl_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            hotshot_xl_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            hotshot_xl_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            hotshot_xl_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.hotshot_xl_output.controls = []\n",
        "      page.hotshot_xl_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def hotshot_xl_help(e):\n",
        "      def close_hotshot_xl_dlg(e):\n",
        "        nonlocal hotshot_xl_help_dlg\n",
        "        hotshot_xl_help_dlg.open = False\n",
        "        page.update()\n",
        "      hotshot_xl_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Hotshot-XL Text-To-GIF\"), content=Column([\n",
        "          Text(\"You‚Äôll be able to make GIFs with any existing or newly fine-tuned SDXL model you may want to use. If you'd like to make GIFs of personalized subjects, you can load your own SDXL based LORAs, and not have to worry about fine-tuning Hotshot-XL. This is awesome because it‚Äôs usually much easier to find suitable images for training data than it is to find videos. It also hopefully fits into everyone's existing LORA usage/workflows. Hotshot-XL is compatible with SDXL ControlNet to make GIFs in the composition/layout you‚Äôd like. Hotshot-XL was trained to generate 1 second GIFs at 8 FPS. Hotshot-XL was trained on various aspect ratios. For best results with the base Hotshot-XL model, we recommend using it with an SDXL model that has been fine-tuned with 512x512 images.\"),\n",
        "          Markdown(\"[Project Website](https://www.hotshot.co/) | [GitHub repository](https://github.com/hotshotco/Hotshot-XL)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"‚ô®Ô∏è  Hot Stuff... \", on_click=close_hotshot_xl_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = hotshot_xl_help_dlg\n",
        "      hotshot_xl_help_dlg.open = True\n",
        "      page.update()\n",
        "    def changed_lora_layer(e):\n",
        "        hotshot_xl_prefs['lora_layer'] = e.control.value\n",
        "        custom_lora_layer.visible = e.control.value == \"Custom\"\n",
        "        custom_lora_layer.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        hotshot_xl_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=hotshot_xl_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=hotshot_xl_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    video_length = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=hotshot_xl_prefs, key='video_length', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    video_duration = SliderRow(label=\"Video Duration\", min=1, max=6000, divisions=5999, suffix=\"ms\", pref=hotshot_xl_prefs, key='video_duration', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=hotshot_xl_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=hotshot_xl_prefs, key='guidance_scale')\n",
        "    #fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=hotshot_xl_prefs, key='fps')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 video file instead of Animated GIF\", content=Switcher(label=\"Export to Video\", value=hotshot_xl_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1024, divisions=24, multiple=16, suffix=\"px\", pref=hotshot_xl_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1024, divisions=24, multiple=16, suffix=\"px\", pref=hotshot_xl_prefs, key='height')\n",
        "    scheduler = Dropdown(label=\"Scheduler\", options=[dropdown.Option(\"EulerAncestralDiscreteScheduler\"), dropdown.Option(\"EulerDiscreteScheduler\")], width=300, value=hotshot_xl_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))\n",
        "    gif = FileInput(label=\"Init Animated GIF (optional)\", pref=hotshot_xl_prefs, expand=True, key='gif', ftype=\"gif\", page=page)\n",
        "    controlnet_type = Dropdown(label=\"ControlNet Image Layer\", width=177, options=[dropdown.Option(\"Canny\"), dropdown.Option(\"Depth\")], value=hotshot_xl_prefs['controlnet_type'], on_change=lambda e: changed(e, 'controlnet_type'))\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='conditioning_scale', tooltip=\"Strength of the ControlNet Mask.\")\n",
        "    control_guidance_start = SliderRow(label=\"Control Guidance Start\", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='control_guidance_start', tooltip=\"The percentage of total steps at which the controlnet starts applying.\")\n",
        "    control_guidance_end = SliderRow(label=\"Control Guidance End\", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='control_guidance_end', tooltip=\"The percentage of total steps at which the controlnet stops applying.\")\n",
        "    lora_layer = Dropdown(label=\"SDXL LoRA Layer\", options=[dropdown.Option(\"None\"), dropdown.Option(\"Custom\")], value=hotshot_xl_prefs['lora_layer'], on_change=changed_lora_layer)\n",
        "    custom_lora_layer = TextField(label=\"Custom LoRA Safetensor (URL or Path)\", value=hotshot_xl_prefs['custom_lora_layer'], expand=True, visible=hotshot_xl_prefs['lora_layer']==\"Custom\", on_change=lambda e:changed(e,'custom_lora_layer'))\n",
        "    for lora in SDXL_LoRA_models:\n",
        "        lora_layer.options.insert(2, dropdown.Option(lora['name']))\n",
        "    num_images = NumberPicker(label=\"Number of Animations: \", min=1, max=8, value=hotshot_xl_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))\n",
        "    batch_folder_name = TextField(label=\"Video Folder Name\", value=hotshot_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(hotshot_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=hotshot_xl_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=2, max=4, divisions=1, suffix=\"x\", pref=hotshot_xl_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=hotshot_xl_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_hotshot_xl = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    ESRGAN_settings.height = None if hotshot_xl_prefs['apply_ESRGAN_upscale'] else 0\n",
        "    page.hotshot_xl_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.hotshot_xl_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üî•  Hotshot-XL Text-To-GIF with SDXL\", \"Generate Animated GIFs with any fine-tuned SDXL model... (Work in Progress)\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Hotshot-XL Settings\", on_click=hotshot_xl_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([export_to_video, lower_memory]),\n",
        "        video_length,\n",
        "        video_duration,\n",
        "        num_inference_row,\n",
        "        #guidance,\n",
        "        width_slider, height_slider,\n",
        "        Row([controlnet_type, gif]),\n",
        "        ResponsiveRow([control_guidance_start, control_guidance_end, conditioning_scale]),\n",
        "        Divider(height=4),\n",
        "        Row([lora_layer, custom_lora_layer]),\n",
        "        Row([scheduler, export_to_video]),\n",
        "        page.ESRGAN_block_hotshot_xl,\n",
        "        Row([num_images, seed, batch_folder_name]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"ü´†  Run Hotshot-XL\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hotshot_xl(page)),\n",
        "        ]),\n",
        "        page.hotshot_xl_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "rerender_a_video_prefs = {\n",
        "    'init_video': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'a_prompt': '',\n",
        "    'control_task': 'Canny',\n",
        "    'controlnet_strength': 0.8,\n",
        "    'x0_strength': 0.95,\n",
        "    'mask_strength': 0.5,\n",
        "    'inner_strength': 0.9,\n",
        "    \"dreambooth_lora\": \"realisticVisionV20_v20\",\n",
        "    'custom_lora': '',\n",
        "    'frame_count': 102,\n",
        "    'max_size': 512,\n",
        "    'low_threshold': 50, #1-255 canny\n",
        "    'high_threshold': 100, #1-255\n",
        "    'crop': {'top': 0, 'left':0, 'right':0, 'bottom':0},\n",
        "    'enable_freeu': True,\n",
        "    'freeu_args': {'b1': 1.1, 'b2':1.2, 's1':1.0, 's2':0.2},\n",
        "    'interval': 10,\n",
        "    'style_update_freq': 10,\n",
        "    'prompt_strength': 7.5,\n",
        "    'start_time': 0.0,\n",
        "    'end_time': 0.0,\n",
        "    'duration': 0.0,\n",
        "    'smooth_boundary': False,\n",
        "    'color_preserve': False,\n",
        "    'loose_cfattn': False,\n",
        "    'first_frame': False,\n",
        "    'show_console': True,\n",
        "    'save_frames': True,\n",
        "    'seed': 0,\n",
        "    'file_prefix': 'rerender-',\n",
        "    'output_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildRerender_a_video(page):\n",
        "    global rerender_a_video_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            rerender_a_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            rerender_a_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            rerender_a_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.rerender_a_video_output.controls = []\n",
        "      page.rerender_a_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def rerender_a_video_help(e):\n",
        "      def close_rerender_a_video_dlg(e):\n",
        "        nonlocal rerender_a_video_help_dlg\n",
        "        rerender_a_video_help_dlg.open = False\n",
        "        page.update()\n",
        "      rerender_a_video_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Rerender-a-Video\"), content=Column([\n",
        "          Text(\"The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos.\"),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"\"\"Features:\n",
        "* Temporal consistency: cross-frame constraints for low-level temporal consistency.\n",
        "* Zero-shot: no training or fine-tuning required.\n",
        "* Flexibility: compatible with off-the-shelf models (e.g., ControlNet, LoRA) for customized translation.\n",
        "\n",
        "[GitHub Page](https://github.com/williamyang1991/Rerender_A_Video) | [HuggingFace Space](https://huggingface.co/spaces/Anonymous-sub/Rerender) | [Project Page](https://www.mmlab-ntu.com/project/rerender/)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" one learns your condition. The \"locked\" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The \"zero convolution\" is 1√ó1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"HED - A monochrome image with white soft edges on a black background.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üì∫  Change Reality... \", on_click=close_rerender_a_video_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = rerender_a_video_help_dlg\n",
        "      rerender_a_video_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        rerender_a_video_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        changed(e,'control_task')\n",
        "        canny_threshold.height = None if rerender_a_video_prefs['control_task'] == \"Canny\" or rerender_a_video_prefs['control_task'] == \"Canny21\" else 0\n",
        "        canny_threshold.update()\n",
        "    def changed_lora(e):\n",
        "      rerender_a_video_prefs['dreambooth_lora'] = e.control.value\n",
        "      custom_lora.visible = e.control.value == \"Custom\"\n",
        "      custom_lora.update()\n",
        "    def toggle_freeu(e):\n",
        "        changed(e,'enable_freeu')\n",
        "        freeu_args.height = None if rerender_a_video_prefs['enable_freeu'] else 0\n",
        "        freeu_args.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=rerender_a_video_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=rerender_a_video_prefs['negative_prompt'], multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    a_prompt = TextField(label=\"Additional Prompt Text\", value=rerender_a_video_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=150, options=[dropdown.Option(\"HED\"), dropdown.Option(\"Canny\"), dropdown.Option(\"Depth\")], value=rerender_a_video_prefs['control_task'], on_change=change_task)\n",
        "    #conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=rerender_a_video_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `rerender_a_video_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    controlnet_strength = SliderRow(label=\"ControlNet Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='controlnet_strength', tooltip=\"How much influence the controlnet annotator's output is used to guide the denoising process.\")\n",
        "    x0_strength = SliderRow(label=\"Denoise Strength\", min=0.0, max=1.05, divisions=21, round=2, pref=rerender_a_video_prefs, key='x0_strength', tooltip=\"Repaint degree, low to make output look more like init video. 0: fully recover the input.1.05: fully rerender the input.\")\n",
        "    mask_strength = SliderRow(label=\"Mask Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='mask_strength', tooltip=\"\")\n",
        "    inner_strength = SliderRow(label=\"Inner Strength\", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='inner_strength', tooltip=\"\")\n",
        "    init_video = FileInput(label=\"Init Video Clip\", pref=rerender_a_video_prefs, key='init_video', ftype=\"video\", expand=True, page=page)\n",
        "    #init_video = TextField(label=\"Init Video Clip\", value=rerender_a_video_prefs['init_video'], expand=True, on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    #fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=rerender_a_video_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=rerender_a_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=rerender_a_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    duration = TextField(label=\"Duration (0 for all)\", value=rerender_a_video_prefs['duration'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'duration', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([Row([start_time, end_time, duration])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=5))\n",
        "    interval = SliderRow(label=\"Number of Intervals\", min=1, max=100, divisions=99, pref=rerender_a_video_prefs, key='interval', tooltip=\"Uniformly sample the key frame every K frames. Small value for large or fast motions.\")\n",
        "    frame_count = SliderRow(label=\"Frame Count\", min=1, max=300, divisions=299, pref=rerender_a_video_prefs, key='frame_count', tooltip=\"The final output video will have K*M+1 frames with M+1 key frames.\")\n",
        "    style_update_freq = SliderRow(label=\"Style Update Frequency\", min=1, max=100, divisions=99, pref=rerender_a_video_prefs, key='style_update_freq', tooltip=\"\")\n",
        "    prompt_strength = SliderRow(label=\"Prompt Strength\", min=0, max=30, divisions=60, round=1, pref=rerender_a_video_prefs, key='prompt_strength', tooltip=\"How much influence the prompt has on the output. Guidance Scale.\")\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=rerender_a_video_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip=\"Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.\")\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=rerender_a_video_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip=\"Higher value decreases the amount of noise but could result in missing some true edges.\")\n",
        "    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    canny_threshold.height = None if rerender_a_video_prefs['control_task'] == \"Canny\" else 0\n",
        "    crop_left = SliderRow(label=\"Crop Left\", min=0, max=512, divisions=512, suffix=\"px\", pref=rerender_a_video_prefs['crop'], key='left', expand=True, col={'lg':6}, tooltip=\"\")\n",
        "    crop_right = SliderRow(label=\"Crop Right\", min=0, max=512, divisions=512, suffix=\"px\", pref=rerender_a_video_prefs['crop'], key='right', expand=True, col={'lg':6})\n",
        "    crop_top = SliderRow(label=\"Crop Top\", min=0, max=512, divisions=512, suffix=\"px\", pref=rerender_a_video_prefs['crop'], key='top', expand=True, col={'lg':6})\n",
        "    crop_bottom = SliderRow(label=\"Crop Bottom\", min=0, max=512, divisions=512, suffix=\"px\", pref=rerender_a_video_prefs['crop'], key='bottom', expand=True, col={'lg':6})\n",
        "\n",
        "    b1 = SliderRow(label=\"b1\", min=1.0, max=1.2, divisions=4, round=2, pref=rerender_a_video_prefs['freeu_args'], key='b1', expand=True, col={'md':6}, tooltip=\"Backbone factor of the first stage block of decoder.\")\n",
        "    b2 = SliderRow(label=\"b2\", min=1.2, max=1.6, divisions=8, round=2, pref=rerender_a_video_prefs['freeu_args'], key='b2', expand=True, col={'md':6}, tooltip=\"Backbone factor of the second stage block of decoder.\")\n",
        "    s1 = SliderRow(label=\"s1\", min=0, max=1, divisions=20, round=1, pref=rerender_a_video_prefs['freeu_args'], key='s1', expand=True, col={'md':6}, tooltip=\"Skip factor of the first stage block of decoder.\")\n",
        "    s2 = SliderRow(label=\"s2\", min=0, max=1, divisions=20, round=1, pref=rerender_a_video_prefs['freeu_args'], key='s2', expand=True, col={'md':6}, tooltip=\"Skip factor of the second stage block of decoder.\")\n",
        "    freeu_args = Container(content=Column([ResponsiveRow([b1, b2, s1, s2])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=0, left=13), height = None if rerender_a_video_prefs['enable_freeu'] else 0)\n",
        "    enable_freeu = Switcher(label=\"Enable FreeU: Free Lunch UNET\", value=rerender_a_video_prefs['enable_freeu'], tooltip=\"Improves diffusion model sample quality at no costs. Results will have higher contrast and saturation, richer details, and more vivid colors.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_freeu)\n",
        "    #color_fix = Dropdown(label=\"Color Fix\", width=150, options=[dropdown.Option(\"None\"), dropdown.Option(\"RGB\"), dropdown.Option(\"HSV\"), dropdown.Option(\"Lab\")], value=rerender_a_video_prefs['color_fix'], on_change=lambda e:changed(e,'color_fix'), tooltip=\"Prevent color from drifting due to feedback and model bias by fixing the histogram to the first frame. Specify colorspace for histogram matching\")\n",
        "    #color_amount = SliderRow(label=\"Color Amount\", min=0.0, max=1.0, divisions=10, round=1, pref=rerender_a_video_prefs, key='color_amount', expand=True, tooltip=\"Blend between the original color and the color matched version.\")\n",
        "    #max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=rerender_a_video_prefs, key='max_size')\n",
        "    dreambooth_lora = Dropdown(label=\"DreamBooth LoRA\", options=[dropdown.Option(\"Custom\")], value=rerender_a_video_prefs['dreambooth_lora'], on_change=changed_lora)\n",
        "    for lora in animate_diff_loras:\n",
        "      dreambooth_lora.options.insert(1, dropdown.Option(lora['name']))\n",
        "    custom_lora = TextField(label=\"Custom LoRA Safetensor (URL or Path)\", value=rerender_a_video_prefs['custom_lora'], expand=True, visible=rerender_a_video_prefs['dreambooth_lora']==\"Custom\", on_change=lambda e:changed(e,'custom_lora'))\n",
        "    max_size = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=48, multiple=16, suffix=\"px\", pref=rerender_a_video_prefs, key='max_size')\n",
        "    smooth_boundary = Switcher(label=\"Smooth Boundary\", value=rerender_a_video_prefs['smooth_boundary'], tooltip=\"Prevents artifacts at fusion boundaries.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'smooth_boundary'))\n",
        "    color_preserve = Switcher(label=\"Color Preserve\", value=rerender_a_video_prefs['color_preserve'], tooltip=\"Keep the color of the input video\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'color_preserve'))\n",
        "    loose_cfattn = Switcher(label=\"Loose Cross-Frame Attention\", value=rerender_a_video_prefs['loose_cfattn'], tooltip=\"Results will better match the input video, thus reducing ghosting artifacts caused by inconsistencies.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'loose_cfattn'))\n",
        "    show_console = Switcher(label=\"Show Console Output\", value=rerender_a_video_prefs['show_console'], tooltip=\"Outputs the progress run log in the console window. Gets messy, but useful.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_console'))\n",
        "    first_frame = Switcher(label=\"First Frame Only\", value=rerender_a_video_prefs['first_frame'], tooltip=\"Rerender only the first key frame as test.\", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'first_frame'))\n",
        "    seed = TextField(label=\"Seed\", value=rerender_a_video_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=rerender_a_video_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    output_name = TextField(label=\"Output Name\", value=rerender_a_video_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=rerender_a_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=rerender_a_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=rerender_a_video_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=rerender_a_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.rerender_a_video_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.rerender_a_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìπ  Rerender-a-Video\", \"Zero-Shot Text-Guided Video-to-Video Translation... (Note: May need 24GB VRAM to run)\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNet Vid2Vid Settings\", on_click=rerender_a_video_help)]),\n",
        "        ResponsiveRow([prompt, a_prompt]),\n",
        "        negative_prompt,\n",
        "        Row([control_task, init_video]),\n",
        "        canny_threshold,\n",
        "        #vid_params,\n",
        "        controlnet_strength,\n",
        "        x0_strength,\n",
        "        mask_strength,\n",
        "        inner_strength,\n",
        "        #prompt_strength,\n",
        "        frame_count,\n",
        "        interval,\n",
        "        style_update_freq,\n",
        "        Row([dreambooth_lora, custom_lora]),\n",
        "        #Row([color_fix, color_amount]),\n",
        "        max_size,\n",
        "        ResponsiveRow([crop_left, crop_right]),\n",
        "        ResponsiveRow([crop_top, crop_bottom]),\n",
        "        #ResponsiveRow([motion_alpha, motion_sigma]),\n",
        "        #ResponsiveRow([max_dimension, min_dimension]),\n",
        "        enable_freeu,\n",
        "        freeu_args,\n",
        "        Row([smooth_boundary, color_preserve, loose_cfattn, first_frame]),\n",
        "        Row([seed, output_name, batch_folder_name]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üì∏  Run Rerender on Video\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_rerender_a_video(page))]),\n",
        "        page.rerender_a_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "materialdiffusion_prefs = {\n",
        "    \"material_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"material-\",\n",
        "    \"num_outputs\": 1,\n",
        "    \"steps\":50,\n",
        "    \"eta\":0.4,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":7.5,\n",
        "    \"seed\":0,\n",
        "    \"init_image\": '',\n",
        "    \"prompt_strength\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    #\"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildMaterialDiffusion(page):\n",
        "    global prefs, materialdiffusion_prefs, status\n",
        "\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            materialdiffusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            materialdiffusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            materialdiffusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def changed_pref(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "        status['changed_parameters'] = True\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                materialdiffusion_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                materialdiffusion_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        materialdiffusion_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "        has_changed = True\n",
        "    def change_strength(e):\n",
        "        strength_value.value = f\" {int(e.control.value * 100)}%\"\n",
        "        strength_value.update()\n",
        "        guidance.update()\n",
        "        changed(e, 'prompt_strength', ptype=\"float\")\n",
        "\n",
        "    material_prompt = TextField(label=\"Material Prompt\", value=materialdiffusion_prefs['material_prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'material_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=materialdiffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=materialdiffusion_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=materialdiffusion_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=materialdiffusion_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Inference Steps\", value=materialdiffusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    eta = TextField(label=\"DDIM ETA\", value=materialdiffusion_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype=\"float\"))\n",
        "    steps = SliderRow(label=\"Inference Steps\", min=0, max=200, divisions=200, pref=materialdiffusion_prefs, key='steps')\n",
        "    eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=20, round=1, pref=materialdiffusion_prefs, key='eta')\n",
        "    seed = TextField(label=\"Seed\", value=materialdiffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, file_prefix, NumberPicker(label=\"Output Images\", min=1, max=8, step=1, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))], col={'xs':12, 'md':6}),\n",
        "                      Column([steps, eta, seed], col={'xs':12, 'lg':6})], vertical_alignment=CrossAxisAlignment.START)\n",
        "    batch_row = Row([batch_folder_name, file_prefix], col={'xs':12, 'lg':6})\n",
        "    number_row = Row([NumberPicker(label=\"Output Images\", min=1, max=4, step=3, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\")), seed], col={'xs':12, 'md':6})\n",
        "    param_rows = ResponsiveRow([number_row, batch_row], vertical_alignment=CrossAxisAlignment.START)\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=materialdiffusion_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=materialdiffusion_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=materialdiffusion_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=materialdiffusion_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    mask_image = TextField(label=\"Mask Image\", value=materialdiffusion_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=materialdiffusion_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}\", round=2, value=materialdiffusion_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    strength_value = Text(f\" {int(materialdiffusion_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "    strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    #strength_slider = SliderRow(label=\"Prompt Strength\", min=0.1, max=0.9, divisions=16, suffix=\"%\", pref=materialdiffusion_prefs, key='prompt_strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    api_instructions = Markdown(\"Get **Replicate API Token** from [https://replicate.com/account](https://replicate.com/account)\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "    Replicate_api = TextField(label=\"Replicate API Key\", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e, 'Replicate_api_key'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=materialdiffusion_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=materialdiffusion_prefs, key='enlarge_scale')\n",
        "    #face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=materialdiffusion_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=materialdiffusion_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_material = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_material.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not materialdiffusion_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí®   Run Material Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_materialdiffusion(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.materialdiffusion_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üß±  Replicate Material Diffusion\", \"Create Seamless Tiled Textures with your Prompt. Requires account at Replicate.com and your Key.\"),\n",
        "            material_prompt,\n",
        "            steps,\n",
        "            guidance,\n",
        "            eta,\n",
        "            width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            img_block, page.ESRGAN_block_material,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            api_instructions,\n",
        "            Replicate_api,\n",
        "            param_rows,\n",
        "            #batch_row,\n",
        "            #number_row,\n",
        "            parameters_row,\n",
        "            page.materialdiffusion_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,\n",
        "    return c\n",
        "\n",
        "ImageNet_classes = {'ATM': 480, 'Acinonyx jubatus': 293, 'Aepyceros melampus': 352, 'Afghan': 160, 'Afghan hound': 160, 'African chameleon': 47, 'African crocodile': 49, 'African elephant': 386, 'African gray': 87, 'African grey': 87, 'African hunting dog': 275, 'Ailuropoda melanoleuca': 388, 'Ailurus fulgens': 387, 'Airedale': 191, 'Airedale terrier': 191, 'Alaska crab': 121, 'Alaska king crab': 121, 'Alaskan king crab': 121, 'Alaskan malamute': 249, 'Alligator mississipiensis': 50, 'Alopex lagopus': 279, 'Ambystoma maculatum': 28, 'Ambystoma mexicanum': 29, 'American Staffordshire terrier': 180, 'American alligator': 50, 'American black bear': 295, 'American chameleon': 40, 'American coot': 137, 'American eagle': 22, 'American egret': 132, 'American lobster': 122, 'American pit bull terrier': 180, 'American robin': 15, 'Angora': 332, 'Angora rabbit': 332, 'Anolis carolinensis': 40, 'Appenzeller': 240, 'Aptenodytes patagonica': 145, 'Arabian camel': 354, 'Aramus pictus': 135, 'Aranea diademata': 74, 'Araneus cavaticus': 73, 'Arctic fox': 279, 'Arctic wolf': 270, 'Arenaria interpres': 139, 'Argiope aurantia': 72, 'Ascaphus trui': 32, 'Asiatic buffalo': 346, 'Ateles geoffroyi': 381, 'Australian terrier': 193, 'Band Aid': 419, 'Bedlington terrier': 181, 'Bernese mountain dog': 239, 'Biro': 418, 'Blenheim spaniel': 156, 'Bonasa umbellus': 82, 'Border collie': 232, 'Border terrier': 182, 'Boston bull': 195, 'Boston terrier': 195, 'Bouvier des Flandres': 233, 'Bouviers des Flandres': 233, 'Brabancon griffon': 262, 'Bradypus tridactylus': 364, 'Brittany spaniel': 215, 'Bubalus bubalis': 346, 'CD player': 485, 'CRO': 688, 'CRT screen': 782, 'Cacatua galerita': 89, 'Camelus dromedarius': 354, 'Cancer irroratus': 119, 'Cancer magister': 118, 'Canis dingo': 273, 'Canis latrans': 272, 'Canis lupus': 269, 'Canis lupus tundrarum': 270, 'Canis niger': 271, 'Canis rufus': 271, 'Cape hunting dog': 275, 'Capra ibex': 350, 'Carassius auratus': 1, 'Carcharodon carcharias': 2, 'Cardigan': 264, 'Cardigan Welsh corgi': 264, 'Carduelis carduelis': 11, 'Caretta caretta': 33, 'Carphophis amoenus': 52, 'Carpodacus mexicanus': 12, 'Cavia cobaya': 338, 'Cebus capucinus': 378, 'Cerastes cornutus': 66, 'Chamaeleo chamaeleon': 47, 'Chesapeake Bay retriever': 209, 'Chihuahua': 151, 'Chlamydosaurus kingi': 43, 'Christmas stocking': 496, 'Ciconia ciconia': 127, 'Ciconia nigra': 128, 'Constrictor constrictor': 61, 'Crock Pot': 521, 'Crocodylus niloticus': 49, 'Crotalus adamanteus': 67, 'Crotalus cerastes': 68, 'Cuon alpinus': 274, 'Cygnus atratus': 100, 'Cypripedium calceolus': 986, 'Cypripedium parviflorum': 986, 'Danaus plexippus': 323, 'Dandie Dinmont': 194, 'Dandie Dinmont terrier': 194, 'Dermochelys coriacea': 34, 'Doberman': 236, 'Doberman pinscher': 236, 'Dugong dugon': 149, 'Dungeness crab': 118, 'Dutch oven': 544, 'Egretta albus': 132, 'Egretta caerulea': 131, 'Egyptian cat': 285, 'Elephas maximus': 385, 'English cocker spaniel': 219, 'English foxhound': 167, 'English setter': 212, 'English springer': 217, 'English springer spaniel': 217, 'EntleBucher': 241, 'Erolia alpina': 140, 'Erythrocebus patas': 371, 'Eschrichtius gibbosus': 147, 'Eschrichtius robustus': 147, 'Eskimo dog': 248, 'Euarctos americanus': 295, 'European fire salamander': 25, 'European gallinule': 136, 'Felis concolor': 286, 'Felis onca': 290, 'French bulldog': 245, 'French horn': 566, 'French loaf': 930, 'Fringilla montifringilla': 10, 'Fulica americana': 137, 'Galeocerdo cuvieri': 3, 'German police dog': 235, 'German shepherd': 235, 'German shepherd dog': 235, 'German short-haired pointer': 210, 'Gila monster': 45, 'Gordon setter': 214, 'Gorilla gorilla': 366, 'Granny Smith': 948, 'Great Dane': 246, 'Great Pyrenees': 257, 'Greater Swiss Mountain dog': 238, 'Grifola frondosa': 996, 'Haliaeetus leucocephalus': 22, 'Heloderma suspectum': 45, 'Hippopotamus amphibius': 344, 'Holocanthus tricolor': 392, 'Homarus americanus': 122, 'Hungarian pointer': 211, 'Hylobates lar': 368, 'Hylobates syndactylus': 369, 'Hypsiglena torquata': 60, 'Ibizan Podenco': 173, 'Ibizan hound': 173, 'Iguana iguana': 39, 'Indian cobra': 63, 'Indian elephant': 385, 'Indri brevicaudatus': 384, 'Indri indri': 384, 'Irish setter': 213, 'Irish terrier': 184, 'Irish water spaniel': 221, 'Irish wolfhound': 170, 'Italian greyhound': 171, 'Japanese spaniel': 152, 'Kakatoe galerita': 89, 'Kerry blue terrier': 183, 'Komodo dragon': 48, 'Komodo lizard': 48, 'Labrador retriever': 208, 'Lacerta viridis': 46, 'Lakeland terrier': 189, 'Latrodectus mactans': 75, 'Lemur catta': 383, 'Leonberg': 255, 'Lepisosteus osseus': 395, 'Lhasa': 204, 'Lhasa apso': 204, 'Loafer': 630, 'Loxodonta africana': 386, 'Lycaon pictus': 275, 'Madagascar cat': 383, 'Maine lobster': 122, 'Maltese': 153, 'Maltese dog': 153, 'Maltese terrier': 153, 'Melursus ursinus': 297, 'Mergus serrator': 98, 'Mexican hairless': 268, 'Model T': 661, 'Mustela nigripes': 359, 'Mustela putorius': 358, 'Naja naja': 63, 'Nasalis larvatus': 376, 'Newfoundland': 256, 'Newfoundland dog': 256, 'Nile crocodile': 49, 'Norfolk terrier': 185, 'Northern lobster': 122, 'Norwegian elkhound': 174, 'Norwich terrier': 186, 'Old English sheepdog': 229, 'Oncorhynchus kisutch': 391, 'Orcinus orca': 148, 'Ornithorhynchus anatinus': 103, 'Ovis canadensis': 349, 'Pan troglodytes': 367, 'Panthera leo': 291, 'Panthera onca': 290, 'Panthera pardus': 288, 'Panthera tigris': 292, 'Panthera uncia': 289, 'Paralithodes camtschatica': 121, 'Passerina cyanea': 14, 'Peke': 154, 'Pekinese': 154, 'Pekingese': 154, 'Pembroke': 263, 'Pembroke Welsh corgi': 263, 'Persian cat': 283, 'Petri dish': 712, 'Phalangium opilio': 70, 'Phascolarctos cinereus': 105, 'Polaroid Land camera': 732, 'Polaroid camera': 732, 'Polyporus frondosus': 996, 'Pomeranian': 259, 'Pongo pygmaeus': 365, 'Porphyrio porphyrio': 136, 'Psittacus erithacus': 87, 'Python sebae': 62, 'R.V.': 757, 'RV': 757, 'Rana catesbeiana': 30, 'Rhodesian ridgeback': 159, 'Rocky Mountain bighorn': 349, 'Rocky Mountain sheep': 349, 'Rottweiler': 234, 'Russian wolfhound': 169, 'Saimiri sciureus': 382, 'Saint Bernard': 247, 'Salamandra salamandra': 25, 'Saluki': 176, 'Samoyed': 258, 'Samoyede': 258, 'Sciurus niger': 335, 'Scotch terrier': 199, 'Scottie': 199, 'Scottish deerhound': 177, 'Scottish terrier': 199, 'Sealyham': 190, 'Sealyham terrier': 190, 'Shetland': 230, 'Shetland sheep dog': 230, 'Shetland sheepdog': 230, 'Shih-Tzu': 155, 'Siamese': 284, 'Siamese cat': 284, 'Siberian husky': 250, 'St Bernard': 247, 'Staffordshire bull terrier': 179, 'Staffordshire bullterrier': 179, 'Staffordshire terrier': 180, 'Strix nebulosa': 24, 'Struthio camelus': 9, 'Sus scrofa': 342, 'Sussex spaniel': 220, 'Sydney silky': 201, 'Symphalangus syndactylus': 369, 'T-shirt': 610, 'Thalarctos maritimus': 296, 'Tibetan mastiff': 244, 'Tibetan terrier': 200, 'Tinca tinca': 0, 'Tringa totanus': 141, 'Triturus vulgaris': 26, 'Turdus migratorius': 15, 'U-boat': 833, 'Urocyon cinereoargenteus': 280, 'Ursus Maritimus': 296, 'Ursus americanus': 295, 'Ursus arctos': 294, 'Ursus ursinus': 297, 'Varanus komodoensis': 48, 'Virginia fence': 912, 'Vulpes macrotis': 278, 'Vulpes vulpes': 277, 'Walker foxhound': 166, 'Walker hound': 166, 'Weimaraner': 178, 'Welsh springer spaniel': 218, 'West Highland white terrier': 203, 'Windsor tie': 906, 'Yorkshire terrier': 187, 'abacus': 398, 'abaya': 399, 'academic gown': 400, 'academic robe': 400, 'accordion': 401, 'acorn': 988, 'acorn squash': 941, 'acoustic guitar': 402, 'admiral': 321, 'aegis': 461, 'affenpinscher': 252, 'agama': 42, 'agaric': 992, 'ai': 364, 'aircraft carrier': 403, 'airliner': 404, 'airship': 405, 'albatross': 146, 'all-terrain bike': 671, 'alligator lizard': 44, 'alp': 970, 'alsatian': 235, 'altar': 406, 'ambulance': 407, 'amphibian': 408, 'amphibious vehicle': 408, 'analog clock': 409, 'ananas': 953, 'anemone': 108, 'anemone fish': 393, 'anole': 40, 'ant': 310, 'anteater': 102, 'apiary': 410, 'apron': 411, 'armadillo': 363, 'armored combat vehicle': 847, 'armoured combat vehicle': 847, 'army tank': 847, 'artichoke': 944, 'articulated lorry': 867, 'ash bin': 412, 'ash-bin': 412, 'ashbin': 412, 'ashcan': 412, 'assault gun': 413, 'assault rifle': 413, 'attack aircraft carrier': 403, 'automated teller': 480, 'automated teller machine': 480, 'automatic teller': 480, 'automatic teller machine': 480, 'automatic washer': 897, 'axolotl': 29, 'baboon': 372, 'back pack': 414, 'backpack': 414, 'badger': 362, 'bagel': 931, 'bakehouse': 415, 'bakery': 415, 'bakeshop': 415, 'balance beam': 416, 'bald eagle': 22, 'balloon': 417, 'ballpen': 418, 'ballplayer': 981, 'ballpoint': 418, 'ballpoint pen': 418, 'balusters': 421, 'balustrade': 421, 'banana': 954, 'bandeau': 459, 'banded gecko': 38, 'banister': 421, 'banjo': 420, 'bannister': 421, 'barbell': 422, 'barber chair': 423, 'barbershop': 424, 'barn': 425, 'barn spider': 73, 'barometer': 426, 'barracouta': 389, 'barrel': 427, 'barrow': 428, 'bars': 702, 'baseball': 429, 'baseball player': 981, 'basenji': 253, 'basketball': 430, 'basset': 161, 'basset hound': 161, 'bassinet': 431, 'bassoon': 432, 'bath': 435, 'bath towel': 434, 'bathing cap': 433, 'bathing trunks': 842, 'bathing tub': 435, 'bathroom tissue': 999, 'bathtub': 435, 'beach waggon': 436, 'beach wagon': 436, 'beacon': 437, 'beacon light': 437, 'beagle': 162, 'beaker': 438, 'beam': 416, 'bear cat': 387, 'bearskin': 439, 'beaver': 337, 'bee': 309, 'bee eater': 92, 'bee house': 410, 'beer bottle': 440, 'beer glass': 441, 'beigel': 931, 'bell': 494, 'bell cot': 442, 'bell cote': 442, 'bell pepper': 945, 'bell toad': 32, 'bib': 443, 'bicycle-built-for-two': 444, 'bighorn': 349, 'bighorn sheep': 349, 'bikini': 445, 'billfish': 395, 'billfold': 893, 'billiard table': 736, 'binder': 446, 'binoculars': 447, 'birdhouse': 448, 'bison': 347, 'bittern': 133, 'black Maria': 734, 'black and gold garden spider': 72, 'black bear': 295, 'black grouse': 80, 'black stork': 128, 'black swan': 100, 'black widow': 75, 'black-and-tan coonhound': 165, 'black-footed ferret': 359, 'bloodhound': 163, 'blow drier': 589, 'blow dryer': 589, 'blower': 545, 'blowfish': 397, 'blue jack': 391, 'blue jean': 608, 'bluetick': 164, 'boa': 552, 'boa constrictor': 61, 'boar': 342, 'board': 532, 'boat paddle': 693, 'boathouse': 449, 'bob': 450, 'bobsled': 450, 'bobsleigh': 450, 'bobtail': 229, 'bola': 451, 'bola tie': 451, 'bolete': 997, 'bolo': 451, 'bolo tie': 451, 'bonnet': 452, 'book jacket': 921, 'bookcase': 453, 'bookshop': 454, 'bookstall': 454, 'bookstore': 454, 'borzoi': 169, 'bottle screw': 512, 'bottlecap': 455, 'bow': 456, 'bow tie': 457, 'bow-tie': 457, 'bowtie': 457, 'box tortoise': 37, 'box turtle': 37, 'boxer': 242, 'bra': 459, 'brain coral': 109, 'brambling': 10, 'brass': 458, 'brassiere': 459, 'breakwater': 460, 'breastplate': 461, 'briard': 226, 'bridegroom': 982, 'broccoli': 937, 'broom': 462, 'brown bear': 294, 'bruin': 294, 'brush kangaroo': 104, 'brush wolf': 272, 'bubble': 971, 'bucket': 463, 'buckeye': 990, 'buckle': 464, 'buckler': 787, 'bulbul': 16, 'bull mastiff': 243, 'bullet': 466, 'bullet train': 466, 'bulletproof vest': 465, 'bullfrog': 30, 'bulwark': 460, 'burrito': 965, 'busby': 439, 'bustard': 138, 'butcher shop': 467, 'butternut squash': 942, 'cab': 468, 'cabbage butterfly': 324, 'cairn': 192, 'cairn terrier': 192, 'caldron': 469, 'can opener': 473, 'candle': 470, 'candy store': 509, 'cannon': 471, 'canoe': 472, 'capitulum': 998, 'capuchin': 378, 'car mirror': 475, 'car wheel': 479, 'carabid beetle': 302, 'carbonara': 959, 'cardigan': 474, 'cardoon': 946, 'carousel': 476, \"carpenter's kit\": 477, \"carpenter's plane\": 726, 'carriage': 705, 'carriage dog': 251, 'carrier': 403, 'carrion fungus': 994, 'carrousel': 476, 'carton': 478, 'cash dispenser': 480, 'cash machine': 480, 'cask': 427, 'cassette': 481, 'cassette player': 482, 'castle': 483, 'cat bear': 387, 'catamaran': 484, 'catamount': 287, 'cathode-ray oscilloscope': 688, 'cauldron': 469, 'cauliflower': 938, 'cell': 487, 'cello': 486, 'cellphone': 487, 'cellular phone': 487, 'cellular telephone': 487, 'centipede': 79, 'cerastes': 66, 'chain': 488, 'chain armor': 490, 'chain armour': 490, 'chain mail': 490, 'chain saw': 491, 'chainlink fence': 489, 'chainsaw': 491, 'chambered nautilus': 117, 'cheeseburger': 933, 'cheetah': 293, 'chest': 492, 'chetah': 293, 'chickadee': 19, 'chiffonier': 493, 'chime': 494, 'chimp': 367, 'chimpanzee': 367, 'china cabinet': 495, 'china closet': 495, 'chiton': 116, 'chocolate sauce': 960, 'chocolate syrup': 960, 'chopper': 499, 'chow': 260, 'chow chow': 260, 'chrysanthemum dog': 200, 'chrysomelid': 304, 'church': 497, 'church building': 497, 'chute': 701, 'cicada': 316, 'cicala': 316, 'cimarron': 349, 'cinema': 498, 'claw': 600, 'cleaver': 499, 'cliff': 972, 'cliff dwelling': 500, 'cloak': 501, 'clog': 502, 'closet': 894, 'clumber': 216, 'clumber spaniel': 216, 'coach': 705, 'coach dog': 251, 'coast': 978, 'coat-of-mail shell': 116, 'cock': 7, 'cocker': 219, 'cocker spaniel': 219, 'cockroach': 314, 'cocktail shaker': 503, 'coffee mug': 504, 'coffeepot': 505, 'coho': 391, 'coho salmon': 391, 'cohoe': 391, 'coil': 506, 'collie': 231, 'colobus': 375, 'colobus monkey': 375, 'combination lock': 507, 'comfort': 750, 'comforter': 750, 'comic book': 917, 'commode': 493, 'common iguana': 39, 'common newt': 26, 'computer keyboard': 508, 'computer mouse': 673, 'conch': 112, 'confectionary': 509, 'confectionery': 509, 'conker': 990, 'consomme': 925, 'container ship': 510, 'container vessel': 510, 'containership': 510, 'convertible': 511, 'coon bear': 388, 'coral fungus': 991, 'coral reef': 973, 'corkscrew': 512, 'corn': 987, 'cornet': 513, 'cot': 520, 'cottontail': 330, 'cottontail rabbit': 330, 'coucal': 91, 'cougar': 286, 'courgette': 939, 'cowboy boot': 514, 'cowboy hat': 515, 'coyote': 272, 'cradle': 516, 'crampfish': 5, 'crane': 517, 'crash helmet': 518, 'crate': 519, 'crawdad': 124, 'crawdaddy': 124, 'crawfish': 124, 'crayfish': 124, 'crib': 520, 'cricket': 312, 'crinoline': 601, 'croquet ball': 522, 'crossword': 918, 'crossword puzzle': 918, 'crutch': 523, 'cucumber': 943, 'cuirass': 524, 'cuke': 943, 'cup': 968, 'curly-coated retriever': 206, 'custard apple': 956, 'daddy longlegs': 70, 'daisy': 985, 'dalmatian': 251, 'dam': 525, 'damselfly': 320, 'dark glasses': 837, 'darning needle': 319, 'day bed': 831, 'deerhound': 177, 'denim': 608, 'desk': 526, 'desktop computer': 527, \"devil's darning needle\": 319, 'devilfish': 147, 'dhole': 274, 'dial phone': 528, 'dial telephone': 528, 'diamondback': 67, 'diamondback rattlesnake': 67, 'diaper': 529, 'digital clock': 530, 'digital watch': 531, 'dike': 525, 'dingo': 273, 'dining table': 532, 'dipper': 20, 'dirigible': 405, 'disc brake': 535, 'dish washer': 534, 'dishcloth': 533, 'dishrag': 533, 'dishwasher': 534, 'dishwashing machine': 534, 'disk brake': 535, 'dock': 536, 'dockage': 536, 'docking facility': 536, 'dog sled': 537, 'dog sleigh': 537, 'dogsled': 537, 'dome': 538, 'doormat': 539, 'dough': 961, 'dowitcher': 142, 'dragon lizard': 48, 'dragonfly': 319, 'drake': 97, 'drilling platform': 540, 'dromedary': 354, 'drop': 972, 'drop-off': 972, 'drum': 541, 'drumstick': 542, 'duck-billed platypus': 103, 'duckbill': 103, 'duckbilled platypus': 103, 'dugong': 149, 'dumbbell': 543, 'dung beetle': 305, 'dunlin': 140, 'dust cover': 921, 'dust jacket': 921, 'dust wrapper': 921, 'dustbin': 412, 'dustcart': 569, 'dyke': 525, 'ear': 998, 'earthstar': 995, 'eastern fox squirrel': 335, 'eatery': 762, 'eating house': 762, 'eating place': 762, 'echidna': 102, 'eel': 390, 'eft': 27, 'eggnog': 969, 'egis': 461, 'electric fan': 545, 'electric guitar': 546, 'electric locomotive': 547, 'electric ray': 5, 'electric switch': 844, 'electrical switch': 844, 'elkhound': 174, 'emmet': 310, 'entertainment center': 548, 'envelope': 549, 'espresso': 967, 'espresso maker': 550, 'essence': 711, 'estate car': 436, 'ewer': 725, 'face powder': 551, 'feather boa': 552, 'ferret': 359, 'fiddle': 889, 'fiddler crab': 120, 'field glasses': 447, 'fig': 952, 'file': 553, 'file cabinet': 553, 'filing cabinet': 553, 'fire engine': 555, 'fire screen': 556, 'fire truck': 555, 'fireboat': 554, 'fireguard': 556, 'fitch': 358, 'fixed disk': 592, 'flagpole': 557, 'flagstaff': 557, 'flamingo': 130, 'flat-coated retriever': 205, 'flattop': 403, 'flatworm': 110, 'flowerpot': 738, 'flute': 558, 'fly': 308, 'folding chair': 559, 'food market': 582, 'football helmet': 560, 'footstall': 708, 'foreland': 976, 'forklift': 561, 'foulmart': 358, 'foumart': 358, 'fountain': 562, 'fountain pen': 563, 'four-poster': 564, 'fox squirrel': 335, 'freight car': 565, 'frilled lizard': 43, 'frying pan': 567, 'frypan': 567, 'fur coat': 568, 'gar': 395, 'garbage can': 412, 'garbage truck': 569, 'garden cart': 428, 'garden spider': 74, 'garfish': 395, 'garpike': 395, 'garter snake': 57, 'gas helmet': 570, 'gas pump': 571, 'gasmask': 570, 'gasoline pump': 571, 'gazelle': 353, 'gazelle hound': 176, 'geta': 502, 'geyser': 974, 'giant lizard': 48, 'giant panda': 388, 'giant schnauzer': 197, 'gibbon': 368, 'glasshouse': 580, 'globe artichoke': 944, 'globefish': 397, 'go-kart': 573, 'goblet': 572, 'golden retriever': 207, 'goldfinch': 11, 'goldfish': 1, 'golf ball': 574, 'golf cart': 575, 'golfcart': 575, 'gondola': 576, 'gong': 577, 'goose': 99, 'gorilla': 366, 'gown': 578, 'grampus': 148, 'grand': 579, 'grand piano': 579, 'grass snake': 57, 'grasshopper': 311, 'gray fox': 280, 'gray whale': 147, 'gray wolf': 269, 'great gray owl': 24, 'great grey owl': 24, 'great white heron': 132, 'great white shark': 2, 'green lizard': 46, 'green mamba': 64, 'green snake': 55, 'greenhouse': 580, 'grey fox': 280, 'grey whale': 147, 'grey wolf': 269, 'grille': 581, 'grocery': 582, 'grocery store': 582, 'groenendael': 224, 'groin': 460, 'groom': 982, 'ground beetle': 302, 'groyne': 460, 'grunter': 341, 'guacamole': 924, 'guenon': 370, 'guenon monkey': 370, 'guillotine': 583, 'guinea pig': 338, 'gyromitra': 993, 'hack': 468, 'hair drier': 589, 'hair dryer': 589, 'hair slide': 584, 'hair spray': 585, 'half track': 586, 'hammer': 587, 'hammerhead': 4, 'hammerhead shark': 4, 'hamper': 588, 'hamster': 333, 'hand blower': 589, 'hand-held computer': 590, 'hand-held microcomputer': 590, 'handbasin': 896, 'handkerchief': 591, 'handrail': 421, 'hankey': 591, 'hankie': 591, 'hanky': 591, 'hard disc': 592, 'hard disk': 592, 'hare': 331, 'harmonica': 593, 'harp': 594, 'hartebeest': 351, 'harvester': 595, 'harvestman': 70, 'hatchet': 596, 'hautbois': 683, 'hautboy': 683, 'haversack': 414, 'hay': 958, 'head': 976, 'head cabbage': 936, 'headland': 976, 'hedgehog': 334, 'helix': 506, 'hen': 8, 'hen of the woods': 996, 'hen-of-the-woods': 996, 'hermit crab': 125, 'high bar': 602, 'hip': 989, 'hippo': 344, 'hippopotamus': 344, 'hockey puck': 746, 'hodometer': 685, 'hog': 341, 'hognose snake': 54, 'holothurian': 329, 'holster': 597, 'home theater': 598, 'home theatre': 598, 'honeycomb': 599, 'hook': 600, 'hoopskirt': 601, 'hopper': 311, 'horizontal bar': 602, 'horn': 566, 'hornbill': 93, 'horned asp': 66, 'horned rattlesnake': 68, 'horned viper': 66, 'horse cart': 603, 'horse chestnut': 990, 'horse-cart': 603, 'hot dog': 934, 'hot pot': 926, 'hotdog': 934, 'hotpot': 926, 'hourglass': 604, 'house finch': 12, 'howler': 379, 'howler monkey': 379, 'hummingbird': 94, 'hunting spider': 77, 'husky': 248, 'hussar monkey': 371, 'hyaena': 276, 'hyena': 276, 'hyena dog': 275, 'iPod': 605, 'ibex': 350, 'ice bear': 296, 'ice cream': 928, 'ice lolly': 929, 'icebox': 760, 'icecream': 928, 'igniter': 626, 'ignitor': 626, 'iguana': 39, 'impala': 352, 'indigo bird': 14, 'indigo bunting': 14, 'indigo finch': 14, 'indri': 384, 'indris': 384, 'internet site': 916, 'iron': 606, 'island dispenser': 571, 'isopod': 126, 'jacamar': 95, 'jack': 955, \"jack-o'-lantern\": 607, 'jackfruit': 955, 'jaguar': 290, 'jak': 955, 'jammies': 697, 'jay': 17, 'jean': 608, 'jeep': 609, 'jellyfish': 107, 'jersey': 610, 'jetty': 460, \"jeweler's loupe\": 633, 'jigsaw puzzle': 611, 'jinrikisha': 612, 'joystick': 613, \"judge's robe\": 400, 'junco': 13, 'kangaroo bear': 105, 'keeshond': 261, 'kelpie': 227, 'keypad': 508, 'killer': 148, 'killer whale': 148, 'kimono': 614, 'king crab': 121, 'king of beasts': 291, 'king penguin': 145, 'king snake': 56, 'kingsnake': 56, 'kit fox': 278, 'kite': 21, 'knapsack': 414, 'knee pad': 615, 'knot': 616, 'koala': 105, 'koala bear': 105, 'komondor': 228, 'kuvasz': 222, 'lab coat': 617, 'laboratory coat': 617, 'labyrinth': 646, 'lacewing': 318, 'lacewing fly': 318, 'ladle': 618, 'lady beetle': 301, 'ladybeetle': 301, 'ladybird': 301, 'ladybird beetle': 301, 'ladybug': 301, 'lakeshore': 975, 'lakeside': 975, 'lamp shade': 619, 'lampshade': 619, 'landrover': 609, 'langouste': 123, 'langur': 374, 'laptop': 620, 'laptop computer': 620, 'lavabo': 896, 'lawn cart': 428, 'lawn mower': 621, 'leaf beetle': 304, 'leafhopper': 317, 'leatherback': 34, 'leatherback turtle': 34, 'leathery turtle': 34, 'lemon': 951, 'lens cap': 622, 'lens cover': 622, 'leopard': 288, 'lesser panda': 387, 'letter box': 637, 'letter opener': 623, 'library': 624, 'lifeboat': 625, 'light': 626, 'lighter': 626, 'lighthouse': 437, 'limo': 627, 'limousine': 627, 'limpkin': 135, 'liner': 628, 'linnet': 12, 'lion': 291, 'lionfish': 396, 'lip rouge': 629, 'lipstick': 629, 'little blue heron': 131, 'llama': 355, 'loggerhead': 33, 'loggerhead turtle': 33, 'lollipop': 929, 'lolly': 929, 'long-horned beetle': 303, 'longicorn': 303, 'longicorn beetle': 303, 'lorikeet': 90, 'lotion': 631, 'loudspeaker': 632, 'loudspeaker system': 632, 'loupe': 633, 'lumbermill': 634, 'lycaenid': 326, 'lycaenid butterfly': 326, 'lynx': 287, 'macaque': 373, 'macaw': 88, 'magnetic compass': 635, 'magpie': 18, 'mail': 490, 'mailbag': 636, 'mailbox': 637, 'maillot': 639, 'malamute': 249, 'malemute': 249, 'malinois': 225, 'man-eater': 2, 'man-eating shark': 2, 'maned wolf': 271, 'manhole cover': 640, 'mantid': 315, 'mantis': 315, 'manufactured home': 660, 'maraca': 641, 'marimba': 642, 'market': 582, 'marmoset': 377, 'marmot': 336, 'marsh hen': 137, 'mashed potato': 935, 'mask': 643, 'matchstick': 644, 'maypole': 645, 'maze': 646, 'measuring cup': 647, 'meat cleaver': 499, 'meat loaf': 962, 'meat market': 467, 'meatloaf': 962, 'medicine cabinet': 648, 'medicine chest': 648, 'meerkat': 299, 'megalith': 649, 'megalithic structure': 649, 'membranophone': 541, 'memorial tablet': 458, 'menu': 922, 'merry-go-round': 476, 'microphone': 650, 'microwave': 651, 'microwave oven': 651, 'mierkat': 299, 'mike': 650, 'mileometer': 685, 'military plane': 895, 'military uniform': 652, 'milk can': 653, 'milkweed butterfly': 323, 'milometer': 685, 'mini': 655, 'miniature pinscher': 237, 'miniature poodle': 266, 'miniature schnauzer': 196, 'minibus': 654, 'miniskirt': 655, 'minivan': 656, 'mink': 357, 'missile': 744, 'mitten': 658, 'mixing bowl': 659, 'mobile home': 660, 'mobile phone': 487, 'modem': 662, 'mole': 460, 'mollymawk': 146, 'monarch': 323, 'monarch butterfly': 323, 'monastery': 663, 'mongoose': 298, 'monitor': 664, 'monkey dog': 252, 'monkey pinscher': 252, 'monocycle': 880, 'mop': 840, 'moped': 665, 'mortar': 666, 'mortarboard': 667, 'mosque': 668, 'mosquito hawk': 319, 'mosquito net': 669, 'motor scooter': 670, 'mountain bike': 671, 'mountain lion': 286, 'mountain tent': 672, 'mouse': 673, 'mousetrap': 674, 'mouth harp': 593, 'mouth organ': 593, 'movie house': 498, 'movie theater': 498, 'movie theatre': 498, 'moving van': 675, 'mower': 621, 'mud hen': 137, 'mud puppy': 29, 'mud turtle': 35, 'mushroom': 947, 'muzzle': 676, 'nail': 677, 'napkin': 529, 'nappy': 529, 'native bear': 105, 'nautilus': 117, 'neck brace': 678, 'necklace': 679, 'nematode': 111, 'nematode worm': 111, 'night snake': 60, 'nipple': 680, 'notebook': 681, 'notebook computer': 681, 'notecase': 893, 'nudibranch': 115, 'numbfish': 5, 'nursery': 580, 'obelisk': 682, 'oboe': 683, 'ocarina': 684, 'ocean liner': 628, 'odometer': 685, 'off-roader': 671, 'offshore rig': 540, 'oil filter': 686, 'one-armed bandit': 800, 'opera glasses': 447, 'orang': 365, 'orange': 950, 'orangutan': 365, 'orangutang': 365, 'orca': 148, 'organ': 687, 'oscilloscope': 688, 'ostrich': 9, 'otter': 360, 'otter hound': 175, 'otterhound': 175, 'ounce': 289, 'overskirt': 689, 'ox': 345, 'oxcart': 690, 'oxygen mask': 691, 'oyster catcher': 143, 'oystercatcher': 143, 'packet': 692, 'packsack': 414, 'paddle': 693, 'paddle wheel': 694, 'paddlewheel': 694, 'paddy wagon': 734, 'padlock': 695, 'pail': 463, 'paintbrush': 696, 'painter': 286, 'pajama': 697, 'palace': 698, 'paling': 716, 'panda': 388, 'panda bear': 388, 'pandean pipe': 699, 'panpipe': 699, 'panther': 290, 'paper knife': 623, 'paper towel': 700, 'paperknife': 623, 'papillon': 157, 'parachute': 701, 'parallel bars': 702, 'park bench': 703, 'parking meter': 704, 'partridge': 86, 'passenger car': 705, 'patas': 371, 'patio': 706, 'patrol wagon': 734, 'patten': 502, 'pay-phone': 707, 'pay-station': 707, 'peacock': 84, 'pearly nautilus': 117, 'pedestal': 708, 'pelican': 144, 'pencil box': 709, 'pencil case': 709, 'pencil eraser': 767, 'pencil sharpener': 710, 'penny bank': 719, 'perfume': 711, 'petrol pump': 571, 'pharos': 437, 'photocopier': 713, 'piano accordion': 401, 'pick': 714, 'pickelhaube': 715, 'picket fence': 716, 'pickup': 717, 'pickup truck': 717, 'picture palace': 498, 'pier': 718, 'pig': 341, 'pigboat': 833, 'piggy bank': 719, 'pill bottle': 720, 'pillow': 721, 'pineapple': 953, 'ping-pong ball': 722, 'pinwheel': 723, 'pipe organ': 687, 'pirate': 724, 'pirate ship': 724, 'pismire': 310, 'pit bull terrier': 180, 'pitcher': 725, 'pizza': 963, 'pizza pie': 963, \"pj's\": 697, 'plane': 726, 'planetarium': 727, 'plaque': 458, 'plastic bag': 728, 'plate': 923, 'plate rack': 729, 'platyhelminth': 110, 'platypus': 103, 'plectron': 714, 'plectrum': 714, 'plinth': 708, 'plough': 730, 'plow': 730, \"plumber's helper\": 731, 'plunger': 731, 'pocketbook': 893, 'poke bonnet': 452, 'polar bear': 296, 'pole': 733, 'polecat': 361, 'police van': 734, 'police wagon': 734, 'polyplacophore': 116, 'pomegranate': 957, 'poncho': 735, 'pool table': 736, 'pop bottle': 737, 'popsicle': 929, 'porcupine': 334, 'postbag': 636, 'pot': 738, 'potpie': 964, \"potter's wheel\": 739, 'power drill': 740, 'prairie chicken': 83, 'prairie fowl': 83, 'prairie grouse': 83, 'prairie wolf': 272, 'prayer mat': 741, 'prayer rug': 741, 'press': 894, 'pretzel': 932, 'printer': 742, 'prison': 743, 'prison house': 743, 'proboscis monkey': 376, 'projectile': 744, 'projector': 745, 'promontory': 976, 'ptarmigan': 81, 'puck': 746, 'puff': 750, 'puff adder': 54, 'puffer': 397, 'pufferfish': 397, 'pug': 254, 'pug-dog': 254, 'puma': 286, 'punch bag': 747, 'punchball': 747, 'punching bag': 747, 'punching ball': 747, 'purse': 748, 'pyjama': 697, 'quail': 85, 'quill': 749, 'quill pen': 749, 'quilt': 750, 'race car': 751, 'racer': 751, 'racing car': 751, 'racket': 752, 'racquet': 752, 'radiator': 753, 'radiator grille': 581, 'radio': 754, 'radio reflector': 755, 'radio telescope': 755, 'rain barrel': 756, 'ram': 348, 'rapeseed': 984, 'reaper': 595, 'recreational vehicle': 757, 'red fox': 277, 'red hot': 934, 'red panda': 387, 'red setter': 213, 'red wine': 966, 'red wolf': 271, 'red-backed sandpiper': 140, 'red-breasted merganser': 98, 'redbone': 168, 'redshank': 141, 'reel': 758, 'reflex camera': 759, 'refrigerator': 760, 'remote': 761, 'remote control': 761, 'respirator': 570, 'restaurant': 762, 'revolver': 763, 'rhinoceros beetle': 306, 'ribbed toad': 32, 'ricksha': 612, 'rickshaw': 612, 'rifle': 764, 'rig': 867, 'ring armor': 490, 'ring armour': 490, 'ring mail': 490, 'ring snake': 53, 'ring-binder': 446, 'ring-necked snake': 53, 'ring-tailed lemur': 383, 'ringlet': 322, 'ringlet butterfly': 322, 'ringneck snake': 53, 'ringtail': 378, 'river horse': 344, 'roach': 314, 'robin': 15, 'rock beauty': 392, 'rock crab': 119, 'rock lobster': 123, 'rock python': 62, 'rock snake': 62, 'rocker': 765, 'rocking chair': 765, 'rose hip': 989, 'rosehip': 989, 'rotisserie': 766, 'roundabout': 476, 'roundworm': 111, 'rubber': 767, 'rubber eraser': 767, 'rucksack': 414, 'ruddy turnstone': 139, 'ruffed grouse': 82, 'rugby ball': 768, 'rule': 769, 'ruler': 769, 'running shoe': 770, 'sabot': 502, 'safe': 771, 'safety pin': 772, 'salt shaker': 773, 'saltshaker': 773, 'sand bar': 977, 'sand viper': 66, 'sandal': 774, 'sandbar': 977, 'sarong': 775, 'sawmill': 634, 'sax': 776, 'saxophone': 776, 'scabbard': 777, 'scale': 778, 'schipperke': 223, 'school bus': 779, 'schooner': 780, 'scooter': 670, 'scope': 688, 'scoreboard': 781, 'scorpion': 71, 'screen': 782, 'screw': 783, 'screwdriver': 784, 'scuba diver': 983, 'sea anemone': 108, 'sea cradle': 116, 'sea crawfish': 123, 'sea cucumber': 329, 'sea lion': 150, 'sea slug': 115, 'sea snake': 65, 'sea star': 327, 'sea urchin': 328, 'sea wolf': 148, 'sea-coast': 978, 'seacoast': 978, 'seashore': 978, 'seat belt': 785, 'seatbelt': 785, 'seawall': 460, 'semi': 867, 'sewing machine': 786, 'sewing needle': 319, 'shades': 837, 'shako': 439, 'shield': 787, 'shoe shop': 788, 'shoe store': 788, 'shoe-shop': 788, 'shoji': 789, 'shopping basket': 790, 'shopping cart': 791, 'shovel': 792, 'shower cap': 793, 'shower curtain': 794, 'siamang': 369, 'sidewinder': 68, 'silky terrier': 201, 'silver salmon': 391, 'site': 916, 'six-gun': 763, 'six-shooter': 763, 'skeeter hawk': 319, 'ski': 795, 'ski mask': 796, 'skillet': 567, 'skunk': 361, 'sleeping bag': 797, 'sleuthhound': 163, 'slide rule': 798, 'sliding door': 799, 'slipstick': 798, 'slot': 800, 'sloth bear': 297, 'slug': 114, 'smoothing iron': 606, 'snail': 113, 'snake doctor': 319, 'snake feeder': 319, 'snake fence': 912, 'snake-rail fence': 912, 'snoek': 389, 'snooker table': 736, 'snorkel': 801, 'snow leopard': 289, 'snowbird': 13, 'snowmobile': 802, 'snowplough': 803, 'snowplow': 803, 'soap dispenser': 804, 'soccer ball': 805, 'sock': 806, 'soda bottle': 737, 'soft-coated wheaten terrier': 202, 'solar collector': 807, 'solar dish': 807, 'solar furnace': 807, 'sombrero': 808, 'sorrel': 339, 'soup bowl': 809, 'space bar': 810, 'space heater': 811, 'space shuttle': 812, 'spaghetti squash': 940, 'spatula': 813, 'speaker': 632, 'speaker system': 632, 'speaker unit': 632, 'speedboat': 814, 'spider monkey': 381, 'spider web': 815, \"spider's web\": 815, 'spike': 998, 'spindle': 816, 'spiny anteater': 102, 'spiny lobster': 123, 'spiral': 506, 'spoonbill': 129, 'sport car': 817, 'sports car': 817, 'spot': 818, 'spotlight': 818, 'spotted salamander': 28, 'squealer': 341, 'squeeze box': 401, 'squirrel monkey': 382, 'stage': 819, 'standard poodle': 267, 'standard schnauzer': 198, 'starfish': 327, 'station waggon': 436, 'station wagon': 436, 'steam locomotive': 820, 'steel arch bridge': 821, 'steel drum': 822, 'stethoscope': 823, 'stick insect': 313, 'stingray': 6, 'stinkhorn': 994, 'stole': 824, 'stone wall': 825, 'stop watch': 826, 'stoplight': 920, 'stopwatch': 826, 'stove': 827, 'strainer': 828, 'strawberry': 949, 'street sign': 919, 'streetcar': 829, 'stretcher': 830, 'studio couch': 831, 'stupa': 832, 'sturgeon': 394, 'sub': 833, 'submarine': 833, 'suit': 834, 'suit of clothes': 834, 'sulfur butterfly': 325, 'sulphur butterfly': 325, 'sulphur-crested cockatoo': 89, 'sun blocker': 838, 'sunblock': 838, 'sundial': 835, 'sunglass': 836, 'sunglasses': 837, 'sunscreen': 838, 'suspension bridge': 839, 'swab': 840, 'sweatshirt': 841, 'sweet potato': 684, 'swimming cap': 433, 'swimming trunks': 842, 'swing': 843, 'switch': 844, 'swob': 840, 'syringe': 845, 'syrinx': 699, 'tabby': 281, 'tabby cat': 281, 'table lamp': 846, 'tailed frog': 32, 'tailed toad': 32, 'tam-tam': 577, 'tandem': 444, 'tandem bicycle': 444, 'tank': 847, 'tank suit': 639, 'tape player': 848, 'taper': 470, 'tarantula': 76, 'taxi': 468, 'taxicab': 468, 'teapot': 849, 'teddy': 850, 'teddy bear': 850, 'tee shirt': 610, 'television': 851, 'television system': 851, 'ten-gallon hat': 515, 'tench': 0, 'tennis ball': 852, 'terrace': 706, 'terrapin': 36, 'thatch': 853, 'thatched roof': 853, 'theater curtain': 854, 'theatre curtain': 854, 'thimble': 855, 'thrasher': 856, 'three-toed sloth': 364, 'thresher': 856, 'threshing machine': 856, 'throne': 857, 'thunder snake': 52, 'tick': 78, 'tiger': 292, 'tiger beetle': 300, 'tiger cat': 282, 'tiger shark': 3, 'tile roof': 858, 'timber wolf': 269, 'tin opener': 473, 'titi': 380, 'titi monkey': 380, 'toaster': 859, 'tobacco shop': 860, 'tobacconist': 860, 'tobacconist shop': 860, 'toilet paper': 999, 'toilet seat': 861, 'toilet tissue': 999, 'tool kit': 477, 'tope': 832, 'torch': 862, 'torpedo': 5, 'totem pole': 863, 'toucan': 96, 'tow car': 864, 'tow truck': 864, 'toy poodle': 265, 'toy terrier': 158, 'toyshop': 865, 'trackless trolley': 874, 'tractor': 866, 'tractor trailer': 867, 'traffic light': 920, 'traffic signal': 920, 'trailer truck': 867, 'tram': 829, 'tramcar': 829, 'transverse flute': 558, 'trash barrel': 412, 'trash bin': 412, 'trash can': 412, 'tray': 868, 'tree frog': 31, 'tree-frog': 31, 'trench coat': 869, 'triceratops': 51, 'tricycle': 870, 'trifle': 927, 'trike': 870, 'trilobite': 69, 'trimaran': 871, 'tripod': 872, 'triumphal arch': 873, 'trolley': 829, 'trolley car': 829, 'trolley coach': 874, 'trolleybus': 874, 'trombone': 875, 'trucking rig': 867, 'trump': 513, 'trumpet': 513, 'tub': 876, 'tup': 348, 'turnstile': 877, 'tusker': 101, 'two-piece': 445, 'tympan': 541, 'typewriter keyboard': 878, 'umbrella': 879, 'unicycle': 880, 'upright': 881, 'upright piano': 881, 'vacuum': 882, 'vacuum cleaner': 882, 'vale': 979, 'valley': 979, 'vase': 883, 'vat': 876, 'vault': 884, 'velocipede': 870, 'velvet': 885, 'vending machine': 886, 'vestment': 887, 'viaduct': 888, 'vine snake': 59, 'violin': 889, 'violoncello': 486, 'vizsla': 211, 'volcano': 980, 'volleyball': 890, 'volute': 506, 'vulture': 23, 'waffle iron': 891, 'waggon': 436, 'wagon': 734, 'walking stick': 313, 'walkingstick': 313, 'wall clock': 892, 'wallaby': 104, 'wallet': 893, 'wardrobe': 894, 'warplane': 895, 'warragal': 273, 'warrigal': 273, 'warthog': 343, 'wash-hand basin': 896, 'washbasin': 896, 'washbowl': 896, 'washer': 897, 'washing machine': 897, 'wastebin': 412, 'water bottle': 898, 'water buffalo': 346, 'water hen': 137, 'water jug': 899, 'water ouzel': 20, 'water ox': 346, 'water snake': 58, 'water tower': 900, 'wax light': 470, 'weasel': 356, 'web site': 916, 'website': 916, 'weevil': 307, 'weighing machine': 778, 'welcome mat': 539, 'wheelbarrow': 428, 'whippet': 172, 'whiptail': 41, 'whiptail lizard': 41, 'whirligig': 476, 'whiskey jug': 901, 'whistle': 902, 'white fox': 279, 'white shark': 2, 'white stork': 127, 'white wolf': 270, 'whorl': 506, 'wig': 903, 'wild boar': 342, 'window screen': 904, 'window shade': 905, 'wine bottle': 907, 'wing': 908, 'wire-haired fox terrier': 188, 'wireless': 754, 'wok': 909, 'wolf spider': 77, 'wombat': 106, 'wood pussy': 361, 'wood rabbit': 330, 'wooden spoon': 910, 'woodworking plane': 726, 'wool': 911, 'woolen': 911, 'woollen': 911, 'worm fence': 912, 'worm snake': 52, 'wreck': 913, 'wrecker': 864, 'xylophone': 642, 'yawl': 914, \"yellow lady's slipper\": 986, 'yellow lady-slipper': 986, 'yurt': 915, 'zebra': 340, 'zucchini': 939}\n",
        "DiT_prefs = {\n",
        "    'prompt': '',\n",
        "    'batch_folder_name': '',\n",
        "    'guidance_scale': 4.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildDiT(page):\n",
        "    global DiT_prefs, prefs, pipe_DiT\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            DiT_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            DiT_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            DiT_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_DiT_output(o):\n",
        "      page.DiT_output.controls.append(o)\n",
        "      page.DiT_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_DiT_output = add_to_DiT_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.DiT_output.controls = []\n",
        "      page.DiT_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def DiT_help(e):\n",
        "      def close_DiT_dlg(e):\n",
        "        nonlocal DiT_help_dlg\n",
        "        DiT_help_dlg.open = False\n",
        "        page.update()\n",
        "      DiT_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DiT Pipeline\"), content=Column([\n",
        "          Text(\"Provide a comma separated list of general ImageNet Classes to create images. Press Class List to see availble classes, click to copy a token to clipboard then paste in textfield.\"),\n",
        "          Text(\"We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.\"),\n",
        "          Markdown(\"The DiT model in diffusers comes from  can be found here: [Scalable Diffusion Models with Transformers](https://www.wpeebles.com/DiT) (DiT) and [facebookresearch/dit](https://github.com/facebookresearch/dit)..\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Interesting... \", on_click=close_DiT_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = DiT_help_dlg\n",
        "      DiT_help_dlg.open = True\n",
        "      page.update()\n",
        "    def copy_class(e):\n",
        "      page.set_clipboard(e.control.text)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã   Class {e.control.text} copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "    def show_classes(e):\n",
        "      classes = []\n",
        "      for c in ImageNet_classes.keys():\n",
        "        #TODO Copy to clipboard on click\n",
        "        classes.append(TextButton(c, col={'sm':4, 'md':3, 'lg':2,}, on_click=copy_class))\n",
        "      alert_msg(page, \"ImageNET Class List\", content=Container(Column([ResponsiveRow(\n",
        "        controls=classes,\n",
        "        expand=True,\n",
        "      )], spacing=0), width=(page.width if page.web else page.window_width) - 150), okay=\"üò≤  That's a lot...\", sound=False)\n",
        "    guidance_scale = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=DiT_prefs, key='guidance_scale')\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        DiT_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"ImageNet Class Names (separated by commas)\", value=DiT_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(DiT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=DiT_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=DiT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(DiT_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(DiT_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(DiT_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=DiT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=DiT_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=DiT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiT.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not DiT_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.DiT_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.DiT_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚ößÔ∏è  DiT Models with Transformers Class-to-Image Generator\", \"Scalable Diffusion Models with Transformers...\", actions=[ft.OutlinedButton(\"Class List\", on_click=show_classes), IconButton(icon=icons.HELP, tooltip=\"Help with DiT Settings\", on_click=DiT_help)]),\n",
        "        prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance_scale,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=DiT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_DiT,\n",
        "        Row([ElevatedButton(content=Text(\"üîÄ   Get DiT Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page, from_list=True))\n",
        "             ]),\n",
        "      ]\n",
        "    )), page.DiT_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "dall_e_prefs = {\n",
        "    'prompt': '',\n",
        "    'size': '512x512',\n",
        "    'num_images': 1,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'variation': False,\n",
        "    \"invert_mask\": False,\n",
        "    'file_prefix': 'dalle2-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "    \"batch_folder_name\": '',\n",
        "}\n",
        "\n",
        "def buildDallE2(page):\n",
        "    global dall_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            dall_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            dall_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            dall_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            dalle = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            dalle.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(dalle)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(dalle[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                dall_e_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                dall_e_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        dalle = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                dalle.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(dalle)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        dall_e_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "\n",
        "    prompt = TextField(label=\"Prompt Text\", value=dall_e_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=dall_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=dall_e_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_images = NumberPicker(label=\"Num of Images\", min=1, max=10, step=9, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #num_images = TextField(label=\"num_images\", value=dall_e_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=dall_e_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    #steps = TextField(label=\"Inference Steps\", value=dall_e_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    #eta = TextField(label=\"DDIM ETA\", value=dall_e_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype=\"float\"))\n",
        "    #seed = TextField(label=\"Seed\", value=dall_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    size = Dropdown(label=\"Image Size\", width=120, options=[dropdown.Option(\"256x256\"), dropdown.Option(\"512x512\"), dropdown.Option(\"1024x1024\")], value=dall_e_prefs['size'], on_change=lambda e:changed(e,'size'))\n",
        "    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size, NumberPicker(label=\" Number of Images\", min=1, max=10, step=1, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))], col={'lg':6})])\n",
        "\n",
        "    #width = Slider(min=128, max=1024, divisions=6, label=\"{value}px\", value=dall_e_prefs['width'], on_change=change_width, expand=True)\n",
        "    #width_value = Text(f\" {int(dall_e_prefs['width'])}px\", weight=FontWeight.BOLD)\n",
        "    #width_slider = Row([Text(f\"Width: \"), width_value, width])\n",
        "    #height = Slider(min=128, max=1024, divisions=6, label=\"{value}px\", value=dall_e_prefs['height'], on_change=change_height, expand=True)\n",
        "    #height_value = Text(f\" {int(dall_e_prefs['height'])}px\", weight=FontWeight.BOLD)\n",
        "    #height_slider = Row([Text(f\"Height: \"), height_value, height])\n",
        "    init_image = TextField(label=\"Init Image (optional)\", value=dall_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init, col={\"*\":1, \"md\":3}))\n",
        "    mask_image = TextField(label=\"Mask Image (optional)\", value=dall_e_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={\"*\":1, \"md\":3}))\n",
        "    variation = Checkbox(label=\"Variation   \", tooltip=\"Creates Variation of Init Image. Disregards the Prompt and Mask.\", value=dall_e_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=dall_e_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={\"md\":6}), Row([mask_image, invert_mask], col={\"md\":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=dall_e_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    #strength_value = Text(f\" {int(dall_e_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "    #strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=dall_e_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(dall_e_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", round=1, value=dall_e_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=dall_e_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=dall_e_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not dall_e_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page, from_list=True))\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üñºÔ∏è   Run DALL‚Ä¢E 2\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.dall_e_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üë∫  OpenAI DALL‚Ä¢E 2\", \"Generates Images using your OpenAI API Key. Note: Uses same credits as official website.\"),\n",
        "            prompt,\n",
        "            img_block, page.ESRGAN_block_dalle,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            param_rows,\n",
        "            parameters_row,\n",
        "            page.dall_e_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "dall_e_3_prefs = {\n",
        "    'prompt': '',\n",
        "    'size': '1024x1024',\n",
        "    'num_images': 1,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'variation': False,\n",
        "    \"invert_mask\": False,\n",
        "    'hd_quality': False,\n",
        "    'natural_style': False,\n",
        "    'file_prefix': 'dalle3-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "    \"batch_folder_name\": '',\n",
        "}\n",
        "\n",
        "def buildDallE3(page):\n",
        "    global dall_e_3_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            dall_e_3_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            dall_e_3_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            dall_e_3_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            dalle = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            dalle.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(dalle)\n",
        "            print(str(src_path))\n",
        "            print(str(dalle[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            init_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                dall_e_3_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                dall_e_3_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        dalle = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                dalle.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(dalle)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        dall_e_3_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "\n",
        "    prompt = TextField(label=\"Prompt Text\", value=dall_e_3_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=dall_e_3_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=dall_e_3_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_images = TextField(label=\"num_images\", value=dall_e_3_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #num_images = NumberPicker(label=\"Num of Images\", min=1, max=10, step=9, value=dall_e_3_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    size = Dropdown(label=\"Image Size\", width=130, options=[dropdown.Option(\"1024x1024\"), dropdown.Option(\"1024x1792\"), dropdown.Option(\"1792x1024\")], value=dall_e_3_prefs['size'], on_change=lambda e:changed(e,'size'))\n",
        "    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size], col={'lg':6})])\n",
        "    init_image = TextField(label=\"Init Image\", value=dall_e_3_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init, col={\"*\":1, \"md\":3}))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=dall_e_3_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={\"*\":1, \"md\":3}))\n",
        "    variation = Checkbox(label=\"Variation   \", tooltip=\"Creates Variation of Init Image. Disregards the Prompt and Mask.\", value=dall_e_3_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=dall_e_3_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={\"md\":6}), Row([mask_image, invert_mask], col={\"md\":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    hd_quality = Switcher(label=\"HD Quality\", value=dall_e_3_prefs['hd_quality'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'hd_quality'), tooltip=\"Creates images with finer details and greater consistency across the image\")\n",
        "    natural_style = Switcher(label=\"Natural Style\", value=dall_e_3_prefs['natural_style'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'natural_style'), tooltip=\"Vivid is default, leaning towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.\")\n",
        "    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=dall_e_3_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(dall_e_3_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", round=1, value=dall_e_3_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=dall_e_3_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=dall_e_3_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not dall_e_3_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e_3(page, from_list=True))\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üôå   Run DALL‚Ä¢E 3\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e_3(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    dall_e_3_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üèá  OpenAI DALL‚Ä¢E 3\", \"Generates Images using your OpenAI API Key. Note: Uses same credits as official website.\"),\n",
        "            prompt,\n",
        "            Row([hd_quality, natural_style]),\n",
        "            #img_block,\n",
        "            param_rows,\n",
        "            page.ESRGAN_block_dalle,\n",
        "            parameters_row,\n",
        "            dall_e_3_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "kandinsky_3_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"steps\":25,\n",
        "    \"width\": 1024,\n",
        "    \"height\":1024,\n",
        "    \"guidance_scale\":4,\n",
        "    #'prior_guidance_scale': 4.0,\n",
        "    #'prior_steps': 25,\n",
        "    \"init_image\": '',\n",
        "    \"strength\": 0.3,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"cpu_offload\": False,\n",
        "    \"seed\": 0,\n",
        "    \"kandinsky_model\": \"Kandinsky 3\",\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinsky3(page):\n",
        "    global prefs, kandinsky_3_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_3_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_3_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_3_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def kandinsky_3_help(e):\n",
        "      def close_kandinsky_3_dlg(e):\n",
        "        nonlocal kandinsky_3_help_dlg\n",
        "        kandinsky_3_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_3_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Pipeline\"), content=Column([\n",
        "          Markdown(\"Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively. Its architecture includes 3 main components: 1) FLAN-UL2, which is an encoder decoder model based on the T5 architecture. 2) New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining the same number of parameters. 3) Sber-MoVQGAN is a decoder proven to have superior results in image restoration.  Kandinsky inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"Kandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh). Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§§  Quality... \", on_click=close_kandinsky_3_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_3_help_dlg\n",
        "      kandinsky_3_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_3_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_version(e):\n",
        "        status['kandinsky_version'] = e.control.value\n",
        "        if '2.1' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky21(page)\n",
        "        elif '2.2' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky(page)\n",
        "        elif '3.0' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky3(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky\":\n",
        "            t.content = page.Kandinsky\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=kandinsky_3_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=kandinsky_3_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_3_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_3_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_3_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_3_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_3_prefs, key='steps')\n",
        "    #prior_guidance_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=20, round=1, expand=True, pref=kandinsky_3_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})\n",
        "    #prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, expand=True, pref=kandinsky_3_prefs, key='prior_steps', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_3_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=kandinsky_3_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=kandinsky_3_prefs, key='height')\n",
        "    init_image = FileInput(label=\"Init Image (optional)\", pref=kandinsky_3_prefs, key='init_image', expand=True, page=page)\n",
        "    mask_image = FileInput(label=\"Mask Image\", pref=kandinsky_3_prefs, key='mask_image', expand=True, page=page)\n",
        "    #, mask_image, invert_mask\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky_3_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    strength_slider = SliderRow(label=\"Init Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_3_prefs, key='strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=kandinsky_3_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(kandinsky_3_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky_3_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_3_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_3_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_3_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_3_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    kandinsky_version = Dropdown(width=155, options=[dropdown.Option(\"Kandinsky 3.0\"), dropdown.Option(\"Kandinsky 2.2\"), dropdown.Option(\"Kandinsky 2.1\")], value=status['kandinsky_version'], on_change=change_version)\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"‚ú®   Run Kandinsky 3\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.Kandinsky_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text(\"Switch to 2.1\", size=18), on_click=switch_version)\n",
        "            Header(\"üéé  Kandinsky 3.0\", \"A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...\", actions=[kandinsky_version, IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_3_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            #ResponsiveRow([prior_steps, prior_guidance_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            img_block,\n",
        "            page.ESRGAN_block_kandinsky,\n",
        "            ResponsiveRow([Row([n_images, seed, cpu_offload], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            parameters_row,\n",
        "            page.Kandinsky_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "kandinsky_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"steps\":25,\n",
        "    #\"ddim_eta\":0.05,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":4,\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'prior_steps': 25,\n",
        "    \"init_image\": '',\n",
        "    \"strength\": 0.3,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"seed\": 0,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinsky(page):\n",
        "    global prefs, kandinsky_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def kandinsky_help(e):\n",
        "      def close_kandinsky_dlg(e):\n",
        "        nonlocal kandinsky_help_dlg\n",
        "        kandinsky_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Pipeline\"), content=Column([\n",
        "          #Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          #Text(\"Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Markdown(\"Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§§  Quality... \", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_help_dlg\n",
        "      kandinsky_help_dlg.open = True\n",
        "      page.update()\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                kandinsky_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                kandinsky_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def switch_version(e):\n",
        "        status['kandinsky_2_2'] = False\n",
        "        page.Kandinsky = buildKandinsky21(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky\":\n",
        "            t.content = page.Kandinsky\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    def change_version(e):\n",
        "        status['kandinsky_version'] = e.control.value\n",
        "        if '2.1' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky21(page)\n",
        "        elif '2.2' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky(page)\n",
        "        elif '3.0' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky3(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky\":\n",
        "            t.content = page.Kandinsky\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=kandinsky_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=kandinsky_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    #ddim_eta = TextField(label=\"DDIM ETA\", value=kandinsky_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype=\"float\"))\n",
        "    #dynamic_threshold_v = TextField(label=\"Dynamic Threshold\", value=kandinsky_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype=\"float\"))\n",
        "    #sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),\n",
        "                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      #], vertical_alignment=CrossAxisAlignment.START)\n",
        "    #denoised_type = Dropdown(label=\"Denoised Type\", width=180, options=[dropdown.Option(\"dynamic_threshold\"), dropdown.Option(\"clip_denoised\")], value=kandinsky_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})\n",
        "    #dropdown_row = ResponsiveRow([sampler])#, denoised_type])\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_prefs, key='steps')\n",
        "    prior_guidance_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=20, round=1, expand=True, pref=kandinsky_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, expand=True, pref=kandinsky_prefs, key='prior_steps', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=kandinsky_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    mask_image = TextField(label=\"Mask Image\", value=kandinsky_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    strength_slider = SliderRow(label=\"Init Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_prefs, key='strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(kandinsky_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"‚ú®   Run Kandinsky 2.2\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    #label=\"Kandinsky Version\", \n",
        "    kandinsky_version = Dropdown(width=150, options=[dropdown.Option(\"Kandinsky 3.0\"), dropdown.Option(\"Kandinsky 2.2\"), dropdown.Option(\"Kandinsky 2.1\")], value=status['kandinsky_version'], on_change=change_version)\n",
        "    page.kandinsky_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text(\"Switch to 2.1\", size=18, on_click=switch_version))\n",
        "            Header(\"üéé  Kandinsky 2.2\", \"A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...\", actions=[kandinsky_version, IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            #param_rows, #dropdown_row,\n",
        "            ResponsiveRow([prior_steps, prior_guidance_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            img_block,\n",
        "            #Row([batch_folder_name, file_prefix]),\n",
        "            page.ESRGAN_block_kandinsky,\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.kandinsky_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "kandinsky21_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"steps\":100,\n",
        "    \"ddim_eta\":0.05,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":4,\n",
        "    'prior_cf_scale': 4,\n",
        "    'prior_steps': \"25\",\n",
        "    \"dynamic_threshold_v\":99.5,\n",
        "    \"sampler\": \"ddim_sampler\",\n",
        "    \"denoised_type\": \"dynamic_threshold\",\n",
        "    \"init_image\": '',\n",
        "    \"strength\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinsky21(page):\n",
        "    global prefs, kandinsky21_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky21_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky21_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky21_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def kandinsky21_help(e):\n",
        "      def close_kandinsky21_dlg(e):\n",
        "        nonlocal kandinsky21_help_dlg\n",
        "        kandinsky21_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky21_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Pipeline\"), content=Column([\n",
        "          Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§§  Quality... \", on_click=close_kandinsky21_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky21_help_dlg\n",
        "      kandinsky21_help_dlg.open = True\n",
        "      page.update()\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                kandinsky21_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                kandinsky21_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky21_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def switch_version(e):\n",
        "        status['kandinsky_2_2'] = True\n",
        "        page.Kandinsky = buildKandinsky(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky\":\n",
        "            t.content = page.Kandinsky\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    def change_version(e):\n",
        "        status['kandinsky_version'] = e.control.value\n",
        "        if '2.1' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky21(page)\n",
        "        elif '2.2' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky(page)\n",
        "        elif '3.0' in status['kandinsky_version']:\n",
        "            page.Kandinsky = buildKandinsky3(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky\":\n",
        "            t.content = page.Kandinsky\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=kandinsky21_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky21_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky21_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky21_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky21_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky21_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky21_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    ddim_eta = TextField(label=\"DDIM ETA\", value=kandinsky21_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype=\"float\"))\n",
        "    dynamic_threshold_v = TextField(label=\"Dynamic Threshold\", value=kandinsky21_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype=\"float\"))\n",
        "    sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky21_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky21_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),\n",
        "                      Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    denoised_type = Dropdown(label=\"Denoised Type\", width=180, options=[dropdown.Option(\"dynamic_threshold\"), dropdown.Option(\"clip_denoised\")], value=kandinsky21_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})\n",
        "    dropdown_row = ResponsiveRow([sampler])#, denoised_type])\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky21_prefs, key='steps')\n",
        "    prior_cf_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=10, pref=kandinsky21_prefs, key='prior_cf_scale', col={'xs':12, 'md':6})\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky21_prefs, key='prior_steps', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky21_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky21_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky21_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=kandinsky21_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    mask_image = TextField(label=\"Mask Image\", value=kandinsky21_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky21_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    strength_slider = SliderRow(label=\"Init Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky21_prefs, key='strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky21_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky21_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky21_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky21_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky21 = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky21.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky21_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    kandinsky_version = Dropdown(width=155, options=[dropdown.Option(\"Kandinsky 3.0\"), dropdown.Option(\"Kandinsky 2.2\"), dropdown.Option(\"Kandinsky 2.1\")], value=status['kandinsky_version'], on_change=change_version)\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"‚ú®   Run Kandinsky 2.1\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.kandinsky21_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text(\"Switch to 2.2\", size=18), on_click=switch_version)\n",
        "            Header(\"üéé  Kandinsky 2.1\", \"A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...\", actions=[kandinsky_version, IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky21_help)]),\n",
        "            prompt,\n",
        "            #param_rows, #dropdown_row,\n",
        "            ResponsiveRow([prior_steps, prior_cf_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            img_block,\n",
        "            page.ESRGAN_block_kandinsky21,\n",
        "            ResponsiveRow([Row([n_images, sampler], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.kandinsky21_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "kandinsky_fuse_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"mixes\": [],\n",
        "    \"steps\":25,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":8,\n",
        "    #'prior_cf_scale': 4,\n",
        "    #'prior_steps': \"25\",\n",
        "    #\"sampler\": \"ddim_sampler\",\n",
        "    \"init_image\": '',\n",
        "    \"weight\": 0.5,\n",
        "    #\"mask_image\": '',\n",
        "    #\"invert_mask\": False,\n",
        "    \"seed\": 0,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinskyFuse(page):\n",
        "    global prefs, kandinsky_fuse_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_fuse_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_fuse_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_fuse_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            init_image.value = fname\n",
        "            init_image.update()\n",
        "            kandinsky_fuse_prefs['init_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_fuse_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def kandinsky_help(e):\n",
        "      def close_kandinsky_dlg(e):\n",
        "        nonlocal kandinsky_help_dlg\n",
        "        kandinsky_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Fuse Pipeline\"), content=Column([\n",
        "          #Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental\"),\n",
        "          Text(\"Kandinsky 2.2 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü´¢  Possibility Overload... \", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_help_dlg\n",
        "      kandinsky_help_dlg.open = True\n",
        "      page.update()\n",
        "    def switch_version(e):\n",
        "        status['kandinsky_fuse_2_2'] = False\n",
        "        page.KandinskyFuse = buildKandinsky21Fuse(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky Fuse\":\n",
        "            t.content = page.KandinskyFuse\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    def add_image(e):\n",
        "        if not bool(kandinsky_fuse_prefs['init_image']): return\n",
        "        layer = {'init_image': kandinsky_fuse_prefs['init_image'], 'weight': kandinsky_fuse_prefs['weight']}\n",
        "        kandinsky_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Image Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky_fuse_prefs['init_image'] = \"\"\n",
        "        init_image.value = \"\"\n",
        "        init_image.update()\n",
        "    def add_prompt(e):\n",
        "        if not bool(kandinsky_fuse_prefs['prompt']): return\n",
        "        layer = {'prompt': kandinsky_fuse_prefs['prompt'], 'weight': kandinsky_fuse_prefs['weight']}\n",
        "        kandinsky_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Text Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Text Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky_fuse_prefs['prompt'] = \"\"\n",
        "        prompt.value = \"\"\n",
        "        prompt.update()\n",
        "    def delete_layer(e):\n",
        "        kandinsky_fuse_prefs['mixes'].remove(e.control.data)\n",
        "        for c in fuse_layers.controls:\n",
        "            if 'prompt' in c.data:\n",
        "                if c.data['prompt'] == e.control.data['prompt']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "            else:\n",
        "                if c.data['init_image'] == e.control.data['init_image']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "        fuse_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        kandinsky_fuse_prefs['mixes'].clear()\n",
        "        fuse_layers.controls.clear()\n",
        "        fuse_layers.update()\n",
        "    def move_down(e):\n",
        "        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx < (len(kandinsky_fuse_prefs['mixes']) - 1):\n",
        "          d = kandinsky_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky_fuse_prefs['mixes'].insert(idx+1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx+1, dr)\n",
        "          fuse_layers.update()\n",
        "    def move_up(e):\n",
        "        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = kandinsky_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky_fuse_prefs['mixes'].insert(idx-1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx-1, dr)\n",
        "          fuse_layers.update()\n",
        "    def edit_layer(e):\n",
        "        data = e.control.data\n",
        "        layer_type = \"prompt\" if \"prompt\" in data else \"image\"\n",
        "        def pick_files_result(e: FilePickerResultEvent):\n",
        "            if e.files:\n",
        "                img = e.files\n",
        "                uf = []\n",
        "                fname = img[0]\n",
        "                src_path = page.get_upload_url(fname.name, 600)\n",
        "                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "                pick_files_dialog.upload(uf)\n",
        "                dst_path = os.path.join(root_dir, fname.name)\n",
        "                image_mix.value = dst_path\n",
        "        pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "        page.overlay.append(pick_files_dialog)\n",
        "        def file_picker_result(e: FilePickerResultEvent):\n",
        "            if e.files != None:\n",
        "                upload_files(e)\n",
        "        def on_upload_progress(e: FilePickerUploadEvent):\n",
        "            if e.progress == 1:\n",
        "                if not slash in e.file_name:\n",
        "                  fname = os.path.join(root_dir, e.file_name)\n",
        "                else:\n",
        "                  fname = e.file_name\n",
        "                image_mix.value = fname\n",
        "                image_mix.update()\n",
        "                data['init_image'] = fname\n",
        "                page.update()\n",
        "        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "        def upload_files(e):\n",
        "            uf = []\n",
        "            if file_picker.result != None and file_picker.result.files != None:\n",
        "                for f in file_picker.result.files:\n",
        "                  if page.web:\n",
        "                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "                  else:\n",
        "                    on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                file_picker.upload(uf)\n",
        "        page.overlay.append(file_picker)\n",
        "        def pick_image(e):\n",
        "            file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "        #name = e.control.title.controls[0].value\n",
        "        #path = e.control.title.controls[1].value\n",
        "        if layer_type == \"prompt\":\n",
        "            prompt_value = data[\"prompt\"]\n",
        "            image_value = \"\"\n",
        "        else:\n",
        "            prompt_value = \"\"\n",
        "            image_value = data[\"init_image\"]\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_layer(e):\n",
        "            layer = None\n",
        "            for l in kandinsky_fuse_prefs['mixes']:\n",
        "                if \"prompt\" in l:\n",
        "                  if layer_type == \"prompt\":\n",
        "                      if data[\"prompt\"] == l[\"prompt\"]:\n",
        "                        layer = l\n",
        "                        layer['prompt'] = prompt_text.value\n",
        "                        break\n",
        "                else:\n",
        "                    if layer_type == \"image\":\n",
        "                      if data[\"init_image\"] == l[\"init_image\"]:\n",
        "                        layer = l\n",
        "                        layer['init_image'] = image_mix.value\n",
        "                        break\n",
        "            for c in fuse_layers.controls:\n",
        "                if 'prompt' in c.data:\n",
        "                    if 'prompt' not in data: continue\n",
        "                    if c.data['prompt'] == data['prompt']:\n",
        "                        c.title.controls[0].value = layer['prompt']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "                else:\n",
        "                    if 'init_image' not in data: continue\n",
        "                    if c.data['init_image'] == data['init_image']:\n",
        "                        c.title.controls[0].value = layer['init_image']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "            layer['prompt'] = prompt_text.value\n",
        "            #layer['weight'] = model_path.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        prompt_text = TextField(label=\"Fuse Prompt Text\", value=prompt_value, multiline=True, visible=layer_type == \"prompt\")\n",
        "        image_mix = TextField(label=\"Fuse Image Path\", value=image_value, visible=layer_type == \"image\", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))\n",
        "        edit_weights = SliderRow(label=\"Weight/Strength\", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip=\"Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit Kandinsky Fuse {layer_type.title()} Mix\"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window_width) - 100)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Layer \", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    add_prompt_btn = ft.FilledButton(\"‚ûï Add Prompt\", width=150, on_click=add_prompt)\n",
        "    #add_prompt_btn = IconButton(icons.ADD, tooltip=\"Add Text Prompt\", on_click=add_prompt)\n",
        "    add_image_btn = ft.FilledButton(\"‚ûï Add Image\", width=150, on_click=add_image)\n",
        "    #add_image_btn = IconButton(icons.ADD, tooltip=\"Add Image to Mix\", on_click=add_image)\n",
        "    prompt = TextField(label=\"Mix Prompt Text\", value=kandinsky_fuse_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))\n",
        "    prompt_row = Row([prompt, add_prompt_btn])\n",
        "    init_image = TextField(label=\"Mixing Image\", value=kandinsky_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    image_row = Row([init_image, add_image_btn])\n",
        "    weight_slider = SliderRow(label=\"Text or Image Weight\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_fuse_prefs, key='weight')\n",
        "    fuse_layers = Column([], spacing=0)\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    #sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),\n",
        "    #                  Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "    #                  #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "    #                  ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_fuse_prefs, key='steps')\n",
        "    #prior_cf_scale = SliderRow(label=\"Prior CF Scale\", min=0, max=10, divisions=10, pref=kandinsky_fuse_prefs, key='prior_cf_scale')\n",
        "    #prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='prior_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_fuse_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_fuse_prefs, key='height')\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(kandinsky_fuse_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=kandinsky_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    #invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky_fuse_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_fuse_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_fuse_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_fuse_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_fuse = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_fuse.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí•   Run Kandinsky 2.2 Fuser\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_fuse(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.kandinsky_fuse_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üí£  Kandinsky 2.2 Fuse\", \"Mix multiple Images and Prompts together to Interpolate. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...\", actions=[ft.OutlinedButton(content=Text(\"Switch to 2.1\", size=18), on_click=switch_version), IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_help)]),\n",
        "            prompt_row,\n",
        "            image_row,\n",
        "            weight_slider,\n",
        "            Divider(height=5, thickness=4),\n",
        "            fuse_layers,\n",
        "            #Divider(height=2, thickness=2),\n",
        "            #param_rows, #dropdown_row,\n",
        "            steps,\n",
        "            #prior_steps, prior_cf_scale,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            #Row([batch_folder_name, file_prefix]),\n",
        "            #Row([n_images, sampler]),\n",
        "            page.ESRGAN_block_kandinsky_fuse,\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.kandinsky_fuse_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "kandinsky21_fuse_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"mixes\": [],\n",
        "    \"steps\":100,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":8,\n",
        "    'prior_cf_scale': 4,\n",
        "    'prior_steps': \"25\",\n",
        "    \"sampler\": \"ddim_sampler\",\n",
        "    \"init_image\": '',\n",
        "    \"weight\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinsky21Fuse(page):\n",
        "    global prefs, kandinsky21_fuse_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky21_fuse_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky21_fuse_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky21_fuse_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            init_image.value = fname\n",
        "            init_image.update()\n",
        "            kandinsky21_fuse_prefs['init_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky21_fuse_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def kandinsky21_help(e):\n",
        "      def close_kandinsky21_dlg(e):\n",
        "        nonlocal kandinsky21_help_dlg\n",
        "        kandinsky21_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky21_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Fuse Pipeline\"), content=Column([\n",
        "          Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental\"),\n",
        "          Text(\"Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü´¢  Possibility Overload... \", on_click=close_kandinsky21_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky21_help_dlg\n",
        "      kandinsky21_help_dlg.open = True\n",
        "      page.update()\n",
        "    def switch_version(e):\n",
        "        status['kandinsky_fuse_2_2'] = True\n",
        "        page.KandinskyFuse = buildKandinskyFuse(page)\n",
        "        for t in page.ImageAIs.tabs:\n",
        "          if t.text == \"Kandinsky Fuse\":\n",
        "            t.content = page.KandinskyFuse\n",
        "            break\n",
        "        page.ImageAIs.update()\n",
        "        page.update()\n",
        "    def add_image(e):\n",
        "        if not bool(kandinsky21_fuse_prefs['init_image']): return\n",
        "        layer = {'init_image': kandinsky21_fuse_prefs['init_image'], 'weight': kandinsky21_fuse_prefs['weight']}\n",
        "        kandinsky21_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Image Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky21_fuse_prefs['init_image'] = \"\"\n",
        "        init_image.value = \"\"\n",
        "        init_image.update()\n",
        "    def add_prompt(e):\n",
        "        if not bool(kandinsky21_fuse_prefs['prompt']): return\n",
        "        layer = {'prompt': kandinsky21_fuse_prefs['prompt'], 'weight': kandinsky21_fuse_prefs['weight']}\n",
        "        kandinsky21_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Text Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Text Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky21_fuse_prefs['prompt'] = \"\"\n",
        "        prompt.value = \"\"\n",
        "        prompt.update()\n",
        "    def delete_layer(e):\n",
        "        kandinsky21_fuse_prefs['mixes'].remove(e.control.data)\n",
        "        for c in fuse_layers.controls:\n",
        "            if 'prompt' in c.data:\n",
        "                if c.data['prompt'] == e.control.data['prompt']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "            else:\n",
        "                if c.data['init_image'] == e.control.data['init_image']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "        fuse_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        kandinsky21_fuse_prefs['mixes'].clear()\n",
        "        fuse_layers.controls.clear()\n",
        "        fuse_layers.update()\n",
        "    def move_down(e):\n",
        "        idx = kandinsky21_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx < (len(kandinsky21_fuse_prefs['mixes']) - 1):\n",
        "          d = kandinsky21_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky21_fuse_prefs['mixes'].insert(idx+1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx+1, dr)\n",
        "          fuse_layers.update()\n",
        "    def move_up(e):\n",
        "        idx = kandinsky21_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = kandinsky21_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky21_fuse_prefs['mixes'].insert(idx-1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx-1, dr)\n",
        "          fuse_layers.update()\n",
        "    def edit_layer(e):\n",
        "        data = e.control.data\n",
        "        layer_type = \"prompt\" if \"prompt\" in data else \"image\"\n",
        "        def pick_files_result(e: FilePickerResultEvent):\n",
        "            if e.files:\n",
        "                img = e.files\n",
        "                uf = []\n",
        "                fname = img[0]\n",
        "                src_path = page.get_upload_url(fname.name, 600)\n",
        "                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "                pick_files_dialog.upload(uf)\n",
        "                dst_path = os.path.join(root_dir, fname.name)\n",
        "                image_mix.value = dst_path\n",
        "        pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "        page.overlay.append(pick_files_dialog)\n",
        "        def file_picker_result(e: FilePickerResultEvent):\n",
        "            if e.files != None:\n",
        "                upload_files(e)\n",
        "        def on_upload_progress(e: FilePickerUploadEvent):\n",
        "            if e.progress == 1:\n",
        "                if not slash in e.file_name:\n",
        "                  fname = os.path.join(root_dir, e.file_name)\n",
        "                else:\n",
        "                  fname = e.file_name\n",
        "                image_mix.value = fname\n",
        "                image_mix.update()\n",
        "                data['init_image'] = fname\n",
        "                page.update()\n",
        "        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "        def upload_files(e):\n",
        "            uf = []\n",
        "            if file_picker.result != None and file_picker.result.files != None:\n",
        "                for f in file_picker.result.files:\n",
        "                  if page.web:\n",
        "                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "                  else:\n",
        "                    on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                file_picker.upload(uf)\n",
        "        page.overlay.append(file_picker)\n",
        "        def pick_image(e):\n",
        "            file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "        #name = e.control.title.controls[0].value\n",
        "        #path = e.control.title.controls[1].value\n",
        "        if layer_type == \"prompt\":\n",
        "            prompt_value = data[\"prompt\"]\n",
        "            image_value = \"\"\n",
        "        else:\n",
        "            prompt_value = \"\"\n",
        "            image_value = data[\"init_image\"]\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_layer(e):\n",
        "            layer = None\n",
        "            for l in kandinsky21_fuse_prefs['mixes']:\n",
        "                if \"prompt\" in l:\n",
        "                  if layer_type == \"prompt\":\n",
        "                      if data[\"prompt\"] == l[\"prompt\"]:\n",
        "                        layer = l\n",
        "                        layer['prompt'] = prompt_text.value\n",
        "                        break\n",
        "                else:\n",
        "                    if layer_type == \"image\":\n",
        "                      if data[\"init_image\"] == l[\"init_image\"]:\n",
        "                        layer = l\n",
        "                        layer['init_image'] = image_mix.value\n",
        "                        break\n",
        "            for c in fuse_layers.controls:\n",
        "                if 'prompt' in c.data:\n",
        "                    if 'prompt' not in data: continue\n",
        "                    if c.data['prompt'] == data['prompt']:\n",
        "                        c.title.controls[0].value = layer['prompt']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "                else:\n",
        "                    if 'init_image' not in data: continue\n",
        "                    if c.data['init_image'] == data['init_image']:\n",
        "                        c.title.controls[0].value = layer['init_image']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "            layer['prompt'] = prompt_text.value\n",
        "            #layer['weight'] = model_path.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        prompt_text = TextField(label=\"Fuse Prompt Text\", value=prompt_value, multiline=True, visible=layer_type == \"prompt\")\n",
        "        image_mix = TextField(label=\"Fuse Image Path\", value=image_value, visible=layer_type == \"image\", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))\n",
        "        edit_weights = SliderRow(label=\"Weight/Strength\", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip=\"Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit Kandinsky Fuse {layer_type.title()} Mix\"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window_width) - 100)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Layer \", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    add_prompt_btn = ft.FilledButton(\"‚ûï Add Prompt\", width=150, on_click=add_prompt)\n",
        "    #add_prompt_btn = IconButton(icons.ADD, tooltip=\"Add Text Prompt\", on_click=add_prompt)\n",
        "    add_image_btn = ft.FilledButton(\"‚ûï Add Image\", width=150, on_click=add_image)\n",
        "    #add_image_btn = IconButton(icons.ADD, tooltip=\"Add Image to Mix\", on_click=add_image)\n",
        "    prompt = TextField(label=\"Mix Prompt Text\", value=kandinsky21_fuse_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))\n",
        "    prompt_row = Row([prompt, add_prompt_btn])\n",
        "    init_image = TextField(label=\"Mixing Image\", value=kandinsky21_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    image_row = Row([init_image, add_image_btn])\n",
        "    weight_slider = SliderRow(label=\"Text or Image Weight\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky21_fuse_prefs, key='weight')\n",
        "    fuse_layers = Column([], spacing=0)\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky21_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky21_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky21_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky21_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky21_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky21_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky21_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky21_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),\n",
        "                      Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky21_fuse_prefs, key='steps')\n",
        "    prior_cf_scale = SliderRow(label=\"Prior Guidance Scale\", min=0, max=10, divisions=10, pref=kandinsky21_fuse_prefs, key='prior_cf_scale', col={'xs':12, 'md':6})\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky21_fuse_prefs, key='prior_steps', col={'xs':12, 'md':6})\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky21_fuse_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky21_fuse_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky21_fuse_prefs, key='height')\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=kandinsky21_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    #invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky21_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky21_fuse_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky21_fuse_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky21_fuse_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky21_fuse_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky21_fuse = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky21_fuse.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky21_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí•   Run Kandinsky 2.1 Fuser\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky21_fuse(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.kandinsky21_fuse_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üí£  Kandinsky 2.1 Fuse\", \"Mix multiple Images and Prompts together to Interpolate. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...\", actions=[ft.OutlinedButton(content=Text(\"Switch to 2.2\", size=18), on_click=switch_version), IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky21_help)]),\n",
        "            prompt_row,\n",
        "            image_row,\n",
        "            weight_slider,\n",
        "            Divider(height=5, thickness=4),\n",
        "            fuse_layers,\n",
        "            #Divider(height=2, thickness=2),\n",
        "            #param_rows, #dropdown_row,\n",
        "            ResponsiveRow([prior_steps, prior_cf_scale]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            page.ESRGAN_block_kandinsky21_fuse,\n",
        "            ResponsiveRow([Row([n_images, sampler], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.kandinsky21_fuse_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "kandinsky_controlnet_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"batch_size\": 1,\n",
        "    \"steps\": 50,\n",
        "    \"prior_steps\": 25,\n",
        "    #\"ddim_eta\":0.05,\n",
        "    \"width\": 768,\n",
        "    \"height\":768,\n",
        "    \"guidance_scale\":4,\n",
        "    \"init_image\": '',\n",
        "    \"strength\": 0.5,\n",
        "    \"prior_strength\": 0.8,\n",
        "    #\"mask_image\": '',\n",
        "    #\"invert_mask\": False,\n",
        "    \"seed\": 0,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinskyControlNet(page):\n",
        "    global prefs, kandinsky_controlnet_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_controlnet_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_controlnet_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_controlnet_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def kandinsky_controlnet_help(e):\n",
        "      def close_kandinsky_controlnet_dlg(e):\n",
        "        nonlocal kandinsky_controlnet_help_dlg\n",
        "        kandinsky_controlnet_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_controlnet_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky ControlNet Pipeline\"), content=Column([\n",
        "          #Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"Kandinsky 2.2 includes KandinskyV22ControlnetImg2ImgPipeline that will allow you to add control to the image generation process with both the image and its depth map. This pipeline works really well with KandinskyV22PriorEmb2EmbPipeline, which generates image embeddings based on both a text prompt and an image. For our robot cat example, we will pass the prompt and cat image together to the prior pipeline to generate an image embedding. We will then use that image embedding and the depth map of the cat to further control the image generation process.\"),\n",
        "          Markdown(\"Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§§  Quality... \", on_click=close_kandinsky_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_controlnet_help_dlg\n",
        "      kandinsky_controlnet_help_dlg.open = True\n",
        "      page.update()\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                kandinsky_controlnet_prefs['init_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=kandinsky_controlnet_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=kandinsky_controlnet_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_controlnet_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky_controlnet_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_controlnet_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    #sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky_controlnet_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    batch_size = NumberPicker(label=\"Batch Size\", min=1, max=6, step=1, value=kandinsky_controlnet_prefs['batch_size'], on_change=lambda e:changed(e,'batch_size', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_controlnet_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),\n",
        "                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      #], vertical_alignment=CrossAxisAlignment.START)\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='steps')\n",
        "    prior_steps = SliderRow(label=\"Number of Prior Steps\", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='prior_steps')\n",
        "    #prior_cf_scale = SliderRow(label=\"Prior CF Scale\", min=0, max=10, divisions=10, pref=kandinsky_controlnet_prefs, key='prior_cf_scale')\n",
        "    #prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='prior_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_controlnet_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_controlnet_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=kandinsky_controlnet_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=kandinsky_controlnet_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    strength_slider = SliderRow(label=\"Init Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='strength', tooltip=\"Indicates how much to transform the reference image.\")\n",
        "    prior_strength_slider = SliderRow(label=\"Prior Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='prior_strength', tooltip=\"Indicates how much to transform the reference text embeddings.\")\n",
        "    #img_block = Container(Column([init_image, strength_slider, prior_strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(kandinsky_controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=kandinsky_controlnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_controlnet_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_controlnet_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_controlnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_controlnet_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üå∑   Run Kandinsky ControlNet\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.kandinsky_controlnet_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üè©  Kandinsky 2.2 ControlNet Text+Image-to-Image\", \"Image-to-Image Generation with ControlNet Conditioning and depth-estimation from transformers.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_controlnet_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            init_image, strength_slider, prior_strength_slider,\n",
        "            #img_block,\n",
        "            #param_rows, #dropdown_row,\n",
        "            steps,\n",
        "            prior_steps,\n",
        "            #prior_cf_scale,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            #Row([batch_folder_name, file_prefix]),\n",
        "            page.ESRGAN_block_kandinsky_controlnet,\n",
        "            ResponsiveRow([Row([n_images, batch_size, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.kandinsky_controlnet_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,\n",
        "    return c\n",
        "\n",
        "\n",
        "deep_daze_prefs = {\n",
        "    'prompt': '',\n",
        "    'num_layers': 32,\n",
        "    'save_every': 20,\n",
        "    'max_size': 512,\n",
        "    'save_progress': False,\n",
        "    'learning_rate': 1e-5,\n",
        "    'iterations': 1050,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'daze-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDeepDaze(page):\n",
        "    global deep_daze_prefs, prefs, pipe_deep_daze\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            deep_daze_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            deep_daze_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            deep_daze_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_deep_daze_output(o):\n",
        "      page.deep_daze_output.controls.append(o)\n",
        "      page.deep_daze_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_deep_daze_output = add_to_deep_daze_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.deep_daze_output.controls = []\n",
        "      page.deep_daze_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def deep_daze_help(e):\n",
        "      def close_deep_daze_dlg(e):\n",
        "        nonlocal deep_daze_help_dlg\n",
        "        deep_daze_help_dlg.open = False\n",
        "        page.update()\n",
        "      deep_daze_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DeepDaze Pipeline\"), content=Column([\n",
        "          Text(\"Text to image generation using OpenAI's CLIP and Siren. Credit goes to Ryan Murdock for the discovery of this technique (and for coming up with the great name)!\"),\n",
        "          Text(\"Heavily influenced by Alexander Mordvintsev's Deep Dream, this work uses CLIP to match an image learned by a SIREN network with a given textual description.\"),\n",
        "          #Markdown(\"\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòµ‚Äçüí´  Why not... \", on_click=close_deep_daze_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = deep_daze_help_dlg\n",
        "      deep_daze_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        deep_daze_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=deep_daze_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=deep_daze_prefs, key='max_size')\n",
        "    #num_layers = TextField(label=\"Inference Steps\", value=str(deep_daze_prefs['num_layers']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_layers', ptype='int'))\n",
        "    learning_rate = TextField(label=\"Learning Rate\", width=130, value=float(deep_daze_prefs['learning_rate']), keyboard_type=KeyboardType.NUMBER, tooltip=\"The learning rate of the neural net.\", on_change=lambda e:changed(e,'learning_rate', ptype='float'))\n",
        "    num_layers_row = SliderRow(label=\"Number of Layers\", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='num_layers', tooltip=\"The number of hidden layers to use with Siren neural network\")\n",
        "    save_every_row = SliderRow(label=\"Save/Show Every x Steps\", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='save_every', tooltip=\"Generate an image every time iterations is a multiple of this number.\")\n",
        "    iterations_row = SliderRow(label=\"Number of Iterations\", min=1, max=2000, divisions=1999, pref=deep_daze_prefs, key='iterations', tooltip=\"The number of times to calculate and backpropogate loss in a given epoch.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=deep_daze_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=deep_daze_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    save_progress = Tooltip(message=\"Whether or not to save images generated before training Siren is complete.\", content=Switcher(label=\"Save Progress Steps\", value=deep_daze_prefs['save_progress'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_progress')))\n",
        "    #eta = TextField(label=\"ETA\", value=str(deep_daze_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(deep_daze_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(deep_daze_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=deep_daze_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=deep_daze_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=deep_daze_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deep_daze = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deep_daze.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not deep_daze_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.deep_daze_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.deep_daze_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëÄ  DeepDaze Text-to-Image Generator\", \"An alternative method using OpenAI's CLIP and Siren. Made a few years ago but still facinating results....\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DeepDaze Settings\", on_click=deep_daze_help)]),\n",
        "        prompt,\n",
        "        num_layers_row,\n",
        "        iterations_row,\n",
        "        Row([learning_rate, save_progress]),\n",
        "        save_every_row,\n",
        "        max_row,\n",
        "        #NumberPicker(label=\"Number of Images: \", min=1, max=20, value=deep_daze_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_deep_daze,\n",
        "        Row([ElevatedButton(content=Text(\"üò∂   Get DeepDaze Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page, from_list=True))\n",
        "        ]),\n",
        "      ]\n",
        "    )), page.deep_daze_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "CLIPstyler_prefs = {\n",
        "    'source':'a photo',\n",
        "    'prompt_text': 'Detailed oil painting',\n",
        "    'batch_folder_name': 'clipstyler',\n",
        "    'crop_size': 128,\n",
        "    'num_crops': 64,\n",
        "    'original_image': '',\n",
        "    'image_dir': \"\",\n",
        "    'training_iterations': 100,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildCLIPstyler(page):\n",
        "    global CLIPstyler, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            CLIPstyler_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            CLIPstyler_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            CLIPstyler_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is original or mask?\n",
        "            original_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            original_image.value = fname\n",
        "            original_image.update()\n",
        "            CLIPstyler_prefs['original_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick original Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        CLIPstyler_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "        has_changed = True\n",
        "    prompt_text = TextField(label=\"Stylized Prompt Text\", value=CLIPstyler_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=CLIPstyler_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    source = TextField(label=\"Source Type\", value=CLIPstyler_prefs['source'], on_change=lambda e:changed(e,'source'))\n",
        "    #training_iterations = TextField(label=\"Training Iterations\", value=CLIPstyler_prefs['training_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'training_iterations', ptype=\"int\"))\n",
        "    crop_size = TextField(label=\"Crop Size\", value=CLIPstyler_prefs['crop_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'crop_size', ptype=\"int\"))\n",
        "    num_crops = TextField(label=\"Number of Crops\", value=CLIPstyler_prefs['num_crops'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_crops', ptype=\"int\"))\n",
        "    param_rows = Column([Row([batch_folder_name, source]), Row([crop_size, num_crops])])\n",
        "    iterations = SliderRow(label=\"Training Iterations\", min=50, max=500, divisions=90, pref=CLIPstyler_prefs, key='training_iterations')\n",
        "\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=CLIPstyler_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=CLIPstyler_prefs, key='height')\n",
        "    original_image = TextField(label=\"Original Image\", value=CLIPstyler_prefs['original_image'], on_change=lambda e:changed(e,'original_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original, col={\"*\":1, \"md\":3}))\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=CLIPstyler_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={\"*\":1, \"md\":3}))\n",
        "    #invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=CLIPstyler_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    image_picker = Container(content=Row([original_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=CLIPstyler_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    #strength_value = Text(f\" {int(CLIPstyler_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "    #strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=CLIPstyler_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=CLIPstyler_prefs, key='enlarge_scale')\n",
        "    #face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=CLIPstyler_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=CLIPstyler_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_styler = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_styler.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not CLIPstyler_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üìé   Run CLIP-Styler\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_CLIPstyler(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.CLIPstyler_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üòé   CLIP-Styler\", \"Transfers a Text Guided Style onto your Image From Prompt Description...\"),\n",
        "            image_picker, prompt_text,\n",
        "            param_rows, iterations, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            page.ESRGAN_block_styler,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),\n",
        "            parameters_row,\n",
        "            page.CLIPstyler_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, crop_size, num_crops,\n",
        "    return c\n",
        "\n",
        "semantic_prefs = {\n",
        "    'prompt': '',\n",
        "    'editing_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 7.5,\n",
        "    'edit_momentum_scale': 0.1,\n",
        "    'edit_mom_beta': 0.4,\n",
        "    'editing_prompts': [],\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 960,\n",
        "    'height': 768,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildSemanticGuidance(page):\n",
        "    global semantic_prefs, prefs, pipe_semantic\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            semantic_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            semantic_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            semantic_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_semantic_output(o):\n",
        "      page.semantic_output.controls.append(o)\n",
        "      page.semantic_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.semantic_output.controls = []\n",
        "      page.semantic_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def semantic_help(e):\n",
        "      def close_semantic_dlg(e):\n",
        "        nonlocal semantic_help_dlg\n",
        "        semantic_help_dlg.open = False\n",
        "        page.update()\n",
        "      semantic_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Semantic Guidance\"), content=Column([\n",
        "          Text(\"SEGA allows applying or removing one or more concepts from an image. The strength of the concept can also be controlled. I.e. the smile concept can be used to incrementally increase or decrease the smile of a portrait. Similar to how classifier free guidance provides guidance via empty prompt inputs, SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts can be applied simultaneously. Each conceptual prompt can either add or remove their concept depending on if the guidance is applied positively or negatively.\"),\n",
        "          Text(\"Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion process instead of performing any explicit gradient-based optimization.\"),\n",
        "          Text(\"Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Diffusion using Semantic Dimensions and provides strong semantic control over the image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, and stay true to the original image composition.\"),\n",
        "          Text(\"Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility.\"),\n",
        "          Markdown(\"[HuggingFace Documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion) - [Paper](https://arxiv.org/abs/2301.12247)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòñ  More Control Please... \", on_click=close_semantic_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = semantic_help_dlg\n",
        "      semantic_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        semantic_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {semantic_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def semantic_tile(semantic_prompt):\n",
        "        params = []\n",
        "        for k, v in semantic_prompt.items():\n",
        "            if k == 'editing_prompt': continue\n",
        "            params.append(f'{to_title(k)}: {v}')\n",
        "        sub = ', '.join(params)\n",
        "        return ListTile(title=Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=semantic_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Semantic Prompt\", on_click=lambda e: edit_semantic(semantic_prompt), data=semantic_prompt),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Semantic Prompt\", on_click=lambda e: del_semantic(semantic_prompt), data=semantic_prompt)]), on_click=lambda e: edit_semantic(semantic_prompt))\n",
        "    def edit_semantic(edit=None):\n",
        "        semantic_prompt = edit if bool(edit) else editing_prompt.copy()\n",
        "        edit_prompt = edit['editing_prompt'] if bool(edit) else \"\"\n",
        "        if not bool(edit):\n",
        "            semantic_prompt['editing_prompt'] = semantic_prefs['prompt']\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def changed_p(e, pref=None):\n",
        "            if pref is not None:\n",
        "                semantic_prompt[pref] = e.control.value\n",
        "        def save_semantic_prompt(e):\n",
        "            if edit == None:\n",
        "                semantic_prefs['editing_prompts'].append(semantic_prompt)\n",
        "                page.semantic_prompts.controls.append(semantic_tile(semantic_prompt))\n",
        "                page.semantic_prompts.update()\n",
        "            else:\n",
        "                for s in semantic_prefs['editing_prompts']:\n",
        "                    if s['editing_prompt'] == edit_prompt:\n",
        "                        s = semantic_prompt\n",
        "                        break\n",
        "                for t in page.semantic_prompts.controls:\n",
        "                    if t.data['editing_prompt'] == edit_prompt:\n",
        "                        params = []\n",
        "                        for k, v in semantic_prompt.items():\n",
        "                            if k == 'editing_prompt': continue\n",
        "                            params.append(f'{to_title(k)}: {v}')\n",
        "                        sub = ', '.join(params)\n",
        "                        t.title = Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)\n",
        "                        t.subtitle = Text(sub)\n",
        "                        t.data = semantic_prompt\n",
        "                        t.update()\n",
        "                        break\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        semantic_editing_prompt = TextField(label=\"Semantic Editing Prompt Modifier\", value=semantic_prompt['editing_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'editing_prompt'))\n",
        "        edit_warmup_steps = SliderRow(label=\"Edit Warmup Steps\", min=0, max=50, divisions=50, pref=semantic_prompt, key='edit_warmup_steps', tooltip=\"Number of diffusion steps (for each prompt) for which semantic guidance will not be applied. Momentum will still be calculated for those steps and applied once all warmup periods are over.\")\n",
        "        edit_guidance_scale = SliderRow(label=\"Edit Guidance Scale\", min=0, max=20, divisions=40, round=1, pref=semantic_prompt, key='edit_guidance_scale', tooltip=\"Guidance scale for semantic guidance. If provided as list values should correspond to `editing_prompt`.\")\n",
        "        edit_threshold = SliderRow(label=\"Edit Threshold\", min=0, max=1, divisions=40, round=3, pref=semantic_prompt, key='edit_threshold', tooltip=\"Threshold of semantic guidance.\")\n",
        "        edit_weights = SliderRow(label=\"Edit Weights\", min=0, max=10, divisions=20, round=1, pref=semantic_prompt, key='edit_weights', tooltip=\"Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.\")\n",
        "        reverse_editing_direction = Checkbox(label=\"Reverse Editing Direction\", value=semantic_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip=\"Whether the corresponding prompt in `editing_prompt` should be increased or decreased.\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è {'Edit' if bool(edit) else 'Add'} Semantic Prompt\"), content=Container(Column([\n",
        "            semantic_editing_prompt, edit_warmup_steps, edit_guidance_scale, edit_threshold, edit_weights, reverse_editing_direction,\n",
        "        ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_semantic_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def del_semantic(edit=None):\n",
        "        for s in semantic_prefs['editing_prompts']:\n",
        "            if s['editing_prompt'] == edit['editing_prompt']:\n",
        "                semantic_prefs['editing_prompts'].remove(s)\n",
        "                break\n",
        "        for t in page.semantic_prompts.controls:\n",
        "            if t.data['editing_prompt'] == edit['editing_prompt']:\n",
        "                page.semantic_prompts.controls.remove(t)\n",
        "                break\n",
        "        page.semantic_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_semantic_prompts(e):\n",
        "        semantic_prefs['editing_prompts'].clear()\n",
        "        page.semantic_prompts.controls.clear()\n",
        "        page.semantic_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    prompt = TextField(label=\"Base Prompt Text\", value=semantic_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=semantic_prefs['negative_prompt'], col={'md':3}, filled=True, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(semantic_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=semantic_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=semantic_prefs, key='guidance_scale')\n",
        "    edit_momentum_scale = SliderRow(label=\"Edit Momentum Scale\", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_momentum_scale', tooltip=\"Scale of the momentum to be added to the semantic guidance at each diffusion step. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `sld_warmup_steps`. Momentum will only be added to latent guidance once all warmup periods are finished.\")\n",
        "    edit_mom_beta = SliderRow(label=\"Edit Momentum Beta\", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_mom_beta', tooltip=\"Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `edit_warmup_steps`.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(semantic_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", round=2, value=float(semantic_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {semantic_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, eta])\n",
        "    page.etas.append(eta_row)\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=semantic_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=semantic_prefs, key='height')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=semantic_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=semantic_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=semantic_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=semantic_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_semantic = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_semantic.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.semantic_prompts = Column([], spacing=0)\n",
        "    page.semantic_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.semantic_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üß©  Semantic Guidance for Diffusion Models - SEGA\", \"Text-to-Image Generation with Latent Editing to apply or remove multiple concepts from an image with advanced controls....\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Semantic Guidance Settings\", on_click=semantic_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([Text(\"Editing Semantic Prompts\", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),\n",
        "                    Row([ft.FilledTonalButton(\"Clear Prompts\", on_click=clear_semantic_prompts), ft.FilledButton(\"Add Editing Prompt\", on_click=lambda e: edit_semantic(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        page.semantic_prompts,\n",
        "        Divider(thickness=2, height=4),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        edit_momentum_scale, edit_mom_beta,\n",
        "        eta_row,\n",
        "        width_slider, height_slider,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=semantic_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_semantic,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üé≥  Run Semantic Guidance\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page, from_list=True))\n",
        "        ]),\n",
        "        page.semantic_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "demofusion_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"negative_prompt\": '',\n",
        "    'view_batch_size': 16,\n",
        "    'cosine_scale_1': 3.0,\n",
        "    'cosine_scale_2': 1.0,\n",
        "    'cosine_scale_3': 1.0,\n",
        "    'sigma': 0.8,\n",
        "    'multi_decoder': True,\n",
        "    'show_image': False,\n",
        "    \"steps\":50,\n",
        "    \"width\": 3072,\n",
        "    \"height\": 3072,\n",
        "    'sigma': 4.0,\n",
        "    'stride': 64,\n",
        "    \"guidance_scale\":7.5,\n",
        "    \"seed\": 0,\n",
        "    'cpu_offload': True,\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"demofusion-\",\n",
        "    \"num_images\": 1,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildDemoFusion(page):\n",
        "    global prefs, demofusion_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            demofusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            demofusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            demofusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def demofusion_help(e):\n",
        "      def close_demofusion_dlg(e):\n",
        "        nonlocal demofusion_help_dlg\n",
        "        demofusion_help_dlg.open = False\n",
        "        page.update()\n",
        "      demofusion_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DemoFusion Pipeline\"), content=Column([\n",
        "          Text('High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as \"previews\", facilitating rapid prompt iteration.'),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"[Paper](https://arxiv.org/abs/2311.16973) | [Original GitHub](https://github.com/PRIS-CV/DemoFusion) | [Ruoyi Du](https://github.com/RuoyiDu)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üè°  Go Big or...\", on_click=close_demofusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = demofusion_help_dlg\n",
        "      demofusion_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        demofusion_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=demofusion_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt = TextField(label=\"Negative Prompt Text\", value=demofusion_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=demofusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=demofusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #steps = TextField(label=\"Number of Steps\", value=demofusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=demofusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=100, divisions=100, pref=demofusion_prefs, key='steps')\n",
        "    sigma = SliderRow(label=\"Sigma\", min=0, max=6, divisions=60, round=1, pref=demofusion_prefs, key='sigma', col={'xs':12, 'md':6}, tooltip=\"The standard value of the Gaussian filter. Larger sigma promotes the global guidance of dilated sampling, but has the potential of over-smoothing.\")\n",
        "    stride = SliderRow(label=\"Stride\", min=0, max=100, divisions=100, pref=demofusion_prefs, key='stride', tooltip=\"The stride of moving local patches. A smaller stride is better for alleviating seam issues, but it also introduces additional computational overhead and inference time.\")\n",
        "    view_batch_size = SliderRow(label=\"View Batch Size\", min=0, max=50, divisions=50, pref=demofusion_prefs, key='view_batch_size', tooltip=\"The batch size for multiple denoising paths. Typically, a larger batch size can result in higher efficiency but comes with increased GPU memory requirements.\")\n",
        "    cosine_scale_1 = SliderRow(label=\"Cosine Scale 1\", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_1', col={'xs':12, 'md':4}, tooltip=\"Control the strength of skip-residual. For specific impacts, please refer to Appendix C in the DemoFusion paper.\")\n",
        "    cosine_scale_2 = SliderRow(label=\"Cosine Scale 2\", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_2', col={'xs':12, 'md':4}, tooltip=\"Control the strength of dilated sampling. For specific impacts, please refer to Appendix C in the DemoFusion paper.\")\n",
        "    cosine_scale_3 = SliderRow(label=\"Cosine Scale 3\", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_3', col={'xs':12, 'md':4}, tooltip=\"Control the strength of the Gaussian filter. For specific impacts, please refer to Appendix C in the DemoFusion paper.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=20, divisions=40, round=1, pref=demofusion_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=4096, divisions=31, multiple=128, suffix=\"px\", pref=demofusion_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=4096, divisions=31, multiple=128, suffix=\"px\", pref=demofusion_prefs, key='height')\n",
        "    multi_decoder = Switcher(label=\"Multi Decoder\", value=demofusion_prefs['multi_decoder'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'multi_decoder'), tooltip=\"Whether to use a tiled decoder. Generally, when the resolution exceeds 3072x3072, a tiled decoder becomes necessary.\")\n",
        "    show_image = Switcher(label=\"Show Intermediates\", value=demofusion_prefs['show_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_image'), tooltip=\"Whether to show intermediate results during generation.\")\n",
        "    cpu_offload = Switcher(label=\"CPU Offload\", value=demofusion_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip=\"Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.\")\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(demofusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switcher(label=\"Apply ESRGAN Upscale\", value=demofusion_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=demofusion_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=demofusion_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=demofusion_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_demofusion = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_demofusion.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not demofusion_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí•   Run DemoFusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page))\n",
        "    from_list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), tooltip=\"Uses all queued Image Parameters per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page, from_list=True))\n",
        "    from_list_with_params_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List /w these Parameters\", size=20), tooltip=\"Uses above settings per prompt in Prompt List\", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page, from_list=True, with_params=True))\n",
        "    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN\n",
        "    page.demofusion_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üí£  DemoFusion\", \"Democratising High-Resolution Image Generation With No $$$. SDXL with Clean Upscaling, 3 Phase Denoising/Decoding, slow but real quality...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DemoFusion Settings\", on_click=demofusion_help)]),\n",
        "            ResponsiveRow([prompt, negative_prompt]),\n",
        "            #ResponsiveRow([stride, sigma]),\n",
        "            stride, sigma,\n",
        "            view_batch_size,\n",
        "            ResponsiveRow([cosine_scale_1, cosine_scale_2, cosine_scale_3]),\n",
        "            steps,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),\n",
        "            Row([multi_decoder, show_image, cpu_offload]),\n",
        "            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),\n",
        "            page.ESRGAN_block_demofusion,\n",
        "            parameters_row,\n",
        "            page.demofusion_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "class State:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "state = State()\n",
        "#TODO: Waiting for Scribbler addon integration\n",
        "def buildDreamMask(page):\n",
        "    #prog_bars: Dict[str, ProgressRing] = {}\n",
        "    files = Ref[Column]()\n",
        "    #upload_button = Ref[ElevatedButton]()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        files.current.controls.clear()\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        load_img = PILImage.open(fname)\n",
        "        w, h = load_img.size\n",
        "        bg_img.width=w\n",
        "        bg_img.height=h\n",
        "        stack_box.width=w\n",
        "        stack_box.height=h\n",
        "        bg_img.src = fname\n",
        "        bg_img.update()\n",
        "        stack_box.update()\n",
        "        clear_canvas(e)\n",
        "        #files.current.controls.append(Row([Text(f\"Done uploading {root_dir}{e.file_name}\")]))\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    bg_img = Img(src=\"https://picsum.photos/200/300\", fit=ImageFit.CONTAIN, width=1024, height=512)#float(\"inf\")\n",
        "    stroke_width = 50\n",
        "    def pan_start(e: ft.DragStartEvent):\n",
        "        state.x = e.local_x\n",
        "        state.y = e.local_y\n",
        "    def pan_update(e: ft.DragUpdateEvent):\n",
        "        cp.shapes.append(cv.Line(state.x, state.y, e.local_x, e.local_y, paint=ft.Paint(stroke_width=stroke_width, stroke_join=ft.StrokeJoin.ROUND, stroke_cap=ft.StrokeCap.ROUND, style=ft.PaintingStyle.STROKE))) #, blend_mode=ft.BlendMode.CLEAR\n",
        "        cp.update()\n",
        "        state.x = e.local_x\n",
        "        state.y = e.local_y\n",
        "    cp = cv.Canvas(\n",
        "        content=ft.GestureDetector(\n",
        "            on_pan_start=pan_start,\n",
        "            on_pan_update=pan_update,\n",
        "            drag_interval=10,\n",
        "        ),\n",
        "        expand=False,\n",
        "    )\n",
        "    def clear_canvas(e):\n",
        "        cp.shapes.clear()\n",
        "        cp.update()\n",
        "    def change_stroke(e):\n",
        "        nonlocal stroke_width\n",
        "        stroke_width = e.control.value\n",
        "    stack_box = Container(\n",
        "            Stack(\n",
        "                [\n",
        "                    #Container(content=Text(\"Picture\")),\n",
        "                    bg_img,\n",
        "                    #Img(src=\"https://picsum.photos/200/300\", fit=ImageFit.FILL, width=float(\"inf\")),\n",
        "                    cp,\n",
        "                ]\n",
        "            ),\n",
        "            #border_radius=5,\n",
        "            border=ft.border.all(2, ft.colors.BLACK),\n",
        "            width=1024,#float(\"inf\"),\n",
        "            height=512,\n",
        "            expand=False,\n",
        "        )\n",
        "    c = Column([\n",
        "        Header(\"üé≠   Dream Mask Maker\"),\n",
        "        ElevatedButton(\n",
        "            \"Select Init Image to Mask...\",\n",
        "            icon=icons.FOLDER_OPEN,\n",
        "            on_click=lambda _: file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\" ),\n",
        "        ),\n",
        "        Column(ref=files),\n",
        "        stack_box,\n",
        "        Row([IconButton(icon=icons.CLEAR, tooltip=\"Clear\", on_click=clear_canvas), Text(\"Stroke Width\"), Slider(min=1, max=100, divisions=99, round=0, expand=True, label=\"{value}px\", value=stroke_width, on_change=change_stroke)])\n",
        "    ], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "dreambooth_prefs = {\n",
        "    'instance_prompt': '',\n",
        "    'prior_preservation': False,\n",
        "    'prior_preservation_class_prompt': \"\",\n",
        "    'num_class_images': 12,\n",
        "    'sample_batch_size': 2,\n",
        "    'train_batch_size': 1,\n",
        "    'prior_loss_weight': 0.5,\n",
        "    'prior_preservation_class_folder': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 5e-06,\n",
        "    'max_train_steps': 450,\n",
        "    'seed': 222476,\n",
        "    'name_of_your_concept': \"\",\n",
        "    'save_concept': True,\n",
        "    'where_to_save_concept': \"Public Library\",\n",
        "    'max_size': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildDreamBooth(page):\n",
        "    global prefs, dreambooth_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              dreambooth_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              dreambooth_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              dreambooth_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_dreambooth_output(o):\n",
        "        page.dreambooth_output.controls.append(o)\n",
        "        page.dreambooth_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.dreambooth_output.controls = []\n",
        "        page.dreambooth_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def db_help(e):\n",
        "        def close_db_dlg(e):\n",
        "          nonlocal db_help_dlg\n",
        "          db_help_dlg.open = False\n",
        "          page.update()\n",
        "        db_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sleepy_face:') + \"  Got it... \", on_click=close_db_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = db_help_dlg\n",
        "        db_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.db_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.db_file_list.controls[i]\n",
        "              page.db_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.db_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.db_file_list.controls.clear()\n",
        "        page.db_file_list.update()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.db_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ])))\n",
        "        if update: page.db_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_concept')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_concept')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          concept_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = concept_image.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          concept_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    instance_prompt = TextField(label=\"Instance Prompt Token Text\", value=dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))\n",
        "    prior_preservation_class_prompt = TextField(label=\"Prior Preservation Class Prompt\", value=dreambooth_prefs['prior_preservation_class_prompt'], on_change=lambda e:changed(e,'prior_preservation_class_prompt'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    num_class_images = TextField(label=\"Number of Class Images\", value=dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160)\n",
        "    sample_batch_size = TextField(label=\"Sample Batch Size\", value=dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160)\n",
        "    prior_loss_weight = TextField(label=\"Prior Loss Weight\", value=dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160)\n",
        "    max_train_steps = TextField(label=\"Max Training Steps\", value=dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    save_concept = Checkbox(label=\"Save Concept    \", tooltip=\"\", value=dreambooth_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))\n",
        "    where_to_save_concept = Dropdown(label=\"Where to Save Concept\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=dreambooth_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))\n",
        "    prior_preservation_class_folder = TextField(label=\"Prior Preservation Class Folder\", value=dreambooth_prefs['prior_preservation_class_folder'], on_change=lambda e:changed(e,'prior_preservation_class_folder'))\n",
        "    name_of_your_concept = TextField(label=\"Name of your Concept\", value=dreambooth_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=dreambooth_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.db_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #seed = TextField(label=\"Seed\", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusdreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.dreambooth_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dreambooth_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Create Custom DreamBooth Concept Model\", \"Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DreamBooth Settings\", on_click=db_help)]),\n",
        "        Row([instance_prompt, name_of_your_concept]),\n",
        "        Row([num_class_images, sample_batch_size, prior_loss_weight]),\n",
        "        Row([max_train_steps, learning_rate, seed]),\n",
        "        Row([save_concept, where_to_save_concept]),\n",
        "        readme_description,\n",
        "        #Row([prior_preservation_class_folder]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.db_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Run DreamBooth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth(page))]),\n",
        "        page.dreambooth_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "textualinversion_prefs = {\n",
        "    'what_to_teach': 'object',\n",
        "    'placeholder_token': '',\n",
        "    'initializer_token': '',\n",
        "    'scale_lr': True,\n",
        "    'max_train_steps': 3000,\n",
        "    'train_batch_size': 1,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'seed': 22276,\n",
        "    'repeats': 100,\n",
        "    'validation_prompt': '',\n",
        "    'validation_steps': 1,\n",
        "    'num_vectors': 2,\n",
        "    'output_dir': os.path.join(root_dir, \"sd-concept-output\"),\n",
        "    'learning_rate': 5e-04,\n",
        "    'name_of_your_concept': \"\",\n",
        "    'save_concept': True,\n",
        "    'where_to_save_concept': \"Public Library\",\n",
        "    'max_size': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'use_SDXL': False,\n",
        "    'urls': [],\n",
        "}\n",
        "def buildTextualInversion(page):\n",
        "    global prefs, textualinversion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              textualinversion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              textualinversion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              textualinversion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_textualinversion_output(o):\n",
        "        page.textualinversion_output.controls.append(o)\n",
        "        page.textualinversion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.textualinversion_output.controls = []\n",
        "        page.textualinversion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def ti_help(e):\n",
        "        def close_ti_dlg(e):\n",
        "          nonlocal ti_help_dlg\n",
        "          ti_help_dlg.open = False\n",
        "          page.update()\n",
        "        ti_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Textual-Inversion\"), content=Column([\n",
        "            Text(\"\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  I'll figure it out... \", on_click=close_ti_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = ti_help_dlg\n",
        "        ti_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.ti_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.ti_file_list.controls[i]\n",
        "              page.ti_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.ti_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.ti_file_list.controls.clear()\n",
        "        page.ti_file_list.update()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.ti_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ])))\n",
        "        if update: page.ti_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_concept')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_concept')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          concept_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = concept_image.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          concept_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    what_to_teach = Dropdown(label=\"What to Teach\", width=250, options=[dropdown.Option(\"object\"), dropdown.Option(\"style\")], value=textualinversion_prefs['what_to_teach'], on_change=lambda e: changed(e, 'what_to_teach'))\n",
        "    placeholder_token = TextField(label=\"Placeholder <Token> Keyword\", value=textualinversion_prefs['placeholder_token'], on_change=lambda e:changed(e,'placeholder_token'))\n",
        "    initializer_token = TextField(label=\"Initializer Token Category Summary\", value=textualinversion_prefs['initializer_token'], on_change=lambda e:changed(e,'initializer_token'))\n",
        "    validation_prompt = TextField(label=\"Validation <Token> Prompt\", value=textualinversion_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))\n",
        "    gradient_accumulation_steps = TextField(label=\"Gradient Accumulation Steps\", value=textualinversion_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160)\n",
        "    scale_lr = Checkbox(label=\"Scale Learning Rate\", tooltip=\"\", value=textualinversion_prefs['scale_lr'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'scale_lr'))\n",
        "    validation_steps = TextField(label=\"Validation Steps\", value=textualinversion_prefs['validation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_steps', ptype='int'), width = 145)\n",
        "    num_vectors = TextField(label=\"Number of Vectors\", value=textualinversion_prefs['num_vectors'], tooltip=\"How many textual inversion vectors shall be used to learn the concept.\", keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_vectors', ptype='int'), width = 145)\n",
        "    repeats = TextField(label=\"Repeats\", value=textualinversion_prefs['repeats'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'repeats', ptype='int'), width = 160)\n",
        "    train_batch_size = TextField(label=\"Train Batch Size\", value=textualinversion_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='float'), width = 160)\n",
        "    max_train_steps = TextField(label=\"Max Training Steps\", value=textualinversion_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=textualinversion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    use_SDXL = Switcher(label=\"Train with SDXL\", value=textualinversion_prefs['use_SDXL'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'use_SDXL'))\n",
        "    save_concept = Checkbox(label=\"Save Concept    \", tooltip=\"\", value=textualinversion_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))\n",
        "    where_to_save_concept = Dropdown(label=\"Where to Save Concept\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=textualinversion_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))\n",
        "    output_dir = TextField(label=\"Prior Preservation Class Folder\", value=textualinversion_prefs['output_dir'], on_change=lambda e:changed(e,'output_dir'))\n",
        "    name_of_your_concept = TextField(label=\"Name of your Concept\", value=textualinversion_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=textualinversion_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=textualinversion_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=textualinversion_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.ti_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #seed = TextField(label=\"Seed\", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfustextualinversion_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=textualinversion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.textualinversion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.textualinversion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Create Cusom Textual-Inversion Concept Model\", \"Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Textual-Inversion Settings\", on_click=ti_help)]),\n",
        "        Row([what_to_teach, initializer_token]),\n",
        "        Row([placeholder_token, name_of_your_concept]),\n",
        "        use_SDXL,\n",
        "        Row([validation_prompt, validation_steps, num_vectors]),\n",
        "        scale_lr,\n",
        "        Row([gradient_accumulation_steps, repeats, train_batch_size]),\n",
        "        Row([max_train_steps, learning_rate, seed]),\n",
        "        Row([save_concept, where_to_save_concept]),\n",
        "        readme_description,\n",
        "        #Row([output_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.ti_file_list,\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Run Textual-Inversion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_textualinversion(page)),\n",
        "        page.textualinversion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "LoRA_dreambooth_prefs = {\n",
        "    'instance_prompt': '', #The prompt with identifier specifying the instance\n",
        "    'class_prompt': '',\n",
        "    'prior_preservation': False, #Flag to add prior preservation loss.\n",
        "    'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\n",
        "    'sample_batch_size': 4, #Batch size (per device) for sampling images.\n",
        "    'train_batch_size': 1, #\"Batch size (per device) for the training dataloader.\n",
        "    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints\n",
        "    'lr_scheduler': 'constant', #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.\n",
        "    'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
        "    'lr_power': 1, #Power factor of the polynomial scheduler.\n",
        "    'prior_loss_weight': 1.0, #The weight of prior preservation loss.\n",
        "    'class_data_dir': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.\n",
        "    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n",
        "    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'seed': 0,\n",
        "    'use_SDXL': False,\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100\n",
        "LoRA_prefs = {\n",
        "    'instance_prompt': '', #The prompt with identifier specifying the instance\n",
        "    'class_prompt': '',\n",
        "    'prior_preservation': False, #Flag to add prior preservation loss.\n",
        "    #'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\n",
        "    #'sample_batch_size': 4, #Batch size (per device) for sampling images.\n",
        "    'train_batch_size': 1, #\"Batch size (per device) for the training dataloader.\n",
        "    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints\n",
        "    'resume_from_checkpoint': '', #Whether training should be resumed from a previous checkpoint. Use a path saved by\" `--checkpointing_steps`, or `latest` to automatically select the last available checkpoint.\n",
        "    'lr_scheduler': 'constant', #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.\n",
        "    #'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
        "    #'lr_power': 1, #Power factor of the polynomial scheduler.\n",
        "    #'prior_loss_weight': 1.0, #The weight of prior preservation loss.\n",
        "    'class_data_dir': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.\n",
        "    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n",
        "    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'seed': 0,\n",
        "    'validation_prompt': '', #A prompt that is sampled during training for inference.\n",
        "    'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.\n",
        "    'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt\n",
        "    'use_SDXL': False,\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildLoRA_Dreambooth(page):\n",
        "    global prefs, LoRA_dreambooth_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              LoRA_dreambooth_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              LoRA_dreambooth_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              LoRA_dreambooth_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_LoRA_dreambooth_output(o):\n",
        "        page.LoRA_dreambooth_output.controls.append(o)\n",
        "        page.LoRA_dreambooth_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.LoRA_dreambooth_output.controls = []\n",
        "        page.LoRA_dreambooth_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def lora_dreambooth_help(e):\n",
        "        def close_lora_dreambooth_dlg(e):\n",
        "          nonlocal lora_dreambooth_help_dlg\n",
        "          lora_dreambooth_help_dlg.open = False\n",
        "          page.update()\n",
        "        lora_dreambooth_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with LoRA DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Markdown(\"\"\"Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*\n",
        "In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:\n",
        "- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
        "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
        "- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + \"  Neato... \", on_click=close_lora_dreambooth_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = lora_dreambooth_help_dlg\n",
        "        lora_dreambooth_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.lora_dreambooth_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.lora_dreambooth_file_list.controls[i]\n",
        "              page.lora_dreambooth_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.lora_dreambooth_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.lora_dreambooth_file_list.controls.clear()\n",
        "        page.lora_dreambooth_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        alert_msg(e.page, \"Image Details\", content=Image(src=img), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.lora_dreambooth_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        if update: page.lora_dreambooth_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_model')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_model')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = LoRA_dreambooth_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=LoRA_dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=LoRA_dreambooth_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    class_prompt = TextField(label=\"Class Prompt\", value=LoRA_dreambooth_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    lr_scheduler = Dropdown(label=\"Learning Rate Scheduler\", width=250, options=[dropdown.Option(\"constant\"), dropdown.Option(\"constant_with_warmup\"), dropdown.Option(\"linear\"), dropdown.Option(\"cosine\"), dropdown.Option(\"cosine_with_restarts\"), dropdown.Option(\"polynomial\")], value=LoRA_dreambooth_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=LoRA_dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    gradient_checkpointing = Checkbox(label=\"Gradient Checkpointing   \", tooltip=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", value=LoRA_dreambooth_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))\n",
        "    num_class_images = Tooltip(message=\"Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\", content=TextField(label=\"Number of Class Images\", value=LoRA_dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))\n",
        "    sample_batch_size = Tooltip(message=\"Batch size (per device) for sampling images.\", content=TextField(label=\"Sample Batch Size\", value=LoRA_dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))\n",
        "    train_batch_size = Tooltip(message=\"Batch size (per device) for the training dataloader.\", content=TextField(label=\"Train Batch Size\", value=LoRA_dreambooth_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))\n",
        "    prior_loss_weight = Tooltip(message=\"The weight of prior preservation loss.\", content=TextField(label=\"Prior Loss Weight\", value=LoRA_dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))\n",
        "    max_train_steps = Tooltip(message=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\", content=TextField(label=\"Max Training Steps\", value=LoRA_dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))\n",
        "    gradient_accumulation_steps = Tooltip(message=\"Number of updates steps to accumulate before performing a backward/update pass.\", content=TextField(label=\"Gradient Accumulation Steps\", value=LoRA_dreambooth_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))\n",
        "    learning_rate = Tooltip(message=\"Initial learning rate (after the potential warmup period) to use.\", content=TextField(label=\"Learning Rate\", value=LoRA_dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))\n",
        "    lr_warmup_steps = Tooltip(message=\"Number of steps for the warmup in the lr scheduler.\", content=TextField(label=\"LR Warmup Steps\", value=LoRA_dreambooth_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))\n",
        "    lr_num_cycles = Tooltip(message=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\", content=TextField(label=\"LR Number of Cycles\", value=LoRA_dreambooth_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))\n",
        "    lr_power = Tooltip(message=\"Power factor of the polynomial scheduler.\", content=TextField(label=\"LR Power\", value=LoRA_dreambooth_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))\n",
        "    seed = Tooltip(message=\"0 or -1 for Random. Pick any number.\", content=TextField(label=\"Seed\", value=LoRA_dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))\n",
        "    #save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=LoRA_dreambooth_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    use_SDXL = Switcher(label=\"Train with SDXL\", value=LoRA_dreambooth_prefs['use_SDXL'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'use_SDXL'))\n",
        "    save_model = Switcher(label=\"Save Model to HuggingFace\", value=LoRA_dreambooth_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save, tooltip=\"Requires WRITE access on API Key to Upload Checkpoint\")\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=LoRA_dreambooth_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=LoRA_dreambooth_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=LoRA_dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=LoRA_dreambooth_prefs, key='resolution')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=LoRA_dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.lora_dreambooth_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']\n",
        "    readme_description.visible = LoRA_dreambooth_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusLoRA_dreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=LoRA_dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.LoRA_dreambooth_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.LoRA_dreambooth_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåá  Training with Low-Rank Adaptation of Large Language Models (LoRA DreamBooth)\", \"Provide a collection of images to train. Adds on to the currently loaded Model Checkpoint...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LoRA DreamBooth Settings\", on_click=lora_dreambooth_help)]),\n",
        "        ResponsiveRow([instance_prompt, name_of_your_model]),\n",
        "        use_SDXL,\n",
        "        Row([num_class_images, sample_batch_size, train_batch_size, prior_loss_weight]),\n",
        "        Row([prior_preservation, gradient_checkpointing, lr_scheduler]),\n",
        "        Row([learning_rate, lr_warmup_steps, lr_num_cycles, lr_power]),\n",
        "        Row([max_train_steps, gradient_accumulation_steps, seed]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.lora_dreambooth_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üåÑ  Run LoRA DreamBooth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA_dreambooth(page))]),\n",
        "        page.LoRA_dreambooth_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "def buildLoRA(page):\n",
        "    global prefs, LoRA_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              LoRA_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              LoRA_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              LoRA_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_LoRA_output(o):\n",
        "        page.LoRA_output.controls.append(o)\n",
        "        page.LoRA_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.LoRA_output.controls = []\n",
        "        page.LoRA_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def lora_help(e):\n",
        "        def close_lora_dlg(e):\n",
        "          nonlocal lora_help_dlg\n",
        "          lora_help_dlg.open = False\n",
        "          page.update()\n",
        "        lora_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with LoRA DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Markdown(\"\"\"Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*\n",
        "In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:\n",
        "- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
        "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
        "- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + \"  Neato... \", on_click=close_lora_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = lora_help_dlg\n",
        "        lora_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.lora_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.lora_file_list.controls[i]\n",
        "              page.lora_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.lora_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.lora_file_list.controls.clear()\n",
        "        page.lora_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        #TODO: Get file size & resolution\n",
        "        alert_msg(e.page, \"Image Details\", content=Column([Text(img), Img(src=img, gapless_playback=True)]), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.lora_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), subtitle=TextField(label=\"Caption Image Description\", height=55, filled=True, content_padding=padding.only(top=12, left=12)), data=fpath, on_click=image_details))\n",
        "        if update: page.lora_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_model')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_model')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = LoRA_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = LoRA_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    validation_prompt = Container(content=Tooltip(message=\"A prompt that is sampled during training for inference.\", content=TextField(label=\"Validation Prompt Text\", value=LoRA_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=LoRA_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    #class_prompt = TextField(label=\"Class Prompt\", value=LoRA_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    #'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.\n",
        "    #'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt\n",
        "    num_validation_images = Tooltip(message=\"Number of images that should be generated during validation with `validation_prompt`\", content=TextField(label=\"# of Validation Images\", value=LoRA_prefs['num_validation_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_validation_images', ptype='int'), width = 160))\n",
        "    validation_epochs = Tooltip(message=\"Run fine-tuning validation every X epochs. The validation process consists of running the prompt\", content=TextField(label=\"Validation Epochs\", value=LoRA_prefs['validation_epochs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_epochs', ptype='int'), width = 160))\n",
        "    lr_scheduler = Dropdown(label=\"Learning Rate Scheduler\", width=250, options=[dropdown.Option(\"constant\"), dropdown.Option(\"constant_with_warmup\"), dropdown.Option(\"linear\"), dropdown.Option(\"cosine\"), dropdown.Option(\"cosine_with_restarts\"), dropdown.Option(\"polynomial\")], value=LoRA_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=LoRA_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    gradient_checkpointing = Checkbox(label=\"Gradient Checkpointing   \", tooltip=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", value=LoRA_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))\n",
        "    #num_class_images = Tooltip(message=\"Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\", content=TextField(label=\"Number of Class Images\", value=LoRA_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))\n",
        "    #sample_batch_size = Tooltip(message=\"Batch size (per device) for sampling images.\", content=TextField(label=\"Sample Batch Size\", value=LoRA_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))\n",
        "    train_batch_size = Tooltip(message=\"Batch size (per device) for the training dataloader.\", content=TextField(label=\"Train Batch Size\", value=LoRA_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))\n",
        "    #prior_loss_weight = Tooltip(message=\"The weight of prior preservation loss.\", content=TextField(label=\"Prior Loss Weight\", value=LoRA_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))\n",
        "    max_train_steps = Tooltip(message=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\", content=TextField(label=\"Max Training Steps\", value=LoRA_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))\n",
        "    gradient_accumulation_steps = Tooltip(message=\"Number of updates steps to accumulate before performing a backward/update pass.\", content=TextField(label=\"Gradient Accumulation Steps\", value=LoRA_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))\n",
        "    learning_rate = Tooltip(message=\"Initial learning rate (after the potential warmup period) to use.\", content=TextField(label=\"Learning Rate\", value=LoRA_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))\n",
        "    lr_warmup_steps = Tooltip(message=\"Number of steps for the warmup in the lr scheduler.\", content=TextField(label=\"LR Warmup Steps\", value=LoRA_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))\n",
        "    #lr_num_cycles = Tooltip(message=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\", content=TextField(label=\"LR Number of Cycles\", value=LoRA_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))\n",
        "    #lr_power = Tooltip(message=\"Power factor of the polynomial scheduler.\", content=TextField(label=\"LR Power\", value=LoRA_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))\n",
        "    seed = Tooltip(message=\"0 or -1 for Random. Pick any number.\", content=TextField(label=\"Seed\", value=LoRA_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))\n",
        "    save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=LoRA_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switcher(label=\"Save Model to HuggingFace    \", value=LoRA_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=LoRA_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=LoRA_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=LoRA_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=64, suffix=\"px\", pref=LoRA_prefs, key='resolution')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=LoRA_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.lora_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    where_to_save_model.visible = LoRA_prefs['save_model']\n",
        "    readme_description.visible = LoRA_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusLoRA_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=LoRA_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.LoRA_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.LoRA_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üå´Ô∏è  Training Text-to-Image Low-Rank Adaptation of Large Language Models (LoRA)\", \"Provide a collection of images to train. Smaller sized. Adds on to the currently loaded Model Checkpoint...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LoRA DreamBooth Settings\", on_click=lora_help)]),\n",
        "        ResponsiveRow([validation_prompt, name_of_your_model]),\n",
        "        Row([num_validation_images, validation_epochs, train_batch_size]),\n",
        "        Row([prior_preservation, gradient_checkpointing]),\n",
        "        Row([learning_rate, lr_warmup_steps, lr_scheduler]),\n",
        "        Row([max_train_steps, gradient_accumulation_steps, seed]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.lora_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üèÑ  Run LoRA Training\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA(page))]),\n",
        "        page.LoRA_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "converter_prefs = {\n",
        "    'from_format': 'ckpt',\n",
        "    'to_format': 'pytorch',\n",
        "    'model_path': '',\n",
        "    'model_name': '',\n",
        "    'base_model': '',\n",
        "    'model_type': 'SD v1.x text2image',\n",
        "    'scheduler_type': 'pndm',\n",
        "    'half_percision': True,\n",
        "    'save_model': False,\n",
        "    'where_to_save_model': \"Public HuggingFace\",\n",
        "    'readme_description': '',\n",
        "    'load_custom_model': True,\n",
        "}\n",
        "\n",
        "def buildConverter(page):\n",
        "    global prefs, converter_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              converter_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              converter_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              converter_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_converter_output(o):\n",
        "        page.converter_output.controls.append(o)\n",
        "        page.converter_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.converter_output.controls = []\n",
        "        page.converter_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def converter_help(e):\n",
        "        def close_converter_dlg(e):\n",
        "          nonlocal converter_help_dlg\n",
        "          converter_help_dlg.open = False\n",
        "          page.update()\n",
        "        converter_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Converters\"), content=Column([\n",
        "            Text(\"Because there have been so many competing formats for Stable Diffusion models, we here have standardized with HuggingFace Diffusers, which is great but doesn't support all the Checkpoint Model types that are out there in the wild.  This should allow you to take other peoples custom trained model files and convert it to the better Diffusers PyTorch format, and then it'll save your Custom Model Checkpoint to HuggingFace (free) to reuse and/or share.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéà  Handy... \", on_click=close_converter_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = converter_help_dlg\n",
        "        converter_help_dlg.open = True\n",
        "        page.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = converter_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = converter_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def change_from_format(e):\n",
        "        changed(e, 'from_format')\n",
        "        base_model_row.visible = converter_prefs['from_format'] == \"lora_safetensors\"\n",
        "        base_model_row.update()\n",
        "\n",
        "    from_format = Dropdown(label=\"From Format\", width=250, options=[dropdown.Option(\"ckpt\"), dropdown.Option(\"safetensors\"), dropdown.Option(\"lora_safetensors\"), dropdown.Option(\"controlnet\"), dropdown.Option(\"KerasCV\")], value=converter_prefs['from_format'], on_change=change_from_format, col={'lg':6})\n",
        "    to_format = Dropdown(label=\"To Format\", width=250, options=[dropdown.Option(\"pytorch\"), dropdown.Option(\"safetensors\"), dropdown.Option(\"dance_diffusion\")], value=converter_prefs['to_format'], on_change=lambda e: changed(e, 'to_format'), col={'lg':6})\n",
        "    #instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=converter_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    from_model_path = TextField(label=\"Model Path to HuggingFace or .ckpt or .safetensors file\", value=converter_prefs['model_path'], on_change=lambda e:changed(e,'model_path'), col={'md':6})\n",
        "    from_model_name = TextField(label=\"Name of your Model\", value=converter_prefs['model_name'], on_change=lambda e:changed(e,'model_name'), col={'md':6})\n",
        "    model_type = Dropdown(label=\"Model Type\", width=250, options=[dropdown.Option(\"SD v1.x text2image\"), dropdown.Option(\"SD v2.x text2image\")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})\n",
        "    base_model = TextField(label=\"Base Model Path to HuggingFace Diffusers\", value=converter_prefs['base_model'], on_change=lambda e:changed(e,'base_model'), col={'md':6})\n",
        "    base_model_row = ResponsiveRow([base_model])\n",
        "    base_model_row.visible = converter_prefs['from_format'] == \"lora_safetensors\"\n",
        "    #sd_version = Dropdown(label=\"Stable Diffusion Version\", width=250, options=[dropdown.Option(\"text2image\")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})\n",
        "    #class_prompt = TextField(label=\"Class Prompt\", value=converter_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    scheduler_type = Dropdown(label=\"Original Scheduler Mode\", hint_text=\"Hopefuly you know what Scheduler/Sampler they used in training\", width=200,\n",
        "            options=[\n",
        "                dropdown.Option(\"pndm\"),\n",
        "                dropdown.Option(\"lms\"),\n",
        "                dropdown.Option(\"ddim\"),\n",
        "                dropdown.Option(\"euler\"),\n",
        "                dropdown.Option(\"euler-ancestral\"),\n",
        "                dropdown.Option(\"dpm\"),\n",
        "            ], value=converter_prefs['scheduler_type'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_type'), col={'lg':6},\n",
        "        )\n",
        "    #save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=converter_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switcher(label=\"Save Model to HuggingFace    \", value=converter_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    half_percision = Tooltip(message=\"Save weights in half precision.\", content=Switcher(label=\"Save Half Percision Float16    \", value=converter_prefs['half_percision'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'half_percision')))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=converter_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=converter_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=converter_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    load_custom_model = Checkbox(label=\"Load Custom Model\", tooltip=\"After conversion is done, will put it in your Custom Model setting, ready to test out\", value=converter_prefs['load_custom_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'load_custom_model'))\n",
        "    #resolution = Slider(min=256, max=1024, divisions=6, label=\"{value}px\", value=float(converter_prefs['resolution']), expand=True, on_change=lambda e:changed(e,'resolution', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), resolution])\n",
        "    #image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=converter_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    #add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    #page.converter_file_list = Column([], tight=True, spacing=0)\n",
        "    #load_images()\n",
        "    where_to_save_model.visible = converter_prefs['save_model']\n",
        "    readme_description.visible = converter_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusconverter_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=converter_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.converter_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.converter_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîÄ  Model Converter Tool\", \"Lets you Convert Format of Model Checkpoints to work with Diffusers...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Model Converters Settings\", on_click=converter_help)]),\n",
        "        ResponsiveRow([from_format, to_format]),\n",
        "        ResponsiveRow([from_model_path, from_model_name]),\n",
        "        base_model_row,\n",
        "        ResponsiveRow([model_type, scheduler_type]),\n",
        "        half_percision,\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        load_custom_model,\n",
        "        #max_row,\n",
        "        #Row([image_path, add_image_button]),\n",
        "        #page.converter_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"„ÄΩÔ∏è  Run Model Converter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_converter(page))]),\n",
        "        page.converter_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "checkpoint_merger_prefs = {\n",
        "    'pretrained_model': '',\n",
        "    'selected_model': 'Stable Diffusion v1.5',\n",
        "    'pretrained_models': [], #A list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.\n",
        "    'alpha': 0.5, #The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2\n",
        "    'interp': 'weighted_sum', #The interpolation method to use for the merging. Supports \"sigmoid\", \"inv_sigmoid\", \"add_difference\" and None. Passing None uses the default interpolation which is weighted sum interpolation. For merging three checkpoints, only \"add_difference\" is supported.\n",
        "    'force': False, #Whether to ignore mismatch in model_config.json for the current models. Defaults to False.\n",
        "    'validation_prompt': '',\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'readme_description': '',\n",
        "}\n",
        "\n",
        "def buildCheckpointMerger(page):\n",
        "    global prefs, checkpoint_merger_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              checkpoint_merger_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              checkpoint_merger_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              checkpoint_merger_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def checkpoint_merger_help(e):\n",
        "        def close_checkpoint_merger_dlg(e):\n",
        "          nonlocal checkpoint_merger_help_dlg\n",
        "          checkpoint_merger_help_dlg.open = False\n",
        "          page.update()\n",
        "        checkpoint_merger_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Checkpoint Merger\"), content=Column([\n",
        "            Text(\"Provide a list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.  Merges the Checkpoint Weights into a new model that you can save for free to HuggingFace to reuse.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçª  Sure thing... \", on_click=close_checkpoint_merger_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = checkpoint_merger_help_dlg\n",
        "        checkpoint_merger_help_dlg.open = True\n",
        "        page.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = checkpoint_merger_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = checkpoint_merger_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def remove_model(e):\n",
        "        f = e.control.data\n",
        "        for i, fl in enumerate(page.checkpoint_merger_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "                del page.checkpoint_merger_file_list.controls[i]\n",
        "                page.checkpoint_merger_file_list.update()\n",
        "                continue\n",
        "    def remove_all_models(e):\n",
        "        checkpoint_merger_prefs['pretrained_models'].clear()\n",
        "        page.checkpoint_merger_file_list.controls.clear()\n",
        "        page.checkpoint_merger_file_list.update()\n",
        "    def move_down(e):\n",
        "        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)\n",
        "        if idx < (len(checkpoint_merger_prefs['pretrained_models']) - 1):\n",
        "          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)\n",
        "          checkpoint_merger_prefs['pretrained_models'].insert(idx+1, d)\n",
        "          dr = page.checkpoint_merger_file_list.controls.pop(idx)\n",
        "          page.checkpoint_merger_file_list.controls.insert(idx+1, dr)\n",
        "          page.checkpoint_merger_file_list.update()\n",
        "    def move_up(e):\n",
        "        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)\n",
        "          checkpoint_merger_prefs['pretrained_models'].insert(idx-1, d)\n",
        "          dr = page.checkpoint_merger_file_list.controls.pop(idx)\n",
        "          page.checkpoint_merger_file_list.controls.insert(idx-1, dr)\n",
        "          page.checkpoint_merger_file_list.update()\n",
        "    def add_selected_model(e):\n",
        "        name = checkpoint_merger_prefs['selected_model']\n",
        "        m = {'name':''}\n",
        "        if name == \"Stable Diffusion v2.1 x768\":\n",
        "            m = {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1'}\n",
        "        elif name == \"Stable Diffusion v2.1 x512\":\n",
        "            m = {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base'}\n",
        "        elif name == \"Stable Diffusion v2.0\":\n",
        "            m = {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2'}\n",
        "        elif name == \"Stable Diffusion v2.0 x768\":\n",
        "            m = {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2'}\n",
        "        elif name == \"Stable Diffusion v2.0 x512\":\n",
        "            m = {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base'}\n",
        "        elif name == \"Stable Diffusion v1.5\":\n",
        "            m = {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5'}\n",
        "        elif name == \"Stable Diffusion v1.4\":\n",
        "            m = {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4'}\n",
        "        else:\n",
        "            m = get_finetuned_model(name)\n",
        "            if not bool(m['name']):\n",
        "                m = get_dreambooth_model(name)\n",
        "        if bool(m['path']):\n",
        "            add_model(m['path'])\n",
        "    def add_custom_model(e):\n",
        "        mpath = checkpoint_merger_prefs['pretrained_model']\n",
        "        add_model(mpath)\n",
        "    def add_model(mpath, update=True):\n",
        "        if mpath in checkpoint_merger_prefs['pretrained_models']:\n",
        "            alert_msg(page, \"That model path is already in your list...\")\n",
        "            return\n",
        "        page.checkpoint_merger_file_list.controls.append(ListTile(title=Text(mpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Remove Model\", on_click=remove_model, data=mpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Remove All\", on_click=remove_all_models, data=mpath),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=mpath),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=mpath),\n",
        "          ]), data=mpath))\n",
        "        checkpoint_merger_prefs['pretrained_models'].append(mpath)\n",
        "        if update: page.checkpoint_merger_file_list.update()\n",
        "    validation_prompt = Container(content=Tooltip(message=\"Optional prompt to test after the merger is finished.\", content=TextField(label=\"Validation Test Prompt\", value=checkpoint_merger_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of New Model\", value=checkpoint_merger_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    force = Checkbox(label=\"Force if Mismatch\", tooltip=\"Whether to ignore mismatch in model_config.json for the current models.\", value=checkpoint_merger_prefs['force'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'force'))\n",
        "    alpha_row = SliderRow(label=\"Alpha Interpolation\", min=0.0, max=1.0, divisions=20, round=2, pref=checkpoint_merger_prefs, key='alpha', tooltip=\"The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2\")\n",
        "    interp = Dropdown(label=\"Interpolation Method\", width=250, options=[dropdown.Option(\"weighted_sum\"), dropdown.Option(\"sigmoid\"), dropdown.Option(\"inv_sigmoid\"), dropdown.Option(\"add_difference\")], value=checkpoint_merger_prefs['interp'], on_change=lambda e: changed(e, 'interp'))\n",
        "    #The interpolation method to use for the merging. Supports \"sigmoid\", \"inv_sigmoid\", \"add_difference\" and None. For merging three checkpoints, only \"add_difference\" is supported.\n",
        "    model_ckpt = Dropdown(label=\"Model Checkpoint\", width=300, options=[\n",
        "        dropdown.Option(\"Stable Diffusion v2.1 x768\"), dropdown.Option(\"Stable Diffusion v2.1 x512\"),\n",
        "        dropdown.Option(\"Stable Diffusion v2.0 x768\"), dropdown.Option(\"Stable Diffusion v2.0 x512\"), dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\")], value=checkpoint_merger_prefs['selected_model'], on_change=lambda e: changed(e, 'selected_model'))\n",
        "    for mod in finetuned_models:\n",
        "        model_ckpt.options.append(dropdown.Option(mod[\"name\"]))\n",
        "    for db in dreambooth_models:\n",
        "        model_ckpt.options.append(dropdown.Option(db[\"name\"]))\n",
        "    add_selected_model_button = ElevatedButton(content=Text(\"Add Selected Model\"), on_click=add_selected_model)\n",
        "    pretrained_model = TextField(label=\"HuggingFace Path or Local Path to Merge\", value=checkpoint_merger_prefs['pretrained_model'], on_change=lambda e:changed(e,'pretrained_model'), expand=1)\n",
        "    add_model_button = ElevatedButton(content=Text(\"Add Model Path\"), on_click=add_custom_model)\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switcher(label=\"Save Model to HuggingFace    \", value=checkpoint_merger_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=checkpoint_merger_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=checkpoint_merger_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "\n",
        "    page.checkpoint_merger_file_list = Column([], tight=True, spacing=0)\n",
        "    page.checkpoint_merger_output = Column([])\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üë•  Checkpoint Merger Tool\", \"Combine together two or more custom models to create a mixture of weights...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Checkpoint Merger Settings\", on_click=checkpoint_merger_help)]),\n",
        "        Row([model_ckpt, add_selected_model_button]),\n",
        "        Row([pretrained_model, add_model_button]),\n",
        "        page.checkpoint_merger_file_list,\n",
        "        Divider(thickness=3, height=6),\n",
        "        Row([interp, force]),\n",
        "        alpha_row,\n",
        "        ResponsiveRow([name_of_your_model, validation_prompt]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        Row([ElevatedButton(content=Text(\"ü§ó  Run Checkpoint Merger\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_checkpoint_merger(page))]),\n",
        "        page.checkpoint_merger_output,\n",
        "        #clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "tortoise_prefs = {\n",
        "    'text': '',\n",
        "    'preset': 'standard', #\"ultra_fast\", \"fast\", \"standard\", \"high_quality\"\n",
        "    'voice': [],\n",
        "    'voices': ['angie', 'applejack', 'daniel', 'deniro', 'emma', 'freeman', 'geralt', 'halle', 'jlaw', 'lj', 'mol', 'myself', 'pat', 'pat2', 'rainbow', 'snakes', 'tim_reynolds', 'tom', 'train_atkins', 'train_daws', 'train_dotrice', 'train_dreams', 'train_empire', 'train_grace', 'train_kennard', 'train_lescault', 'train_mouse', 'weaver', 'william'],\n",
        "    'train_custom': False,\n",
        "    'custom_voice_name': '',\n",
        "    'custom_wavs': [],\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'tts-',\n",
        "}\n",
        "\n",
        "def buildTortoiseTTS(page):\n",
        "    global prefs, tortoise_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              tortoise_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              tortoise_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              tortoise_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_tortoise_output(o):\n",
        "        page.tortoise_output.controls.append(o)\n",
        "        page.tortoise_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.tortoise_output.controls = []\n",
        "        page.tortoise_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def tortoise_help(e):\n",
        "        def close_tortoise_dlg(e):\n",
        "          nonlocal tortoise_help_dlg\n",
        "          tortoise_help_dlg.open = False\n",
        "          page.update()\n",
        "        tortoise_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Tortoise-TTS\"), content=Column([\n",
        "            Text(\"Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips. These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.\"),\n",
        "            Text(\"This comes with several pre-packaged voices. Voices prepended with 'train_' came from the training set and perform far better than the others. If your goal is high quality speech, we recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others.\"),\n",
        "            Text(\"To add new voices to Tortoise, you will need to do the following: Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section. Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing. Save the clips as a WAV file with floating point format and a 22,050 sample rate.\"),\n",
        "            Text(\"You can do prompt engineering with Tortoise to get the performance you want. For example, you can evoke emotion by including things like [I am really sad], before your text. It redacts the phrase in brackets, but keeps the context of the meaning to the voice reading. Experiment with grammar and spelling, and use an audio editor to perfect the vocals later.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üëÑ  What to say... \", on_click=close_tortoise_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = tortoise_help_dlg\n",
        "        tortoise_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_audio(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.tortoise_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.tortoise_file_list.controls[i]\n",
        "              page.tortoise_file_list.update()\n",
        "              if f in tortoise_prefs['custom_wavs']:\n",
        "                tortoise_prefs['custom_wavs'].remove(f)\n",
        "              continue\n",
        "    def delete_all_audios(e):\n",
        "        for fl in page.tortoise_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.tortoise_file_list.controls.clear()\n",
        "        page.tortoise_file_list.update()\n",
        "        tortoise_prefs['custom_wavs'].clear()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.tortoise_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Audio\", on_click=delete_audio, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_audios, data=fpath),\n",
        "          ])))\n",
        "        tortoise_prefs['custom_wavs'].append(fpath)\n",
        "        if update: page.tortoise_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'tortoise-audio')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "            shutil.move(fname, fpath)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "            shutil.copy(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"wav\", \"WAV\", \"mp3\", \"MP3\"], dialog_title=\"Pick Voice WAV or MP3 Files to Train\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_wav(e):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        if wav_path.value.startswith('http'):\n",
        "            import requests\n",
        "            from io import BytesIO\n",
        "            #response = requests.get(wav_path.value)\n",
        "            fpath = download_file(wav_path.value)\n",
        "            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "            add_file(fpath)\n",
        "        elif os.path.isfile(wav_path.value):\n",
        "          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "          shutil.copy(wav_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(wav_path.value):\n",
        "          for f in os.listdir(wav_path.value):\n",
        "            file_path = os.path.join(wav_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(wav_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        wav_path.value = \"\"\n",
        "        wav_path.update()\n",
        "    def load_wavs():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_custom(e):\n",
        "        changed(e, 'train_custom')\n",
        "        custom_box.height = None if tortoise_prefs['train_custom'] else 0\n",
        "        custom_box.update()\n",
        "        custom_voice_name.visible = tortoise_prefs['train_custom']\n",
        "        custom_voice_name.update()\n",
        "    text = TextField(label=\"Text to Read\", value=tortoise_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    preset = Dropdown(label=\"Quality Preset\", width=250, options=[dropdown.Option(\"ultra_fast\"), dropdown.Option(\"fast\"), dropdown.Option(\"standard\"), dropdown.Option(\"high_quality\")], value=tortoise_prefs['preset'], on_change=lambda e: changed(e, 'preset'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=tortoise_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=tortoise_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    page.tortoise_voices = ResponsiveRow(controls=[])\n",
        "    for v in tortoise_prefs['voices']:\n",
        "      page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    if len(prefs['tortoise_custom_voices']) > 0:\n",
        "      for custom in prefs['tortoise_custom_voices']:\n",
        "        page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    custom_voice_name = TextField(label=\"Custom Voice Name\", value=tortoise_prefs['custom_voice_name'], on_change=lambda e:changed(e,'custom_voice_name'))\n",
        "    train_custom = Switcher(label=\"Train Custom Voice  \", value=tortoise_prefs['train_custom'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_custom)\n",
        "    wav_path = TextField(label=\"Audio Files or Folder Path or URL to Train\", value=tortoise_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_wav_button = ElevatedButton(content=Text(\"Add Audio Files\"), on_click=add_wav)\n",
        "    page.tortoise_file_list = Column([], tight=True, spacing=0)\n",
        "    custom_box = Container(Column([Text(\"Provide 3 or more ~10 second clips of voice as mp3 or wav files with 22050 sample rate:\"),\n",
        "        Row([wav_path, add_wav_button]),\n",
        "        page.tortoise_file_list,]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    custom_box.height = None if tortoise_prefs['train_custom'] else 0\n",
        "    custom_voice_name.visible = tortoise_prefs['train_custom']\n",
        "    load_wavs()\n",
        "    #seed = TextField(label=\"Seed\", value=tortoise_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfustortoise_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=tortoise_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.tortoise_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.tortoise_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üê¢  Tortoise Text-to-Speech Voice Modeling\", \"Reads your text in a realistic AI voice, train your own to mimic vocal performances...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Tortoise-TTS Settings\", on_click=tortoise_help)]),\n",
        "        text,\n",
        "        preset,\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        Row([Text(\"Select one or more voices:\", weight=FontWeight.BOLD), Text(\"(none for random or custom)\")]),\n",
        "        page.tortoise_voices,\n",
        "        Row([train_custom, custom_voice_name], vertical_alignment=CrossAxisAlignment.START),\n",
        "        #Row([output_dir]),\n",
        "        custom_box,\n",
        "        ElevatedButton(content=Text(\"üó£Ô∏è  Run Tortoise-TTS\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tortoise_tts(page)),\n",
        "        page.tortoise_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "audioLDM_prefs = {\n",
        "    'text': '',\n",
        "    'duration': 5.0,\n",
        "    'guidance_scale': 2.5,\n",
        "    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation\n",
        "    'seed': 0,\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ldm-',\n",
        "}\n",
        "\n",
        "def buildAudioLDM(page):\n",
        "    global prefs, audioLDM_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              audioLDM_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              audioLDM_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              audioLDM_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_audioLDM_output(o):\n",
        "        page.audioLDM_output.controls.append(o)\n",
        "        page.audioLDM_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.audioLDM_output.controls = []\n",
        "        page.audioLDM_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def audioLDM_help(e):\n",
        "        def close_audioLDM_dlg(e):\n",
        "          nonlocal audioLDM_help_dlg\n",
        "          audioLDM_help_dlg.open = False\n",
        "          page.update()\n",
        "        audioLDM_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-LDM\"), content=Column([\n",
        "            Text(\"AudioLDM is a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.\"),\n",
        "            Markdown(\"They built the model with data from [AudioSet](http://research.google.com/audioset/), [Freesound](https://freesound.org/) and [BBC Sound Effect library](https://sound-effects.bbcrewind.co.uk/). We share this demo based on the [UK copyright exception](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/375954/Research.pdf) of data for academic research.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üîî  Good to hear... \", on_click=close_audioLDM_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = audioLDM_help_dlg\n",
        "        audioLDM_help_dlg.open = True\n",
        "        page.update()\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=20, divisions=38, round=1, suffix=\"s\", pref=audioLDM_prefs, key='duration')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=5, divisions=10, round=1, pref=audioLDM_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    text = TextField(label=\"Text Prompt to Auditorialize\", value=audioLDM_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=audioLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=audioLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_candidates = Tooltip(message=\"Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.\", content=NumberPicker(label=\"Number of Candidates:   \", min=1, max=5, value=audioLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))\n",
        "    seed = TextField(label=\"Seed\", value=audioLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.audioLDM_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.audioLDM_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü¶ª  Audio LDM Modeling\", \"Text-to-Audio Generation with Latent Diffusion Model...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=audioLDM_help)]),\n",
        "        text,\n",
        "        duration_row,\n",
        "        guidance,\n",
        "        Row([n_candidates, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üëè  Run AudioLDM\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm(page)),\n",
        "        page.audioLDM_output,\n",
        "        clear_button,\n",
        "        #AudioPlayer(audio_file=os.path.join(assets, \"snd-drop.mp3\"), display=\"tester\", page=page)\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "audioLDM2_prefs = {\n",
        "    'text': '',\n",
        "    'negative_prompt':'',\n",
        "    'transcription': '',\n",
        "    'model_name': 'cvssp/audioldm2',\n",
        "    'duration': 10.0,\n",
        "    'steps': 200,\n",
        "    'guidance_scale': 3.5,\n",
        "    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation\n",
        "    'seed': 0,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ldm2-',\n",
        "    'save_mp3': False,\n",
        "}\n",
        "\n",
        "def buildAudioLDM2(page):\n",
        "    global prefs, audioLDM2_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              audioLDM2_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              audioLDM2_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              audioLDM2_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_audioLDM2_output(o):\n",
        "        page.audioLDM2_output.controls.append(o)\n",
        "        page.audioLDM2_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.audioLDM2_output.controls = []\n",
        "        page.audioLDM2_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def audioLDM2_help(e):\n",
        "        def close_audioLDM2_dlg(e):\n",
        "          nonlocal audioLDM2_help_dlg\n",
        "          audioLDM2_help_dlg.open = False\n",
        "          page.update()\n",
        "        audioLDM2_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-LDM 2\"), content=Column([\n",
        "            Text(\"AudioLDM-2 is a novel and versatile audio generation model that capable of performing conditional audio, music, and intelligible speech generation. The proposed method is based on a universal representation of audio, which enables large-scale self-supervised pretraining of the core latent diffusion model without audio annotation and helps to combine the advantages of both the auto-regressive and the latent diffusion model. AudioLDM 2 achieves state-of-the-art performance in text-to-audio and text-to-music generation, while also delivering competitive results in text-to-speech generation, comparable to the current SoTA.\"),\n",
        "            Text(\"Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called 'language of audio' (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches.\"),\n",
        "            Markdown(\"[Project Page](https://audioldm.github.io/audioldm2/) | [Paper](https://arxiv.org/abs/2308.05734) | [GitHub Code](https://github.com/haoheliu/audioldm2)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üîî  Good to hear... \", on_click=close_audioLDM2_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = audioLDM2_help_dlg\n",
        "        audioLDM2_help_dlg.open = True\n",
        "        page.update()\n",
        "    def change_model(e):\n",
        "        changed(e, 'model_name')\n",
        "        if 'speech' in audioLDM2_prefs['model_name']:\n",
        "            transcription.visible = True\n",
        "        else:\n",
        "            transcription.visible = False\n",
        "        transcription.update()\n",
        "    model_name = Dropdown(label=\"Audio-LDM2 Model\", width=350, options=[dropdown.Option(\"cvssp/audioldm2\"), dropdown.Option(\"cvssp/audioldm2-music\"), dropdown.Option(\"cvssp/audioldm2-large\")], value=audioLDM2_prefs['model_name'], on_change=change_model)\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=320, divisions=319, round=1, suffix=\"s\", pref=audioLDM2_prefs, key='duration')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=audioLDM2_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    steps_row = SliderRow(label=\"Number of Steps\", min=1, max=300, divisions=299, pref=audioLDM2_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    text = TextField(label=\"Text Prompt to Auditorialize\", value=audioLDM2_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'), col={'md':9})\n",
        "    negative_prompt = TextField(label=\"Negative Prompt\", value=audioLDM2_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})\n",
        "    transcription = TextField(label=\"Text Transcript to Speak\", value=audioLDM2_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))\n",
        "    save_mp3 = Checkbox(label=\"Save as mp3\", tooltip=\"Otherwise saves larger wav file.\", value=audioLDM2_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=audioLDM2_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=audioLDM2_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_candidates = Tooltip(message=\"Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.\", content=NumberPicker(label=\"Number of Candidates:   \", min=1, max=5, value=audioLDM2_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:  \", min=1, max=10, value=audioLDM2_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=audioLDM2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.audioLDM2_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.audioLDM2_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üì¢  Audio LDM-2 Modeling\", \"Holistic Audio Generation with Self-supervised Pretraining...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=audioLDM2_help)]),\n",
        "        ResponsiveRow([text, negative_prompt]),\n",
        "        transcription,\n",
        "        model_name,\n",
        "        duration_row,\n",
        "        guidance,\n",
        "        steps_row,\n",
        "        Row([batch_size, seed, save_mp3]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üéô  Run AudioLDM-2\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm2(page)),\n",
        "        page.audioLDM2_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "musicLDM_prefs = {\n",
        "    'text': '',\n",
        "    'negative_prompt':'',\n",
        "    'model_name': 'cvssp/musicldm',\n",
        "    'duration': 10.0,\n",
        "    'steps': 200,\n",
        "    'guidance_scale': 3.5,\n",
        "    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation\n",
        "    'seed': 0,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ldm-',\n",
        "    'save_mp3': False,\n",
        "}\n",
        "\n",
        "def buildMusicLDM(page):\n",
        "    global prefs, musicLDM_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              musicLDM_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              musicLDM_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              musicLDM_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_musicLDM_output(o):\n",
        "        page.musicLDM_output.controls.append(o)\n",
        "        page.musicLDM_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.musicLDM_output.controls = []\n",
        "        page.musicLDM_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def musicLDM_help(e):\n",
        "        def close_musicLDM_dlg(e):\n",
        "          nonlocal musicLDM_help_dlg\n",
        "          musicLDM_help_dlg.open = False\n",
        "          page.update()\n",
        "        musicLDM_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Music-LDM\"), content=Column([\n",
        "            Text(\"MusicLDM takes a text prompt as input and predicts the corresponding music sample. Inspired by Stable Diffusion and AudioLDM, MusicLDM is a text-to-music latent diffusion model (LDM) that learns continuous audio representations from CLAP latents. MusicLDM is trained on a corpus of 466 hours of music data. Beat-synchronous data augmentation strategies are applied to the music samples, both in the time domain and in the latent space. Using beat-synchronous data augmentation strategies encourages the model to interpolate between the training samples, but stay within the domain of the training data. The result is generated music that is more diverse while staying faithful to the corresponding style.\"),\n",
        "            Text(\"It was proposed in MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\"),\n",
        "            Markdown(\"[Paper](https://huggingface.co/papers/2308.01546) | [GitHub Code](https://github.com/haoheliu/musicldm)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü™®  Let's Rock... \", on_click=close_musicLDM_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = musicLDM_help_dlg\n",
        "        musicLDM_help_dlg.open = True\n",
        "        page.update()\n",
        "    def change_model(e):\n",
        "        changed(e, 'model_name') #dropdown.Option(\"cvssp/audioldm-s-full-v2\"), \n",
        "    model_name = Dropdown(label=\"Music-LDM Model\", width=350, options=[dropdown.Option(\"cvssp/musicldm\"), dropdown.Option(\"sanchit-gandhi/musicldm-full\")], value=musicLDM_prefs['model_name'], on_change=change_model)\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=320, divisions=319, round=1, suffix=\"s\", pref=musicLDM_prefs, key='duration')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=musicLDM_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    steps_row = SliderRow(label=\"Number of Steps\", min=1, max=300, divisions=299, pref=musicLDM_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    text = TextField(label=\"Text Prompt to Describe Music\", value=musicLDM_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'), col={'md':9})\n",
        "    negative_prompt = TextField(label=\"Negative Prompt\", value=musicLDM_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})\n",
        "    #transcription = TextField(label=\"Text Transcript to Speak\", value=musicLDM_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))\n",
        "    save_mp3 = Checkbox(label=\"Save as mp3\", tooltip=\"Otherwise saves larger wav file.\", value=musicLDM_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=musicLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=musicLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_candidates = Tooltip(message=\"Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.\", content=NumberPicker(label=\"Number of Candidates:   \", min=1, max=5, value=musicLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:  \", min=1, max=10, value=musicLDM_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=musicLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.musicLDM_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.musicLDM_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé∏  MusicLDM Song Modeling\", \"Text-to-Music Generation: Enhancing Novelty in Beat-Synchronous Mixup...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Music LDM-TTS Settings\", on_click=musicLDM_help)]),\n",
        "        ResponsiveRow([text, negative_prompt]),\n",
        "        model_name,\n",
        "        duration_row,\n",
        "        guidance,\n",
        "        steps_row,\n",
        "        Row([batch_size, seed, save_mp3]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"ü§ò  Run MusicLDM\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_ldm(page)),\n",
        "        page.musicLDM_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "bark_prefs = {\n",
        "    'text': '',\n",
        "    'text_temp': 0.7,\n",
        "    'waveform_temp': 0.7,\n",
        "    'acoustic_prompt': 'Unconditional',\n",
        "    'n_iterations': 1,\n",
        "    'seed': 0,\n",
        "    'use_bettertransformer': False,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'bark-',\n",
        "}\n",
        "\n",
        "def buildBark(page):\n",
        "    global prefs, bark_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              bark_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              bark_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              bark_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_bark_output(o):\n",
        "        page.bark_output.controls.append(o)\n",
        "        page.bark_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.bark_output.controls = []\n",
        "        page.bark_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def bark_help(e):\n",
        "        def close_bark_dlg(e):\n",
        "          nonlocal bark_help_dlg\n",
        "          bark_help_dlg.open = False\n",
        "          page.update()\n",
        "        bark_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Bark AI\"), content=Column([\n",
        "            Text(\"Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference.\"),\n",
        "            Text(\"Below is a list of some known non-speech sounds, but we are finding more every day. Please let us know if you find patterns that work particularly well on Discord!\"),\n",
        "            Text(\"[laughter], [laughs], [sighs], [music], [gasps], [clears throat], ‚Äî or ‚Ä¶ for hesitations, ‚ô™ for song lyrics, capitalization for emphasis of a word, MAN/WOMAN: for bias towards speaker\"),\n",
        "            Markdown(\"Checkout their [GitHub Project](https://github.com/suno-ai/bark), [HuggingFace Space](https://huggingface.co/spaces/suno/bark) and [Suno Discord](https://discord.com/invite/J2B2vsjKuE).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü¶ä  Woof... \", on_click=close_bark_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = bark_help_dlg\n",
        "        bark_help_dlg.open = True\n",
        "        page.update()\n",
        "    #duration_row = SliderRow(label=\"Duration\", min=1, max=20, divisions=38, round=1, suffix=\"s\", pref=bark_prefs, key='duration')\n",
        "    text_temp = SliderRow(label=\"Text Temperature\", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='text_temp', tooltip=\"1.0 more diverse, 0.0 more conservative\")\n",
        "    waveform_temp = SliderRow(label=\"Wave Temperature\", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='waveform_temp', tooltip=\"1.0 more diverse, 0.0 more conservative\")\n",
        "    text = TextField(label=\"Text Prompt to Vocalize\", value=bark_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    acoustic_prompt = Dropdown(label=\"Acoustic Prompt\", width=250, options=[dropdown.Option(\"Unconditional\"), dropdown.Option(\"Announcer\")], value=bark_prefs['acoustic_prompt'], on_change=lambda e: changed(e, 'acoustic_prompt'))\n",
        "    langs = [\"en\", \"de\", \"es\", \"fr\", \"hi\", \"it\", \"ja\", \"ko\", \"pl\", \"pt\", \"ru\", \"tr\", \"zh\"]\n",
        "    for lang in langs:\n",
        "        for n in range(10):\n",
        "            #label = f\"Speaker {n} ({lang})\"\n",
        "            acoustic_prompt.options.append(dropdown.Option(f\"{lang}_speaker_{n}\"))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=bark_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=bark_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_iterations = Tooltip(message=\"Make multiple for quality control. Generates candidates and choose the best.\", content=NumberPicker(label=\"Number of Iterations:   \", min=1, max=10, value=bark_prefs['n_iterations'], on_change=lambda e: changed(e, 'n_iterations')))\n",
        "    seed = TextField(label=\"Seed\", value=bark_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    use_bettertransformer = Switcher(label=\"Use Optimum BetterTransformer\", value=bark_prefs['use_bettertransformer'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_bettertransformer'), tooltip=\"Enables HuggingFace Optimum Transformers to go 25% faster.\")\n",
        "    page.bark_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.bark_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üê∂  Bark AI\", \"Text-to-Audio Generation for Multilingual Speech, Music and Sound Effects...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=bark_help)]),\n",
        "        text,\n",
        "        text_temp,\n",
        "        waveform_temp,\n",
        "        Row([acoustic_prompt, n_iterations]),\n",
        "        #Row([n_iterations, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        use_bettertransformer,\n",
        "        ElevatedButton(content=Text(\"üêï  Run Bark\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_bark(page)),\n",
        "        page.bark_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "riffusion_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'audio_file': '',\n",
        "    'duration': 5.0,\n",
        "    'steps': 50,\n",
        "    'strength': 0.5,\n",
        "    'guidance_scale': 7.0,\n",
        "    'batch_size': 1,\n",
        "    'max_size': 768,\n",
        "    'seed': 0,\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'riff-',\n",
        "    'loaded_pipe': '',\n",
        "}\n",
        "\n",
        "def buildRiffusion(page):\n",
        "    global prefs, riffusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              riffusion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              riffusion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              riffusion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_riffusion_output(o):\n",
        "        page.riffusion_output.controls.append(o)\n",
        "        page.riffusion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.riffusion_output.controls = []\n",
        "        page.riffusion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def riffusion_help(e):\n",
        "        def close_riffusion_dlg(e):\n",
        "          nonlocal riffusion_help_dlg\n",
        "          riffusion_help_dlg.open = False\n",
        "          page.update()\n",
        "        riffusion_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Riffusion\"), content=Column([\n",
        "            Text(\"This is the v1.5 stable diffusion model with no modifications, just fine-tuned on images of spectrograms paired with text. Audio processing happens downstream of the model. It can generate infinite variations of a prompt by varying the seed. All the same web UIs and techniques like img2img, inpainting, negative prompts, and interpolation work out of the box.\"),\n",
        "            Markdown(\"[Project Page](https://www.riffusion.com/about), [Codebase](https://github.com/riffusion/riffusion), [Discord](https://discord.gg/yu6SRwvX4v)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üé∫  Let's Jam... \", on_click=close_riffusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = riffusion_help_dlg\n",
        "        riffusion_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              riffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              riffusion_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            riffusion_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp3\", \"wav\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Input Audio File (optional)\", value=riffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "\n",
        "    def change_duration(e):\n",
        "        changed(e, 'duration', ptype=\"float\")\n",
        "        duration_value.value = f\" {riffusion_prefs['duration']}s\"\n",
        "        duration_value.update()\n",
        "    duration = Slider(min=1, max=20, divisions=38, label=\"{value}s\", round=1, value=float(riffusion_prefs['duration']), expand=True, on_change=change_duration)\n",
        "    duration_value = Text(f\" {float(riffusion_prefs['duration'])}s\", weight=FontWeight.BOLD)\n",
        "    duration_row = Row([Text(\"Duration: \"), duration_value, duration])\n",
        "    steps_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=riffusion_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=riffusion_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    prompt = TextField(label=\"Musical Text Prompt\", value=riffusion_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'), col={'md':9})\n",
        "    negative_prompt = TextField(label=\"Negative Prompt\", value=riffusion_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})\n",
        "    max_size_row = SliderRow(label=\"Max Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=riffusion_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=riffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=riffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:   \", min=1, max=5, value=riffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=riffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.riffusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.riffusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üíΩ  Riffusion Spectrogram Sound Modeling\", \"Stable Diffusion for real-time music generation...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=riffusion_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #audio_file,\n",
        "        #duration_row,\n",
        "        guidance,\n",
        "        steps_row,\n",
        "        max_size_row,\n",
        "        Row([batch_size, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé§Ô∏è  Run Riffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_riffusion(page)),\n",
        "        page.riffusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "mubert_prefs = {\n",
        "    'prompt': '',\n",
        "    'duration': 30,\n",
        "    'is_loop': False,\n",
        "    'email': '',\n",
        "    'tags': '',\n",
        "    'tag_list': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'mu-'\n",
        "}\n",
        "\n",
        "def buildMubert(page):\n",
        "    global mubert_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              mubert_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              mubert_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              mubert_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_mubert_output(o):\n",
        "        page.mubert_output.controls.append(o)\n",
        "        page.mubert_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.mubert_output.controls = []\n",
        "        page.mubert_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def mubert_help(e):\n",
        "        def close_mubert_dlg(e):\n",
        "          nonlocal mubert_help_dlg\n",
        "          mubert_help_dlg.open = False\n",
        "          page.update()\n",
        "        mubert_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Mubert\"), content=Column([\n",
        "            Text(\"Mubert AI is a cutting-edge tool that allows you to create realistic and infinite music by learning from a large dataset of existing music. The result is a high-quality and original music stream that sounds like it was composed by a professional musician.\"),\n",
        "            Text(\"This uses the Mubert Developer API which is actually quite expensive to use, so don't be surprised if your generation fails because the API Key has used it's monthy quota. Sorry, it's not free/opensource...\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•Å  Gimme a Beat... \", on_click=close_mubert_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = mubert_help_dlg\n",
        "        mubert_help_dlg.open = True\n",
        "        page.update()\n",
        "    prompt = TextField(label=\"Prompt to generate a track (genre, theme, etc.)\", value=mubert_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=250, divisions=249, suffix=\"s\", pref=mubert_prefs, key='duration')\n",
        "    is_loop = Checkbox(label=\"Is Audio Loop   \", value=mubert_prefs['is_loop'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'is_loop'))\n",
        "    email = TextField(label=\"Email Address (for API use)\", keyboard_type=KeyboardType.EMAIL, value=mubert_prefs['email'], on_change=lambda e:changed(e,'email'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=mubert_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=mubert_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #page.mubert_songs = ResponsiveRow(controls=[])\n",
        "    #for v in mubert_prefs['tag_list']:\n",
        "    #  page.mubert_songs.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    page.mubert_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.mubert_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéº  Mubert Music Generator\", \"AI music is generated by Mubert API. Pretty good grooves...(may not work if API maxed)\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Mubert Settings\", on_click=mubert_help)]),\n",
        "        prompt,\n",
        "        duration_row,\n",
        "        is_loop,\n",
        "        email,\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        #Row([Text(\"Select one or more tags:\", weight=FontWeight.BOLD), Text(\"(none for random or custom)\")]),\n",
        "        #page.mubert_songs,\n",
        "        ElevatedButton(content=Text(\"üéª  Run Mubert Music\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_mubert(page)),\n",
        "        page.mubert_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "whisper_prefs = {\n",
        "    'audio_file': '',\n",
        "    'model_size': 'base',\n",
        "    'trim_audio': True,\n",
        "    'AI_engine': 'ChatGPT-3.5 Turbo',\n",
        "    'AI_temperature': 0.7,\n",
        "    'simple_transcribe': True,\n",
        "    'detect_language': False,\n",
        "    'reformat': False,\n",
        "    'rewrite': False,\n",
        "    'summarize': False,\n",
        "    'describe': False,\n",
        "    'article': False,\n",
        "    'keypoints': False,\n",
        "    'keywords': False,\n",
        "    'translate': False,\n",
        "    'to_language': '',\n",
        "}\n",
        "whisper_requests = {\n",
        "        'reformat': 'Format the following text to have correct punctuations, grammar and sentence structure.',\n",
        "        'rewrite': 'Rewrite and edit the following text to be more proper.',\n",
        "        'summarize': 'Write a summary of the following text to reflect the most important information.',\n",
        "        'describe': 'Write a description of the following text to explain the subject and discussed topics.',\n",
        "        'article': 'Write an article based on the following text, using the interesting points to discuss.',\n",
        "        'keypoints': 'Outline the key points of the following text to summarize the useful information contained.',\n",
        "        'keywords': 'List the relevant keywords and phrases of the following text for SEO search engine optimization.',\n",
        "        'translate': 'Translate the following text into ',\n",
        "    }\n",
        "\n",
        "def buildWhisper(page):\n",
        "    global prefs, whisper_prefs, whisper_requests\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              whisper_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              whisper_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              whisper_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.whisper_output.controls = []\n",
        "        page.whisper_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def whisper_help(e):\n",
        "        def close_whisper_dlg(e):\n",
        "          nonlocal whisper_help_dlg\n",
        "          whisper_help_dlg.open = False\n",
        "          page.update()\n",
        "        whisper_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Whisper-AI\"), content=Column([\n",
        "            Text(\"Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English. We are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing.\"),\n",
        "            Text(\"The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation.\"),\n",
        "            Markdown(\"[Project Page](https://openai.com/research/whisper) | [GitHub Code](https://github.com/openai/whisper)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üôâ  Understood... \", on_click=close_whisper_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = whisper_help_dlg\n",
        "        whisper_help_dlg.open = True\n",
        "        page.update()\n",
        "    #audio_file = TextField(label=\"Input Media File (MP3, MP4, AVI, URL or YouTube URL)\", value=whisper_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "    audio_file = FileInput(label=\"Input Media File (MP3, MP4, AVI, URL or YouTube URL)\", pref=whisper_prefs, key='audio_file', ftype=\"media\", page=page)\n",
        "    model_size = Dropdown(label=\"Whisper Model Size\", width=200, options=[dropdown.Option(\"tiny\"), dropdown.Option(\"base\"), dropdown.Option(\"small\"), dropdown.Option(\"medium\"), dropdown.Option(\"large\"), dropdown.Option(\"large-v2\"), dropdown.Option(\"large-v3\")], value=whisper_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))\n",
        "    trim_audio = Checkbox(label=\"Trim Audio to 30s\", value=whisper_prefs['trim_audio'], tooltip=\"Prefers a short audio chunk\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=200, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"OpenAI GPT-4\"), dropdown.Option(\"GPT-4 Turbo\"), dropdown.Option(\"Google Gemini\")], value=whisper_prefs['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    AI_temperature = SliderRow(label=\"AI Temperature\", min=0, max=1, divisions=10, round=1, expand=True, pref=whisper_prefs, key=\"AI_temperature\")\n",
        "    reformat = Checkbox(label=\"Reformat grammar and structure of transcript\", value=whisper_prefs['reformat'], tooltip=whisper_requests['reformat'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'reformat'))\n",
        "    rewrite = Checkbox(label=\"Rewrite and edit content of transcript\", value=whisper_prefs['rewrite'], tooltip=whisper_requests['rewrite'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'rewrite'))\n",
        "    summarize = Checkbox(label=\"Summarize the information of transcript\", value=whisper_prefs['summarize'], tooltip=whisper_requests['summarize'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'summarize'))\n",
        "    describe = Checkbox(label=\"Describe the subject of transcript\", value=whisper_prefs['describe'], tooltip=whisper_requests['describe'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'describe'))\n",
        "    article = Checkbox(label=\"Write article about content of transcript\", value=whisper_prefs['article'], tooltip=whisper_requests['article'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'article'))\n",
        "    keypoints = Checkbox(label=\"Outline key points of transcript\", value=whisper_prefs['keypoints'], tooltip=whisper_requests['keypoints'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keypoints'))\n",
        "    keywords = Checkbox(label=\"List SEO Keywords in transcript\", value=whisper_prefs['keywords'], tooltip=whisper_requests['keywords'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keywords'))\n",
        "    translate = Checkbox(label=\"Translate transcript\", value=whisper_prefs['translate'], tooltip=whisper_requests['translate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'translate'))\n",
        "    to_language = TextField(label=\"to Language\", value=whisper_prefs['to_language'], width=120, on_change=lambda e:changed(e,'to_language'))\n",
        "    translate_to = Container(Row([translate, to_language], vertical_alignment=CrossAxisAlignment.START), col={'md':6, 'lg':4, 'xl':3})\n",
        "    page.whisper_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.whisper_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßè  OpenAI Whisper-AI Speech-To-Text\", \"Generate Text Transcriptions from Speech Recordings, then optionally process text with GPT...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio Diffusion-TTS Settings\", on_click=whisper_help)]),\n",
        "        audio_file,\n",
        "        Row([model_size, trim_audio]),\n",
        "        Row([AI_engine, AI_temperature]),\n",
        "        ResponsiveRow([reformat, rewrite, summarize, describe, article, keypoints, keywords, translate_to]),\n",
        "        ElevatedButton(content=Text(\"üëÇ  Run Whisper\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_whisper(page)),\n",
        "        page.whisper_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "voice_fixer_prefs = {\n",
        "    'audio_file': '',\n",
        "    'mode': 0,\n",
        "    'mode_0': True,\n",
        "    'mode_1': False,\n",
        "    'mode_2': False,\n",
        "    'audio_name': '',\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'voicefixer-',\n",
        "}\n",
        "\n",
        "def buildVoiceFixer(page):\n",
        "    global prefs, voice_fixer_prefs, voice_fixer_requests\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              voice_fixer_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              voice_fixer_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              voice_fixer_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.voice_fixer_output.controls = []\n",
        "        page.voice_fixer_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def voice_fixer_help(e):\n",
        "        def close_voice_fixer_dlg(e):\n",
        "          nonlocal voice_fixer_help_dlg\n",
        "          voice_fixer_help_dlg.open = False\n",
        "          page.update()\n",
        "        voice_fixer_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with VoiceFixer\"), content=Column([\n",
        "            Text(\"Voicefixer aims to restore human speech regardless how serious it's degraded. It can handle noise, reveberation, low resolution (2kHz~44.1kHz) and clipping (0.1-1.0 threshold) effect within one model.  This package provides a pretrained Voicefixer, which is build based on neural vocoder and pretrained 44.1k universal speaker-independent neural vocoder.\"),\n",
        "            Markdown(\"[Arxiv Paper](https://arxiv.org/pdf/2109.13731.pdf) | [GitHub Code](https://github.com/haoheliu/voicefixer) | [Demo Page](https://haoheliu.github.io/demopage-voicefixer/)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü¶ú  Loud & clear... \", on_click=close_voice_fixer_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = voice_fixer_help_dlg\n",
        "        voice_fixer_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              voice_fixer_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])\n",
        "              voice_fixer_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            voice_fixer_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"wav\", \"mp3\", \"flac\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Input Audio File (WAV, MP3, URL or YouTube URL)\", value=voice_fixer_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "    #model_size = Dropdown(label=\"VoiceFixer Model Size\", width=200, options=[dropdown.Option(\"tiny\"), dropdown.Option(\"base\"), dropdown.Option(\"small\"), dropdown.Option(\"medium\"), dropdown.Option(\"large\")], value=voice_fixer_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))\n",
        "    #trim_audio = Checkbox(label=\"Trim Audio to 30s\", value=voice_fixer_prefs['trim_audio'], tooltip=\"Prefers a short audio chunk\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))\n",
        "    mode_0 = Checkbox(label=\"Mode 0\", value=voice_fixer_prefs['mode_0'], tooltip=\"\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_0'))\n",
        "    mode_1 = Checkbox(label=\"Mode 1\", value=voice_fixer_prefs['mode_1'], tooltip=\"\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_1'))\n",
        "    mode_2 = Checkbox(label=\"Mode 2\", value=voice_fixer_prefs['mode_2'], tooltip=\"\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_2'))\n",
        "    audio_name = TextField(label=\"Audio File Name\", value=voice_fixer_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=voice_fixer_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=voice_fixer_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "\n",
        "    page.voice_fixer_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.voice_fixer_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üí¨  Voice Fixer - Speech Restoration with Neural Vocoder\", \"Cleans up bad vocals and fixes the unwanted noise...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio Diffusion-TTS Settings\", on_click=voice_fixer_help)]),\n",
        "        audio_file,\n",
        "        Row([mode_0, mode_1, mode_2]),\n",
        "        #Row([model_size, trim_audio]),\n",
        "        Row([audio_name, batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üó£  Run VoiceFixer\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_voice_fixer(page)),\n",
        "        page.voice_fixer_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "def buildCustomModelManager(page):\n",
        "    global prefs\n",
        "    def title_header(title, type):\n",
        "        return Row([Text(title, style=TextThemeStyle.BODY_LARGE),\n",
        "                    ft.FilledButton(f\"Add {type} Model\", on_click=lambda e: add_model(type))], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    def model_tile(name, path, token, type, weights=\"\"):\n",
        "        return ListTile(title=Row([Text(name, weight=FontWeight.BOLD), Text(path + (f\" - {weights}\" if bool(weights) else \"\")), Text(token)], alignment=MainAxisAlignment.SPACE_BETWEEN), data=type, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Custom Model\", on_click=lambda e: edit_model(e, name), data=type),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Custom Model\", on_click=lambda e: del_model(e, name), data=type)]), on_click=lambda e: edit_model(e, name))\n",
        "    def load_customs():\n",
        "        #custom_models.controls.append(title_header(\"Stable Diffusion Finetuned Models\", type=\"Finetuned\"))\n",
        "        for mod in prefs['custom_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            custom_models.controls.append(model_tile(mod['name'], mod['path'], token, \"Finetuned\"))\n",
        "            #page.custom_models.controls.append(ListTile(title=Row([Text(f\".{slash}{dir.rpartition(slash)[2]}\", weight=FontWeight.BOLD), Text(\"\")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            #  items=[PopupMenuItem(icon=icons.DELETE, text=\"Delete Custom Model\", on_click=del_model, data=dir)])))\n",
        "        #custom_models.update()\n",
        "        for mod in prefs['custom_LoRA_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            weights = mod['weights'] if 'weights' in mod else \"\"\n",
        "            custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, \"LoRA\", weights=weights))\n",
        "        #custom_LoRA_models.update()\n",
        "        for mod in prefs['custom_SDXL_LoRA_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            weights = mod['weights'] if 'weights' in mod else \"\"\n",
        "            custom_SDXL_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, \"SDXL LoRA\", weights=weights))\n",
        "        for mod in prefs['custom_dance_diffusion_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], token, \"DanceDiffusion\"))\n",
        "        #custom_dance_diffusion_models.update()\n",
        "        for mod in prefs['tortoise_custom_voices']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], token, \"Tortoise\"))\n",
        "        #tortoise_custom_voices.update()\n",
        "    def model_list(type):\n",
        "        if type == \"Finetuned\":\n",
        "            return prefs[\"custom_models\"]\n",
        "        elif type == \"LoRA\":\n",
        "            return prefs[\"custom_LoRA_models\"]\n",
        "        elif type == \"SDXL LoRA\":\n",
        "            return prefs[\"custom_SDXL_LoRA_models\"]\n",
        "        elif type == \"DanceDiffusion\":\n",
        "            return prefs[\"custom_dance_diffusion_models\"]\n",
        "        elif type == \"Tortoise\":\n",
        "            return prefs[\"tortoise_custom_voices\"]\n",
        "    def edit_model(e, name):\n",
        "        #name = e.control.title.controls[0].value\n",
        "        #path = e.control.title.controls[1].value\n",
        "        type = e.control.data\n",
        "        mod = None\n",
        "        for sub in model_list(type):\n",
        "            if sub['name'] == name:\n",
        "                mod = sub\n",
        "                break\n",
        "        if mod is None: return\n",
        "        path = mod['path' if type != \"Tortoise\" else 'folder']\n",
        "        if 'LoRA' in type:\n",
        "            weights = mod['weights'] if 'LoRA' in type and 'weights' in mod else \"\"\n",
        "        else: weights = None\n",
        "        #print(str(mod))\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_model(e):\n",
        "            nonlocal weights\n",
        "            if type == \"Finetuned\":\n",
        "                for m in custom_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            elif type == \"LoRA\":\n",
        "                for m in custom_LoRA_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.title.controls[2].value = model_weights.value\n",
        "                        m.update()\n",
        "            elif type == \"SDXL LoRA\":\n",
        "                for m in custom_SDXL_LoRA_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.title.controls[2].value = model_weights.value\n",
        "                        m.update()\n",
        "            elif type == \"DanceDiffusion\":\n",
        "                for m in custom_dance_diffusion_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            elif type == \"Tortoise\":\n",
        "                for m in tortoise_custom_voices.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            mod['name'] = model_name.value\n",
        "            mod['path' if type != \"Tortoise\" else 'folder'] = model_path.value\n",
        "            if weights != None:\n",
        "                mod['weights'] = model_weights.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        model_name = TextField(label=\"Custom Model Name\", value=name)\n",
        "        model_path = TextField(label=\"Model Path\", value=path)\n",
        "        model_weights = TextField(label=\"Model Weights (safetensor)\", value=weights or \"\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit {type} Model Info\"), content=Container(Column([model_name, model_path, model_weights if weights != None else Container(content=None)], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Model \", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def add_model(type):\n",
        "        mod_list = model_list(type)\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_model(e):\n",
        "            mod = {'name': model_name.value, 'path' if type != \"Tortoise\" else 'folder': model_path.value}\n",
        "            if 'LoRA' in type:\n",
        "                mod['weights'] = model_weights.value\n",
        "            mod_list.append(mod)\n",
        "            if type == \"Finetuned\":\n",
        "                custom_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"Finetuned\"))\n",
        "                custom_models.update()\n",
        "            elif type == \"LoRA\":\n",
        "                weights = mod['weights'] if 'weights' in mod else \"\"\n",
        "                custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"LoRA\", weights=weights))\n",
        "                custom_LoRA_models.update()\n",
        "            elif type == \"SDXL LoRA\":\n",
        "                weights = mod['weights'] if 'weights' in mod else \"\"\n",
        "                custom_SDXL_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"SDXL LoRA\", weights=weights))\n",
        "                custom_SDXL_LoRA_models.update()\n",
        "            elif type == \"DanceDiffusion\":\n",
        "                custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"DanceDiffusion\"))\n",
        "                custom_dance_diffusion_models.update()\n",
        "            elif type == \"Tortoise\":\n",
        "                tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], \"\", \"Tortoise\"))\n",
        "                tortoise_custom_voices.update()\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        model_name = TextField(label=\"Custom Model Name\")\n",
        "        model_path = TextField(label=\"Model Path\")\n",
        "        model_weights = TextField(label=\"Model Weights (safetensor)\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Add Custom {type} Model\"), content=Container(Column([model_name, model_path, model_weights if 'LoRA' in type else Container(content=None)], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Model \", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def del_model(e, name):\n",
        "        type = e.control.data\n",
        "        mod_list = model_list(type)\n",
        "        for i, sub in enumerate(mod_list):\n",
        "            if sub['name'] == name:\n",
        "                mod = sub\n",
        "                del mod_list[i]\n",
        "                break\n",
        "        if type == \"Finetuned\":\n",
        "            for i, l in enumerate(custom_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_models.controls[i]\n",
        "                    custom_models.update()\n",
        "                    break\n",
        "        elif type == \"LoRA\":\n",
        "            for i, l in enumerate(custom_LoRA_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_LoRA_models.controls[i]\n",
        "                    custom_LoRA_models.update()\n",
        "                    break\n",
        "        elif type == \"SDXL LoRA\":\n",
        "            for i, l in enumerate(custom_SDXL_LoRA_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_SDXL_LoRA_models.controls[i]\n",
        "                    custom_SDXL_LoRA_models.update()\n",
        "                    break\n",
        "        elif type == \"DanceDiffusion\":\n",
        "            for i, l in enumerate(custom_dance_diffusion_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_dance_diffusion_models.controls[i]\n",
        "                    custom_dance_diffusion_models.update()\n",
        "                    break\n",
        "        elif type == \"Tortoise\":\n",
        "            for i, l in enumerate(tortoise_custom_voices.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del tortoise_custom_voices.controls[i]\n",
        "                    tortoise_custom_voices.update()\n",
        "                    break\n",
        "        if prefs['enable_sounds']: e.page.snd_delete.play()\n",
        "    def update_list(e):\n",
        "        page.finetuned_model.options.clear()\n",
        "        for cust in model_list(\"Finetuned\"):\n",
        "            page.finetuned_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in finetuned_models:\n",
        "            page.finetuned_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        try: page.finetuned_model.update()\n",
        "        except: pass\n",
        "        page.LoRA_model.options.clear()\n",
        "        for cust in model_list(\"LoRA\"):\n",
        "            page.LoRA_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in LoRA_models:\n",
        "            page.LoRA_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        page.LoRA_model.options.append(dropdown.Option(\"Custom LoRA Path\"))\n",
        "        try: page.LoRA_model.update()\n",
        "        except: pass\n",
        "        page.SDXL_LoRA_model.options.clear()\n",
        "        for cust in model_list(\"SDXL LoRA\"):\n",
        "            page.SDXL_LoRA_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in SDXL_LoRA_models:\n",
        "            page.SDXL_LoRA_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        page.SDXL_LoRA_model.options.append(dropdown.Option(\"Custom SDXL LoRA Path\"))\n",
        "        try: page.SDXL_LoRA_model.update()\n",
        "        except: pass\n",
        "        page.community_dance_diffusion_model.options.clear()\n",
        "        for cust in model_list(\"DanceDiffusion\"):\n",
        "            page.community_dance_diffusion_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in community_dance_diffusion_models:\n",
        "            page.community_dance_diffusion_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        try: page.community_dance_diffusion_model.update()\n",
        "        except: pass\n",
        "        page.tortoise_voices.controls.clear()\n",
        "        for v in tortoise_prefs['voices']:\n",
        "            page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "        if len(prefs['tortoise_custom_voices']) > 0:\n",
        "            for custom in prefs['tortoise_custom_voices']:\n",
        "                page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "        try: page.tortoise_voices.update()\n",
        "        except: pass\n",
        "        save_settings_file(e.page)\n",
        "        if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    custom_models = Column([], spacing=0)\n",
        "    custom_LoRA_models = Column([], spacing=0)\n",
        "    custom_SDXL_LoRA_models = Column([], spacing=0)\n",
        "    custom_dance_diffusion_models = Column([], spacing=0)\n",
        "    tortoise_custom_voices = Column([], spacing=0)\n",
        "    load_customs()\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßß   Manage your Saved Custom Models\", \"Add or Edit your favorite models from HuggingFace, URL or Local Path\"),\n",
        "        title_header(\"Custom Finetuned Models\", \"Finetuned\"),\n",
        "        custom_models,\n",
        "        title_header(\"Custom LoRA Models\", \"LoRA\"),\n",
        "        custom_LoRA_models,\n",
        "        title_header(\"Custom SDXL LoRA Models\", \"SDXL LoRA\"),\n",
        "        custom_SDXL_LoRA_models,\n",
        "        title_header(\"Custom Tortoise Voice Models\", \"Tortoise\"),\n",
        "        tortoise_custom_voices,\n",
        "        title_header(\"Custom Dance Diffusion Models\", \"DanceDiffusion\"),\n",
        "        custom_dance_diffusion_models,\n",
        "        ElevatedButton(content=Text(\"üõÑ  Update Custom Dropdowns\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=update_list),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def get_directory_size(directory):\n",
        "    total = 0\n",
        "    for entry in os.scandir(directory):\n",
        "        if entry.is_file():\n",
        "            total += entry.stat().st_size\n",
        "        elif entry.is_dir():\n",
        "            try:\n",
        "                total += get_directory_size(entry.path)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    return total\n",
        "def convert_bytes(num):\n",
        "    step_unit = 1000.0 #1024 bad the size\n",
        "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if num < step_unit:\n",
        "            return \"%3.1f %s\" % (num, x)\n",
        "        num /= step_unit\n",
        "\n",
        "def buildCachedModelManager(page):\n",
        "    global prefs\n",
        "    def scan_cache(e):\n",
        "      if not bool(prefs['cache_dir']):\n",
        "        alert_msg(page, \"You haven't set a Cache Directory in your Settings...\")\n",
        "        return\n",
        "      elif not os.path.isdir(prefs['cache_dir']):\n",
        "        alert_msg(page, \"The Cache Directory in your Settings can't be found...\")\n",
        "        return\n",
        "      if len(page.cached_folders.controls) > 1:\n",
        "        page.cached_folders.controls.clear()\n",
        "        page.cached_folders.update()\n",
        "      page.cached_folders.controls.append(Installing(f\"Scanning {prefs['cache_dir']}\"))\n",
        "      dirs = [f.path for f in os.scandir(prefs['cache_dir']) if f.is_dir()]\n",
        "      del page.cached_folders.controls[-1]\n",
        "      page.cached_folders.update()\n",
        "      for dir in dirs:\n",
        "        page.cached_folders.controls.append(ListTile(title=Row([Text(f\".{slash}{dir.rpartition(slash)[2]}\", weight=FontWeight.BOLD), Text(\"\")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.DELETE, text=\"Delete Model Directory\", on_click=del_dir, data=dir)])))\n",
        "        page.cached_folders.update()\n",
        "      for l in page.cached_folders.controls:\n",
        "        size = convert_bytes(get_directory_size(l.data))\n",
        "        l.title.controls[1].value = size\n",
        "        l.title.controls[1].update()\n",
        "        page.cached_folders.update()\n",
        "    def del_dir(e):\n",
        "      dir = e.control.data\n",
        "      shutil.rmtree(dir, ignore_errors=True)\n",
        "      for i, l in enumerate(page.cached_folders.controls):\n",
        "        if l.data == dir:\n",
        "          del page.cached_folders.controls[i]\n",
        "          page.cached_folders.update()\n",
        "          break\n",
        "      if prefs['enable_sounds']: e.page.snd_delete.play()\n",
        "    page.cached_folders = Column([])\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üóÇÔ∏è   Manage your Cache Directory Saved Models\", \"If you're cacheing your model files, it can fill up your drive space quickly, so you can trim the fat as needed... Redownloads when used.\"),\n",
        "        page.cached_folders,\n",
        "        ElevatedButton(content=Text(\"üîç  Scan Cache Dirctory\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=scan_cache),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "#if 'pipe' in locals():\n",
        "#    clear_pipes()\n",
        "use_custom_scheduler = False\n",
        "retry_attempts_if_NSFW = 3\n",
        "unet = None\n",
        "pipe = None\n",
        "pipe_img2img = None\n",
        "pipe_SD = None\n",
        "pipe_SDXL = None\n",
        "pipe_SDXL_refiner = None\n",
        "compel_proc = None\n",
        "compel_base = None\n",
        "compel_refiner = None\n",
        "pipe_interpolation = None\n",
        "pipe_clip_guided = None\n",
        "pipe_conceptualizer = None\n",
        "pipe_repaint = None\n",
        "pipe_imagic = None\n",
        "pipe_composable = None\n",
        "pipe_safe = None\n",
        "pipe_versatile = None\n",
        "pipe_versatile_text2img = None\n",
        "pipe_versatile_variation = None\n",
        "pipe_versatile_dualguided = None\n",
        "pipe_upscale = None\n",
        "pipe_depth = None\n",
        "pipe_image_variation = None\n",
        "pipe_semantic = None\n",
        "pipe_DiffEdit = None\n",
        "pipe_EDICT = None\n",
        "text_encoder_EDICT = None\n",
        "pipe_null_text = None\n",
        "pipe_unCLIP = None\n",
        "pipe_unCLIP_image_variation = None\n",
        "pipe_unCLIP_interpolation = None\n",
        "pipe_unCLIP_image_interpolation = None\n",
        "pipe_wuerstchen = None\n",
        "pipe_pixart_alpha = None\n",
        "pipe_pixart_alpha_encoder = None\n",
        "pipe_magic_mix = None\n",
        "pipe_paint_by_example = None\n",
        "pipe_instruct_pix2pix = None\n",
        "pipe_alt_diffusion = None\n",
        "pipe_alt_diffusion_img2img = None\n",
        "pipe_demofusion = None\n",
        "pipe_SAG = None\n",
        "pipe_attend_and_excite = None\n",
        "pipe_lmd_plus = None\n",
        "pipe_lcm = None\n",
        "pipe_lcm_interpolation = None\n",
        "pipe_ldm3d = None\n",
        "pipe_ldm3d_upscale = None\n",
        "pipe_svd = None\n",
        "pipe_panorama = None\n",
        "pipe_DiT = None\n",
        "pipe_dance = None\n",
        "pipe_kandinsky = None\n",
        "pipe_kandinsky_prior = None\n",
        "pipe_tortoise_tts = None\n",
        "pipe_audio_ldm = None\n",
        "pipe_audio_ldm2 = None\n",
        "pipe_music_ldm = None\n",
        "pipe_riffusion = None\n",
        "pipe_audio_diffusion = None\n",
        "pipe_music_gen = None\n",
        "pipe_voice_fixer = None\n",
        "pipe_whisper = None\n",
        "pipe_text_to_video = None\n",
        "pipe_text_to_video_zero = None\n",
        "pipe_video_to_video = None\n",
        "pipe_infinite_zoom = None\n",
        "pipe_deepfloyd = None\n",
        "pipe_deepfloyd2 = None\n",
        "pipe_deepfloyd3 = None\n",
        "pipe_amused = None\n",
        "pipe_gpt2 = None\n",
        "pipe_distil_gpt2 = None\n",
        "pipe_background_remover = None\n",
        "pipe_shap_e = None\n",
        "pipe_zoe_depth = None\n",
        "pipe_marigold_depth = None\n",
        "pipe_stable_lm = None\n",
        "tokenizer_stable_lm = None\n",
        "depth_estimator = None\n",
        "fuyu_tokenizer = None\n",
        "fuyu_model = None\n",
        "fuyu_processor = None\n",
        "pipe_blip_diffusion = None\n",
        "pipe_anytext = None\n",
        "pipe_reference = None\n",
        "pipe_ip_adapter = None\n",
        "pipe_controlnet_qr = None\n",
        "pipe_controlnet_segment = None\n",
        "pipe_kandinsky_controlnet_prior = None\n",
        "pipe_controlnet = None\n",
        "controlnet = None\n",
        "controlnet_models = {\"Canny Map Edge\":None, \"Scribble\":None, \"OpenPose\":None, \"Depth\":None, \"HED\":None, \"M-LSD\":None, \"Normal Map\":None, \"Segmented\":None, \"LineArt\":None, \"Shuffle\":None, \"Instruct Pix2Pix\":None}\n",
        "controlnet_xl_models = {\"Canny Map Edge\":None, \"OpenPose\":None, \"Depth\":None, \"Softedge\":None, \"Segmented\":None, \"LineArt\":None, \"Shuffle\":None, \"Instruct Pix2Pix\":None}\n",
        "stability_api = None\n",
        "safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}\n",
        "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "inpaint_model = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "#\"runwayml/stable-diffusion-inpainting\"\n",
        "scheduler = None\n",
        "scheduler_clip = None\n",
        "if is_Colab:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "def get_model(name):\n",
        "  #dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\", dropdown.Option(\"Community Finetuned Model\", dropdown.Option(\"DreamBooth Library Model\"), dropdown.Option(\"Custom Model Path\")\n",
        "  if name == \"Stable Diffusion v2.1 x768\":\n",
        "    return {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.1 x512\":\n",
        "    return {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base', 'prefix':''}\n",
        "  elif name == \"Stable Diffusion v2.0\":\n",
        "    return {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.0 x768\":\n",
        "    return {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.0 x512\":\n",
        "    return {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v1.5\":\n",
        "    return {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v1.4\":\n",
        "    return {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Community Finetuned Model\":\n",
        "    return get_finetuned_model(prefs['finetuned_model'])\n",
        "  elif name == \"DreamBooth Library Model\":\n",
        "    return get_dreambooth_model(prefs['dreambooth_model'])\n",
        "  elif name == \"Custom Model Path\":\n",
        "    return {'name':'Custom Model', 'path':prefs['custom_model'], 'prefix':''}\n",
        "  else:\n",
        "    return {'name':'', 'path':'', 'prefix':''}\n",
        "\n",
        "def get_finetuned_model(name):\n",
        "  for mod in finetuned_models:\n",
        "      if mod['name'] == name:\n",
        "        return mod\n",
        "  for mod in prefs['custom_models']:\n",
        "      if mod['name'] == name:\n",
        "        return mod\n",
        "  return {'name':'', 'path':'', 'prefix':''}\n",
        "def get_dreambooth_model(name):\n",
        "  for mod in dreambooth_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':f'sd-dreambooth-library/{mod[\"name\"]}', 'prefix':mod['token']}\n",
        "  return {'name':'', 'path':'', 'prefix':''}\n",
        "def get_LoRA_model(name):\n",
        "  if name == \"Custom LoRA Path\":\n",
        "      return {'name':\"Custom LoRA Model\", 'path':prefs['custom_LoRA_model'], 'weights':None}\n",
        "  for mod in LoRA_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}\n",
        "  if len(prefs['custom_LoRA_models']) > 0:\n",
        "    for mod in prefs['custom_LoRA_models']:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}\n",
        "  return {'name':'', 'path':''}\n",
        "def get_SDXL_LoRA_model(name):\n",
        "  if name == \"Custom SDXL LoRA Path\":\n",
        "      return {'name':\"Custom SDXL LoRA Model\", 'path':prefs['custom_SDXL_LoRA_model'], 'weights':None}\n",
        "  for mod in SDXL_LoRA_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}\n",
        "  if len(prefs['custom_SDXL_LoRA_models']) > 0:\n",
        "    for mod in prefs['custom_SDXL_LoRA_models']:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}\n",
        "  return {'name':'', 'path':''}\n",
        "def get_SDXL_model(name):\n",
        "  if name == \"Custom Model\":\n",
        "      return {'name':\"Custom SDXL Model\", 'path':prefs['SDXL_custom_model'], 'prefix':'', 'revision': 'fp16'}\n",
        "  for mod in SDXL_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path'], 'prefix':mod['prefix'], 'revision': mod['variant'] if 'variant' in mod else mod['revision'] if 'variant' in mod else 'fp16'}\n",
        "\n",
        "HFapi = None\n",
        "def get_diffusers(page):\n",
        "    global scheduler, model_path, prefs, status, HFapi\n",
        "    torch_installed = False\n",
        "    try:\n",
        "        import torch\n",
        "        torch_installed = True\n",
        "    except:\n",
        "        pass\n",
        "    if torch_installed:\n",
        "        if version.parse(torch.__version__) < version.parse(\"2.0.0\"):\n",
        "            torch_installed = False\n",
        "    if not torch_installed:\n",
        "        import importlib\n",
        "        page.console_msg(\"Upgrading Torch 2.1.0 Packages... You may need to restart session.\")\n",
        "        run_process(\"pip uninstall --yes torch torchaudio torchvision torchtext torchdata\", page=page)\n",
        "        if is_Colab:\n",
        "            run_process(\"pip install torch torchaudio torchvision torchtext torchdata\", page=page)\n",
        "        else: #TODO: Check OS and run platform specific\n",
        "            run_process(\"pip install torch torchvision torchaudio torchtext torchdata --index-url https://download.pytorch.org/whl/cu121\", page=page)\n",
        "        import torch\n",
        "        importlib.reload(torch)\n",
        "    if prefs['enable_xformers']:#prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':\n",
        "        try:\n",
        "            import xformers\n",
        "            if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "        except ModuleNotFoundError:\n",
        "            page.console_msg(\"Installing FaceBook's Xformers Memory Efficient Package...\")\n",
        "            run_process(\"pip install --pre -U triton\", page=page)\n",
        "            run_process(\"pip install -U xformers==0.0.18\", page=page)\n",
        "            import xformers\n",
        "            page.console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "            pass\n",
        "        #run_process(\"pip install pyre-extensions==0.0.23\", page=page)\n",
        "        #run_process(\"pip install -i https://test.pypi.org/simple/ formers==0.0.15.dev376\", page=page)\n",
        "        #run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "        #run_process(\"pip install https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\", page=page)\n",
        "        #if install_xformers(page):\n",
        "        status['installed_xformers'] = True\n",
        "    page.console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "    if prefs['enable_bitsandbytes']:\n",
        "        try:\n",
        "            os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "            import bitsandbytes\n",
        "        except ModuleNotFoundError:\n",
        "            page.status(\"...installing bitsandbytes\")\n",
        "            if sys.platform.startswith(\"win\"):\n",
        "                run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "            else:\n",
        "                run_sp(\"pip install bitsandbytes\", realtime=False)\n",
        "            import bitsandbytes\n",
        "            page.status()\n",
        "            pass\n",
        "        pip_install(\"sentencepiece\")\n",
        "    try:\n",
        "        import imwatermark\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing invisible_watermark\")\n",
        "        run_sp(\"pip install invisible-watermark\", realtime=False) #pip install --no-deps invisible-watermark>=0.2.0\n",
        "        page.status()\n",
        "        pass\n",
        "    try:\n",
        "        import peft\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing peft\")\n",
        "        run_sp(\"pip install --upgrade git+https://github.com/huggingface/peft.git\", realtime=False)\n",
        "        page.status()\n",
        "        pass\n",
        "    \n",
        "    page.console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "    try:\n",
        "        import transformers\n",
        "        #print(f\"transformers=={transformers.__version__}\")\n",
        "        if version.parse(transformers.__version__) == version.parse(\"4.21.3\"): #Workaround because CLIP-Interrogator required other version\n",
        "            page.status(\"...uninstalling transformers\")\n",
        "            run_process(\"pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "            run_process(\"pip uninstall -y clip-interrogator\", realtime=False)\n",
        "            run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "        elif version.parse(transformers.__version__).base_version < version.parse(\"4.37.0\").base_version:\n",
        "          import importlib\n",
        "          page.status(f\"...uninstalling transformers {transformers.__version__}\")\n",
        "          run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "          page.status(\"...installing transformers\")\n",
        "          run_process(\"pip install --upgrade git+https://github.com/huggingface/transformers.git@main#egg=transformers[sentencepiece]\", page=page)\n",
        "          importlib.reload(transformers)\n",
        "    except ModuleNotFoundError:\n",
        "        pass\n",
        "    try:\n",
        "        import transformers\n",
        "        #if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing transformers\")\n",
        "        run_process(\"pip install --upgrade git+https://github.com/huggingface/transformers.git@main#egg=transformers[sentencepiece]\", page=page)\n",
        "        #run_process(\"pip install --upgrade transformers~=4.28\", page=page)\n",
        "        page.status()\n",
        "        pass\n",
        "    #run_process(\"pip install -q huggingface_hub\", page=page)\n",
        "    '''try:\n",
        "      from huggingface_hub import notebook_login, HfApi, HfFolder, login\n",
        "      from diffusers import StableDiffusionPipeline, logging\n",
        "      import transformers\n",
        "    except ModuleNotFoundError as e:#ModuleNotFoundError as e:'''\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing accelerate\")\n",
        "        run_sp(\"pip install git+https://github.com/huggingface/accelerate.git\", realtime=False)\n",
        "        import accelerate\n",
        "        page.status()\n",
        "        pass\n",
        "    try:\n",
        "        import diffusers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing diffusers\")\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git\", page=page)\n",
        "        run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]\", page=page)\n",
        "        page.status()\n",
        "        pass\n",
        "    if prefs['enable_deepcache']:\n",
        "        try:\n",
        "            import DeepCache\n",
        "        except ModuleNotFoundError:\n",
        "            page.status(\"...installing DeepCache\")\n",
        "            run_process(\"pip install -upgrade DeepCache\", page=page)\n",
        "            page.status()\n",
        "            pass\n",
        "    try:\n",
        "        import scipy\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing scipy\")\n",
        "        run_process(\"pip install -upgrade scipy\", page=page)\n",
        "        page.status()\n",
        "        pass\n",
        "    try:\n",
        "        import ftfy\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing ftfy\")\n",
        "        run_process(\"pip install --upgrade ftfy\", page=page)\n",
        "        page.status()\n",
        "        pass\n",
        "    try:\n",
        "        import safetensors\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing safetensors\")\n",
        "        run_process(\"pip install --upgrade safetensors~=0.3\", page=page)\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "        page.status()\n",
        "        pass\n",
        "    try:\n",
        "        import ipywidgets\n",
        "    except ModuleNotFoundError:\n",
        "        page.status(\"...installing ipywidgets\")\n",
        "        run_process('pip install -qq \"ipywidgets>=7,<8\"', page=page)\n",
        "        page.status()\n",
        "        pass\n",
        "\n",
        "    from huggingface_hub import notebook_login, HfApi, HfFolder, login\n",
        "    #from diffusers import StableDiffusionPipeline, logging\n",
        "    from diffusers import logging\n",
        "    if not os.path.exists(HfFolder.path_token):\n",
        "        run_process(\"git config --global credential.helper store\", page=page)\n",
        "    logging.set_verbosity_error()\n",
        "    if not os.path.exists(HfFolder.path_token):\n",
        "        #from huggingface_hub.commands.user import _login\n",
        "        #_login(HfApi(), token=prefs['HuggingFace_api_key'])\n",
        "        try:\n",
        "          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)\n",
        "        except Exception:\n",
        "          alert_msg(page, \"ERROR Logging into HuggingFace... Check your API Key or Internet conenction.\")\n",
        "          return\n",
        "    # TODO: Get Username to prefs\n",
        "    HFapi = HfApi()\n",
        "    prefs['HuggingFace_username'] = HFapi.whoami()[\"name\"]\n",
        "    #if prefs['model_ckpt'] == \"Stable Diffusion v1.5\": model_path =  \"runwayml/stable-diffusion-v1-5\"\n",
        "    #elif prefs['model_ckpt'] == \"Stable Diffusion v1.4\": model_path =  \"CompVis/stable-diffusion-v1-4\"\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    model_path = model['path']\n",
        "    #try:\n",
        "    #  scheduler = model_scheduler(model_path)\n",
        "    #except Exception as e:\n",
        "    #  alert_msg(page, f\"ERROR: {prefs['scheduler_mode']} Scheduler couldn't load for {model_path}\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "    #  pass\n",
        "    status['finetuned_model'] = False if model['name'].startswith(\"Stable\") else True\n",
        "\n",
        "def save_file(file, folder=\"\", filename=None, media=\"images\"):\n",
        "    if 'save_to_HF' in prefs and prefs['save_to_HF']:\n",
        "        upload_HF(file=file, folder=folder, filename=filename, media=media)\n",
        "    \n",
        "def upload_HF(file, folder=\"\", filename=None, media=\"images\"):\n",
        "    global HFapi\n",
        "    import huggingface_hub\n",
        "    from huggingface_hub import HfApi\n",
        "    if HFapi == None:\n",
        "        HFapi = HfApi()\n",
        "    if 'HuggingFace_username' not in prefs or prefs['HuggingFace_username'] == \"\":\n",
        "        prefs['HuggingFace_username'] = HFapi.whoami()[\"name\"]\n",
        "    repo_name = f\"{prefs['HuggingFace_username']/{media}}\"\n",
        "    if folder==\"\":\n",
        "        path=filename\n",
        "    else:\n",
        "        path=f\"{folder}/{filename}\"\n",
        "    if filename==None:\n",
        "        print(\"upload_HF Error: Must give filename\")\n",
        "    try:\n",
        "        HFapi.upload_file(\n",
        "            path_or_fileobj=file,\n",
        "            path_in_repo=path,#f\"{folder}{filename}\",#path.split(\"/\")[-1],\n",
        "            repo_id=repo_name,\n",
        "            repo_type=\"dataset\",\n",
        "        )\n",
        "    except huggingface_hub.utils._errors.RepositoryNotFoundError:\n",
        "        HFapi.create_repo(\n",
        "            repo_id=repo_name,\n",
        "            repo_type=\"dataset\",\n",
        "            private=True,\n",
        "        )\n",
        "        try:\n",
        "            HFapi.upload_file(\n",
        "                path_or_fileobj=file,\n",
        "                path_in_repo=path,\n",
        "                repo_id=repo_name,\n",
        "                repo_type=\"dataset\",\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Create upload_HF Error: {e}\")\n",
        "            return None\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"upload_HF Error: {e}\")\n",
        "        return None\n",
        "    print(f\"Saved https://huggingface.co/datasets/{repo_name}/blob/main/{path}\")\n",
        "    return f\" https://huggingface.co/datasets/{repo_name}/blob/main/{path}\"\n",
        "\n",
        "def model_scheduler(model, big3=False):\n",
        "    scheduler_mode = prefs['scheduler_mode']\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      s = PNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif big3:\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DPM Solver\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DPM Solver Singlestep\":\n",
        "      from diffusers import DPMSolverSinglestepScheduler\n",
        "      s = DPMSolverSinglestepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DPM Solver Inverse\":\n",
        "      from diffusers import DPMSolverMultistepInverseScheduler\n",
        "      s = DPMSolverMultistepInverseScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-Euler Discrete\":\n",
        "      from diffusers import EulerDiscreteScheduler\n",
        "      s = EulerDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-Euler Ancestral\":\n",
        "      from diffusers import EulerAncestralDiscreteScheduler\n",
        "      s = EulerAncestralDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"Karras-LMS\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "      scheduler_config = s.get_scheduler_config()\n",
        "      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"DPM Stochastic\":\n",
        "      from diffusers import DPMSolverSDEScheduler\n",
        "      s = DPMSolverSDEScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"SDE-DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "      s.config.algorithm_type = 'sde-dpmsolver++'\n",
        "    elif scheduler_mode == \"DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\",\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        #predict_epsilon=True,\n",
        "        prediction_type=\"v_prediction\" if model.startswith('stabilityai') else \"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        solver_order=2,\n",
        "        #denoise_final=True,\n",
        "        lower_order_final=True,\n",
        "      )\n",
        "    elif scheduler_mode == \"Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"Karras Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\", use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"K-DPM2 Ancestral\":\n",
        "      from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "      s = KDPM2AncestralDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-DPM2 Discrete\":\n",
        "      from diffusers import KDPM2DiscreteScheduler\n",
        "      s = KDPM2DiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"IPNDM\":\n",
        "      from diffusers import IPNDMScheduler\n",
        "      s = IPNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DEIS Multistep\":\n",
        "      from diffusers import DEISMultistepScheduler\n",
        "      s = DEISMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"UniPC Multistep\":\n",
        "      from diffusers import UniPCMultistepScheduler\n",
        "      s = UniPCMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    #elif scheduler_mode == \"Score-SDE-Vp\":\n",
        "    #  from diffusers import ScoreSdeVpScheduler\n",
        "    #  s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format=\"np\")\n",
        "    #  use_custom_scheduler = True\n",
        "    #elif scheduler_mode == \"Score-SDE-Ve\":\n",
        "    #  from diffusers import ScoreSdeVeScheduler\n",
        "    #  s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format=\"pt\"\n",
        "    #  use_custom_scheduler = True\n",
        "    #elif scheduler_mode == \"Karras-Ve\":\n",
        "    #  from diffusers import KarrasVeScheduler\n",
        "    #  s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format=\"pt\")\n",
        "    #  use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"DDPM\":\n",
        "      from diffusers import DDPMScheduler\n",
        "      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, variance_type=\"fixed_small\", clip_sample=True, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"LMS\": #no more\n",
        "      from diffusers import LMSScheduler\n",
        "      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, timestep_values=None, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    #print(f\"Loaded Schedueler {scheduler_mode} {type(scheduler)}\")\n",
        "    else:\n",
        "      print(f\"Unknown scheduler request {scheduler_mode} - Using LMS Discrete\")\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    return s\n",
        "\n",
        "def pipeline_scheduler(p, big3=False, from_scheduler = True, scheduler=None, trailing=False):\n",
        "    global status\n",
        "    scheduler_mode = prefs['scheduler_mode'] if scheduler is None else scheduler\n",
        "    args = {} if not trailing else {'timestep_spacing': 'trailing'}\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      s = PNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif big3:\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DPM Solver\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DPM Solver Singlestep\":\n",
        "      from diffusers import DPMSolverSinglestepScheduler\n",
        "      s = DPMSolverSinglestepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DPM Solver Inverse\":\n",
        "      from diffusers import DPMSolverMultistepInverseScheduler\n",
        "      s = DPMSolverMultistepInverseScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DPM Stochastic\":\n",
        "      from diffusers import DPMSolverSDEScheduler\n",
        "      s = DPMSolverSDEScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"SDE-DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "      s.config.algorithm_type = 'sde-dpmsolver++'\n",
        "    elif scheduler_mode == \"K-Euler Discrete\":\n",
        "      from diffusers import EulerDiscreteScheduler\n",
        "      s = EulerDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"K-Euler Ancestral\":\n",
        "      from diffusers import EulerAncestralDiscreteScheduler\n",
        "      s = EulerAncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"Karras-LMS\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "      scheduler_config = s.get_scheduler_config()\n",
        "      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True, **args)\n",
        "    elif scheduler_mode == \"DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      try:\n",
        "        p_model = p.model\n",
        "      except Exception:\n",
        "        p_model = \"\"\n",
        "        pass\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config,\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        #predict_epsilon=True,\n",
        "        prediction_type=\"v_prediction\" if p_model.startswith('stabilityai') else \"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        solver_order=2,\n",
        "        #denoise_final=True,\n",
        "        lower_order_final=True,\n",
        "        **args\n",
        "      )\n",
        "    elif scheduler_mode == \"Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"Karras Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, use_karras_sigmas=True, **args)\n",
        "    elif scheduler_mode == \"K-DPM2 Ancestral\":\n",
        "      from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "      s = KDPM2AncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"K-DPM2 Discrete\":\n",
        "      from diffusers import KDPM2DiscreteScheduler\n",
        "      s = KDPM2DiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"IPNDM\":\n",
        "      from diffusers import IPNDMScheduler\n",
        "      s = IPNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"DEIS Multistep\":\n",
        "      from diffusers import DEISMultistepScheduler\n",
        "      s = DEISMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"UniPC Multistep\":\n",
        "      from diffusers import UniPCMultistepScheduler\n",
        "      s = UniPCMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    elif scheduler_mode == \"LCM\":\n",
        "      from diffusers import LCMScheduler\n",
        "      s = LCMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)\n",
        "    #elif scheduler_mode == \"Score-SDE-Vp\":\n",
        "    #  from diffusers import ScoreSdeVpScheduler\n",
        "    #  s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format=\"np\")\n",
        "    #  use_custom_scheduler = True\n",
        "    #elif scheduler_mode == \"Score-SDE-Ve\":\n",
        "    #  from diffusers import ScoreSdeVeScheduler\n",
        "    #  s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format=\"pt\"\n",
        "    #  use_custom_scheduler = True\n",
        "    #elif scheduler_mode == \"Karras-Ve\":\n",
        "    #  from diffusers import KarrasVeScheduler\n",
        "    #  s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format=\"pt\")\n",
        "    #  use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"DDPM\":\n",
        "      from diffusers import DDPMScheduler\n",
        "      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, variance_type=\"fixed_small\", clip_sample=True, tensor_format=\"pt\", **args)\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"LMS\": #no more\n",
        "      from diffusers import LMSScheduler\n",
        "      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", **args)\n",
        "      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, timestep_values=None, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    #print(f\"Loaded Schedueler {scheduler_mode} {type(scheduler)}\")\n",
        "    else:\n",
        "      print(f\"Unknown scheduler request {scheduler_mode} - Using LMS Discrete\")\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config, **args)\n",
        "    p.scheduler = s\n",
        "    status['loaded_scheduler'] = scheduler_mode\n",
        "    return p\n",
        "\n",
        "#if is_Colab:\n",
        "#    os.remove(\"/usr/local/lib/python3.8/dist-packages/torch/lib/libcudnn.so.8\")\n",
        "#    download_file(\"https://github.com/Skquark/diffusers/blob/main/utils/libcudnn.so.8?raw=true\", to=\"/usr/local/lib/python3.8/dist-packages/torch/lib/\")\n",
        "torch_device = \"cuda\"\n",
        "try:\n",
        "    from packaging import version\n",
        "    import torch\n",
        "    if version.parse(torch.__version__) < version.parse(\"2.0.1\") and torch.cuda.is_available():\n",
        "      raise ModuleNotFoundError(\"\")\n",
        "except ModuleNotFoundError:\n",
        "    #page.console_msg(\"Installing PyTorch with CUDA 1.17\")\n",
        "    print(\"Installing PyTorch 2.1.0 with CUDA 1.18\")\n",
        "    run_sp(\"pip install -U --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\", realtime=False)\n",
        "    #pip install --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "    #run_sp(\"pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117\", realtime=False)\n",
        "    try:\n",
        "      import torch\n",
        "    except ModuleNotFoundError:\n",
        "      print(\"Failed Installing Torch Packages...\")\n",
        "      run_sp(\"pip install -q torch\")\n",
        "      import torch\n",
        "      pass\n",
        "    pass\n",
        "finally:\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if torch_device == \"cpu\": print(\"WARNING: CUDA is only available with CPU, so GPU tasks are limited. Can use Stability-API, AIHorde & OpenAI, but not Diffusers...\")\n",
        "\n",
        "if torch_device == \"cuda\":\n",
        "    try:\n",
        "        import transformers\n",
        "        if version.parse(transformers.__version__).base_version < version.parse(\"4.37.0\").base_version:\n",
        "            import importlib\n",
        "            print(f\"Uninstalling old transformers v{transformers.__version__}\")\n",
        "            run_sp(\"pip uninstall -y transformers\", realtime=False)\n",
        "            print(\"Installing newest transformers package...\")\n",
        "            run_sp(\"pip install --upgrade -q git+https://github.com/huggingface/transformers.git\", realtime=True)\n",
        "            print(\"Installing newest accelerate package...\")\n",
        "            run_sp(\"pip install --upgrade -q git+https://github.com/huggingface/peft.git\", realtime=True)\n",
        "            run_sp(\"pip install --upgrade huggingface_hub\", realtime=False)\n",
        "            print(\"Restart Runtime to apply updates...\")\n",
        "            #importlib.reload(transformers)\n",
        "            #try:\n",
        "            #    sys.exit()\n",
        "            #except SystemExit:\n",
        "            raise SystemExit(\"Please Restart Session and run all again to Upgrade... Sorry, only workaround.\")\n",
        "    except ModuleNotFoundError:\n",
        "        pass\n",
        "\n",
        "if not is_Colab:\n",
        "    try:\n",
        "        subprocess.check_call([\"git\", \"--version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    except subprocess.CalledProcessError:\n",
        "        os_name = os.name.lower()\n",
        "        if os_name == \"posix\":\n",
        "            install_command = \"curl -fsSL https://git-scm.com/download/linux/latest.tar.gz | tar xz && mv git-*/bin/git /usr/bin/git\"\n",
        "        elif os_name == \"nt\":\n",
        "            install_command = \"curl -fsSL https://git-scm.com/download/Win64/Git-2.39.0-windows-x64.exe | tar xz && mv Git/bin/git.exe /usr/bin/git.exe\"\n",
        "        else:\n",
        "            print(f\"Unsupported OS for Git: {os_name}. Install manually.\")\n",
        "            pass\n",
        "        try:\n",
        "            print(\"Installing latest GIT library....\")\n",
        "            subprocess.run(install_command, shell=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Git installation failed on {os_name}. Please manually install it...\")\n",
        "        pass\n",
        "\n",
        "status['cpu_memory'] = psutil.virtual_memory().total / (1024 * 1024 * 1024)\n",
        "try:\n",
        "    status['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)\n",
        "except Exception:\n",
        "    status['gpu_memory'] = \"N/A\"\n",
        "    pass\n",
        "pb = ProgressBar(width=420, bar_height=8)\n",
        "total_steps = args['steps']\n",
        "def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "    callback_fn.has_been_called = True\n",
        "    global total_steps, pb\n",
        "    if total_steps is None: total_steps = timestep\n",
        "    if total_steps == 0: total_steps = len(latents)\n",
        "    multiplier = 1\n",
        "    if prefs['scheduler_mode'].startswith(\"Heun\") or prefs['scheduler_mode'].startswith(\"K-DPM\"):\n",
        "      multiplier = 2\n",
        "    percent = (step +1)/ (total_steps * multiplier)\n",
        "    pb.value = percent\n",
        "    pb.tooltip = f\"[{step +1} / {total_steps * multiplier}] (Timestep: {timestep})\"\n",
        "    #print(f\"step: {step}, total: {total_steps}, latent: {len(latents)}\")\n",
        "    #if step == 0:\n",
        "        #latents = latents.detach().cpu().numpy()\n",
        "        #assert latents.shape == (1, 4, 64, 64)\n",
        "        #latents_slice = latents[0, -3:, -3:, -1]\n",
        "        #expected_slice = np.array([1.8285, 1.2857, -0.1024, 1.2406, -2.3068, 1.0747, -0.0818, -0.6520, -2.9506])\n",
        "        #assert np.abs(latents_slice.flatten() - expected_slice).max() < 1e-3\n",
        "    pb.update()\n",
        "\n",
        "start_step = 0\n",
        "start_callback = 0\n",
        "def callback_step(pipe, i, t, callback_kwargs):\n",
        "    callback_step.has_been_called = True\n",
        "    global pb, start_step, start_callback, abort_run\n",
        "    now = time.time()\n",
        "    itsec = \"\"\n",
        "    try:\n",
        "        steps = pipe.num_timesteps\n",
        "    except:\n",
        "        global total_steps\n",
        "        steps = total_steps\n",
        "        pass\n",
        "    if i < 2:\n",
        "        start_callback = now\n",
        "    else:\n",
        "        itsec = f\" - {its(now - start_step)} - Elapsed: {elapsed(start_callback, now)}\"\n",
        "    start_step = now\n",
        "    percent = (i +1)/ (steps)\n",
        "    pb.value = percent\n",
        "    pb.tooltip = f\"[{i +1} / {steps}] (Timestep: {t}){itsec}\"\n",
        "    pb.update()\n",
        "    if abort_run:\n",
        "        pipe._interrupt = True\n",
        "    return callback_kwargs\n",
        "\n",
        "def optimize_pipe(p, vae_slicing=False, unet=False, no_cpu=False, vae_tiling=False, to_gpu=True, tome=True, torch_compile=True, model_offload=False, freeu=True, lora=True):\n",
        "    global prefs, status\n",
        "    if model_offload:\n",
        "      p.enable_model_cpu_offload()\n",
        "    #if prefs['memory_optimization'] == 'Attention Slicing':\n",
        "    if prefs['enable_attention_slicing']:\n",
        "      #if not model['name'].startswith('Stable Diffusion v2'): #TEMP hack until it updates my git with fix\n",
        "      if prefs['sequential_cpu_offload'] and not no_cpu:\n",
        "        p.enable_attention_slicing(1)\n",
        "      else:\n",
        "        p.enable_attention_slicing()#prefs['memory_optimization'] == 'Xformers Mem Efficient Attention'\n",
        "    elif prefs['enable_xformers'] and status['installed_xformers']:\n",
        "      #p.set_use_memory_efficient_attention_xformers(True)\n",
        "      p.enable_xformers_memory_efficient_attention()\n",
        "    elif prefs['enable_xformers']:\n",
        "      p.enable_attention_slicing()\n",
        "    if prefs['vae_slicing'] and vae_slicing:\n",
        "      p.enable_vae_slicing()\n",
        "    if prefs['vae_tiling'] and vae_tiling:\n",
        "      p.enable_vae_tiling()\n",
        "    if unet:\n",
        "      p.unet = torch.compile(p.unet)\n",
        "    if lora:\n",
        "      p = apply_LoRA(p)\n",
        "    '''if prefs['use_LoRA_model']:\n",
        "      lora = get_LoRA_model(prefs['LoRA_model'])\n",
        "      if bool(lora['weights']):\n",
        "        p.load_lora_weights(lora['path'], weight_name=lora['weights'])\n",
        "      else:\n",
        "        p.load_lora_weights(lora['path'])'''\n",
        "      #TODO: , weight_name=lora_filename\n",
        "      #p.unet.load_attn_procs(lora['path'])\n",
        "    if prefs['enable_freeu'] and freeu:\n",
        "      p.enable_freeu(**prefs['freeu_args'])\n",
        "    if prefs['sequential_cpu_offload'] and not no_cpu:\n",
        "      p.enable_sequential_cpu_offload()\n",
        "    else:\n",
        "      if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:\n",
        "        p = p.to(torch_device)\n",
        "    if prefs['enable_torch_compile'] and torch_compile:\n",
        "      p.unet.to(memory_format=torch.channels_last)\n",
        "      p.unet = torch.compile(p.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "    if prefs['enable_tome'] and tome:\n",
        "      try:\n",
        "        import tomesd\n",
        "      except Exception:\n",
        "        run_sp(\"pip install tomesd\", realtime=False)\n",
        "        import tomesd\n",
        "        pass\n",
        "      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])\n",
        "    if prefs['enable_deepcache']:\n",
        "        try:\n",
        "            from DeepCache import DeepCacheSDHelper\n",
        "        except Exception:\n",
        "            run_sp(\"pip install DeepCache\", realtime=False)\n",
        "            from DeepCache import DeepCacheSDHelper\n",
        "            pass\n",
        "        helper = DeepCacheSDHelper(pipe=p)\n",
        "        helper.set_params(cache_interval=3, cache_branch_id=0)\n",
        "        helper.enable()\n",
        "    status['loaded_scheduler'] = prefs['scheduler_mode']\n",
        "    status['loaded_model'] = get_model(prefs['model_ckpt'])['path']\n",
        "    return p\n",
        "\n",
        "def optimize_SDXL(p, vae_slicing=False, no_cpu=False, vae_tiling=True, torch_compile=True, model_offload=False, freeu=True, tome=True, lora=True):\n",
        "    global prefs, status\n",
        "    low_ram = int(status['cpu_memory']) <= 12\n",
        "    to_gpu = True\n",
        "    if model_offload and not low_ram:\n",
        "      p.enable_model_cpu_offload()\n",
        "      to_gpu = False\n",
        "    elif prefs['sequential_cpu_offload'] and not no_cpu:\n",
        "      p.enable_sequential_cpu_offload()\n",
        "      to_gpu = False\n",
        "    if prefs['enable_xformers'] and status['installed_xformers']:\n",
        "      #p.set_use_memory_efficient_attention_xformers(True)\n",
        "      p.enable_xformers_memory_efficient_attention()\n",
        "    if prefs['vae_slicing'] and vae_slicing:\n",
        "      p.enable_vae_slicing()\n",
        "    if prefs['vae_tiling'] and vae_tiling:\n",
        "      p.enable_vae_tiling()\n",
        "    if lora:\n",
        "      p = apply_LoRA(p, SDXL=True)\n",
        "    '''if prefs['use_LoRA_model']:\n",
        "      lora = get_SDXL_LoRA_model(prefs['SDXL_LoRA_model'])\n",
        "      if bool(lora['weights']):\n",
        "        p.load_lora_weights(lora['path'], weight_name=lora['weights'], torch_dtype=torch.float16)\n",
        "      else:\n",
        "        p.load_lora_weights(lora['path'], torch_dtype=torch.float16)'''\n",
        "      #p.unfuse_lora() TODO\n",
        "      #p.fuse_lora(lora_scale=0.5)\n",
        "      #p.unet.load_attn_procs(lora['path'])\n",
        "    #if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:\n",
        "    if prefs['enable_freeu'] and freeu:\n",
        "      p.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)#**prefs['freeu_args'])\n",
        "      #s1=0.9, s2=0.2, b1=1.2, b2=1.4\n",
        "    if prefs['enable_torch_compile'] and torch_compile:\n",
        "      #p.unet.to(memory_format=torch.channels_last)\n",
        "      p.unet = torch.compile(p.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "    if prefs['enable_tome'] and tome:\n",
        "      try:\n",
        "        import tomesd\n",
        "      except Exception:\n",
        "        run_sp(\"pip install tomesd\", realtime=False)\n",
        "        import tomesd\n",
        "        pass\n",
        "      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])\n",
        "    if prefs['enable_deepcache']:\n",
        "        try:\n",
        "            from DeepCache import DeepCacheSDHelper\n",
        "        except Exception:\n",
        "            run_sp(\"pip install DeepCache\", realtime=False)\n",
        "            from DeepCache import DeepCacheSDHelper\n",
        "            pass\n",
        "        helper = DeepCacheSDHelper(pipe=p)\n",
        "        helper.set_params(cache_interval=3, cache_branch_id=0)\n",
        "        helper.enable()\n",
        "    if to_gpu:\n",
        "      p = p.to(torch_device)\n",
        "    p = pipeline_scheduler(p)\n",
        "    status['loaded_scheduler'] = prefs['scheduler_mode']\n",
        "    status['loaded_SDXL_model'] = get_SDXL_model(prefs['SDXL_model'])['path']\n",
        "    p.set_progress_bar_config(disable=True)\n",
        "    return p\n",
        "\n",
        "def apply_LoRA(p, SDXL=False):\n",
        "    active = p.get_active_adapters()\n",
        "    layers = 'active_SDXL_LoRA_layers' if SDXL else 'active_LoRA_layers'\n",
        "    if prefs['use_LoRA_model'] and len(prefs[layers]) > 0:\n",
        "      adapters = []\n",
        "      scales = []\n",
        "      for l in prefs[layers]:\n",
        "        adapters.append(l['name'])\n",
        "        scales.append(l['scale'])\n",
        "        weight_args = {}\n",
        "        if 'weights' in l and bool(l['weights']):\n",
        "          weight_args['weight_name'] = l['weights']\n",
        "        p.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)\n",
        "      p.set_adapters(adapters, adapter_weights=scales)\n",
        "    else:\n",
        "      if len(active) > 0:\n",
        "        p.disable_lora()\n",
        "    return p\n",
        "\n",
        "def install_xformers(page):\n",
        "    ''' No longer needed, they finally updated to make it easier'''\n",
        "    run_process(\"pip install -U --pre triton\", page=page)\n",
        "    from subprocess import getoutput\n",
        "\n",
        "    s = getoutput('nvidia-smi')\n",
        "    if 'T4' in s:\n",
        "      gpu = 'T4'\n",
        "    elif 'P100' in s:\n",
        "      gpu = 'P100'\n",
        "    elif 'V100' in s:\n",
        "      gpu = 'V100'\n",
        "    elif 'A100' in s:\n",
        "      gpu = 'A100'\n",
        "    if not (gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'):\n",
        "      alert_msg(page, \"Xformers Error: It seems that your GPU is not supported at the moment\")\n",
        "      return False\n",
        "    print(f\"Installing Xformers for {gpu}\")\n",
        "    if (gpu=='T4'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\", page=page, show=True)\n",
        "    elif (gpu=='P100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    elif (gpu=='V100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    elif (gpu=='A100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    return True\n",
        "\n",
        "def get_text2image(page):\n",
        "    os.chdir(root_dir)\n",
        "    global pipe, unet, scheduler, prefs, model\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      if use_custom_scheduler: # Not really using anymore, maybe later\n",
        "        from transformers import CLIPTextModel, CLIPTokenizer\n",
        "        from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "        # 1. Load the autoencoder model which will be used to decode the latents into image space.\n",
        "        vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
        "        # 2. Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        if prefs['higher_vram_mode']:\n",
        "          unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", device_map=\"auto\")\n",
        "        else:\n",
        "          unet = UNet2DConditionModel.from_pretrained(model_path, variant=\"fp16\", torch_dtype=torch.float16, subfolder=\"unet\", device_map=\"auto\")\n",
        "        vae = vae.to(torch_device)\n",
        "        text_encoder = text_encoder.to(torch_device)\n",
        "        #if enable_attention_slicing:\n",
        "        #  unet.enable_attention_slicing() #slice_size\n",
        "        unet = unet.to(torch_device)\n",
        "      else:\n",
        "        #if status['finetuned_model']: pipe = get_txt2img_pipe()\n",
        "        #else:\n",
        "        #pipe = get_lpw_pipe()\n",
        "        pipe = get_SD_pipe()\n",
        "    except EnvironmentError as e:\n",
        "      model = get_model(prefs['model_ckpt'])\n",
        "      model_url = f\"https://huggingface.co/{model['path']}\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace {model[\"name\"]} Model Cards to use Checkpoint',\n",
        "                content=Column([Markdown(f'[{model_url}]({model_url})', selectable=True, on_tap_link=open_url), Text(str(e), selectable=True), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "    except Exception as e:\n",
        "      model = get_model(prefs['model_ckpt'])\n",
        "      model_url = f\"https://huggingface.co/{model['path']}\"\n",
        "      alert_msg(page, f'ERROR Loading Pipeline with {model[\"name\"]}',\n",
        "                content=Column([Markdown(f'[{model_url}]({model_url})', selectable=True, on_tap_link=open_url), Text(str(e), selectable=True), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "\n",
        "# I thought it's what I wanted, but current implementation does same as mine but doesn't clear memory between\n",
        "def get_mega_pipe():\n",
        "  global pipe, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"stable_diffusion_mega\", safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  else:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"stable_diffusion_mega\", revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = pipeline_scheduler(pipe)\n",
        "  pipe = optimize_pipe(pipe)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  return pipe\n",
        "\n",
        "def get_lpw_pipe():\n",
        "  global pipe, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  model_path = model['path']\n",
        "  os.chdir(root_dir)\n",
        "  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):\n",
        "  #  run_sp(\"wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py\")\n",
        "  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline\n",
        "  if pipe is not None:\n",
        "    if model['path'] != status['loaded_model']:\n",
        "      #clear_txt2img_pipe()\n",
        "      clear_pipes()\n",
        "    elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "      pipe = pipeline_scheduler(pipe)\n",
        "      return pipe\n",
        "    else:\n",
        "      return pipe\n",
        "  if 'revision' in model:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, variant=model['revision'], torch_dtype=torch.float16, **safety)\n",
        "  else:\n",
        "    if 'vae' in model:\n",
        "      from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "      vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    else:\n",
        "      if 'from_ckpt' in model:\n",
        "        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "      else:\n",
        "        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    #pipe = DiffusionPipeline.from_pretrained(model_path, community=\"lpw_stable_diffusion\", scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  #pipe = pipeline_scheduler(pipe)\n",
        "  pipe = optimize_pipe(pipe, vae_slicing=True)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  return pipe\n",
        "\n",
        "def get_SD_pipe(task=\"txt2img\"):\n",
        "  global pipe, model_path, prefs, compel_proc\n",
        "  from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpainting\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  model_path = model['path']\n",
        "  if pipe is not None:\n",
        "    if 'from_ckpt' in model and task != status['loaded_task']:\n",
        "      clear_pipes()\n",
        "    elif task != status['loaded_task']:\n",
        "      if task == \"txt2img\":\n",
        "        pipe = AutoPipelineForText2Image.from_pipe(pipe)\n",
        "      elif task == \"img2img\":\n",
        "        pipe = AutoPipelineForImage2Image.from_pipe(pipe)\n",
        "      elif task == \"inpaint\":\n",
        "        pipe = AutoPipelineForInpainting.from_pipe(pipe)\n",
        "      pipe = pipeline_scheduler(pipe)\n",
        "      pipe = apply_LoRA(pipe)\n",
        "      status['loaded_task'] = task\n",
        "      return pipe\n",
        "    if model['path'] != status['loaded_model'] or task != status['loaded_task']:\n",
        "      clear_pipes()\n",
        "    elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "      pipe = pipeline_scheduler(pipe)\n",
        "      pipe = apply_LoRA(pipe)\n",
        "      return pipe\n",
        "    else:\n",
        "      pipe = apply_LoRA(pipe)\n",
        "      return pipe\n",
        "  if prefs['SD_compel']:\n",
        "      pip_install(\"compel\", upgrade=True)\n",
        "  variant = {'variant': model['revision']} if 'revision' in model else {}\n",
        "  variant = {'variant': model['variant']} if 'variant' in model else variant\n",
        "  if 'vae' in model:\n",
        "    from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "    vae = AutoencoderKL.from_single_file(model_path, subfolder=\"vae\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "    unet = UNet2DConditionModel.from_single_file(model_path, subfolder=\"unet\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "    vae_args = {'vae': vae, 'unet': unet}\n",
        "  else:\n",
        "    vae_args = {}\n",
        "  if 'from_ckpt' in model:\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "    if task == \"txt2img\":\n",
        "      pipe = StableDiffusionPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)\n",
        "    elif task == \"img2img\":\n",
        "      pipe = StableDiffusionImg2ImgPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)\n",
        "    elif task == \"inpaint\":\n",
        "      pipe = StableDiffusionInpaintPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True,**vae_args, **variant, **safety)\n",
        "    #pipe = StableDiffusionPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "  else:\n",
        "    if task == \"txt2img\":\n",
        "      pipe = AutoPipelineForText2Image.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)\n",
        "    elif task == \"img2img\":\n",
        "      pipe = AutoPipelineForImage2Image.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)\n",
        "    elif task == \"inpaint\":\n",
        "      pipe = AutoPipelineForInpainting.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)\n",
        "      #pipe = StableDiffusionPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "  pipe = optimize_pipe(pipe, vae_slicing=task == \"txt2img\")\n",
        "  if prefs['SD_compel']:\n",
        "      from compel import Compel\n",
        "      compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder, truncate_long_prompts=False)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  status['loaded_task'] = task\n",
        "  status['loaded_model'] = model_path\n",
        "  return pipe\n",
        "\n",
        "def get_txt2img_pipe():\n",
        "  global pipe, scheduler, model_path, prefs, status\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  #from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  #if status['finetuned_model']:\n",
        "  #  vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16)\n",
        "  #  unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16)\n",
        "  #pipe = optimize_pipe(pipe, vae=True)\n",
        "  #pipe.set_progress_bar_config(disable=True)\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = get_lpw_pipe()\n",
        "  return pipe\n",
        "\n",
        "def get_compel_pipe():\n",
        "  global pipe, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  os.chdir(root_dir)\n",
        "  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):\n",
        "  #  run_sp(\"wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py\")\n",
        "  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline\n",
        "  if pipe is not None:\n",
        "    if model['path'] != status['loaded_model']:\n",
        "      #clear_txt2img_pipe()\n",
        "      clear_pipes()\n",
        "    elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "      pipe = pipeline_scheduler(pipe)\n",
        "      return pipe\n",
        "    else:\n",
        "      return pipe\n",
        "  if 'revision' in model:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, variant=model['revision'], torch_dtype=torch.float16, **safety)\n",
        "  else:\n",
        "    if 'vae' in model:\n",
        "      from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "      vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    else:\n",
        "      if 'from_ckpt' in model:\n",
        "        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "      else:\n",
        "        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    #pipe = DiffusionPipeline.from_pretrained(model_path, community=\"lpw_stable_diffusion\", scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = pipeline_scheduler(pipe)\n",
        "  pipe = optimize_pipe(pipe, vae_slicing=True)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  return pipe\n",
        "\n",
        "def get_unet_pipe():\n",
        "  global unet, scheduler, model_path, prefs\n",
        "  from transformers import CLIPTextModel, CLIPTokenizer\n",
        "  from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  # 1. Load the autoencoder model which will be used to decode the latents into image space.\n",
        "  vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
        "  # 2. Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "  tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "  text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "  if prefs['higher_vram_mode']:\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", **safety, device_map=\"auto\")\n",
        "  else:\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_path, revision=\"fp16\", torch_dtype=torch.float16, subfolder=\"unet\", **safety, device_map=\"auto\")\n",
        "  vae = vae.to(torch_device)\n",
        "  text_encoder = text_encoder.to(torch_device)\n",
        "  #if enable_attention_slicing:\n",
        "  #  unet.enable_attention_slicing() #slice_size\n",
        "  unet = unet.to(torch_device)\n",
        "  return unet\n",
        "\n",
        "def get_interpolation(page):\n",
        "    from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
        "    import torch, gc\n",
        "    global pipe_interpolation\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if pipe_interpolation is not None:\n",
        "      #print(\"Clearing the ol' pipe first...\")\n",
        "      del pipe_interpolation\n",
        "      flush()\n",
        "      pipe_interpolation = None\n",
        "\n",
        "    pipe_interpolation = get_interpolation_pipe()\n",
        "    run_process(\"pip install watchdog -q\", page=page, realtime=False)\n",
        "    status['loaded_interpolation'] = True\n",
        "\n",
        "def get_interpolation_pipe():\n",
        "    global pipe_interpolation, scheduler, model_path, prefs\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    os.chdir(root_dir)\n",
        "    if not os.path.isfile(os.path.join(root_dir, 'clip_guided_stable_diffusion.py')):\n",
        "      run_sp(\"wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/interpolate_stable_diffusion.py\")\n",
        "    from interpolate_stable_diffusion import StableDiffusionWalkPipeline\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    if pipe_interpolation is not None:\n",
        "      if model['path'] != status['loaded_model']:\n",
        "        clear_interpolation_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_interpolation = pipeline_scheduler(pipe_interpolation)\n",
        "        return pipe_interpolation\n",
        "      else:\n",
        "        return pipe_interpolation\n",
        "    if 'revision' in model:\n",
        "      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    else:\n",
        "      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe_interpolation = pipe_interpolation.to(torch_device)\n",
        "    pipe_interpolation = pipeline_scheduler(pipe_interpolation)\n",
        "    pipe_interpolation = optimize_pipe(pipe_interpolation, freeu=False)\n",
        "    pipe_interpolation.set_progress_bar_config(disable=True)\n",
        "    return pipe_interpolation\n",
        "\n",
        "def get_image2image(page):\n",
        "    from diffusers import StableDiffusionInpaintPipeline, DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
        "    import torch, gc\n",
        "    global pipe_img2img\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if pipe_img2img is not None:\n",
        "      if model['path'] != status['loaded_model']:\n",
        "        clear_img2img_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "        return pipe_img2img\n",
        "      else:\n",
        "        return pipe_img2img\n",
        "    try:\n",
        "      pipe_img2img = get_img2img_pipe()\n",
        "    except EnvironmentError:\n",
        "      model_url = f\"https://huggingface.co/{inpaint_model}\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Inpainting Model Card to use Checkpoint',\n",
        "                content=Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url))\n",
        "    loaded_img2img = True\n",
        "\n",
        "def get_img2img_pipe():\n",
        "  global pipe_img2img, scheduler, model_path, inpaint_model, prefs, callback_fn\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_img2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "        return pipe_img2img\n",
        "      else:\n",
        "        return pipe_img2img\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_img2img = DiffusionPipeline.from_pretrained(\n",
        "        inpaint_model,\n",
        "        custom_pipeline=\"img2img_inpainting\",\n",
        "        #scheduler=model_scheduler(inpaint_model),\n",
        "        use_safetensors=True,\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "      pipe_img2img = DiffusionPipeline.from_pretrained(\n",
        "      inpaint_model,\n",
        "      custom_pipeline=\"img2img_inpainting\",\n",
        "      #scheduler=model_scheduler(inpaint_model),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      use_safetensors=True, variant=\"fp16\",\n",
        "      torch_dtype=torch.float16,\n",
        "      **safety)\n",
        "  #pipe_img2img.to(torch_device)\n",
        "  #if prefs['enable_attention_slicing']: pipe_img2img.enable_attention_slicing() #slice_size\n",
        "  pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "  pipe_img2img = optimize_pipe(pipe_img2img, vae_slicing=True)\n",
        "  pipe_img2img.set_progress_bar_config(disable=True)\n",
        "  #def dummy(images, **kwargs): return images, False\n",
        "  #pipe_img2img.safety_checker = dummy\n",
        "  return pipe_img2img\n",
        "\n",
        "def get_imagic(page):\n",
        "    global pipe_imagic\n",
        "    pipe_imagic = get_imagic_pipe()\n",
        "\n",
        "def get_imagic_pipe():\n",
        "  global pipe_imagic, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline#, DDIMScheduler\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  #ddim = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if pipe_imagic is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_imagic_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_imagic = pipeline_scheduler(pipe_imagic)\n",
        "        return pipe_imagic\n",
        "      else:\n",
        "        return pipe_imagic\n",
        "  if True:\n",
        "    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/imagic_stable_diffusion_mod\", **safety)\n",
        "  else:\n",
        "    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/imagic_stable_diffusion_mod\", revision=\"fp16\", torch_dtype=torch.float16, **safety)\n",
        "  #pipe_imagic = pipe_imagic.to(torch_device)\n",
        "  def dummy(images, **kwargs):\n",
        "    return images, False\n",
        "  if prefs['disable_nsfw_filter']:\n",
        "    pipe_imagic.safety_checker = dummy\n",
        "  pipe_imagic = pipeline_scheduler(pipe_imagic, big3=True)\n",
        "  pipe_imagic = optimize_pipe(pipe_imagic, freeu=False)\n",
        "  #pipe_imagic.set_progress_bar_config(disable=True)\n",
        "  return pipe_imagic\n",
        "\n",
        "def get_composable(page):\n",
        "    global pipe_composable\n",
        "    pipe_composable = get_composable_pipe()\n",
        "\n",
        "def get_composable_pipe():\n",
        "  global pipe_composable, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_composable is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_composable_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_composable = pipeline_scheduler(pipe_composable, big3=True)\n",
        "        return pipe_composable\n",
        "      else:\n",
        "        return pipe_composable\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if True:\n",
        "    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/composable_stable_diffusion_mod\", feature_extractor=None, safety_checker=None)\n",
        "  else:\n",
        "    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/composable_stable_diffusion_mod\", revision=\"fp16\", torch_dtype=torch.float16, feature_extractor=None, safety_checker=None)\n",
        "  #pipe_composable = pipe_composable.to(torch_device)\n",
        "  def dummy(images, **kwargs):\n",
        "    return images, False\n",
        "  if prefs['disable_nsfw_filter']:\n",
        "    pipe_composable.safety_checker = dummy\n",
        "  pipe_composable = pipeline_scheduler(pipe_composable, big3=True)\n",
        "  pipe_composable = optimize_pipe(pipe_composable, freeu=False)\n",
        "  #pipe_composable.set_progress_bar_config(disable=True)\n",
        "  return pipe_composable\n",
        "\n",
        "def get_SDXL(page):\n",
        "    global pipe_SDXL\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      pipe_SDXL = get_SDXL_pipe()\n",
        "      return True\n",
        "    except Exception as er:\n",
        "      SDXL_model = get_SDXL_model(prefs['SDXL_model'])\n",
        "      model_id = SDXL_model['path']\n",
        "      model_url = f\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Stable Diffusion XL Model Card to use Checkpoint',\n",
        "                content=Markdown(f'[{model_id}]({model_url})</br>{er}</br>{traceback.format_exc()}', on_tap_link=open_url))\n",
        "      return False\n",
        "\n",
        "def get_SDXL_pipe(task=\"text2image\"):\n",
        "  global pipe_SDXL, pipe_SDXL_refiner, prefs, status, compel_base, compel_refiner\n",
        "  from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, StableDiffusionXLInpaintPipeline, AutoencoderKL # , AutoencoderTiny\n",
        "  try:\n",
        "      from imwatermark import WatermarkEncoder\n",
        "  except ModuleNotFoundError:\n",
        "      run_sp(\"pip install --no-deps invisible-watermark>=0.2.0\", realtime=False)\n",
        "      pass\n",
        "  if prefs['SDXL_compel']:\n",
        "      pip_install(\"compel\", upgrade=True)\n",
        "  SDXL_model = get_SDXL_model(prefs['SDXL_model'])\n",
        "  model_id = SDXL_model['path']#\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "  refiner_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "  if SDXL_model['path'] != status['loaded_SDXL_model'] or task != status['loaded_SDXL']:\n",
        "      clear_pipes()\n",
        "  if pipe_SDXL is not None:\n",
        "      pipe_SDXL = apply_LoRA(pipe_SDXL, SDXL=True)\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "          pipe_SDXL = pipeline_scheduler(pipe_SDXL)\n",
        "          if pipe_SDXL_refiner is not None:\n",
        "              pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)\n",
        "      return pipe_SDXL\n",
        "  watermark = prefs['SDXL_watermark']\n",
        "  low_ram = int(status['cpu_memory']) <= 12\n",
        "  variant = {'variant': SDXL_model['revision']} if 'revision' in SDXL_model else {}\n",
        "  variant = {'variant': SDXL_model['variant']} if 'variant' in SDXL_model else variant\n",
        "  #vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesdxl\", torch_dtype=torch.float16)\n",
        "  vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\" if not prefs['higher_vram_mode'] else \"stabilityai/sdxl-vae\", torch_dtype=torch.float16, force_upcast=False)\n",
        "  if task == \"text2image\":\n",
        "      status['loaded_SDXL'] = task\n",
        "      pipe_SDXL = StableDiffusionXLPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,\n",
        "          vae=vae,\n",
        "          use_safetensors=True,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)\n",
        "      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(refiner_id, torch_dtype=torch.float16, use_safetensors=True,\n",
        "          text_encoder_2=pipe_SDXL.text_encoder_2,\n",
        "          vae=pipe_SDXL.vae,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)\n",
        "\n",
        "  elif task == \"image2image\":\n",
        "      status['loaded_SDXL'] = task\n",
        "      pipe_SDXL = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "          model_id, torch_dtype=torch.float16, use_safetensors=True,\n",
        "          vae=vae,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)\n",
        "      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "          refiner_id, torch_dtype=torch.float16, use_safetensors=True,\n",
        "          vae=pipe_SDXL.vae,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)\n",
        "\n",
        "  elif task == \"inpainting\":\n",
        "      status['loaded_SDXL'] = task\n",
        "      from diffusers import StableDiffusionXLInpaintPipeline\n",
        "      pipe_SDXL = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "          \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, use_safetensors=True,\n",
        "          vae=vae,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)\n",
        "\n",
        "      pipe_SDXL_refiner = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "          refiner_id,\n",
        "          text_encoder_2=pipe_SDXL.text_encoder_2,\n",
        "          vae=pipe_SDXL.vae,\n",
        "          torch_dtype=torch.float16,\n",
        "          use_safetensors=True,\n",
        "          add_watermarker=watermark,\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **variant, **safety,\n",
        "      )\n",
        "      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)\n",
        "  if prefs['SDXL_compel']:\n",
        "      from compel import Compel, ReturnedEmbeddingsType\n",
        "      compel_base = Compel(tokenizer=[pipe_SDXL.tokenizer, pipe_SDXL.tokenizer_2], text_encoder=[pipe_SDXL.text_encoder, pipe_SDXL.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])\n",
        "      compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer_2], text_encoder=[pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[True])\n",
        "      #compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer, pipe_SDXL_refiner.tokenizer_2] , text_encoder=[pipe_SDXL_refiner.text_encoder, pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])\n",
        "      #compel_refiner = Compel(tokenizer=pipe_SDXL_refiner.tokenizer_2, text_encoder=pipe_SDXL_refiner.text_encoder_2, returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])\n",
        "      #truncate_long_prompts=True,\n",
        "  return pipe_SDXL\n",
        "\n",
        "def get_versatile(page):\n",
        "    import torch, gc\n",
        "    global pipe_versatile_text2img\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      pipe_versatile_text2img = get_versatile_text2img_pipe()\n",
        "    except Exception as er:\n",
        "      model_url = f\"https://huggingface.co/shi-labs/versatile-diffusion\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Versatile Diffusion Model Card to use Checkpoint',\n",
        "                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_versatile_pipe(): # Mega was taking up too much vram and crashing the system\n",
        "  global pipe_versatile, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_composable is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_composable = pipeline_scheduler(pipe_composable)\n",
        "        return pipe_composable\n",
        "      else:\n",
        "        return pipe_composable\n",
        "  pipe_versatile = VersatileDiffusionPipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      #revision=\"fp16\",\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      **safety\n",
        "  )\n",
        "  #pipe_versatile.to(torch_device)\n",
        "  pipe_versatile = pipeline_scheduler(pipe_versatile)\n",
        "  pipe_versatile = optimize_pipe(pipe_versatile, freeu=False)\n",
        "  pipe_versatile.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile\n",
        "\n",
        "def get_versatile_text2img_pipe():\n",
        "  global pipe_versatile_text2img, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionTextToImagePipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_text2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)\n",
        "        return pipe_versatile_text2img\n",
        "      else:\n",
        "        return pipe_versatile_text2img\n",
        "  pipe_versatile_text2img = VersatileDiffusionTextToImagePipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      #revision=\"fp16\",\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      **safety\n",
        "  )\n",
        "  #pipe_versatile_text2img.to(torch_device)\n",
        "  pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)\n",
        "  pipe_versatile_text2img = optimize_pipe(pipe_versatile_text2img, freeu=False)\n",
        "  pipe_versatile_text2img.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_text2img\n",
        "\n",
        "def get_versatile_variation_pipe():\n",
        "  global pipe_versatile_variation, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionImageVariationPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_variation is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)\n",
        "        return pipe_versatile_variation\n",
        "      else:\n",
        "        return pipe_versatile_variation\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #revision=\"fp16\",\n",
        "        torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "        **safety\n",
        "    )\n",
        "  #pipe_versatile_variation.to(torch_device)\n",
        "  pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)\n",
        "  pipe_versatile_variation = optimize_pipe(pipe_versatile_variation, freeu=False)\n",
        "  pipe_versatile_variation.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_variation\n",
        "\n",
        "def get_versatile_dualguided_pipe():\n",
        "  global pipe_versatile_dualguided, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionDualGuidedPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_dualguided is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)\n",
        "        return pipe_versatile_dualguided\n",
        "      else:\n",
        "        return pipe_versatile_dualguided\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #revision=\"fp16\",\n",
        "        torch_dtype=torch.float16,\n",
        "        **safety\n",
        "    )\n",
        "  #pipe_versatile_dualguided.to(torch_device)\n",
        "  pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)\n",
        "  pipe_versatile_dualguided = optimize_pipe(pipe_versatile_dualguided, freeu=False)\n",
        "  pipe_versatile_dualguided.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_dualguided\n",
        "\n",
        "def get_safe(page):\n",
        "    import torch, gc\n",
        "    global pipe_safe\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      pipe_safe = get_safe_pipe()\n",
        "    except Exception as er:\n",
        "      model_url = f\"https://huggingface.co/AIML-TUDA/stable-diffusion-safe\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Safe Model Card to use Checkpoint. Reinstall after accepting TOS.',\n",
        "                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_safe_pipe():\n",
        "  global pipe_safe, scheduler, model_path, prefs, callback_fn\n",
        "  from diffusers import StableDiffusionPipelineSafe\n",
        "  from diffusers.pipelines.stable_diffusion_safe import StableDiffusionPipelineSafe\n",
        "  #from diffusers.pipelines.safety_checker import SafeStableDiffusionPipelineSafe\n",
        "  #from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"AIML-TUDA/stable-diffusion-safe\"\n",
        "  if pipe_safe is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_safe = pipeline_scheduler(pipe_safe)\n",
        "        return pipe_safe\n",
        "      else:\n",
        "        return pipe_safe\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if True:\n",
        "    pipe_safe = StableDiffusionPipelineSafe.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety,\n",
        "    )\n",
        "  else:\n",
        "      pipe_safe = StableDiffusionPipelineSafe.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        revision=\"fp16\",\n",
        "        torch_dtype=torch.float16,\n",
        "        **safety\n",
        "      )\n",
        "  #pipe_safe.to(torch_device)\n",
        "  pipe_safe = pipeline_scheduler(pipe_safe)\n",
        "  pipe_safe = optimize_pipe(pipe_safe, freeu=False)\n",
        "  pipe_safe.set_progress_bar_config(disable=True)\n",
        "  return pipe_safe\n",
        "\n",
        "def get_SAG(page):\n",
        "  global pipe_SAG\n",
        "  #clear_SAG_pipe()\n",
        "  pipe_SAG = get_SAG_pipe()\n",
        "\n",
        "def get_SAG_pipe():\n",
        "  global pipe_SAG, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionSAGPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_SAG is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_SAG_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)\n",
        "        return pipe_SAG\n",
        "      else:\n",
        "        return pipe_SAG\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "      pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      **safety)\n",
        "  pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)\n",
        "  pipe_SAG = optimize_pipe(pipe_SAG, vae_slicing=False, freeu=False)\n",
        "  pipe_SAG.set_progress_bar_config(disable=True)\n",
        "  return pipe_SAG\n",
        "\n",
        "def get_attend_and_excite(page):\n",
        "  global pipe_attend_and_excite\n",
        "  #clear_attend_and_excite_pipe()\n",
        "  pipe_attend_and_excite = get_attend_and_excite_pipe()\n",
        "\n",
        "def get_attend_and_excite_pipe():\n",
        "  global pipe_attend_and_excite, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionAttendAndExcitePipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_attend_and_excite is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_attend_and_excite_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)\n",
        "        return pipe_attend_and_excite\n",
        "      else:\n",
        "        return pipe_attend_and_excite\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "      pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      **safety)\n",
        "  pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)\n",
        "  pipe_attend_and_excite = optimize_pipe(pipe_attend_and_excite, vae_slicing=True, freeu=False)\n",
        "  pipe_attend_and_excite.set_progress_bar_config(disable=True)\n",
        "  return pipe_attend_and_excite\n",
        "\n",
        "def get_panorama(page):\n",
        "  global pipe_panorama\n",
        "  #clear_panorama_pipe()\n",
        "  pipe_panorama = get_panorama_pipe()\n",
        "\n",
        "def get_panorama_pipe():\n",
        "  global pipe_panorama, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionPanoramaPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_panorama is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_panorama_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_panorama = pipeline_scheduler(pipe_panorama)\n",
        "        return pipe_panorama\n",
        "      else:\n",
        "        return pipe_panorama\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety\n",
        "    )\n",
        "  else:\n",
        "      pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      **safety)\n",
        "  pipe_panorama = pipeline_scheduler(pipe_panorama)\n",
        "  pipe_panorama = optimize_pipe(pipe_panorama, vae_slicing=True, freeu=False)\n",
        "  pipe_panorama.set_progress_bar_config(disable=True)\n",
        "  return pipe_panorama\n",
        "\n",
        "def get_upscale(page):\n",
        "    import torch, gc\n",
        "    global pipe_upscale\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    if pipe_upscale is None:\n",
        "      try:\n",
        "        pipe_upscale = get_upscale_pipe()\n",
        "      except Exception as er:\n",
        "        model_url = f\"https://huggingface.co/{model_path}\"\n",
        "        alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Upscale Model Card to use Checkpoint',\n",
        "                  content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_upscale_pipe():\n",
        "  global pipe_upscale, scheduler, prefs\n",
        "  from diffusers import StableDiffusionUpscalePipeline\n",
        "  model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
        "  if pipe_upscale is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)\n",
        "        return pipe_upscale\n",
        "      else:\n",
        "        return pipe_upscale\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "    )\n",
        "  else:\n",
        "    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      revision=\"fp16\",\n",
        "      torch_dtype=torch.float16,\n",
        "      #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "    )\n",
        "  #pipe_upscale.to(torch_device)\n",
        "  pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)\n",
        "  pipe_upscale = optimize_pipe(pipe_upscale)\n",
        "  pipe_upscale.set_progress_bar_config(disable=True)\n",
        "  return pipe_upscale\n",
        "\n",
        "def get_clip(page):\n",
        "    global pipe_clip_guided, model_path\n",
        "    pipe_clip_guided = get_clip_guided_pipe()\n",
        "\n",
        "def get_clip_guided_pipe():\n",
        "    global pipe_clip_guided, scheduler_clip, prefs, model_path\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline\n",
        "    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion\n",
        "    if pipe_clip_guided is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_clip_guided_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "        return pipe_clip_guided\n",
        "      else:\n",
        "        return pipe_clip_guided\n",
        "    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):\n",
        "    #  scheduler_clip = scheduler\n",
        "    #else:\n",
        "    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])\n",
        "\n",
        "    if 'revision' in model:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(\n",
        "              model_path,\n",
        "              custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\",\n",
        "              clip_model=clip_model,\n",
        "              feature_extractor=feature_extractor,\n",
        "              #scheduler=model_scheduler(model_path, big3=True),\n",
        "              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "              safety_checker=None,\n",
        "              torch_dtype=torch.float16,\n",
        "              revision=model['revision'],\n",
        "              #device_map=\"auto\",\n",
        "          )\n",
        "    else:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)\n",
        "    #pipe_clip_guided = pipe_clip_guided.to(torch_device)\n",
        "    '''\n",
        "    pipe_clip_guided = CLIPGuidedStableDiffusion(\n",
        "        unet=pipeline.unet,\n",
        "        vae=pipeline.vae,\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder,\n",
        "        scheduler=scheduler_clip,\n",
        "        clip_model=clip_model,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )'''\n",
        "    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "    pipe_clip_guided = optimize_pipe(pipe_clip_guided, freeu=False)\n",
        "    return pipe_clip_guided\n",
        "\n",
        "def get_clip_guided_img2img_pipe():\n",
        "    global pipe_clip_guided, scheduler_clip, prefs, model_path\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline\n",
        "    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion\n",
        "    if pipe_clip_guided is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_clip_guided_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "        return pipe_clip_guided\n",
        "      else:\n",
        "        return pipe_clip_guided\n",
        "    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):\n",
        "    #  scheduler_clip = scheduler\n",
        "    #else:\n",
        "    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])\n",
        "\n",
        "    if 'revision' in model:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(\n",
        "              model_path,\n",
        "              custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\",\n",
        "              clip_model=clip_model,\n",
        "              feature_extractor=feature_extractor,\n",
        "              #scheduler=model_scheduler(model_path, big3=True),\n",
        "              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "              safety_checker=None,\n",
        "              torch_dtype=torch.float16,\n",
        "              revision=model['revision'],\n",
        "              #device_map=\"auto\",\n",
        "          )\n",
        "    else:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)\n",
        "    #pipe_clip_guided = pipe_clip_guided.to(torch_device)\n",
        "    '''\n",
        "    pipe_clip_guided = CLIPGuidedStableDiffusion(\n",
        "        unet=pipeline.unet,\n",
        "        vae=pipeline.vae,\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder,\n",
        "        scheduler=scheduler_clip,\n",
        "        clip_model=clip_model,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )'''\n",
        "    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "    pipe_clip_guided = optimize_pipe(pipe_clip_guided, freeu=False)\n",
        "    return pipe_clip_guided\n",
        "\n",
        "def get_repaint(page):\n",
        "    global pipe_repaint\n",
        "    pipe_repaint = get_repaint_pipe()\n",
        "\n",
        "def get_repaint_pipe():\n",
        "    global pipe_repaint\n",
        "    from diffusers import UNet2DModel, RePaintScheduler, RePaintPipeline\n",
        "    #model = get_model(prefs['model_ckpt'])\n",
        "    #model_path = model['path']\n",
        "    model_id = \"google/ddpm-ema-celebahq-256\"\n",
        "    unet = UNet2DModel.from_pretrained(model_id)\n",
        "    repaint_scheduler = RePaintScheduler.from_pretrained(model_id)\n",
        "    pipe_repaint = RePaintPipeline(unet=unet, scheduler=repaint_scheduler).to(torch_device)\n",
        "    return pipe_repaint\n",
        "\n",
        "def get_depth2img(page):\n",
        "  global pipe_depth\n",
        "  pipe_depth = get_depth_pipe()\n",
        "\n",
        "def get_depth_pipe():\n",
        "  global pipe_depth, prefs\n",
        "  from diffusers import StableDiffusionDepth2ImgPipeline\n",
        "  model_id = \"stabilityai/stable-diffusion-2-depth\"\n",
        "  if pipe_depth is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_depth = pipeline_scheduler(pipe_depth)\n",
        "        return pipe_depth\n",
        "      else:\n",
        "        return pipe_depth\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "    )\n",
        "  else:\n",
        "    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        variant=\"fp16\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "  #pipe_depth.to(torch_device)\n",
        "  pipe_depth = pipeline_scheduler(pipe_depth)\n",
        "  pipe_depth = optimize_pipe(pipe_depth, freeu=False)\n",
        "  pipe_depth.set_progress_bar_config(disable=True)\n",
        "  return pipe_depth\n",
        "\n",
        "def get_alt_diffusion(page):\n",
        "    global pipe_alt_diffusion\n",
        "    run_process(\"pip install -q sentencepiece\", page=page)\n",
        "    pipe_alt_diffusion = get_alt_diffusion_pipe()\n",
        "\n",
        "def get_alt_diffusion_pipe():\n",
        "    global pipe_alt_diffusion\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from diffusers import AltDiffusionPipeline, StableDiffusionPipeline\n",
        "    #from diffusers.pipelines.alt_diffusion.modeling_roberta_series import (RobertaSeriesConfig, RobertaSeriesModelWithTransformation)\n",
        "    model_id = \"BAAI/AltDiffusion-m9\"\n",
        "    if pipe_alt_diffusion is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)\n",
        "        return pipe_alt_diffusion\n",
        "      else:\n",
        "        return pipe_alt_diffusion\n",
        "    if prefs['higher_vram_mode']:\n",
        "      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **safety\n",
        "      )\n",
        "    else:\n",
        "      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          torch_dtype=torch.float16,\n",
        "          **safety\n",
        "      )\n",
        "    #pipe_alt_diffusion.to(torch_device)\n",
        "    pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)\n",
        "    pipe_alt_diffusion = optimize_pipe(pipe_alt_diffusion)\n",
        "    pipe_alt_diffusion.set_progress_bar_config(disable=True)\n",
        "    return pipe_alt_diffusion\n",
        "\n",
        "def get_alt_diffusion_img2img(page):\n",
        "    global pipe_alt_diffusion_img2img\n",
        "    pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()\n",
        "\n",
        "def get_alt_diffusion_img2img_pipe():\n",
        "    global pipe_alt_diffusion_img2img\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from diffusers import AltDiffusionImg2ImgPipeline, StableDiffusionImg2ImgPipeline\n",
        "    model_id = \"BAAI/AltDiffusion-m9\"\n",
        "    if pipe_alt_diffusion_img2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_alt_diffusion_img2img = pipeline_scheduler(pipe_alt_diffusion_img2img)\n",
        "        return pipe_alt_diffusion_img2img\n",
        "      else:\n",
        "        return pipe_alt_diffusion_img2img\n",
        "    if prefs['higher_vram_mode']:\n",
        "      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          **safety\n",
        "      )\n",
        "    else:\n",
        "      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          torch_dtype=torch.float16,\n",
        "          **safety\n",
        "      )\n",
        "    #pipe_alt_diffusion_img2img.to(torch_device)\n",
        "    pipe_alt_diffusion_img2im = pipeline_scheduler(pipe_alt_diffusion_img2im)\n",
        "    pipe_alt_diffusion_img2img = optimize_pipe(pipe_alt_diffusion_img2img)\n",
        "    pipe_alt_diffusion_img2img.set_progress_bar_config(disable=True)\n",
        "    return pipe_alt_diffusion_img2img\n",
        "\n",
        "def postprocess_latent(pipe, latent):\n",
        "    vae_output = pipe.vae.decode(\n",
        "        latent.images / pipe.vae.config.scaling_factor, return_dict=False\n",
        "    )[0].detach()\n",
        "    return pipe.image_processor.postprocess(vae_output, output_type=\"pil\")[0]\n",
        "\n",
        "SD_sampler = None\n",
        "def get_stability(page):\n",
        "    global prefs, SD_sampler#, stability_api\n",
        "    '''try:\n",
        "      from stability_sdk import client\n",
        "      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "    except ImportError as e:\n",
        "      run_process(\"pip install stability-sdk -q\", page=page)\n",
        "      from stability_sdk import client\n",
        "      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "      pass\n",
        "    stability_api = client.StabilityInference(\n",
        "        key=prefs['Stability_api_key'],\n",
        "        verbose=True,\n",
        "        engine=prefs['model_checkpoint']# if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\",\n",
        "    )\n",
        "    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())'''\n",
        "    # New way, other is obsolete\n",
        "    page.status(\"installing stability\")\n",
        "    import requests\n",
        "    api_host = os.getenv('API_HOST', 'https://api.stability.ai')\n",
        "    stability_url = f\"{api_host}/v1/engines/list\" #user/account\"\n",
        "    response = requests.get(stability_url, headers={\"Authorization\": prefs['Stability_api_key']})\n",
        "    if response.status_code != 200:\n",
        "      alert_msg(page, \"ERROR with Stability-ai: \" + str(response.text))\n",
        "      return\n",
        "    payload = response.json()\n",
        "    #print(str(payload))\n",
        "    status['installed_stability'] = True\n",
        "    page.status()\n",
        "\n",
        "'''\n",
        "def update_stability():\n",
        "    global SD_sampler, stability_api\n",
        "    from stability_sdk import client\n",
        "    stability_api = client.StabilityInference(\n",
        "        key=prefs['Stability_api_key'],\n",
        "        verbose=True,\n",
        "        engine=prefs['model_checkpoint']\n",
        "    )\n",
        "    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())\n",
        "'''\n",
        "def get_AIHorde(page):\n",
        "    global prefs, status\n",
        "    import requests\n",
        "    api_host = os.getenv('API_HOST', 'https://stablehorde.net/api/')\n",
        "    horde_url = f\"{api_host}/v2/find_user\" #user/account\"\n",
        "    response = requests.get(horde_url, headers={\"apikey\": prefs['AIHorde_api_key'], 'accept': 'application/json'})\n",
        "    if response.status_code != 200:\n",
        "      alert_msg(page, \"ERROR {response.status_code} with AIHorde Authentication\", content=Text(str(response.text)))\n",
        "      return\n",
        "    payload = response.json()\n",
        "    print(str(payload))\n",
        "    AI_Horde = os.path.join(dist_dir, \"AI-Horde-CLI\")\n",
        "    if not os.path.exists(AI_Horde) or force_updates:\n",
        "      run_sp(\"git clone https://github.com/db0/AI-Horde-CLI.git\", cwd=dist_dir, realtime=False)\n",
        "      run_sp(\"pip install -r cli_requirements.txt --user\", cwd=AI_Horde, realtime=False)\n",
        "    try:\n",
        "      import yaml\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install pyyaml\", realtime=False)\n",
        "      pass\n",
        "    status['installed_AIHorde'] = True\n",
        "\n",
        "def get_ESRGAN(page, model=None, installer=None):\n",
        "    global status\n",
        "    def stat(msg):\n",
        "        if installer is not None: installer.status(f\"...{msg}\")\n",
        "    ESRGAN_folder = os.path.join(dist_dir, 'Real-ESRGAN')\n",
        "    pretrained = os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models')\n",
        "    if model == None: model = prefs['upscale_model']\n",
        "    if not os.path.isdir(ESRGAN_folder):\n",
        "        os.chdir(dist_dir)\n",
        "        stat(f\"Installing Real-ESRGAN\")\n",
        "        run_sp(f\"git clone https://github.com/xinntao/Real-ESRGAN.git -q\", cwd=dist_dir)\n",
        "        os.chdir(ESRGAN_folder)\n",
        "        pip_install(\"basicsr facexlib gfpgan tqdm\", q=True, cwd=ESRGAN_folder, installer=installer)\n",
        "        stat(f\"Setup Real-ESRGAN\")\n",
        "        run_sp(f\"pip install -r requirements.txt --quiet\", realtime=False, cwd=ESRGAN_folder)\n",
        "        run_sp(f\"python setup.py develop --quiet\", realtime=False, cwd=ESRGAN_folder)\n",
        "    if 'BSRGAN' in model:\n",
        "        run_sp(f\"git clone https://github.com/cszn/BSRGAN.git -q\", cwd=ESRGAN_folder)\n",
        "        run_sp(f\"rm -r BSRGAN/testsets/RealSRSet\", cwd=ESRGAN_folder)\n",
        "    # Search https://openmodeldb.info/?q=real-esrgan\n",
        "    model_url = \"\"\n",
        "    for m in Real_ESRGAN_models:\n",
        "        if m['name'] == model:\n",
        "            model_url = m['url']\n",
        "            break\n",
        "    if not bool(model_url):\n",
        "        print(f\"ESRGAN model {model} not found.\")\n",
        "        return\n",
        "    \n",
        "    '''if model ==\"realesr-general-x4v3\":\n",
        "        model_url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth\"\n",
        "    elif model == \"RealESRGAN_x2plus\":\n",
        "        model_url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\"\n",
        "    elif model == \"RealESRGAN_x4plus\":\n",
        "        model_url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\"\n",
        "    elif model == \"BSRGANx2\":\n",
        "        model_url = \"https://github.com/cszn/KAIR/releases/download/v1.0/BSRGANx2.pth\"\n",
        "    elif model == \"realesr-animevideov3\":\n",
        "        model_url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth\"\n",
        "    elif model == \"4xLSDIRplus\":\n",
        "        model_url = \"https://github.com/Phhofm/models/raw/main/4xLSDIRplus/4xLSDIRplus.pth\"\n",
        "    elif model == \"2xLexicaRRDBNet_Sharp\":\n",
        "        model_url = \"https://github.com/Phhofm/models/raw/main/2xLexicaRRDBNet/2xLexicaRRDBNet_Sharp.pth\"\n",
        "    elif model == \"RealisticRescaler\":\n",
        "        model_url = \"https://drive.google.com/drive/folders/13OC-hQNz_S-kX0EVjVgNO1eoGvcXrTfk?usp=sharing\"\n",
        "    elif model == \"RealESRGAN_x4plus_anime\":\n",
        "        model_url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth\"'''\n",
        "    if not os.path.isfile(os.path.join(pretrained, f\"{model}.pth\")):\n",
        "        stat(f\"downloading {model}.pth\")\n",
        "        if 'drive.google' in model_url:\n",
        "            import gdown\n",
        "            gdown.download(model_url, os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth'), quiet=True)\n",
        "        else:\n",
        "            run_sp(f\"wget {model_url} -P experiments/pretrained_models --quiet\", cwd=ESRGAN_folder)\n",
        "    #run_process(f\"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    os.chdir(root_dir)\n",
        "    status['installed_ESRGAN'] = True\n",
        "\n",
        "def upscale_image(source, target, method=\"Real-ESRGAN\", scale=4, face_enhance=False, model=None, installer=None):\n",
        "    def stat(msg):\n",
        "        if installer is not None: installer.status(f\"...{msg}\")\n",
        "    if not isinstance(source, list): source = [source]\n",
        "    if not os.path.isdir(target): target = os.path.dirname(target)\n",
        "    saves = {}\n",
        "    if model == None: model = prefs['upscale_model']\n",
        "    if method==\"Real-ESRGAN\": #TODO: Add more ESRGAN model options\n",
        "        ESRGAN_folder = os.path.join(dist_dir, 'Real-ESRGAN')\n",
        "        if not status['installed_ESRGAN']:\n",
        "            stat(f\"Installing {method}\")\n",
        "            get_ESRGAN(None, model=model, installer=installer)\n",
        "        model_file = os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth')\n",
        "        if not os.path.isfile(model_file):\n",
        "            stat(f\"Downloading {model}\")\n",
        "            get_ESRGAN(None, model=model, installer=installer)\n",
        "        stat(\"Initializing\")\n",
        "        upload_folder = os.path.join(ESRGAN_folder, 'upload')\n",
        "        if 'BSRGAN' in model:\n",
        "            upload_folder = os.path.join(ESRGAN_folder, 'BSRGAN', 'testsets', 'RealSRSet')\n",
        "        result_folder = os.path.join(ESRGAN_folder, 'results')\n",
        "        if os.path.isdir(upload_folder):\n",
        "            shutil.rmtree(upload_folder)\n",
        "        if os.path.isdir(result_folder):\n",
        "            shutil.rmtree(result_folder)\n",
        "        os.mkdir(upload_folder)\n",
        "        os.mkdir(result_folder)\n",
        "        #TODO: Support source as list\n",
        "        for i in source:\n",
        "            fname = os.path.basename(i)\n",
        "            short_name = f'{fname[:80]}.png'\n",
        "            dst_path = os.path.join(upload_folder, short_name)\n",
        "            shutil.copy(i, dst_path)\n",
        "            saves[short_name] = fname\n",
        "        faceenhance = ' --face_enhance' if face_enhance else ''\n",
        "        stat(f\"Upscaling {method} {scale}X\")\n",
        "        try:\n",
        "            run_sp(f'python inference_realesrgan.py -n {model} -i upload --outscale {scale}{faceenhance}', cwd=ESRGAN_folder, realtime=False)\n",
        "        except Exception as e:\n",
        "            stat(f\"Error running {method}\")\n",
        "            print(f\"Error running {method}: {e}\")\n",
        "            return\n",
        "        out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "        stat(\"Saving output\")\n",
        "        filenames = os.listdir(result_folder)\n",
        "        for oname in filenames:\n",
        "            #fparts = oname.rpartition('_out')\n",
        "            #fname_clean = fparts[0] + fparts[2]\n",
        "            fname_clean = saves[oname]\n",
        "            opath = os.path.join(target, fname_clean)\n",
        "            shutil.move(os.path.join(result_folder, oname), opath)\n",
        "        #shutil.move(os.path.join(result_folder, out_file), target)\n",
        "        # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "    elif method==\"SRFormer\":\n",
        "        SRFormer_dir = os.path.join(dist_dir, \"SRFormer\")\n",
        "        if not os.path.isdir(SRFormer_dir):\n",
        "            stat(f\"Installing {method}...\")\n",
        "            run_sp(\"git clone https://github.com/HVision-NKU/SRFormer\", cwd=root_dir)\n",
        "            pip_install(\"addict future lmdb pyyaml scikit-image scipy tb-nightly tqdm yapf\", q=True, installer=installer)\n",
        "            run_sp(\"python setup.py develop\", cwd=SRFormer_dir)\n",
        "        upload_folder = os.path.join(SRFormer_dir, 'upload')\n",
        "        result_folder = os.path.join(SRFormer_dir, 'results')\n",
        "        if os.path.isdir(upload_folder):\n",
        "            shutil.rmtree(upload_folder)\n",
        "        if os.path.isdir(result_folder):\n",
        "            shutil.rmtree(result_folder)\n",
        "        os.mkdir(upload_folder)\n",
        "        os.mkdir(result_folder)\n",
        "        for i in source:\n",
        "            fname = os.path.basename(i)\n",
        "            short_name = f'{fname[:80]}.png'\n",
        "            dst_path = os.path.join(upload_folder, short_name)\n",
        "            shutil.copy(i, dst_path)\n",
        "            saves[short_name] = fname\n",
        "        x = int(scale)\n",
        "        if x < 2: x = 2\n",
        "        if x > 4: x = 4\n",
        "        stat(f\"Upscaling {method} {scale}X\")\n",
        "        try:\n",
        "            run_sp(f\"python basicsr/infer_sr.py -opt options/test/SRFormer/test_SRFormer_DF2Ksrx{x}.yml --input_dir upload --output_dir results\", cwd=SRFormer_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error running {method}: {e}\")\n",
        "            return\n",
        "        stat(\"Saving output\")\n",
        "        to = os.path.dirname(target)\n",
        "        for f in os.listdir(result_folder):\n",
        "            img = os.path.join(result_folder, f)\n",
        "            out = os.path.join(to, saves[f])\n",
        "            shutil.move(img, out)\n",
        "    else: #TODO: Add SwinIR, DAT and other Upscale methods\n",
        "        print(f\"Unknown upscale method {method}\")\n",
        "    #https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb\n",
        "    \n",
        "\n",
        "def get_concept(name):\n",
        "  for con in concepts:\n",
        "      if con['name'] == name:\n",
        "        return con\n",
        "  return {'name':'', 'token':''}\n",
        "\n",
        "def get_conceptualizer(page):\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "    global pipe_conceptualizer\n",
        "    repo_id_embeds = f\"sd-concepts-library/{prefs['concepts_model']}\"\n",
        "    embeds_url = \"\" #Add the URL or path to a learned_embeds.bin file in case you have one\n",
        "    placeholder_token_string = \"\" #Add what is the token string in case you are uploading your own embed\n",
        "\n",
        "    downloaded_embedding_folder = os.path.join(root_dir, \"downloaded_embedding\")\n",
        "    if not os.path.exists(downloaded_embedding_folder):\n",
        "      os.mkdir(downloaded_embedding_folder)\n",
        "    try:\n",
        "      if(not embeds_url):\n",
        "        embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"learned_embeds.bin\")\n",
        "        token_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"token_identifier.txt\")\n",
        "        shutil.copy(embeds_path, downloaded_embedding_folder)\n",
        "        shutil.copy(token_path, downloaded_embedding_folder)\n",
        "        with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:\n",
        "          placeholder_token_string = file.read()\n",
        "      else:\n",
        "        run_sp(f\"wget -q -O {downloaded_embedding_folder}/learned_embeds.bin {embeds_url}\")\n",
        "        #!wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url\n",
        "    except Exception as e:\n",
        "      alert_msg(page, f\"Error getting concept. May need to accept model at https://huggingface.co/sd-concepts-library/{prefs['concepts_model']}\", content=Text(e))\n",
        "      return\n",
        "    learned_embeds_path = f\"{downloaded_embedding_folder}/learned_embeds.bin\"\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "    def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "      loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
        "      trained_token = list(loaded_learned_embeds.keys())[0]\n",
        "      embeds = loaded_learned_embeds[trained_token]\n",
        "      dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "      embeds.to(dtype)\n",
        "      token = token if token is not None else trained_token\n",
        "      num_added_tokens = tokenizer.add_tokens(token)\n",
        "      if num_added_tokens == 0:\n",
        "        alert_msg(page, f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n",
        "        return\n",
        "      text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "      token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "      text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "    try:\n",
        "      load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)\n",
        "    except Exception as e:\n",
        "      alert_msg(page, f\"Error Loading Concept\", content=Text(e))\n",
        "      return\n",
        "    pipe_conceptualizer = StableDiffusionPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        variant=\"fp16\",\n",
        "        torch_dtype=torch.float16,\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        **safety,\n",
        "    )\n",
        "    pipe_conceptualizer = pipeline_scheduler(pipe_conceptualizer)\n",
        "    pipe_conceptualizer = optimize_pipe(pipe_conceptualizer)\n",
        "    pipe_conceptualizer.set_progress_bar_config(disable=True)\n",
        "    #pipe_conceptualizer = pipe_conceptualizer.to(torch_device)\n",
        "    return pipe_conceptualizer\n",
        "\n",
        "def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "    input_image = input_image.convert(\"RGB\")\n",
        "    W, H = input_image.size\n",
        "    k = float(resolution) / min(H, W)\n",
        "    H *= k\n",
        "    W *= k\n",
        "    H = int(round(H / 64.0)) * 64\n",
        "    W = int(round(W / 64.0)) * 64\n",
        "    img = input_image.resize((W, H), resample=PILImage.LANCZOS)\n",
        "    return img\n",
        "\n",
        "def flush():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    #torch.cuda.clear_autocast_cache()\n",
        "\n",
        "def clear_img2img_pipe():\n",
        "  global pipe_img2img\n",
        "  if pipe_img2img is not None:\n",
        "    #print(\"Clearing out img2img pipeline for more VRAM\")\n",
        "    del pipe_img2img\n",
        "    flush()\n",
        "    pipe_img2img = None\n",
        "def clear_txt2img_pipe():\n",
        "  global pipe, compel_proc\n",
        "  if pipe is not None:\n",
        "    #print(\"Clearing out text2img pipeline for more VRAM\")\n",
        "    del pipe\n",
        "    flush()\n",
        "    pipe = None\n",
        "    if prefs['SD_compel']:\n",
        "      if compel_proc is not None:\n",
        "        del compel_proc\n",
        "        compel_proc = None\n",
        "def clear_unet_pipe():\n",
        "  global unet\n",
        "  if unet is not None:\n",
        "    #print(\"Clearing out unet custom pipeline for more VRAM\")\n",
        "    del unet\n",
        "    flush()\n",
        "    unet = None\n",
        "def clear_SDXL_pipe():\n",
        "  global pipe_SDXL, pipe_SDXL_refiner, compel_base, compel_refiner\n",
        "  if prefs['SDXL_compel']:\n",
        "    if compel_base is not None:\n",
        "      del compel_base, compel_refiner\n",
        "      compel_base = None\n",
        "      compel_refiner = None\n",
        "  if pipe_SDXL is not None:\n",
        "    del pipe_SDXL\n",
        "    flush()\n",
        "    pipe_SDXL = None\n",
        "  if pipe_SDXL_refiner is not None:\n",
        "    del pipe_SDXL_refiner\n",
        "    flush()\n",
        "    pipe_SDXL_refiner = None\n",
        "\n",
        "def clear_clip_guided_pipe():\n",
        "  global pipe_clip_guided\n",
        "  if pipe_clip_guided is not None:\n",
        "    #print(\"Clearing out CLIP Guided pipeline for more VRAM\")\n",
        "    del pipe_clip_guided\n",
        "    flush()\n",
        "    pipe_clip_guided = None\n",
        "def clear_conceptualizer_pipe():\n",
        "  global pipe_conceptualizer\n",
        "  if pipe_conceptualizer is not None:\n",
        "    #print(\"Clearing out CLIP Guided pipeline for more VRAM\")\n",
        "    del pipe_conceptualizer\n",
        "    flush()\n",
        "    pipe_conceptualizer = None\n",
        "def clear_repaint_pipe():\n",
        "  global pipe_repaint\n",
        "  if pipe_repaint is not None:\n",
        "    del pipe_repaint\n",
        "    flush()\n",
        "    pipe_repaint = None\n",
        "def clear_imagic_pipe():\n",
        "  global pipe_imagic\n",
        "  if pipe_imagic is not None:\n",
        "    del pipe_imagic\n",
        "    flush()\n",
        "    pipe_imagic = None\n",
        "def clear_composable_pipe():\n",
        "  global pipe_composable\n",
        "  if pipe_composable is not None:\n",
        "    del pipe_composable\n",
        "    flush()\n",
        "    pipe_composable = None\n",
        "def clear_versatile_pipe():\n",
        "  global pipe_versatile\n",
        "  if pipe_versatile is not None:\n",
        "    del pipe_versatile\n",
        "    flush()\n",
        "    pipe_versatile = None\n",
        "def clear_versatile_text2img_pipe():\n",
        "  global pipe_versatile_text2img\n",
        "  if pipe_versatile_text2img is not None:\n",
        "    del pipe_versatile_text2img\n",
        "    flush()\n",
        "    pipe_versatile_text2img = None\n",
        "def clear_versatile_variation_pipe():\n",
        "  global pipe_versatile_variation\n",
        "  if pipe_versatile_variation is not None:\n",
        "    del pipe_versatile_variation\n",
        "    flush()\n",
        "    pipe_versatile_variation = None\n",
        "def clear_versatile_dualguided_pipe():\n",
        "  global pipe_versatile_dualguided\n",
        "  if pipe_versatile_dualguided is not None:\n",
        "    del pipe_versatile_dualguided\n",
        "    flush()\n",
        "    pipe_versatile_dualguided = None\n",
        "def clear_depth_pipe():\n",
        "  global pipe_depth\n",
        "  if pipe_depth is not None:\n",
        "    del pipe_depth\n",
        "    flush()\n",
        "    pipe_depth = None\n",
        "def clear_interpolation_pipe():\n",
        "  global pipe_interpolation\n",
        "  if pipe_interpolation is not None:\n",
        "    del pipe_interpolation\n",
        "    flush()\n",
        "    pipe_interpolation = None\n",
        "def clear_safe_pipe():\n",
        "  global pipe_safe\n",
        "  if pipe_safe is not None:\n",
        "    del pipe_safe\n",
        "    flush()\n",
        "    pipe_safe = None\n",
        "def clear_upscale_pipe():\n",
        "  global pipe_upscale\n",
        "  if pipe_upscale is not None:\n",
        "    del pipe_upscale\n",
        "    flush()\n",
        "    pipe_upscale = None\n",
        "def clear_image_variation_pipe():\n",
        "  global pipe_image_variation\n",
        "  if pipe_image_variation is not None:\n",
        "    del pipe_image_variation\n",
        "    flush()\n",
        "    pipe_image_variation = None\n",
        "def clear_semantic_pipe():\n",
        "  global pipe_semantic\n",
        "  if pipe_semantic is not None:\n",
        "    del pipe_semantic\n",
        "    flush()\n",
        "    pipe_semantic = None\n",
        "def clear_EDICT_pipe():\n",
        "  global pipe_EDICT, text_encoder_EDICT\n",
        "  if pipe_EDICT is not None:\n",
        "    del pipe_EDICT\n",
        "    del text_encoder_EDICT\n",
        "    flush()\n",
        "    pipe_EDICT = None\n",
        "    text_encoder_EDICT = None\n",
        "def clear_DiffEdit_pipe():\n",
        "  global pipe_DiffEdit\n",
        "  if pipe_DiffEdit is not None:\n",
        "    del pipe_DiffEdit\n",
        "    flush()\n",
        "    pipe_DiffEdit = None\n",
        "def clear_null_text_pipe():\n",
        "  global pipe_null_text\n",
        "  if pipe_null_text is not None:\n",
        "    del pipe_null_text\n",
        "    flush()\n",
        "    pipe_null_text = None\n",
        "def clear_unCLIP_pipe():\n",
        "  global pipe_unCLIP\n",
        "  if pipe_unCLIP is not None:\n",
        "    del pipe_unCLIP\n",
        "    flush()\n",
        "    pipe_unCLIP = None\n",
        "def clear_unCLIP_image_variation_pipe():\n",
        "  global pipe_unCLIP_image_variation\n",
        "  if pipe_unCLIP_image_variation is not None:\n",
        "    del pipe_unCLIP_image_variation\n",
        "    flush()\n",
        "    pipe_unCLIP_image_variation = None\n",
        "def clear_unCLIP_interpolation_pipe():\n",
        "  global pipe_unCLIP_interpolation\n",
        "  if pipe_unCLIP_interpolation is not None:\n",
        "    del pipe_unCLIP_interpolation\n",
        "    flush()\n",
        "    pipe_unCLIP_interpolation = None\n",
        "def clear_unCLIP_image_interpolation_pipe():\n",
        "  global pipe_unCLIP_image_interpolation\n",
        "  if pipe_unCLIP_image_interpolation is not None:\n",
        "    del pipe_unCLIP_image_interpolation\n",
        "    flush()\n",
        "    pipe_unCLIP_image_interpolation = None\n",
        "def clear_wuerstchen_pipe():\n",
        "  global pipe_wuerstchen\n",
        "  if pipe_wuerstchen is not None:\n",
        "    del pipe_wuerstchen.prior_pipe\n",
        "    del pipe_wuerstchen.decoder_pipe\n",
        "    del pipe_wuerstchen\n",
        "    flush()\n",
        "    pipe_wuerstchen = None\n",
        "def clear_pixart_alpha_pipe():\n",
        "  global pipe_pixart_alpha, pipe_pixart_alpha_encoder\n",
        "  if pipe_pixart_alpha is not None:\n",
        "    del pipe_pixart_alpha, pipe_pixart_alpha_encoder\n",
        "    flush()\n",
        "    pipe_pixart_alpha = None\n",
        "    pipe_pixart_alpha_encoder = None\n",
        "def clear_magic_mix_pipe():\n",
        "  global pipe_magic_mix\n",
        "  if pipe_magic_mix is not None:\n",
        "    del pipe_magic_mix\n",
        "    flush()\n",
        "    pipe_magic_mix = None\n",
        "def clear_paint_by_example_pipe():\n",
        "  global pipe_paint_by_example\n",
        "  if pipe_paint_by_example is not None:\n",
        "    del pipe_paint_by_example\n",
        "    flush()\n",
        "    pipe_paint_by_example = None\n",
        "def clear_instruct_pix2pix_pipe():\n",
        "  global pipe_instruct_pix2pix\n",
        "  if pipe_instruct_pix2pix is not None:\n",
        "    del pipe_instruct_pix2pix\n",
        "    flush()\n",
        "    pipe_instruct_pix2pix = None\n",
        "def clear_alt_diffusion_pipe():\n",
        "  global pipe_alt_diffusion\n",
        "  if pipe_alt_diffusion is not None:\n",
        "    del pipe_alt_diffusion\n",
        "    flush()\n",
        "    pipe_alt_diffusion = None\n",
        "def clear_alt_diffusion_img2img_pipe():\n",
        "  global pipe_alt_diffusion_img2img\n",
        "  if pipe_alt_diffusion_img2img is not None:\n",
        "    del pipe_alt_diffusion_img2img\n",
        "    flush()\n",
        "    pipe_alt_diffusion_img2img = None\n",
        "def clear_SAG_pipe():\n",
        "  global pipe_SAG\n",
        "  if pipe_SAG is not None:\n",
        "    del pipe_SAG\n",
        "    flush()\n",
        "    pipe_SAG = None\n",
        "def clear_demofusion_pipe():\n",
        "  global pipe_demofusion\n",
        "  if pipe_demofusion is not None:\n",
        "    del pipe_demofusion\n",
        "    flush()\n",
        "    pipe_demofusion = None\n",
        "def clear_attend_and_excite_pipe():\n",
        "  global pipe_attend_and_excite\n",
        "  if pipe_attend_and_excite is not None:\n",
        "    del pipe_attend_and_excite\n",
        "    flush()\n",
        "    pipe_attend_and_excite = None\n",
        "def clear_lmd_plus_pipe():\n",
        "  global pipe_lmd_plus\n",
        "  if pipe_lmd_plus is not None:\n",
        "    del pipe_lmd_plus\n",
        "    flush()\n",
        "    pipe_lmd_plus = None\n",
        "def clear_lcm_pipe():\n",
        "  global pipe_lcm\n",
        "  if pipe_lcm is not None:\n",
        "    del pipe_lcm\n",
        "    flush()\n",
        "    pipe_lcm = None\n",
        "def clear_lcm_interpolation_pipe():\n",
        "  global pipe_lcm_interpolation\n",
        "  if pipe_lcm_interpolation is not None:\n",
        "    del pipe_lcm_interpolation\n",
        "    flush()\n",
        "    pipe_lcm_interpolation = None\n",
        "def clear_ldm3d_pipe():\n",
        "  global pipe_ldm3d, pipe_ldm3d_upscale\n",
        "  if pipe_ldm3d is not None:\n",
        "    del pipe_ldm3d, pipe_ldm3d_upscale\n",
        "    flush()\n",
        "    pipe_ldm3d = None\n",
        "    pipe_ldm3d_upscale = None\n",
        "def clear_svd_pipe():\n",
        "  global pipe_svd\n",
        "  if pipe_svd is not None:\n",
        "    del pipe_svd\n",
        "    flush()\n",
        "    pipe_svd = None\n",
        "def clear_panorama_pipe():\n",
        "  global pipe_panorama\n",
        "  if pipe_panorama is not None:\n",
        "    del pipe_panorama\n",
        "    flush()\n",
        "    pipe_panorama = None\n",
        "def clear_dance_pipe():\n",
        "  global pipe_dance\n",
        "  if pipe_dance is not None:\n",
        "    del pipe_dance\n",
        "    flush()\n",
        "    pipe_dance = None\n",
        "def clear_audio_diffusion_pipe():\n",
        "  global pipe_audio_diffusion\n",
        "  if pipe_audio_diffusion is not None:\n",
        "    del pipe_audio_diffusion\n",
        "    flush()\n",
        "    pipe_audio_diffusion = None\n",
        "def clear_music_gen_pipe():\n",
        "  global pipe_music_gen\n",
        "  if pipe_music_gen is not None:\n",
        "    del pipe_music_gen\n",
        "    flush()\n",
        "    pipe_music_gen = None\n",
        "def clear_riffusion_pipe():\n",
        "  global pipe_riffusion\n",
        "  if pipe_riffusion is not None:\n",
        "    del pipe_riffusion\n",
        "    flush()\n",
        "    pipe_riffusion = None\n",
        "def clear_voice_fixer_pipe():\n",
        "  global pipe_voice_fixer\n",
        "  if pipe_voice_fixer is not None:\n",
        "    del pipe_voice_fixer\n",
        "    flush()\n",
        "    pipe_voice_fixer = None\n",
        "def clear_whisper_pipe():\n",
        "  global pipe_whisper\n",
        "  if pipe_whisper is not None:\n",
        "    del pipe_whisper\n",
        "    flush()\n",
        "    pipe_whisper = None\n",
        "def clear_text_to_video_pipe():\n",
        "  global pipe_text_to_video\n",
        "  if pipe_text_to_video is not None:\n",
        "    del pipe_text_to_video\n",
        "    flush()\n",
        "    pipe_text_to_video = None\n",
        "def clear_text_to_video_zero_pipe():\n",
        "  global pipe_text_to_video_zero\n",
        "  if pipe_text_to_video_zero is not None:\n",
        "    del pipe_text_to_video_zero\n",
        "    flush()\n",
        "    torch.cuda.ipc_collect()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    pipe_text_to_video_zero = None\n",
        "def clear_video_to_video_pipe():\n",
        "  global pipe_video_to_video\n",
        "  if pipe_video_to_video is not None:\n",
        "    del pipe_video_to_video\n",
        "    flush()\n",
        "    pipe_video_to_video = None\n",
        "def clear_infinite_zoom_pipe():\n",
        "  global pipe_infinite_zoom\n",
        "  if pipe_infinite_zoom is not None:\n",
        "    del pipe_infinite_zoom\n",
        "    flush()\n",
        "    pipe_infinite_zoom = None\n",
        "def clear_deepfloyd_pipe():\n",
        "  global pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3\n",
        "  if pipe_deepfloyd is not None:\n",
        "    del pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3\n",
        "    flush()\n",
        "    pipe_deepfloyd = None\n",
        "    pipe_deepfloyd2 = None\n",
        "    pipe_deepfloyd3 = None\n",
        "def clear_amused_pipe():\n",
        "  global pipe_amused\n",
        "  if pipe_amused is not None:\n",
        "    del pipe_amused\n",
        "    flush()\n",
        "    pipe_amused = None\n",
        "def clear_blip_diffusion_pipe():\n",
        "  global pipe_blip_diffusion\n",
        "  if pipe_blip_diffusion is not None:\n",
        "    del pipe_blip_diffusion\n",
        "    flush()\n",
        "    pipe_blip_diffusion = None\n",
        "def clear_anytext_pipe():\n",
        "  global pipe_anytext\n",
        "  if pipe_anytext is not None:\n",
        "    del pipe_anytext\n",
        "    flush()\n",
        "    pipe_anytext = None\n",
        "def clear_fuyu_pipe():\n",
        "  global fuyu_tokenizer, fuyu_model, fuyu_processor\n",
        "  if fuyu_tokenizer is not None:\n",
        "    del fuyu_tokenizer\n",
        "    del fuyu_model\n",
        "    del fuyu_processor\n",
        "    flush()\n",
        "    fuyu_tokenizer = None\n",
        "    fuyu_model = None\n",
        "    fuyu_processor = None\n",
        "def clear_reference_pipe():\n",
        "  global pipe_reference\n",
        "  if pipe_reference is not None:\n",
        "    del pipe_reference\n",
        "    flush()\n",
        "    pipe_reference = None\n",
        "def clear_ip_adapter_pipe():\n",
        "  global pipe_ip_adapter\n",
        "  if pipe_ip_adapter is not None:\n",
        "    del pipe_ip_adapter\n",
        "    flush()\n",
        "    pipe_ip_adapter = None\n",
        "def clear_controlnet_qr_pipe():\n",
        "  global pipe_controlnet_qr, pipe_controlnet\n",
        "  if pipe_controlnet_qr is not None:\n",
        "    del pipe_controlnet_qr\n",
        "    flush()\n",
        "    pipe_controlnet_qr = None\n",
        "  if pipe_controlnet is not None:\n",
        "    del pipe_controlnet\n",
        "    flush()\n",
        "    pipe_controlnet = None\n",
        "def clear_controlnet_segment_pipe():\n",
        "  global pipe_controlnet_segment, pipe_controlnet\n",
        "  if pipe_controlnet_segment is not None:\n",
        "    del pipe_controlnet_segment\n",
        "    flush()\n",
        "    pipe_controlnet_segment = None\n",
        "  if pipe_controlnet is not None:\n",
        "    del pipe_controlnet\n",
        "    flush()\n",
        "    pipe_controlnet = None\n",
        "def clear_DiT_pipe():\n",
        "  global pipe_DiT\n",
        "  if pipe_DiT is not None:\n",
        "    del pipe_DiT\n",
        "    flush()\n",
        "    pipe_DiT = None\n",
        "def clear_kandinsky_pipe(all=True):\n",
        "  global pipe_kandinsky, pipe_kandinsky_prior, pipe_kandinsky_controlnet_prior, loaded_kandinsky_task, depth_estimator\n",
        "  if pipe_kandinsky is not None:\n",
        "    del pipe_kandinsky\n",
        "    if all:\n",
        "      del pipe_kandinsky_prior\n",
        "      del pipe_kandinsky_controlnet_prior\n",
        "      del depth_estimator\n",
        "    flush()\n",
        "    pipe_kandinsky = None\n",
        "    if all:\n",
        "      pipe_kandinsky_prior = None\n",
        "      pipe_kandinsky_controlnet_prior = None\n",
        "      depth_estimator = None\n",
        "    loaded_kandinsky_task = \"\"\n",
        "def clear_tortoise_tts_pipe():\n",
        "  global pipe_tortoise_tts\n",
        "  if pipe_tortoise_tts is not None:\n",
        "    del pipe_tortoise_tts\n",
        "    flush()\n",
        "    pipe_tortoise_tts = None\n",
        "def clear_audio_ldm_pipe():\n",
        "  global pipe_audio_ldm\n",
        "  if pipe_audio_ldm is not None:\n",
        "    del pipe_audio_ldm\n",
        "    flush()\n",
        "    pipe_audio_ldm = None\n",
        "def clear_audio_ldm2_pipe():\n",
        "  global pipe_audio_ldm2\n",
        "  if pipe_audio_ldm2 is not None:\n",
        "    del pipe_audio_ldm2\n",
        "    flush()\n",
        "    pipe_audio_ldm2 = None\n",
        "def clear_music_ldm_pipe():\n",
        "  global pipe_music_ldm\n",
        "  if pipe_music_ldm is not None:\n",
        "    del pipe_music_ldm\n",
        "    flush()\n",
        "    pipe_music_ldm = None\n",
        "def clear_gpt2_pipe():\n",
        "  global pipe_gpt2\n",
        "  if pipe_gpt2 is not None:\n",
        "    del pipe_gpt2\n",
        "    flush()\n",
        "    pipe_gpt2 = None\n",
        "def clear_distil_gpt2_pipe():\n",
        "  global pipe_distil_gpt2\n",
        "  if pipe_distil_gpt2 is not None:\n",
        "    del pipe_distil_gpt2\n",
        "    flush()\n",
        "    pipe_distil_gpt2 = None\n",
        "def clear_background_remover_pipe():\n",
        "  global pipe_background_remover\n",
        "  if pipe_background_remover is not None:\n",
        "    del pipe_background_remover\n",
        "    flush()\n",
        "    pipe_background_remover = None\n",
        "def clear_shap_e_pipe():\n",
        "  global pipe_shap_e\n",
        "  if pipe_shap_e is not None:\n",
        "    del pipe_shap_e\n",
        "    flush()\n",
        "    pipe_shap_e = None\n",
        "def clear_marigold_depth_pipe():\n",
        "  global pipe_marigold_depth\n",
        "  if pipe_marigold_depth is not None:\n",
        "    del pipe_marigold_depth\n",
        "    flush()\n",
        "    pipe_marigold_depth = None\n",
        "def clear_zoe_depth_pipe():\n",
        "  global pipe_zoe_depth\n",
        "  if pipe_zoe_depth is not None:\n",
        "    del pipe_zoe_depth\n",
        "    flush()\n",
        "    pipe_zoe_depth = None\n",
        "def clear_controlnet_pipe():\n",
        "  global pipe_controlnet, controlnet, controlnet_models, controlnet_xl_models, status\n",
        "  if pipe_controlnet is not None:\n",
        "    del pipe_controlnet\n",
        "    del controlnet\n",
        "    for k, v in controlnet_models.items():\n",
        "      if v != None:\n",
        "        del v\n",
        "        controlnet_models[k] = None\n",
        "    for k, v in controlnet_xl_models.items():\n",
        "      if v != None:\n",
        "        del v\n",
        "        controlnet_xl_models[k] = None\n",
        "    flush()\n",
        "    pipe_controlnet = None\n",
        "    controlnet = None\n",
        "    status['loaded_controlnet'] = None\n",
        "def clear_stable_lm_pipe():\n",
        "  global pipe_stable_lm, tokenizer_stable_lm\n",
        "  if pipe_stable_lm is not None:\n",
        "    del pipe_stable_lm\n",
        "    del tokenizer_stable_lm\n",
        "    flush()\n",
        "    pipe_stable_lm = None\n",
        "    tokenizer_stable_lm = None\n",
        "\n",
        "def clear_pipes(allbut=None):\n",
        "    if torch_device == \"cpu\": return\n",
        "    but = [] if allbut == None else [allbut] if type(allbut) is str else allbut\n",
        "    if not 'txt2img' in but: clear_txt2img_pipe()\n",
        "    if not 'img2img' in but: clear_img2img_pipe()\n",
        "    if not 'SDXL' in but: clear_SDXL_pipe()\n",
        "    if not 'unet' in but: clear_unet_pipe()\n",
        "    if not 'clip_guided' in but: clear_clip_guided_pipe()\n",
        "    if not 'conceptualizer' in but: clear_conceptualizer_pipe()\n",
        "    if not 'repaint' in but: clear_repaint_pipe()\n",
        "    if not 'imagic' in but: clear_imagic_pipe()\n",
        "    if not 'composable': clear_composable_pipe()\n",
        "    if not 'versatile_text2img' in but: clear_versatile_text2img_pipe()\n",
        "    if not 'versatile_variation' in but: clear_versatile_variation_pipe()\n",
        "    if not 'versatile_dualguided' in but: clear_versatile_dualguided_pipe()\n",
        "    if not 'depth' in but: clear_depth_pipe()\n",
        "    if not 'interpolation' in but: clear_interpolation_pipe()\n",
        "    if not 'safe' in but: clear_safe_pipe()\n",
        "    if not 'upscale' in but: clear_upscale_pipe()\n",
        "    if not 'unCLIP' in but: clear_unCLIP_pipe()\n",
        "    if not 'EDICT' in but: clear_EDICT_pipe()\n",
        "    if not 'DiffEdit' in but: clear_DiffEdit_pipe()\n",
        "    if not 'null_text' in but: clear_null_text_pipe()\n",
        "    if not 'unCLIP_image_variation' in but: clear_unCLIP_image_variation_pipe()\n",
        "    if not 'unCLIP_interpolation' in but: clear_unCLIP_interpolation_pipe()\n",
        "    if not 'image_variation' in but: clear_image_variation_pipe()\n",
        "    if not 'semantic' in but: clear_semantic_pipe()\n",
        "    if not 'wuerstchen' in but: clear_wuerstchen_pipe()\n",
        "    if not 'pixart_alpha' in but: clear_pixart_alpha_pipe()\n",
        "    if not 'magic_mix' in but: clear_magic_mix_pipe()\n",
        "    if not 'alt_diffusion' in but: clear_alt_diffusion_pipe()\n",
        "    if not 'alt_diffusion_img2img' in but: clear_alt_diffusion_img2img_pipe()\n",
        "    if not 'paint_by_example' in but: clear_paint_by_example_pipe()\n",
        "    if not 'instruct_pix2pix' in but: clear_instruct_pix2pix_pipe()\n",
        "    if not 'SAG' in but: clear_SAG_pipe()\n",
        "    if not 'demofusion' in but: clear_demofusion_pipe()\n",
        "    if not 'attend_and_excite' in but: clear_attend_and_excite_pipe()\n",
        "    if not 'lmd_plus' in but: clear_lmd_plus_pipe()\n",
        "    if not 'lcm' in but: clear_lcm_pipe()\n",
        "    if not 'lcm_interpolation' in but: clear_lcm_interpolation_pipe()\n",
        "    if not 'ldm3d' in but: clear_ldm3d_pipe()\n",
        "    if not 'svd' in but: clear_svd_pipe()\n",
        "    if not 'deepfloyd' in but: clear_deepfloyd_pipe()\n",
        "    if not 'amused' in but: clear_amused_pipe()\n",
        "    if not 'blip_diffusion' in but: clear_blip_diffusion_pipe()\n",
        "    if not 'anytext' in but: clear_anytext_pipe()\n",
        "    if not 'fuyu' in but: clear_fuyu_pipe()\n",
        "    if not 'ip_adapter' in but: clear_ip_adapter_pipe()\n",
        "    if not 'reference' in but: clear_reference_pipe()\n",
        "    if not 'controlnet_qr' in but: clear_controlnet_qr_pipe()\n",
        "    if not 'controlnet_segment' in but: clear_controlnet_segment_pipe()\n",
        "    if not 'DiT' in but: clear_DiT_pipe()\n",
        "    if not 'controlnet' in but: clear_controlnet_pipe()\n",
        "    if not 'panorama' in but: clear_panorama_pipe()\n",
        "    if not 'kandinsky' in but: clear_kandinsky_pipe(all=not ('kandinsky_prior' in but or 'kandinsky_controlnet_prior' in but))\n",
        "    if not 'dance' in but: clear_dance_pipe()\n",
        "    if not 'riffusion' in but: clear_riffusion_pipe()\n",
        "    if not 'audio_diffusion' in but: clear_audio_diffusion_pipe()\n",
        "    if not 'music_gen' in but: clear_music_gen_pipe()\n",
        "    if not 'voice_fixer' in but: clear_voice_fixer_pipe()\n",
        "    if not 'whisper' in but: clear_whisper_pipe()\n",
        "    if not 'text_to_video' in but: clear_text_to_video_pipe()\n",
        "    if not 'text_to_video_zero' in but: clear_text_to_video_zero_pipe()\n",
        "    if not 'video_to_video' in but: clear_video_to_video_pipe()\n",
        "    if not 'infinite_zoom' in but: clear_infinite_zoom_pipe()\n",
        "    if not 'tortoise_tts' in but: clear_tortoise_tts_pipe()\n",
        "    if not 'audio_ldm' in but: clear_audio_ldm_pipe()\n",
        "    if not 'audio_ldm2' in but: clear_audio_ldm2_pipe()\n",
        "    if not 'music_ldm' in but: clear_music_ldm_pipe()\n",
        "    if not 'gpt2' in but: clear_gpt2_pipe()\n",
        "    if not 'distil_gpt2' in but: clear_distil_gpt2_pipe()\n",
        "    if not 'shap_e' in but: clear_shap_e_pipe()\n",
        "    if not 'zoe_depth' in but: clear_zoe_depth_pipe()\n",
        "    if not 'marigold_depth' in but: clear_marigold_depth_pipe()\n",
        "    if not 'background_remover' in but: clear_background_remover_pipe()\n",
        "    if not 'controlnet' in but: clear_controlnet_pipe()\n",
        "    if not 'stable_lm' in but: clear_stable_lm_pipe()\n",
        "    try:\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    except Exception:\n",
        "        pass\n",
        "import base64\n",
        "def get_base64(image_path):\n",
        "    with open(image_path, \"rb\") as img_file:\n",
        "        base64_string = base64.b64encode(img_file.read()).decode('utf-8')\n",
        "        return base64_string\n",
        "def pil_to_base64(image):\n",
        "    image_stream = io.BytesIO()\n",
        "    image.save(image_stream, format='PNG')\n",
        "    image_bytes = image_stream.getvalue()\n",
        "    base64_string = base64.b64encode(image_bytes).decode('utf-8')\n",
        "    return base64_string\n",
        "\n",
        "def available_file(folder, name, idx=0, ext='png', no_num=False, zfill=None):\n",
        "  available = False\n",
        "  while not available:\n",
        "    # Todo, check if using PyDrive2\n",
        "    if no_num:\n",
        "      if not os.path.isfile(os.path.join(folder, f'{name}.{ext}')):\n",
        "        return os.path.join(folder, f'{name}.{ext}')\n",
        "    i = str(idx) if zfill is None else str(idx).zfill(zfill)\n",
        "    if os.path.isfile(os.path.join(folder, f'{name}-{i}.{ext}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "    \n",
        "  return os.path.join(folder, f'{name}-{i}.{ext}')\n",
        "\n",
        "def available_folder(folder, name, idx):\n",
        "  available = False\n",
        "  while not available:\n",
        "    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}')\n",
        "\n",
        "def filepath_to_url(path):\n",
        "    windows_path_pattern = re.compile(r\"(.)\\:\\/\")\n",
        "    linux_path_pattern = re.compile(r\"^\\/\")\n",
        "    path = path.replace('\\\\', '/')\n",
        "    is_windows_path = windows_path_pattern.match(path)\n",
        "    is_linux_path = linux_path_pattern.match(path)\n",
        "    if is_windows_path:\n",
        "        drive_letter = windows_path_pattern.match(path).group(1)\n",
        "        path = re.sub(windows_path_pattern, f\"file:///{drive_letter}:/\", path)\n",
        "    elif is_linux_path:\n",
        "        path = re.sub(linux_path_pattern, \"file:///\", path)\n",
        "    else:\n",
        "        path = f\"file:///{path}\"\n",
        "    path = path.replace(\" \", \"%20\")\n",
        "    return path\n",
        "\n",
        "#import asyncio\n",
        "#async\n",
        "def start_diffusion(page):\n",
        "  global pipe, unet, pipe_img2img, pipe_clip_guided, pipe_interpolation, pipe_conceptualizer, pipe_imagic, pipe_depth, pipe_composable, pipe_versatile_text2img, pipe_versatile_variation, pipe_versatile_dualguided, pipe_SAG, pipe_attend_and_excite, pipe_alt_diffusion, pipe_alt_diffusion_img2img, pipe_panorama, pipe_safe, pipe_upscale, pipe_SDXL, pipe_SDXL_refiner\n",
        "  global SD_sampler, stability_api, total_steps, pb, prefs, args, total_steps, compel_proc, compel_base, compel_refiner\n",
        "  def prt(line, update=True):\n",
        "    if type(line) == str:\n",
        "      line = Text(line)\n",
        "    try:\n",
        "      page.imageColumn.controls.append(line)\n",
        "      if update:\n",
        "        page.imageColumn.update()\n",
        "    except Exception:\n",
        "      clear_image_output()\n",
        "      pass\n",
        "    if update:\n",
        "      page.Images.update()\n",
        "  def clear_last(update=True):\n",
        "    del page.imageColumn.controls[-1]\n",
        "    if update:\n",
        "      page.imageColumn.update()\n",
        "      page.Images.update()\n",
        "  abort_run = False\n",
        "  def abort_diffusion(e):\n",
        "    nonlocal abort_run\n",
        "    abort_run = True\n",
        "    page.snd_error.play()\n",
        "    page.snd_delete.play()\n",
        "  def callback_cancel(cancel) -> None:\n",
        "    callback_cancel.has_been_called = True\n",
        "    if abort_run:\n",
        "      return True\n",
        "  def download_image(e):\n",
        "    if is_Colab:\n",
        "      print(f\"{type(e.control.data)} {e.control.data}\")\n",
        "      from google.colab import files\n",
        "      if os.path.isfile(e.control.data):\n",
        "        files.download(e.control.data)\n",
        "      else:\n",
        "        time.sleep(5)\n",
        "        files.download(e.control.data)\n",
        "  def clear_image_output():\n",
        "    for co in reversed(page.imageColumn.controls):\n",
        "      del co\n",
        "    page.imageColumn.controls.clear()\n",
        "    try:\n",
        "      page.imageColumn.update()\n",
        "    except Exception as e:\n",
        "      try:\n",
        "        page.imageColumn = Column([], auto_scroll=True, scroll=ScrollMode.AUTO)\n",
        "      except Exception as er:\n",
        "        alert_msg(page, f\"ERROR: Problem Clearing Image Output List. May need to stop script and restart app to recover, sorry...\", content=Text(f'{e}\\n{er}'))\n",
        "        page.Images = buildImages(page)\n",
        "        pass\n",
        "      page.update()\n",
        "      pass\n",
        "# Why getting Exception: control with ID '_3607' not found when re-running after error\n",
        "  #page.Images.content.controls = []\n",
        "  if int(status['cpu_memory']) <= 8 and prefs['use_SDXL'] and status['installed_SDXL']: # and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run SD XL. Either Change runtime type to High-RAM mode and restart or use other pipelines.\")\n",
        "      return\n",
        "  clear_image_output()\n",
        "  #pb = ProgressBar(bar_height=8)\n",
        "  pb.width=(page.width if page.web else page.window_width) - 50\n",
        "  #prt(Row([Text(\"‚ñ∂Ô∏è   Running Stable Diffusion on Batch Prompts List\", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Diffusion Run\", on_click=abort_diffusion)], alignment=MainAxisAlignment.SPACE_BETWEEN))\n",
        "  prt(Header(\"‚ñ∂Ô∏è   Running Stable Diffusion on Batch Prompts List\", actions=[IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Diffusion Run\", on_click=abort_diffusion)]))\n",
        "  import string, shutil, random, gc, io, json\n",
        "  from collections import ChainMap\n",
        "  from PIL.PngImagePlugin import PngInfo\n",
        "  from contextlib import contextmanager, nullcontext\n",
        "  import copy\n",
        "\n",
        "  if status['installed_diffusers']:\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "  os.chdir(stable_dir)\n",
        "  generator = None\n",
        "  clear_repaint_pipe()\n",
        "  output_files = []\n",
        "  pipe_used = \"\"\n",
        "  retry_attempts_if_NSFW = prefs['retry_attempts']\n",
        "  #if (prefs['use_Stability_api'] and status['installed_stability']) or bool(not status['installed_diffusers'] and status['installed_stability']):\n",
        "  #  update_stability()\n",
        "  last_seed = args['seed']\n",
        "  if args['seed'] < 1 or args['seed'] is None:\n",
        "    rand_seed = random.randint(0,2147483647)\n",
        "    if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "      if use_custom_scheduler:\n",
        "        generator = torch.manual_seed(rand_seed)\n",
        "      else:\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(rand_seed)\n",
        "    last_seed = rand_seed\n",
        "  else:\n",
        "    if not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability'])) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "      if use_custom_scheduler:\n",
        "        generator = torch.manual_seed(args['seed'])\n",
        "      else:\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(args['seed'])\n",
        "  strikes = 0\n",
        "  p_idx = 0\n",
        "  if prefs['centipede_prompts_as_init_images']:\n",
        "    os.makedirs(os.path.join(root_dir, 'init_images'), exist_ok=True)\n",
        "  last_image = None\n",
        "  updated_prompts = []\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "      \n",
        "  if not (prefs[\"use_interpolation\"] and status['installed_interpolation']):\n",
        "    for p in prompts:\n",
        "      pr = None\n",
        "      arg = {}\n",
        "      if type(p) == list or type(p) == str:\n",
        "        pr = p\n",
        "        arg = args.copy()\n",
        "      elif isinstance(p, Dream):\n",
        "        pr = p.prompt\n",
        "        arg = merge_dict(args, p.arg)\n",
        "      else: prt(f'Unknown item in list of type {type(p)}')\n",
        "      #print(str(arg))\n",
        "      arg['width'] = int(arg['width'])\n",
        "      arg['height'] = int(arg['height'])\n",
        "      arg['seed'] = int(arg['seed'])\n",
        "      arg['guidance_scale'] = float(arg['guidance_scale'])\n",
        "      arg['batch_size'] = int(arg['batch_size'])\n",
        "      arg['n_iterations'] = int(arg['n_iterations'])\n",
        "      arg['steps'] = int(arg['steps'])\n",
        "      arg['eta'] = float(arg['eta'])\n",
        "      arg['init_image_strength'] = float(arg['init_image_strength'])\n",
        "      p.arg = arg\n",
        "      iterations = arg['n_iterations']\n",
        "      updated_prompts.append(p)\n",
        "      if iterations > 1:\n",
        "        #print(f\"Iterating {iterations} times - {pr}\")\n",
        "        for d in range(iterations - 1):\n",
        "          new_dream = None\n",
        "          if isinstance(p, Dream):\n",
        "            new_dream = copy.copy(p)\n",
        "            new_dream.prompt = pr[0] if type(pr) == list else pr\n",
        "            new_arg = new_dream.arg.copy()\n",
        "            new_arg['seed'] = random.randint(0,2147483647)\n",
        "            new_arg['n_iterations'] = 1\n",
        "            new_dream.arg = new_arg\n",
        "            #new_dream.arg['seed'] = random.randint(0,4294967295)\n",
        "          else:\n",
        "            new_dream = Dream(p, seed=random.randint(0,2147483647), n_iterations=1)\n",
        "          new_dream.arg['n_iterations'] = 1\n",
        "          #prompts.insert(p_idx+1, new_dream)\n",
        "          updated_prompts.append(new_dream)\n",
        "    prefix = \"\"\n",
        "    if bool(model['prefix']):\n",
        "      if model['prefix'][-1] != ' ':\n",
        "        model['prefix'] = model['prefix'] + ' '\n",
        "      prefix = model['prefix']\n",
        "    lcm_lora = False\n",
        "    if prefs['use_LoRA_model']:\n",
        "      if prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "        for l in prefs['active_SDXL_LoRA_layers']:\n",
        "          if 'name' in l:\n",
        "            if 'LCM' in l['name']:\n",
        "              lcm_lora = True\n",
        "          if 'prefix' in l:\n",
        "           if bool(l['prefix']):\n",
        "              prefix += l['prefix']\n",
        "              if prefix[-1] != ' ':\n",
        "                prefix += ' '\n",
        "        #lora = get_SDXL_LoRA_model(prefs['SDXL_LoRA_model'])\n",
        "      else:\n",
        "        for l in prefs['active_LoRA_layers']:\n",
        "          if 'name' in l:\n",
        "            if 'LCM' in l['name']:\n",
        "              lcm_lora = True\n",
        "          if 'prefix' in l:\n",
        "           if bool(l['prefix']):\n",
        "              prefix += l['prefix']\n",
        "              if prefix[-1] != ' ':\n",
        "                prefix += ' '\n",
        "        #lora = get_LoRA_model(prefs['LoRA_model'])\n",
        "    ip_adapter_arg = {}\n",
        "    if prefs['use_ip_adapter']:\n",
        "      ip_adapter_img = None\n",
        "      if prefs['ip_adapter_image'].startswith('http'):\n",
        "        i_response = requests.get(prefs['ip_adapter_image'])\n",
        "        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "      else:\n",
        "        if os.path.isfile(prefs['ip_adapter_image']):\n",
        "          ip_adapter_img = PILImage.open(prefs['ip_adapter_image'])\n",
        "        else:\n",
        "          clear_last()\n",
        "          prt(f\"ERROR: Couldn't find your ip_adapter_image {prefs['ip_adapter_image']}\")\n",
        "      if bool(ip_adapter_img):\n",
        "        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img\n",
        "        \n",
        "    for p in updated_prompts:\n",
        "      pr = \"\"\n",
        "      images = None\n",
        "      usable_image = True\n",
        "      arg = {}\n",
        "      if type(p) == list or type(p) == str:\n",
        "        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):\n",
        "          pr = p\n",
        "        else:\n",
        "          pr = prefix + p\n",
        "        arg = args.copy()\n",
        "      elif isinstance(p, Dream):\n",
        "        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):\n",
        "          pr = prefix + p.prompt\n",
        "        else:\n",
        "          pr = p.prompt\n",
        "        arg = merge_dict(args, p.arg)\n",
        "      else: prt(f\"Unknown object {type(p)} in the prompt list\")\n",
        "      if arg['batch_size'] > 1:\n",
        "        pr = [pr] * arg['batch_size']\n",
        "        #if bool(arg['negative_prompt']):\n",
        "        arg['negative_prompt'] = [arg['negative_prompt']] * arg['batch_size']\n",
        "      if last_seed != arg['seed']:\n",
        "        if arg['seed'] < 1 or arg['seed'] is None:\n",
        "          rand_seed = random.randint(0,2147483647)\n",
        "          if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "            if use_custom_scheduler:\n",
        "              generator = torch.manual_seed(rand_seed)\n",
        "            else:\n",
        "              generator = torch.Generator(\"cuda\").manual_seed(rand_seed)\n",
        "          arg['seed'] = rand_seed\n",
        "        else:\n",
        "          if (not(prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "            if use_custom_scheduler:\n",
        "              generator = torch.manual_seed(arg['seed'])\n",
        "            else:\n",
        "              generator = torch.Generator(\"cuda\").manual_seed(arg['seed'])\n",
        "        last_seed = arg['seed']\n",
        "      if prefs['centipede_prompts_as_init_images'] and last_image is not None:\n",
        "        arg['init_image'] = last_image\n",
        "      p_count = f'[{p_idx + 1} of {len(updated_prompts)}]  '\n",
        "      #if p_idx % 30 == 0 and p_idx > 1:\n",
        "      #  clear_output()\n",
        "      #  print(f\"{Color.BEIGE2}Cleared console display due to memory limit in console logging.  Images still saving.{Color.END}\")\n",
        "      #prt(Divider(height=6, thickness=2), update=False)\n",
        "      prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg[\"seed\"]}     ')]))\n",
        "      time.sleep(0.1)\n",
        "      page.auto_scrolling(False)\n",
        "      #prt(p_count + ('‚îÄ' * 90))\n",
        "      #prt(f'{pr[0] if type(pr) == list else pr} - seed:{arg[\"seed\"]}')\n",
        "      total_steps = arg['steps']\n",
        "      #if prefs['use_Stability_api'] or bool(arg['use_Stability'] or (not status['installed_diffusers'] and status['installed_stability'])):\n",
        "      if status['installed_stability'] and (not status['installed_diffusers'] or prefs['use_Stability_api']) and not (status['installed_AIHorde'] and prefs['use_AIHorde_api']):\n",
        "        #print('use_Stability_api')\n",
        "        if not status['installed_stability']:\n",
        "          alert_msg(page, f\"ERROR: To use Stability-API, you must run the install it first and have proper API key\")\n",
        "          return\n",
        "        else:\n",
        "          prt('Stability API Diffusion ')# + ('‚îÄ' * 100))\n",
        "          #print(f'\"{SD_prompt}\", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')\n",
        "          #strikes = 0\n",
        "          images = []\n",
        "          arg['width'] = multiple_of_64(arg['width'])\n",
        "          arg['height'] = multiple_of_64(arg['height'])\n",
        "          prt(pb)\n",
        "          import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "          answers = response = None\n",
        "\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          import base64\n",
        "          api_host = os.getenv('API_HOST', 'https://api.stability.ai')\n",
        "          engine_id = prefs['model_checkpoint']# if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "          url = f\"{api_host}/v1/generation/{engine_id}/\"#image-to-image\"\n",
        "          headers = {\n",
        "              'Content-Type': 'application/json',\n",
        "              'Accept': 'application/json',#'image/png',\n",
        "              'Authorization': prefs['Stability_api_key'],\n",
        "          }\n",
        "          payload = {\n",
        "              \"cfg_scale\": arg['guidance_scale'],\n",
        "              \"clip_guidance_preset\": prefs['clip_guidance_preset'],\n",
        "              \"height\": arg['height'],\n",
        "              \"width\": arg['width'],\n",
        "              \"sampler\": prefs['generation_sampler'],\n",
        "              \"samples\": arg['batch_size'],\n",
        "              \"seed\": arg['seed'],\n",
        "              \"steps\": arg['steps'],\n",
        "              \"text_prompts\": [\n",
        "                  {\n",
        "                      \"text\": pr[0] if type(pr) == list else pr,\n",
        "                      \"weight\": 1\n",
        "                  }\n",
        "              ],\n",
        "          }\n",
        "          if bool(arg['negative_prompt']):\n",
        "            payload['text_prompts'].append({\"text\": arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt'], \"weight\": -10})\n",
        "          if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):\n",
        "            if not bool(arg['init_image']):\n",
        "              clear_last()\n",
        "              prt(f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "              continue\n",
        "            if arg['init_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['init_image'])\n",
        "              init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['init_image']):\n",
        "                init_img = PILImage.open(arg['init_image'])\n",
        "              else:\n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "            init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.LANCZOS)\n",
        "            buff = BytesIO()\n",
        "            init_img.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            #init_image = preprocess(init_img)\n",
        "            if not arg['alpha_mask']:\n",
        "              if arg['mask_image'].startswith('http'):\n",
        "                i_response = requests.get(arg['mask_image'])\n",
        "                mask_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "              else:\n",
        "                if os.path.isfile(arg['mask_image']):\n",
        "                  mask_img = PILImage.open(arg['mask_image'])\n",
        "                else:\n",
        "                  clear_last()\n",
        "                  prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "              mask = mask_img.resize((arg['width'], arg['height']), resample=PILImage.LANCZOS)\n",
        "\n",
        "              buff = BytesIO()\n",
        "              mask.save(buff, format=\"PNG\")\n",
        "              buff.seek(0)\n",
        "              mask_str = io.BufferedReader(buff).read()\n",
        "            #payload[\"step_schedule_end\"] = 0.01\n",
        "            payload[\"step_schedule_start\"] = 1# - arg['init_image_strength']\n",
        "            files = {\n",
        "                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),\n",
        "                #'mask_image': mask_str,\n",
        "                'mask_source': \"INIT_IMAGE_ALPHA\" if arg['alpha_mask'] else \"MASK_IMAGE_BLACK\" if arg['invert_mask'] else \"MASK_IMAGE_WHITE\",\n",
        "                'options': (None, json.dumps(payload)),\n",
        "            }\n",
        "            if not arg['alpha_mask']:\n",
        "              files['mask_image'] = mask_str\n",
        "            pipe_used = \"Stability-API Inpainting\"\n",
        "            #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "            response = requests.post(url+\"image-to-image/masking\", headers=headers, files=files)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "          elif bool(arg['init_image']):\n",
        "            if arg['init_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['init_image'])\n",
        "              init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['init_image']):\n",
        "                init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "              else:\n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "            init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.LANCZOS)\n",
        "\n",
        "            buff = BytesIO()\n",
        "            init_img.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes(\"raw\")\n",
        "            payload[\"step_schedule_end\"] = 0.01\n",
        "            payload[\"step_schedule_start\"] = 1 - arg['init_image_strength']\n",
        "            files = {\n",
        "                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),\n",
        "                'options': (None, json.dumps(payload)),\n",
        "            }\n",
        "            pipe_used = \"Stability-API Image-to-Image\"\n",
        "            response = requests.post(url+\"image-to-image\", headers=headers, files=files)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "          else:\n",
        "            pipe_used = \"Stability-API Text-to-Image\"\n",
        "            response = requests.post(url+\"text-to-image\", headers=headers, json=payload)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)\n",
        "          clear_last(update=False)\n",
        "          clear_last()\n",
        "          if response != None:\n",
        "            if response.status_code != 200:\n",
        "              if response.status_code == 402:\n",
        "                alert_msg(page, \"Stability-API ERROR: Insufficient Credit Balance. Reload at DreamStudio.com...\", content=Text(str(response.text)))\n",
        "                return\n",
        "              else:\n",
        "                prt(Text(f\"Stability-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "                print(payload)\n",
        "                continue\n",
        "            #with open(output_file, \"wb\") as f:\n",
        "            #  f.write(response.content)\n",
        "            artifacts = json.loads(response.content)\n",
        "            for resp in artifacts['artifacts']:\n",
        "              #print(f'{type(resp)} - {resp[\"seed\"]}')\n",
        "              if resp == None: continue\n",
        "              images.append(PILImage.open(io.BytesIO(base64.b64decode(resp['base64']))))\n",
        "            #print(f'{type(response.content)} {response.content}')\n",
        "          if answers != None:\n",
        "            for resp in answers:\n",
        "              for artifact in resp.artifacts:\n",
        "                print(\"Artifact reason: \" + str(artifact.finish_reason))\n",
        "                if artifact.finish_reason == generation.FILTER:\n",
        "                  usable_image = False\n",
        "                if artifact.finish_reason == generation.ARTIFACT_TEXT:\n",
        "                  usable_image = False\n",
        "                  prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                  images.append(PILImage.open(io.BytesIO(artifact.binary)))\n",
        "      elif prefs['use_AIHorde_api'] and status['installed_AIHorde']:# or bool(prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde'])):\n",
        "        if not status['installed_AIHorde']:\n",
        "          alert_msg(page, f\"ERROR: To use AIHorde-API, you must run the install it first and have proper API key\")\n",
        "          return\n",
        "        stats = Text(\"Stable Horde API Diffusion \")\n",
        "        prt(stats)\n",
        "        #prt('Stable Horde API Diffusion ')# + ('‚îÄ' * 100))\n",
        "        #print(f'\"{SD_prompt}\", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')\n",
        "        #strikes = 0\n",
        "        images = []\n",
        "        arg['width'] = multiple_of_64(arg['width'])\n",
        "        arg['height'] = multiple_of_64(arg['height'])\n",
        "        prt(pb)\n",
        "        answers = None\n",
        "        response = None\n",
        "\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        import base64\n",
        "        api_host = 'https://stablehorde.net/api'\n",
        "        engine_id = prefs['AIHorde_model']\n",
        "        api_check_url = f\"{api_host}/v2/generate/check/\"\n",
        "        api_get_result_url = f\"{api_host}/v2/generate/status/\"\n",
        "        url = f\"{api_host}/v2/generate/async\"\n",
        "        headers = {\n",
        "            #'Content-Type': 'application/json',\n",
        "            #'Accept': 'application/json',\n",
        "            'apikey': prefs['AIHorde_api_key'],\n",
        "        }\n",
        "        text_prompt = pr[0] if type(pr) == list else pr\n",
        "        if bool(arg['negative_prompt']):\n",
        "          text_prompt += \"###\" +arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']\n",
        "        payload = {\n",
        "          \"prompt\": text_prompt,\n",
        "          \"nsfw\": prefs[\"disable_nsfw_filter\"],\n",
        "          \"models\": [prefs[\"AIHorde_model\"]]\n",
        "        }\n",
        "        params = {\n",
        "          \"cfg_scale\": arg['guidance_scale'],\n",
        "          \"denoising_strength\": arg['init_image_strength'],\n",
        "          \"width\": arg['width'],\n",
        "          \"height\": arg['height'],\n",
        "          \"sampler_name\": prefs['AIHorde_sampler'],\n",
        "          \"n\": arg['batch_size'],\n",
        "          \"seed\": str(arg['seed']),\n",
        "          \"steps\": arg['steps'],\n",
        "        }\n",
        "        if prefs['AIHorde_post_processing'] != \"None\":\n",
        "          params['post_processing'] = [prefs['AIHorde_post_processing']]\n",
        "        if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):\n",
        "          if not bool(arg['init_image']):\n",
        "            clear_last()\n",
        "            prt(f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "            continue\n",
        "          if arg['init_image'].startswith('http'):\n",
        "            i_response = requests.get(arg['init_image'])\n",
        "            init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "          else:\n",
        "            if os.path.isfile(arg['init_image']):\n",
        "              init_img = PILImage.open(arg['init_image'])\n",
        "            else:\n",
        "              clear_last()\n",
        "              prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "          init_img = init_img.resize((arg['width'], arg['height']))\n",
        "          buff = BytesIO()\n",
        "          init_img.save(buff, format=\"PNG\")\n",
        "          buff.seek(0)\n",
        "          img_str = io.BufferedReader(buff).read()\n",
        "          #init_image = preprocess(init_img)\n",
        "          if not arg['alpha_mask']:\n",
        "            if arg['mask_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['mask_image'])\n",
        "              mask_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['mask_image']):\n",
        "                mask_img = PILImage.open(arg['mask_image'])\n",
        "              else:\n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "            mask = mask_img.resize((arg['width'], arg['height']))\n",
        "\n",
        "            buff = BytesIO()\n",
        "            mask.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            mask_str = io.BufferedReader(buff).read()\n",
        "          payload['source_image'] = img_str\n",
        "          if not arg['alpha_mask']:\n",
        "            payload['source_mask'] = mask_str\n",
        "          pipe_used = \"Stable Horde-API Inpainting\"\n",
        "          payload['source_processing'] = \"inpainting\"\n",
        "          #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "          #response = requests.post(url+\"image-to-image/masking\", headers=headers, files=files)\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "        elif bool(arg['init_image']):\n",
        "          if arg['init_image'].startswith('http'):\n",
        "            i_response = requests.get(arg['init_image'])\n",
        "            init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "          else:\n",
        "            if os.path.isfile(arg['init_image']):\n",
        "              init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "            else:\n",
        "              clear_last()\n",
        "              prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "          init_img = init_img.resize((arg['width'], arg['height']))\n",
        "\n",
        "          buff = BytesIO()\n",
        "          init_img.save(buff, format=\"PNG\")\n",
        "          buff.seek(0)\n",
        "          img_str = io.BufferedReader(buff).read()\n",
        "          #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes(\"raw\")\n",
        "          payload['source_image'] = img_str\n",
        "          pipe_used = \"Stable Horde-API Image-to-Image\"\n",
        "          payload['source_processing'] = \"img2img\"\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "        else:\n",
        "          pipe_used = \"Stable Horde-API Text-to-Image\"\n",
        "          #response = requests.post(url+\"text-to-image\", headers=headers, json=payload)\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)\n",
        "        payload[\"params\"] = params\n",
        "        #print(params)\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "        if response != None:\n",
        "          if response.status_code != 202:\n",
        "            if response.status_code == 400:\n",
        "              alert_msg(page, \"Stable Horde-API ERROR: Validation Error...\", content=Text(str(response.text)))\n",
        "              return\n",
        "            else:\n",
        "              prt(Text(f\"Stable Horde-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "              print(payload)\n",
        "              continue\n",
        "          #with open(output_file, \"wb\") as f:\n",
        "          #  f.write(response.content)\n",
        "\n",
        "        artifacts = json.loads(response.content)\n",
        "        q_id = artifacts['id']\n",
        "        #print(str(artifacts))\n",
        "        #stats = Text(\"\")\n",
        "        #prt(stats)\n",
        "        elapsed_seconds = 0\n",
        "        try:\n",
        "          while True:\n",
        "            if abort_run: break\n",
        "            check_response = requests.get(api_check_url + q_id)\n",
        "            check = json.loads(check_response.content)\n",
        "            try:\n",
        "              div = check['wait_time'] + elapsed_seconds\n",
        "              percentage = (1 - check['wait_time'] / div)\n",
        "            except Exception:\n",
        "              div = 0\n",
        "              continue\n",
        "              pass\n",
        "            if div == 0: continue\n",
        "            pb.value = percentage\n",
        "            pb.update()\n",
        "            status_txt = f\"Stable Horde API Diffusion - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}\"\n",
        "            stats.value = status_txt #str(check)\n",
        "            stats.update()\n",
        "            if bool(check['done']):\n",
        "              break\n",
        "            time.sleep(1)\n",
        "            elapsed_seconds += 1\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "        get_response = requests.get(api_get_result_url + q_id)\n",
        "        final_results = json.loads(get_response.content)\n",
        "        clear_last(update=False)\n",
        "        clear_last()\n",
        "        for gen in final_results[\"generations\"]:\n",
        "          #print(f'{type(resp)} - {resp[\"seed\"]}')\n",
        "          if gen[\"censored\"]:\n",
        "            prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "            continue\n",
        "          img_response = requests.get(gen['img'])\n",
        "          webp_file = io.BytesIO(img_response.content)\n",
        "          cv_img = cv2.imdecode(np.frombuffer(webp_file.read(), np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "          images.append(PILImage.fromarray(cv_img))\n",
        "          #cv_img = cv2.imdecode(io.BytesIO(base64.b64decode(gen['img'])), cv2.IMREAD_COLOR)\n",
        "          #cv_img = cv2.imdecode(np.frombuffer(base64.b64decode(gen['img']), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "          #decoded_string = base64.b64decode(gen['img'])\n",
        "          #nparr = np.frombuffer(decoded_string, np.uint8)\n",
        "          #cv_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "          #print(gen['img'])\n",
        "          #print(str(type(gen['img'])))\n",
        "          #img_bytes = io.BytesIO(decoded_string)\n",
        "          #img = Image.open(io.BytesIO(img_response.content))\n",
        "          #pil_image = PILImage.open(webp_file)\n",
        "          #images.append(PILImage.open(io.BytesIO(img_response.content)))\n",
        "          #images.append(PILImage.open(pil_image))\n",
        "          #images.append(PILImage.open(img_response.content))\n",
        "          #images.append(PILImage.open(io.BytesIO(base64.b64decode(gen['img']))).convert('RGB'))\n",
        "          #print(f'{type(response.content)} {response.content}')\n",
        "          '''if answers != None:\n",
        "            for resp in answers:\n",
        "              for artifact in resp.artifacts:\n",
        "                #print(\"Artifact reason: \" + str(artifact.finish_reason))\n",
        "                if artifact.finish_reason == generation.FILTER:\n",
        "                  usable_image = False\n",
        "                if artifact.finish_reason == generation.ARTIFACT_TEXT:\n",
        "                  usable_image = False\n",
        "                  prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                  images.append(PILImage.open(io.BytesIO(artifact.binary)))'''\n",
        "      else:\n",
        "        #from torch.amp.autocast_mode import autocast\n",
        "        #precision_scope = autocast if prefs['precision']==\"autocast\" else nullcontext\n",
        "        SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}\n",
        "        if lcm_lora:\n",
        "          arg['guidance_scale'] = 0.0\n",
        "        if (status['installed_SDXL'] and prefs['use_SDXL'] and prefs['SDXL_model'] == \"SDXL-Turbo\") or ((not status['installed_SDXL'] or not prefs['use_SDXL']) and prefs['model_ckpt'] == \"SD-Turbo\"):\n",
        "          arg['guidance_scale'] = 0.0\n",
        "          if arg['steps'] > 8:\n",
        "            arg['steps'] = 4\n",
        "        try:\n",
        "          if use_custom_scheduler and not bool(arg['init_image']) and not bool(arg['mask_image']) and not bool(arg['prompt2']):\n",
        "            # Not implemented correctly anymore, old code but might reuse custom\n",
        "            text_input = tokenizer(pr[0] if type(pr) == list else pr, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "              text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]# We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional text_embeddings (batch_size and seq_length)\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = tokenizer([\"\"] * arg['batch_size'], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "              uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   #For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes.\n",
        "\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])#Generate the intial random noise.\n",
        "            #if generator:\n",
        "            #latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'], arg['width']), generator=generator)\n",
        "            latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'] // 8,  arg['width'] // 8), generator=generator)\n",
        "            #else:\n",
        "            #  latents = torch.randn((batch_size, unet.in_channels, arg['height'] // 8, arg['width'] // 8))\n",
        "            latents = latents.to(torch_device)\n",
        "            latents.shape\n",
        "            #Cool  64√ó64  is expected. The model will transform this latent representation (pure noise) into a 512 √ó 512 image later on.\n",
        "            #Next, we initialize the scheduler with our chosen num_inference_steps. This will compute the sigmas and exact time step values to be used during the denoising process.\n",
        "            scheduler.set_timesteps(arg['steps'])#The LMS Discrete scheduler needs to multiple the `latents` by its `sigma` values. Let's do this here\n",
        "            if prefs['scheduler_mode'] == \"LMS Discrete\" or prefs['scheduler_mode'] == \"Score-SDE-Vp\":\n",
        "              latents = latents * scheduler.sigmas[0]#We are ready to write the denoising loop.\n",
        "            from tqdm.auto import tqdm\n",
        "            clear_pipes(\"unet\")\n",
        "            if unet is None:\n",
        "              unet = get_unet_pipe()\n",
        "            #with precision_scope(\"cuda\"):\n",
        "            #with autocast(\"cuda\"):\n",
        "            for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "              # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "              latent_model_input = torch.cat([latents] * 2)\n",
        "              if prefs['scheduler_mode'] == \"LMS Discrete\" or prefs['scheduler_mode'] == \"Score-SDE-Vp\":\n",
        "                sigma = scheduler.sigmas[i]\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "              # predict the noise residual\n",
        "              if prefs['scheduler_mode'] == \"DDPM\":\n",
        "                #TODO: Work in progress, still not perfect\n",
        "                noisy_sample = torch.randn(1, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)\n",
        "                noisy_residual = unet(sample=noisy_sample, timestep=2)[\"sample\"]\n",
        "                less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample)[\"prev_sample\"]\n",
        "                less_noisy_sample.shape\n",
        "              with torch.no_grad():\n",
        "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).images\n",
        "              # perform guidance\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + arg['guidance_scale'] * (noise_pred_text - noise_pred_uncond)\n",
        "              # compute the previous noisy sample x_t -> x_t-1\n",
        "              latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]#We now use the vae to decode the generated latents back into the image.\n",
        "            # scale and decode the image latents with vae\n",
        "            latents = 1 / 0.18215 * latents\n",
        "            with torch.no_grad():\n",
        "              image = vae.decode(latents)\n",
        "            image = (image / 2 + 0.5).clip(0, 1)\n",
        "            image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "            uint8_images = (image * 255).round().astype(\"uint8\")\n",
        "            #for img in uint8_images: images.append(Image.fromarray(img))\n",
        "            images = [PILImage.fromarray(img) for img in uint8_images]\n",
        "          else:\n",
        "            if bool(arg['use_clip_guided_model']) and status['installed_clip']:\n",
        "              if bool(arg['init_image']) or bool(arg['mask_image']):\n",
        "                #raise ValueError(\"Cannot use CLIP Guided Model with init or mask image yet.\")\n",
        "                alert_msg(page, \"Cannot use CLIP Guided Model with init or mask image yet.\")\n",
        "                return\n",
        "              clear_pipes(\"clip_guided\")\n",
        "              if pipe_clip_guided is None:\n",
        "                prt(Installing(\"Initializing CLIP-Guided Pipeline...\"))\n",
        "                pipe_clip_guided = get_clip_guided_pipe()\n",
        "                clear_last()\n",
        "              clip_prompt = arg[\"clip_prompt\"] if arg[\"clip_prompt\"].strip() != \"\" else None\n",
        "              if bool(arg[\"unfreeze_unet\"]):\n",
        "                pipe_clip_guided.unfreeze_unet()\n",
        "              else:\n",
        "                pipe_clip_guided.freeze_unet()\n",
        "              if bool(arg[\"unfreeze_vae\"]):\n",
        "                pipe_clip_guided.unfreeze_vae()\n",
        "              else:\n",
        "                pipe_clip_guided.freeze_vae()\n",
        "              # TODO: Figure out why it's broken with use_cutouts=False and doesn't generate, hacking it True for now\n",
        "              arg[\"use_cutouts\"] = True\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              pipe_used = \"CLIP Guided\"\n",
        "              images = pipe_clip_guided(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], clip_prompt=clip_prompt, clip_guidance_scale=arg[\"clip_guidance_scale\"], num_cutouts=int(arg[\"num_cutouts\"]) if arg[\"use_cutouts\"] else None, use_cutouts=arg[\"use_cutouts\"], generator=generator).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(prefs['use_conceptualizer']) and status['installed_conceptualizer']:\n",
        "              clear_pipes(\"conceptualizer\")\n",
        "              if pipe_conceptualizer is None:\n",
        "                prt(Installing(\"Initializing Conceptualizer Pipeline...\"))\n",
        "                pipe_conceptualizer = get_conceptualizer(page)\n",
        "                clear_last()\n",
        "              total_steps = arg['steps']\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              pipe_used = f\"Conceptualizer {prefs['concepts_model']}\"\n",
        "              images = pipe_conceptualizer(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['mask_image']) or (not bool(arg['mask_image']) and bool(arg['init_image']) and bool(arg['alpha_mask'])):\n",
        "              if not bool(arg['init_image']):\n",
        "                alert_msg(page, f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "                return\n",
        "              #if pipe_inpainting is None:\n",
        "              #  pipe_inpainting = get_inpainting_pipe()\n",
        "              if prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                clear_pipes(\"img2img\")\n",
        "                if pipe_img2img is None:\n",
        "                  prt(Installing(\"Initializing Inpaint Pipeline...\"))\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                if status['loaded_SDXL'] == \"inpainting\" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:\n",
        "                  clear_pipes(\"SDXL\")\n",
        "                else:\n",
        "                  clear_pipes()\n",
        "                if pipe_SDXL is None:\n",
        "                  prt(Installing(\"Initializing Stable Diffusion XL Inpainting Pipeline...\"))\n",
        "                  get_SDXL_pipe(\"inpainting\")\n",
        "                  clear_last()\n",
        "                elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)\n",
        "                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)\n",
        "                if prefs['SDXL_model'] == \"SDXL-Turbo\":\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])\n",
        "                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              else:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                if pipe is None or status['loaded_task'] != \"inpaint\" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:\n",
        "                  #prt(Installing(\"Initializing Long Prompt Weighting Inpaint Pipeline...\"))\n",
        "                  prt(Installing(\"Initializing Stable Diffusion Inpaint Pipeline...\"))\n",
        "                  pipe = get_SD_pipe(task=\"inpaint\")\n",
        "                  #pipe = get_txt2img_pipe()\n",
        "                  clear_last()\n",
        "                else:\n",
        "                  pipe = get_SD_pipe(task=\"inpaint\")\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              '''if pipe_img2img is None:\n",
        "                try:\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                except NameError:\n",
        "                  prt(f\"{Color.RED}You must install the image2image Pipeline above.{Color.END}\")\n",
        "                finally:\n",
        "                  raise NameError(\"You must install the image2image Pipeline above\")'''\n",
        "              import requests\n",
        "              from io import BytesIO\n",
        "              if arg['init_image'].startswith('http'):\n",
        "                response = requests.get(arg['init_image'])\n",
        "                init_img = PILImage.open(BytesIO(response.content))\n",
        "              else:\n",
        "                if os.path.isfile(arg['init_image']):\n",
        "                  init_img = PILImage.open(arg['init_image'])\n",
        "                else: prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "              #if bool(arg['alpha_mask']):\n",
        "              #  init_img = init_img.convert(\"RGBA\")\n",
        "              #else:\n",
        "              #  init_img = init_img.convert(\"RGB\")\n",
        "              init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "              #init_image = preprocess(init_img)\n",
        "              mask_img = None\n",
        "              if not bool(arg['mask_image']) and bool(arg['alpha_mask']):\n",
        "                mask_img = init_img.convert('RGBA')\n",
        "                red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "                mask_img = alpha.convert('L')\n",
        "              else:\n",
        "                if arg['mask_image'].startswith('http'):\n",
        "                  response = requests.get(arg['mask_image'])\n",
        "                  mask_img = PILImage.open(BytesIO(response.content))\n",
        "                else:\n",
        "                  if os.path.isfile(arg['mask_image']):\n",
        "                    mask_img = PILImage.open(arg['mask_image'])\n",
        "                  else: prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "              if arg['invert_mask'] and not arg['alpha_mask']:\n",
        "                from PIL import ImageOps\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "              mask_img = mask_img.convert(\"L\")\n",
        "              mask_img = mask_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              init_img = init_img.convert(\"RGB\")\n",
        "              #mask = mask_img.resize((arg['width'], arg['height']))\n",
        "              #mask = np.array(mask).astype(np.float32) / 255.0\n",
        "              #mask = np.tile(mask,(4,1,1))\n",
        "              #mask = mask[None].transpose(0, 1, 2, 3)\n",
        "              #mask[np.where(mask != 0.0 )] = 1.0 #make sure mask is actually valid\n",
        "              #mask_img = torch.from_numpy(mask)\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              #with autocast(\"cuda\"):\n",
        "              if prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                pipe_used = \"Diffusers Inpaint\"\n",
        "                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                pipe_used = \"Stable Diffusion XL Inpainting\"\n",
        "                high_noise_frac = prefs['SDXL_high_noise_frac']\n",
        "                total_steps = int(arg['steps'] * high_noise_frac)\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SDXL_compel']:\n",
        "                  prompt_embed, pooled = compel_base(pr)\n",
        "                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])\n",
        "                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  #, target_size=(arg['width'], arg['height'])\n",
        "                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, mask_image=mask_img, strength=1 - arg['init_image_strength'], height=arg['height'], width=arg['width'], output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                else:\n",
        "                  if arg['batch_size'] > 1:\n",
        "                    init_img = [init_img] * arg['batch_size']\n",
        "                    mask_img = [mask_img] * arg['batch_size']\n",
        "                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, strength=1 - arg['init_image_strength'], height=arg['height'], width=arg['width'], output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                if high_noise_frac != 1:\n",
        "                  total_steps = int(arg['steps'] * (1 - high_noise_frac))\n",
        "                  if prefs['SDXL_compel']:\n",
        "                    prompt_embed, pooled = compel_refiner(pr)\n",
        "                    negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])\n",
        "                    #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=image, mask_image=mask_img, strength=1 - arg['init_image_strength'], target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images\n",
        "                    del prompt_embed, negative_embed, pooled, negative_pooled\n",
        "                  else:\n",
        "                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, mask_image=mask_img, strength=1 - arg['init_image_strength'], target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images\n",
        "                #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                flush()\n",
        "              else:\n",
        "                #pipe_used = \"Long Prompt Weight Inpaint\"\n",
        "                #images = pipe.inpaint(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                pipe_used = \"Stable Diffusion Inpaint\"\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SD_compel']:\n",
        "                  pipe_used += \" w/ Compel\"\n",
        "                  prompt_embed = compel_proc.build_conditioning_tensor(pr)\n",
        "                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\")\n",
        "                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, mask_image=mask_img, image=init_img, height=arg['height'], width=arg['width'], strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "                  del prompt_embed, negative_embed\n",
        "                else:\n",
        "                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\", mask_image=mask_img, image=init_img, height=arg['height'], width=arg['width'], strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['init_image']):\n",
        "              if not status['installed_txt2img'] and not (prefs['use_imagic'] and status['installed_imagic']) and not (prefs['use_depth2img'] and status['installed_depth2img']) and not (prefs['use_alt_diffusion'] and status['installed_alt_diffusion']) and not status['installed_SDXL']:\n",
        "                alert_msg(page, f\"CRITICAL ERROR: You have not installed a image2image pipeline yet.  Run in the Installer..\")\n",
        "                continue\n",
        "              if prefs['use_versatile'] and status['installed_versatile']:\n",
        "                if len(pr.strip()) > 2: # Find another way to know the difference\n",
        "                  clear_pipes(\"versatile_dualguided\")\n",
        "                  if pipe_versatile_dualguided is None:\n",
        "                    prt(Installing(\"Initializing Versatile Dual-Guided Pipeline...\"))\n",
        "                    pipe_versatile_dualguided = get_versatile_dualguided_pipe()\n",
        "                    clear_last()\n",
        "                else:\n",
        "                  clear_pipes(\"versatile_variation\")\n",
        "                  if pipe_versatile_variation is None:\n",
        "                    prt(Installing(\"Initializing Versatile Image Variation Pipeline...\"))\n",
        "                    pipe_versatile_variation = get_versatile_variation_pipe()\n",
        "                    clear_last()\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                if status['loaded_SDXL'] == \"image2image\" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:\n",
        "                  clear_pipes(\"SDXL\")\n",
        "                else:\n",
        "                  clear_pipes()\n",
        "                if pipe_SDXL_refiner is None:\n",
        "                  prt(Installing(\"Initializing Stable Diffusion XL Image2Image Pipeline...\"))\n",
        "                  get_SDXL_pipe(\"image2image\")\n",
        "                  clear_last()\n",
        "                elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)\n",
        "                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)\n",
        "                if prefs['SDXL_model'] == \"SDXL-Turbo\":\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])\n",
        "                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                clear_pipes(\"alt_diffusion_img2img\")\n",
        "                if pipe_alt_diffusion_img2img is None:\n",
        "                  prt(Installing(\"Initializing AltDiffusion Image2Image Pipeline...\"))\n",
        "                  pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_depth2img'] and status['installed_depth2img']:\n",
        "                clear_pipes(\"depth\")\n",
        "                if pipe_depth is None:\n",
        "                  prt(Installing(\"Initializing SD2 Depth2Image Pipeline...\"))\n",
        "                  pipe_depth = get_depth_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                clear_pipes(\"img2img\")\n",
        "                if pipe_img2img is None:\n",
        "                  prt(Installing(\"Initializing Inpaint Pipeline...\"))\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                  clear_last()\n",
        "                  if bool(ip_adapter_arg):\n",
        "                    ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                    pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                    pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_imagic'] and status['installed_imagic']:\n",
        "                clear_pipes(\"imagic\")\n",
        "                if pipe_imagic is None:\n",
        "                  prt(Installing(\"Initializing iMagic Image2Image Pipeline...\"))\n",
        "                  pipe_imagic = get_imagic_pipe()\n",
        "                  clear_last()\n",
        "              else:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                if pipe is None or status['loaded_task'] != \"img2img\" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:\n",
        "                  #prt(Installing(\"Initializing Long Prompt Weighting Image2Image Pipeline...\"))\n",
        "                  prt(Installing(\"Initializing Stable Diffusion Image2Image Pipeline...\"))\n",
        "                  pipe = get_SD_pipe(task=\"img2img\")\n",
        "                  #pipe = get_txt2img_pipe()\n",
        "                  clear_last()\n",
        "                else:\n",
        "                  pipe = get_SD_pipe(task=\"img2img\")\n",
        "                if prefs['model_ckpt'] == \"SD-Turbo\":\n",
        "                  pipe = pipeline_scheduler(pipe_SDXL, trailing=True)\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              '''if pipe_img2img is None:\n",
        "                try:\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                except NameError:\n",
        "                  prt(f\"{Color.RED}You must install the image2image Pipeline above.{Color.END}\")\n",
        "                  raise NameError(\"You must install the image2image Pipeline above\")'''\n",
        "                #finally:\n",
        "              import requests\n",
        "              from io import BytesIO\n",
        "              if arg['init_image'].startswith('http'):\n",
        "                response = requests.get(arg['init_image'])\n",
        "                init_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              else:\n",
        "                if os.path.isfile(arg['init_image']):\n",
        "                  init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "                else:\n",
        "                  alert_msg(page, f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "              init_img = init_img.resize((arg['width'], arg['height']))\n",
        "              #init_image = preprocess(init_img)\n",
        "              #white_mask = PILImage.new(\"RGB\", (arg['width'], arg['height']), (255, 255, 255))\n",
        "              prt(pb)\n",
        "              time.sleep(0.4)\n",
        "              page.auto_scrolling(False)\n",
        "              #with autocast(\"cuda\"):\n",
        "              #images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], init_image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              if prefs['use_versatile'] and status['installed_versatile']:\n",
        "                if len(pr.strip()) > 2:\n",
        "                  pipe_used = \"Versatile Dual-Guided\"\n",
        "                  images = pipe_versatile_dualguided(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, text_to_image_strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                else:\n",
        "                  pipe_used = \"Versatile Variation\"\n",
        "                  images = pipe_versatile_variation(negative_prompt=arg['negative_prompt'], image=init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                pipe_used = \"Stable Diffusion XL Image-to-Image\"\n",
        "                high_noise_frac = prefs['SDXL_high_noise_frac']\n",
        "                total_steps = int(arg['steps'] * high_noise_frac)\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SDXL_compel']:\n",
        "                  prompt_embed, pooled = compel_base(pr)\n",
        "                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])\n",
        "                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, strength=1 - arg['init_image_strength'], output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                else:\n",
        "                  if arg['batch_size'] > 1:\n",
        "                    init_img = [init_img] * arg['batch_size']\n",
        "                  #image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                if high_noise_frac != 1:\n",
        "                  total_steps = int(arg['steps'] * (1 - high_noise_frac))\n",
        "                  if prefs['SDXL_compel']:\n",
        "                    pipe_used += \" w/ Compel\"\n",
        "                    prompt_embed, pooled = compel_refiner(pr)\n",
        "                    negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])\n",
        "                    #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=image, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images\n",
        "                    del prompt_embed, negative_embed, pooled, negative_pooled\n",
        "                  else:\n",
        "                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images\n",
        "                #images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                flush()\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                pipe_used = \"AltDiffusion Image-to-Image\"\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                  images = pipe_alt_diffusion_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images\n",
        "              elif prefs['use_depth2img'] and status['installed_depth2img']:\n",
        "                pipe_used = \"Depth-to-Image\"\n",
        "                images = pipe_depth(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images\n",
        "              elif prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                pipe_used = \"Diffusers Inpaint Image-to-Image\"\n",
        "                white_mask = PILImage.new(\"RGB\", (arg['width'], arg['height']), (255, 255, 255))\n",
        "                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **ip_adapter_arg).images\n",
        "              elif prefs['use_imagic'] and status['installed_imagic']:\n",
        "                pipe_used = \"iMagic Image-to-Image\"\n",
        "                #only one element tensors can be converted to Python scalars\n",
        "                total_steps = None\n",
        "                res = pipe_imagic.train(pr, init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                images = []\n",
        "                # TODO: alpha= arguments to customize which to make\n",
        "                total_steps = 0\n",
        "                res = pipe_imagic(alpha=1, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "                res = pipe_imagic(alpha=1.5, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "                res = pipe_imagic(alpha=2, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "              else:\n",
        "                #pipe_used = \"Long Prompt Weight Image-to-Image\"\n",
        "                #images = pipe.img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                pipe_used = \"Stable Diffusion Image-to-Image\"\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SD_compel']:\n",
        "                  pipe_used += \" w/ Compel\"\n",
        "                  prompt_embed = compel_proc.build_conditioning_tensor(pr)\n",
        "                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\")\n",
        "                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, image=init_img, target_size=(arg['height'], arg['width']), strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "                  del prompt_embed, negative_embed\n",
        "                else:\n",
        "                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\", image=init_img, target_size=(arg['height'], arg['width']), strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['prompt2']):\n",
        "              if pipe is None:\n",
        "                pipe = get_txt2img_pipe()\n",
        "              #with precision_scope(\"cuda\"):\n",
        "              #    with torch.no_grad():\n",
        "              pipe_used = \"LPW Tween Lerp\"\n",
        "              images_tween = pipe.lerp_between_prompts(pr, arg[\"prompt2\"], length = arg['tweens'], save=False, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)\n",
        "              #print(str(images_tween))\n",
        "              images = images_tween['images']\n",
        "              #images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)[\"sample\"]\n",
        "            else:\n",
        "              if prefs['use_composable'] and status['installed_composable']:\n",
        "                clear_pipes(\"composable\")\n",
        "                if pipe_composable is None:\n",
        "                  prt(Installing(\"Initializing Composable Text2Image Pipeline...\"))\n",
        "                  pipe_composable = get_composable_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                if status['loaded_SDXL'] == \"text2image\" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:\n",
        "                  clear_pipes(\"SDXL\")\n",
        "                else:\n",
        "                  clear_pipes()\n",
        "                if pipe_SDXL is None:\n",
        "                  prt(Installing(\"Initializing Stable Diffusion XL Text2Image Pipeline...\"))\n",
        "                  get_SDXL_pipe(\"text2image\")\n",
        "                  clear_last()\n",
        "                elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)\n",
        "                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)\n",
        "                if prefs['SDXL_model'] == \"SDXL-Turbo\":\n",
        "                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])\n",
        "                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                clear_pipes(\"alt_diffusion\")\n",
        "                if pipe_alt_diffusion is None:\n",
        "                  prt(Installing(\"Initializing AltDiffusion Text2Image Pipeline...\"))\n",
        "                  pipe_alt_diffusion = get_alt_diffusion_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_SAG'] and status['installed_SAG']:\n",
        "                clear_pipes(\"SAG\")\n",
        "                if pipe_SAG is None:\n",
        "                  prt(Installing(\"Initializing Self-Attention Guidance Text2Image Pipeline...\"))\n",
        "                  pipe_SAG = get_SAG_pipe()\n",
        "                  clear_last()\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe_SAG.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_SAG.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:\n",
        "                clear_pipes(\"attend_and_excite\")\n",
        "                if pipe_attend_and_excite is None:\n",
        "                  prt(Installing(\"Initializing Attend and Excite Text2Image Pipeline...\"))\n",
        "                  pipe_attend_and_excite = get_attend_and_excite_pipe()\n",
        "                  clear_last()\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe_attend_and_excite.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_attend_and_excite.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_versatile'] and status['installed_versatile']:\n",
        "                clear_pipes(\"versatile_text2img\")\n",
        "                if pipe_versatile_text2img is None:\n",
        "                  prt(Installing(\"Initializing Versatile Text2Image Pipeline...\"))\n",
        "                  pipe_versatile_text2img = get_versatile_text2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_safe'] and status['installed_safe']:\n",
        "                clear_pipes(\"safe\")\n",
        "                if pipe_safe is None:\n",
        "                  prt(Installing(\"Initializing Safe Stable Diffusion Pipeline...\"))\n",
        "                  pipe_safe = get_safe_pipe()\n",
        "                  clear_last()\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe_safe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_safe.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              elif prefs['use_panorama'] and status['installed_panorama']:\n",
        "                clear_pipes(\"panorama\")\n",
        "                if pipe_panorama is None:\n",
        "                  prt(Installing(\"Initializing Panorama MultiDiffusion Pipeline...\"))\n",
        "                  pipe_panorama = get_panorama_pipe()\n",
        "                  clear_last()\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe_panorama.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe_panorama.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              else:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                if pipe is None or status['loaded_task'] != \"txt2img\" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:\n",
        "                  #prt(Installing(\"Initializing Long Prompt Weighting Text2Image Pipeline...\"))\n",
        "                  prt(Installing(\"Initializing Stable Diffusion Text2Image Pipeline...\"))\n",
        "                  pipe = get_SD_pipe(task=\"txt2img\")\n",
        "                  #pipe = get_txt2img_pipe()\n",
        "                  clear_last()\n",
        "                else:\n",
        "                  pipe = get_SD_pipe(task=\"txt2img\")\n",
        "                #print(pipe)\n",
        "                if prefs['model_ckpt'] == \"SD-Turbo\":\n",
        "                  pipe = pipeline_scheduler(pipe_SDXL, trailing=True)\n",
        "                if bool(ip_adapter_arg):\n",
        "                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])\n",
        "                  pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "                  pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])\n",
        "              '''with io.StringIO() as buf, redirect_stdout(buf):\n",
        "                get_text2image(page)\n",
        "                output = buf.getvalue()\n",
        "                page.Images.content.controls.append(Text(output.strip())\n",
        "                page.Images.content.update()\n",
        "                page.Images.update()\n",
        "                page.update()'''\n",
        "              total_steps = arg['steps']\n",
        "              prt(pb)\n",
        "              nudge(page.imageColumn, page=page)\n",
        "              page.auto_scrolling(False)\n",
        "              if prefs['use_composable'] and status['installed_composable']:\n",
        "                weights = arg['negative_prompt'] #\" 1 | 1\"  # Equal weight to each prompt. Can be negative\n",
        "                if not bool(weights):\n",
        "                  segments = len(pr.split('|'))\n",
        "                  weights = '|'.join(['1' * segments])\n",
        "                pipe_used = \"Composable Text-to-Image\"\n",
        "                images = pipe_composable(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], weights=weights, generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "                pipe_used = \"Stable Diffusion XL Text-to-Image\"\n",
        "                #if arg['batch_size'] > 1:\n",
        "                #    neg_prompts = [arg['negative_prompt']] * arg['batch_size']\n",
        "                high_noise_frac = prefs['SDXL_high_noise_frac']\n",
        "                #TODO: Figure out batch num_images_per_prompt + option to not refine , image[None, :] , num_images_per_prompt=arg['batch_size']\n",
        "                total_steps = int(arg['steps'] * high_noise_frac)\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SDXL_compel']:\n",
        "                  #print(f\"pr:{pr} - neg: {arg['negative_prompt']}\")\n",
        "                  prompt_embed, pooled = compel_base(pr)\n",
        "                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\")\n",
        "                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                  del pooled, negative_pooled\n",
        "                else:\n",
        "                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\", output_type=\"latent\" if high_noise_frac != 1 else \"pil\", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]\n",
        "                if high_noise_frac != 1:\n",
        "                  total_steps = int(arg['steps'] * (1 - high_noise_frac))\n",
        "                  if prefs['SDXL_compel']:\n",
        "                    prompt_embed_refiner, pooled_refiner = compel_refiner(pr)\n",
        "                    negative_embed_refiner, negative_pooled_refiner = compel_refiner(arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\")\n",
        "                    #[prompt_embed_refiner, negative_embed_refiner] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed_refiner, negative_embed_refiner])\n",
        "                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed_refiner, pooled_prompt_embeds=pooled_refiner, negative_prompt_embeds=negative_embed_refiner, negative_pooled_prompt_embeds=negative_pooled_refiner, image=image, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images\n",
        "                    #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                    del prompt_embed_refiner, negative_embed_refiner, pooled_refiner, negative_pooled_refiner\n",
        "                  else:\n",
        "                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt' if bool(arg['negative_prompt']) else \"blurry\"], image=image, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images\n",
        "                flush()\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                pipe_used = \"AltDiffusion Text-to-Image\"\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                  images = pipe_alt_diffusion(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images\n",
        "              elif prefs['use_SAG'] and status['installed_SAG']:\n",
        "                pipe_used = \"Self-Attention Guidance Text-to-Image\"\n",
        "                #size = pipe_SAG.unet.config.sample_size * pipe_SAG.vae_scale_factor\n",
        "                #arg['width'] = size\n",
        "                #arg['height'] = size\n",
        "                with torch.autocast(\"cuda\"): #, height=arg['height'], width=arg['width']\n",
        "                  images = pipe_SAG(prompt=pr, negative_prompt=arg['negative_prompt'], sag_scale=prefs['sag_scale'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **ip_adapter_arg).images\n",
        "              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:\n",
        "                pipe_used = \"Attend and Excite Text-to-Image\"\n",
        "                size = pipe_attend_and_excite.unet.config.sample_size * pipe_attend_and_excite.vae_scale_factor\n",
        "                arg['width'] = size\n",
        "                arg['height'] = size\n",
        "                token_indices = []\n",
        "                words = []\n",
        "                ptext = pr[0] if type(pr) == list else pr\n",
        "                ntext = arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']\n",
        "                for ti, w in enumerate(ptext.split()):\n",
        "                  if w[:1] == '+':\n",
        "                    token_indices.append(ti + 1)\n",
        "                    words.append(w[1:])\n",
        "                    #print(f'indices: {ti + 1} - {w[1:]}')\n",
        "                  else:\n",
        "                    words.append(w)\n",
        "                ptext = ' '.join(words)\n",
        "                pr = [ptext * arg['batch_size']] if type(pr) == list else ptext\n",
        "                print(f\"token_indices: {token_indices} | ptext: {ptext}\")\n",
        "                if len(token_indices) < 1:\n",
        "                  token_indices = pipe_attend_and_excite.get_indices(ptext)\n",
        "                print(f\"indices: {token_indices}\")\n",
        "                #, negative_prompt=arg['negative_prompt'], num_images_per_prompt=int(arg['batch_size'])\n",
        "                #images = pipe_attend_and_excite(prompt=ptext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                images = pipe_attend_and_excite(prompt=ptext, negative_prompt=ntext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_images_per_prompt=int(arg['batch_size']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **ip_adapter_arg).images\n",
        "              elif prefs['use_versatile'] and status['installed_versatile']:\n",
        "                pipe_used = \"Versatile Text-to-Image\"\n",
        "                images = pipe_versatile_text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_panorama'] and status['installed_panorama']:\n",
        "                pipe_used = \"MultiDiffusion Panorama Text-to-Image\"\n",
        "                arg['width'] = prefs['panorama_width']\n",
        "                arg['height'] = 512\n",
        "                images = pipe_panorama(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, circular_padding=prefs['panorama_circular_padding'], **ip_adapter_arg).images\n",
        "              elif prefs['use_safe'] and status['installed_safe']:\n",
        "                from diffusers.pipelines.stable_diffusion_safe import SafetyConfig\n",
        "                s = prefs['safety_config']\n",
        "                safety = SafetyConfig.WEAK if s == 'Weak' else SafetyConfig.MEDIUM if s == 'Medium' else SafetyConfig.STRONG if s == 'Strong' else SafetyConfig.MAX if s == 'Max' else SafetyConfig.STRONG\n",
        "                pipe_used = f\"Safe Diffusion {safety}\"\n",
        "                images = pipe_safe(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **safety, **ip_adapter_arg).images\n",
        "              else:\n",
        "                pipe_used = \"Stable Diffusion Text-to-Image\"\n",
        "                if pipe is None:\n",
        "                  print(\"pipe is None\")\n",
        "                cross_attention_kwargs = {\"cross_attention_kwargs\": {\"scale\": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}\n",
        "                if prefs['SD_compel']:\n",
        "                  pipe_used += \" w/ Compel\"\n",
        "                  prompt_embed = compel_proc.build_conditioning_tensor(pr)\n",
        "                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\")\n",
        "                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])\n",
        "                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "                  del prompt_embed, negative_embed\n",
        "                else:\n",
        "                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else \"blurry\", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images\n",
        "                #pipe_used = \"Long Prompt Weight Text-to-Image\"\n",
        "                #images = pipe.text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              '''if prefs['precision'] == \"autocast\":\n",
        "                with autocast(\"cuda\"):\n",
        "                  images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)[\"sample\"]\n",
        "              else:\n",
        "                with precision_scope(\"cuda\"):\n",
        "                  with torch.no_grad():\n",
        "                    images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)[\"sample\"]'''\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "        except RuntimeError as e:\n",
        "          clear_last()\n",
        "          if 'out of memory' in str(e):\n",
        "            alert_msg(page, f\"CRITICAL ERROR: GPU ran out of memory! Flushing memory to save session... Try reducing image size.\", content=Text(str(e).strip()))\n",
        "            clear_pipes()\n",
        "            pass\n",
        "          else:\n",
        "            alert_msg(page, f\"RUNTIME ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            pass\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          abort_run = True\n",
        "          pass\n",
        "        finally:\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      txt2img_output = stable_dir #f'{stable_dir}/stable-diffusion/outputs/txt2img-samples'\n",
        "      batch_output = prefs['image_output']\n",
        "      if bool(prefs['batch_folder_name']):\n",
        "        txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'])\n",
        "        batch_output = os.path.join(prefs['image_output'], prefs['batch_folder_name'])\n",
        "      if not os.path.exists(txt2img_output):\n",
        "        os.makedirs(txt2img_output)\n",
        "      if save_to_GDrive or storage_type == \"Colab Google Drive\":\n",
        "        if not os.path.exists(batch_output):\n",
        "          os.makedirs(batch_output)\n",
        "      elif storage_type == \"PyDrive Google Drive\": # TODO: I'm not getting the parent folder id right, their docs got confusing\n",
        "        newFolder = gdrive.CreateFile({'title': prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "        newFolder.Upload()\n",
        "        batch_output = newFolder\n",
        "      else:\n",
        "        if not os.path.exists(batch_output):\n",
        "          os.makedirs(batch_output)\n",
        "\n",
        "      filename = format_filename(pr[0] if type(pr) == list else pr)\n",
        "      if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "        images = []\n",
        "\n",
        "      idx = num = 0\n",
        "      for image in images:\n",
        "        cur_seed = arg['seed']\n",
        "        if idx > 0:\n",
        "          cur_seed += idx\n",
        "          i_count = f'  ({idx + 1} of {len(images)})  '\n",
        "          prt(Row([Text(i_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {cur_seed}     ')]))\n",
        "          #prt(f'{pr[0] if type(pr) == list else pr} - seed:{cur_seed}')\n",
        "        seed_suffix = \"\" if not prefs['file_suffix_seed'] else f\"-{cur_seed}\"\n",
        "        if prefs['use_imagic'] and status['installed_imagic'] and bool(arg['init_image'] and not bool(arg['mask_image'])):\n",
        "          if idx == 0: seed_suffix += '-alpha_1'\n",
        "          if idx == 1: seed_suffix += '-alpha_1_5'\n",
        "          if idx == 2: seed_suffix += '-alpha_2'\n",
        "        fname = f'{prefs[\"file_prefix\"]}{filename}{seed_suffix}'\n",
        "        image_path = available_file(txt2img_output, fname, idx)\n",
        "        num = int(image_path.rpartition('-')[2].partition('.')[0])\n",
        "        #image_path = os.path.join(txt2img_output, f'{fname}-{idx}.png')\n",
        "        image.save(image_path)\n",
        "        #print(f'size:{os.path.getsize(f\"{fname}-{idx}.png\")}')\n",
        "        if os.path.getsize(image_path) < 2000 or not usable_image: #False: #not sum(image.convert(\"L\").getextrema()) in (0, 2): #image.getbbox():#\n",
        "          os.remove(os.path.join(txt2img_output, f'{fname}-{num}.png'))\n",
        "          if strikes >= retry_attempts_if_NSFW:\n",
        "            if retry_attempts_if_NSFW != 0: prt(\"Giving up on finding safe image...\")\n",
        "            strikes = 0\n",
        "            continue\n",
        "          else: strikes += 1\n",
        "          new_dream = None\n",
        "          if isinstance(p, Dream):\n",
        "            new_dream = p\n",
        "            new_dream.prompt = pr[0] if type(pr) == list else pr\n",
        "            new_dream.arg['seed'] = random.randint(0,4294967295)\n",
        "          else:\n",
        "            new_dream = Dream(p, arg=dict(seed=random.randint(0,4294967295)))\n",
        "          updated_prompts.insert(p_idx+1, new_dream)\n",
        "          prt(f\"Filtered NSFW image, retrying prompt with new seed. Attempt {strikes} of {retry_attempts_if_NSFW}...\")\n",
        "          continue\n",
        "        else: strikes = 0\n",
        "        #if not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']:\n",
        "          #print(f\"Image path:{image_path}\")\n",
        "          #time.sleep(0.4)\n",
        "          #prt(Row([Img(src=image_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          #display(image)\n",
        "        #if bool(batch_folder_name):\n",
        "        #  fpath = os.path.join(txt2img_output, batch_folder_name, f'{fname}-{idx}.png')\n",
        "        #fpath = os.path.join(txt2img_output, f'{fname}-{idx}.png')\n",
        "        #fpath = available_file(txt2img_output, fname, idx)\n",
        "        fpath = image_path\n",
        "        if txt2img_output != batch_output:\n",
        "          new_file = available_file(batch_output, fname, num)\n",
        "        else:\n",
        "          new_file = image_path\n",
        "        #print(f'fpath: {fpath} - idx: {idx}')\n",
        "        if prefs['centipede_prompts_as_init_images']:\n",
        "          shutil.copy(fpath, os.path.join(root_dir, 'init_images'))\n",
        "          last_image = os.path.join(root_dir, 'init_images', f'{fname}-{num}.png')\n",
        "        page.auto_scrolling(True)\n",
        "        #if (not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']) and prefs['apply_ESRGAN_upscale']:\n",
        "        if not prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "          #print(f\"Image path:{image_path}\")\n",
        "          upscaled_path = new_file #os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)\n",
        "          #time.sleep(0.2)\n",
        "          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(fpath), width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([GestureDetector(content=Img(src=fpath, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=fpath, data=new_file, width=arg['width'], height=arg['height'], subtitle=pr[0] if type(pr) == list else pr, center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(ImageButton(src=fpath, width=arg['width'], height=arg['height'], data=new_file, subtitle=pr[0] if type(pr) == list else pr, center=True, page=page))\n",
        "          #time.sleep(0.3)\n",
        "          #display(image)\n",
        "        if prefs['use_upscale'] and status['installed_upscale']:\n",
        "          clear_pipes(['upscale'])\n",
        "          if pipe_upscale == None:\n",
        "            prt(Installing(\"Initializing Stable Diffusion 2 Upscale Pipeline...\"))\n",
        "            pipe_upscale = get_upscale_pipe()\n",
        "            clear_last()\n",
        "          prt(Row([Text(\"Upscaling 4X\"), pb]))\n",
        "          try:\n",
        "            output = pipe_upscale(prompt=pr, image=image, guidance_scale=arg['guidance_scale'], generator=generator, noise_level=prefs['upscale_noise_level'], callback=callback_fn, callback_steps=1)\n",
        "            output.images[0].save(fpath)\n",
        "            #clear_upscale()\n",
        "          except Exception as e:\n",
        "            alert_msg(page, \"Error Upscaling Image.  Most likely out of Memory... Reduce image size to less than 512px.\", content=Text(e))\n",
        "            pass\n",
        "          clear_last()\n",
        "          #clear_upscale_pipe()\n",
        "        if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "          w = int(arg['width'] * prefs[\"enlarge_scale\"])\n",
        "          h = int(arg['height'] * prefs[\"enlarge_scale\"])\n",
        "          prt(Row([Text(f'Enlarging {prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "          upscale_image(fpath, fpath, scale=prefs[\"enlarge_scale\"], face_enhance=prefs[\"face_enhance\"])\n",
        "          clear_last(update=False)\n",
        "\n",
        "        config_json = arg.copy()\n",
        "        del config_json['batch_size']\n",
        "        del config_json['n_iterations']\n",
        "        del config_json['precision']\n",
        "        config_json['prompt'] = pr[0] if type(pr) == list else pr\n",
        "        config_json['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "        if bool(prefs['meta_ArtistName']): config_json['artist'] = prefs['meta_ArtistName']\n",
        "        if bool(prefs['meta_Copyright']): config_json['copyright'] = prefs['meta_Copyright']\n",
        "        #TODO: if use_LoRA add layers and scale\n",
        "        if prefs['use_Stability_api']: del config_json['eta']\n",
        "        del config_json['use_Stability']\n",
        "        if not bool(config_json['negative_prompt']): del config_json['negative_prompt']\n",
        "        if not bool(config_json['prompt2']):\n",
        "          del config_json['prompt2']\n",
        "          del config_json['tweens']\n",
        "        if not bool(config_json['init_image']):\n",
        "          del config_json['init_image']\n",
        "          del config_json['init_image_strength']\n",
        "          del config_json['alpha_mask']\n",
        "        if not bool(config_json['mask_image']):\n",
        "          del config_json['mask_image']\n",
        "          del config_json['invert_mask']\n",
        "        if not bool(config_json['use_clip_guided_model']):\n",
        "          del config_json[\"use_clip_guided_model\"]\n",
        "          del config_json[\"clip_prompt\"]\n",
        "          del config_json[\"clip_guidance_scale\"]\n",
        "          del config_json[\"num_cutouts\"]\n",
        "          del config_json[\"use_cutouts\"]\n",
        "          del config_json[\"unfreeze_unet\"]\n",
        "          del config_json[\"unfreeze_vae\"]\n",
        "        else:\n",
        "          config_json[\"clip_model_id\"] = prefs['clip_model_id']\n",
        "        if prefs['apply_ESRGAN_upscale']:\n",
        "          config_json['upscale'] = f\"Upscaled {prefs['enlarge_scale']}x with ESRGAN\" + (\" with GFPGAN Face-Enhance\" if prefs['face_enhance'] else \"\")\n",
        "        sampler_str = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "        config_json['pipeline'] = pipe_used\n",
        "        config_json['scheduler_mode'] = sampler_str\n",
        "        config_json['model_path'] = model_path\n",
        "        if prefs['use_SDXL'] and status['installed_SDXL']:\n",
        "          config_json['model_path'] = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "        if prefs['save_image_metadata']:\n",
        "          img = PILImage.open(fpath)\n",
        "          metadata = PngInfo()\n",
        "          metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "          metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "          metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {prefs['enlarge_scale']}x with ESRGAN\" if prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "          metadata.add_text(\"title\", pr[0] if type(pr) == list else pr)\n",
        "          if prefs['save_config_in_metadata']:\n",
        "            config = f\"prompt: {pr[0] if type(pr) == list else pr}, seed: {cur_seed}, steps: {arg['steps']}, CGS: {arg['guidance_scale']}, iterations: {arg['n_iterations']}\" + f\", eta: {arg['eta']}\" if not prefs['use_Stability_api'] else \"\"\n",
        "            config += f\", sampler: {sampler_str}\"\n",
        "            if bool(arg['init_image']): config += f\", init_image: {arg['init_image']}, init_image_strength: {arg['init_image_strength']}\"\n",
        "            metadata.add_text(\"config\", config)\n",
        "            #metadata.add_text(\"prompt\", p)\n",
        "            metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "          img.save(fpath, pnginfo=metadata)\n",
        "\n",
        "        #new_file = available_file(batch_output if save_to_GDrive else txt2img_output, fname, idx)\n",
        "        #new_file = fname #.rpartition('.')[0] #f'{file_prefix}{filename}'\n",
        "        #if os.path.isfile(os.path.join(batch_output if save_to_GDrive else txt2img_output, f'{new_file}-{idx}.png')):\n",
        "        #  new_file += '-' + random.choice(string.ascii_letters) + random.choice(string.ascii_letters)\n",
        "        #new_file += f'-{idx}.png'\n",
        "        if save_to_GDrive:\n",
        "          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))\n",
        "          #shutil.move(fpath, os.path.join(batch_output, new_file))\n",
        "        elif storage_type == \"PyDrive Google Drive\":\n",
        "          #batch_output\n",
        "          out_file = gdrive.CreateFile({'title': new_file})\n",
        "          out_file.SetContentFile(fpath)\n",
        "          out_file.Upload()\n",
        "        elif bool(prefs['image_output']):\n",
        "          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))\n",
        "        if prefs['save_config_json']:\n",
        "          json_file = new_file.rpartition('.')[0] + '.json'\n",
        "          with open(os.path.join(stable_dir, json_file), \"w\") as f:\n",
        "            json.dump(config_json, f, ensure_ascii=False, indent=4)\n",
        "          #if save_to_GDrive:\n",
        "          shutil.copy(os.path.join(stable_dir, json_file), os.path.join(batch_output, json_file))\n",
        "          if storage_type == \"PyDrive Google Drive\":\n",
        "            #batch_output\n",
        "            out_file = gdrive.CreateFile({'title': json_file})\n",
        "            out_file.SetContentFile(os.path.join(stable_dir, json_file))\n",
        "            out_file.Upload()\n",
        "        output_files.append(os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file))\n",
        "        if (prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale']):\n",
        "          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)\n",
        "          #time.sleep(0.4)\n",
        "          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(upscaled_path), width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([GestureDetector(content=Img(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([Img(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Img(src=upscaled_path))\n",
        "          #upscaled = PILImage.open(os.path.join(batch_output, new_file))\n",
        "          #display(upscaled)\n",
        "        #else:\n",
        "          #time.sleep(0.4)\n",
        "          #prt(Row([Img(src=new_file, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        elif not prefs['apply_ESRGAN_upscale'] or not status['installed_ESRGAN']:\n",
        "          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)\n",
        "          prt(Row([ImageButton(src=upscaled_path, width=arg['width'], height=arg['height'], data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        prt(Row([Text(fpath.rpartition(slash)[2])], alignment=MainAxisAlignment.CENTER))\n",
        "        idx += 1\n",
        "        if abort_run:\n",
        "          prt(Text(\"üõë   Aborting Current Diffusion Run...\"))\n",
        "          abort_run = False\n",
        "          return\n",
        "      p_idx += 1\n",
        "      if abort_run:\n",
        "        prt(Text(\"üõë   Aborting Current Diffusion Run...\"))\n",
        "        abort_run = False\n",
        "        return\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "  else:\n",
        "    clear_pipes(\"interpolation\")\n",
        "    if pipe_interpolation is None:\n",
        "      pipe_interpolation = get_interpolation_pipe()\n",
        "    txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else 'dreams')\n",
        "    batch_output = prefs['image_output']\n",
        "    if not os.path.exists(txt2img_output):\n",
        "      os.makedirs(txt2img_output)\n",
        "    #dream_name = prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else None\n",
        "    #first = prompts[0]\n",
        "    arg = args.copy()\n",
        "    arg['width'] = int(arg['width'])\n",
        "    arg['height'] = int(arg['height'])\n",
        "    arg['seed'] = int(arg['seed'])\n",
        "    arg['guidance_scale'] = float(arg['guidance_scale'])\n",
        "    arg['steps'] = int(arg['steps'])\n",
        "    arg['eta'] = float(arg['eta'])\n",
        "    walk_prompts = []\n",
        "    walk_seeds = []\n",
        "    for p in prompts:\n",
        "      walk_prompts.append(p.prompt)\n",
        "      if int(p.arg['seed']) < 1 or arg['seed'] is None:\n",
        "        walk_seeds.append(random.randint(0,4294967295))\n",
        "      else:\n",
        "        walk_seeds.append(int(p.arg['seed']))\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import LoggingEventHandler, FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self,event):\n",
        "        nonlocal img_idx\n",
        "        if event.is_directory:\n",
        "          return None\n",
        "        elif event.event_type == 'created':\n",
        "          page.auto_scrolling(True)\n",
        "          clear_last()\n",
        "          #p_count = f'[{img_idx + 1} of {(len(walk_prompts) -1) * int(prefs['num_interpolation_steps'])}]  '\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          #prt(Row([Text(p_count), Text(walk_prompts[img_idx], expand=True, weight=FontWeight.BOLD), Text(f'seed: {walk_seeds[img_idx]}')]))\n",
        "          #prt(Row([Img(src=event.src_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=event.src_path, width=arg['width'], height=arg['height'], subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          time.sleep(0.2)\n",
        "          prt(pb)\n",
        "          page.auto_scrolling(False)\n",
        "          img_idx += 1\n",
        "    # TODO: Rename files with to-from prompt text between each\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, txt2img_output, recursive = True)\n",
        "    observer.start()\n",
        "    prt(f\"Interpolating latent space between {len(walk_prompts)} prompts with {int(prefs['num_interpolation_steps'])} steps between each.\")\n",
        "    prt(Divider(height=6, thickness=2))\n",
        "    prt(pb)\n",
        "    page.auto_scrolling(False)\n",
        "    #prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg[\"seed\"]}')]))\n",
        "    images = pipe_interpolation.walk(prompts=walk_prompts, seeds=walk_seeds, num_interpolation_steps=int(prefs['num_interpolation_steps']), batch_size=int(prefs['batch_size']), output_dir=txt2img_output, width=arg['width'], height=arg['height'], guidance_scale=arg['guidance_scale'], num_inference_steps=int(arg['steps']), eta=arg['eta'], callback=callback_fn, callback_steps=1)\n",
        "    observer.stop()\n",
        "    clear_last()\n",
        "    fpath = images[0].rpartition(slash)[0]\n",
        "    bfolder = fpath.rpartition(slash)[2]\n",
        "    if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "      prt('Applying Real-ESRGAN Upscaling to images...')\n",
        "      upscale_image(images, fpath, scale=prefs[\"enlarge_scale\"], face_enhance=prefs[\"face_enhance\"])\n",
        "      '''os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "      upload_folder = 'upload'\n",
        "      result_folder = 'results'\n",
        "      if os.path.isdir(upload_folder):\n",
        "          shutil.rmtree(upload_folder)\n",
        "      if os.path.isdir(result_folder):\n",
        "          shutil.rmtree(result_folder)\n",
        "      os.mkdir(upload_folder)\n",
        "      os.mkdir(result_folder)\n",
        "      for i in images:\n",
        "        fname = i.rpartition(slash)[2]\n",
        "        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, fname)\n",
        "        shutil.move(i, dst_path)\n",
        "      faceenhance = ' --face_enhance' if prefs[\"face_enhance\"] else ''\n",
        "      run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "      filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))\n",
        "      for oname in filenames:\n",
        "        fparts = oname.rpartition('_out')\n",
        "        fname_clean = fparts[0] + fparts[2]\n",
        "        opath = os.path.join(fpath, fname_clean)\n",
        "        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, oname), opath)\n",
        "      os.chdir(stable_dir)'''\n",
        "    os.makedirs(os.path.join(batch_output, bfolder), exist_ok=True)\n",
        "    imgs = os.listdir(fpath)\n",
        "    for i in imgs:\n",
        "      #prt(f'Created {i}')\n",
        "      #fname = i.rpartition(slash)[2]\n",
        "      if save_to_GDrive:\n",
        "        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))\n",
        "      elif storage_type == \"PyDrive Google Drive\":\n",
        "        #batch_output\n",
        "        out_file = gdrive.CreateFile({'title': i})\n",
        "        out_file.SetContentFile(fpath)\n",
        "        out_file.Upload()\n",
        "      elif bool(prefs['image_output']):\n",
        "        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def prompt_parse(prompt):\n",
        "    '''Convert A1111 weights to Compel format'''\n",
        "    if ')' not in prompt and ']' not in prompt:\n",
        "        return prompt\n",
        "    else:\n",
        "        # Find and replace all instances of the colon format with the desired format\n",
        "        prompt = re.sub(r'\\(([^:]+):([\\d.]+)\\)', r'(\\1)\\2', prompt)\n",
        "        # Find and replace square brackets with round brackets and assign weight\n",
        "        prompt = re.sub(r'\\[([^:\\]]+)\\]', r'(\\1)0.909090909', prompt)\n",
        "        # Handle the general case of [x:number] and convert it to (x)0.9\n",
        "        prompt = re.sub(r'\\[([^:]+):[\\d.]+\\]', r'(\\1)0.9', prompt)\n",
        "        return prompt\n",
        "\n",
        "nspterminology = None\n",
        "\n",
        "def nsp_parse(prompt):\n",
        "    import random, os, json\n",
        "    global nspterminology\n",
        "    new_prompt = ''\n",
        "    new_prompts = []\n",
        "    new_dict = {}\n",
        "    ptype = type(prompt)\n",
        "    #if not os.path.exists('./nsp_pantry.json'):\n",
        "    #    wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json', f'.{slash}nsp_pantry.json')\n",
        "    if nspterminology is None:\n",
        "        response = requests.get(\"https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json\")\n",
        "        nspterminology = json.loads(response.content)\n",
        "    if ptype == dict:\n",
        "        for pstep, pvalue in prompt.items():\n",
        "            if type(pvalue) == list:\n",
        "                for prompt in pvalue:\n",
        "                    new_prompt = prompt\n",
        "                    for term in nspterminology:\n",
        "                        tkey = f'_{term}_'\n",
        "                        tcount = prompt.count(tkey)\n",
        "                        for i in range(tcount):\n",
        "                            new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "                    new_prompts.append(new_prompt)\n",
        "                new_dict[pstep] = new_prompts\n",
        "                new_prompts = []\n",
        "        return new_dict\n",
        "    elif ptype == list:\n",
        "        for pstr in prompt:\n",
        "            new_prompt = pstr\n",
        "            for term in nspterminology:\n",
        "                tkey = f'_{term}_'\n",
        "                tcount = new_prompt.count(tkey)\n",
        "                for i in range(tcount):\n",
        "                    new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "            new_prompts.append(new_prompt)\n",
        "            new_prompt = None\n",
        "        return new_prompts\n",
        "    elif ptype == str:\n",
        "        new_prompt = prompt\n",
        "        for term in nspterminology:\n",
        "            tkey = f'_{term}_'\n",
        "            tcount = new_prompt.count(tkey)\n",
        "            for i in range(tcount):\n",
        "                new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "        return new_prompt\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "#Code a function in Python programming language named list_variations, which takes a list and returns a set of lists with possible permutations of the list. Example list_variations([1,2,3]) returns [[1,2,3],[1,2],[1,3],[2,3],[1],[2],[3]] */\n",
        "def list_variations(lst):\n",
        "    result = []\n",
        "    for i in range(len(lst)):\n",
        "        for j in range(i+1, len(lst)+1):\n",
        "            result.append(lst[i:j])\n",
        "    return result\n",
        "#print(str(list_variations([1,2,3])))\n",
        "def and_list(lst):\n",
        "  return \" and \".join([\", \".join(lst[:-1]),lst[-1]] if len(lst) > 2 else lst)\n",
        "\n",
        "generator_request_modes = [\"visually detailed\",\n",
        "  \"with long detailed colorful interesting artistic scenic visual descriptions\",\n",
        "  \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "  \"that are strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "  \"that is technical, wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "  \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "  \"that includes many subjects with descriptions, color details, artistic expression, point of view\",\n",
        "  \"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",]\n",
        "\n",
        "def run_prompt_generator(page):\n",
        "  import random as rnd\n",
        "  global artists, styles, status\n",
        "  if 'GPT' in prefs['prompt_generator']['AI_engine']:\n",
        "    try:\n",
        "      import openai\n",
        "    except:\n",
        "      page.prompt_generator_list.controls.append(Installing(\"Installing OpenAI Library...\"))\n",
        "      page.prompt_generator_list.update()\n",
        "      run_sp(\"pip install --upgrade openai\", realtime=False)\n",
        "      import openai\n",
        "      del page.prompt_generator_list.controls[-1]\n",
        "      page.prompt_generator_list.update()\n",
        "      pass\n",
        "    try:\n",
        "      #openai.api_key = prefs['OpenAI_api_key']\n",
        "      from openai import OpenAI\n",
        "      openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "    except:\n",
        "      alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "      return\n",
        "    status['installed_OpenAI'] = True\n",
        "  if prefs['prompt_generator']['AI_engine'] == \"Google Gemini\":\n",
        "    if not bool(prefs['PaLM_api_key']):\n",
        "      alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "      return\n",
        "    try:\n",
        "      import google.generativeai as genai\n",
        "      if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except:\n",
        "      page.prompt_generator_list.controls.append(Installing(\"Installing Google MakerSuite Library...\"))\n",
        "      page.prompt_generator_list.update()\n",
        "      run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "      import google.generativeai as genai\n",
        "      del page.prompt_generator_list.controls[-1]\n",
        "      page.prompt_generator_list.update()\n",
        "      pass\n",
        "    try:\n",
        "      genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "    except:\n",
        "      alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "      return\n",
        "    gemini_model = genai.GenerativeModel(model_name='models/gemini-pro')\n",
        "  if prefs['prompt_generator']['AI_engine'] == \"Google Gemini Pro\":\n",
        "    if not bool(prefs['PaLM_api_key']):\n",
        "      alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "      return\n",
        "    try:\n",
        "      import vertexai\n",
        "    except:\n",
        "      page.prompt_generator_list.controls.append(Installing(\"Installing Vertex AI Gemini SDK Library...\"))\n",
        "      page.prompt_generator_list.update()\n",
        "      run_sp(\"pip install --upgrade google-cloud-aiplatform\", realtime=False)\n",
        "      from vertexai.preview.generative_models import GenerativeModel\n",
        "      del page.prompt_generator_list.controls[-1]\n",
        "      page.prompt_generator_list.update()\n",
        "      pass\n",
        "    try:\n",
        "      run_sp(\"gcloud auth application-default login\", realtime=True)\n",
        "    except:\n",
        "      alert_msg(page, \"Invalid Google Cloud Authentication. Change in Settings...\")\n",
        "      return\n",
        "  prompts_gen = []\n",
        "  prompt_results = []\n",
        "  subject = \"\"\n",
        "  if bool(prefs['prompt_generator']['subject_detail']):\n",
        "      subject = \", and \" + prefs['prompt_generator']['subject_detail']\n",
        "\n",
        "  def prompt_gen():\n",
        "    prompt = f'''Write a list of {prefs['prompt_generator']['amount'] if prefs['prompt_generator']['phrase_as_subject'] else (prefs['prompt_generator']['amount'] + 4)} image generation prompts about \"{prefs['prompt_generator']['phrase']}\"{subject}, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition:\n",
        "\n",
        "'''\n",
        "    #print(prompt)\n",
        "    if prefs['prompt_generator']['phrase_as_subject']:\n",
        "      prompt += \"\\n* \"\n",
        "    else:\n",
        "      prompt += f\"\"\"* A beautiful painting of a serene landscape with a river running through it, lush trees, golden sun illuminating\n",
        "* Fireflies illuminating autumnal woods, an Autumn in the Brightwood glade, with warm yellow lantern lights\n",
        "* The Fabric of spacetime continuum over a large cosmological vista, pieces of dark matter, space dust and nebula doted with small dots that seem to form fractal patterns and glowing bright lanterns in distances, also with an stardust effect towards the plane of the galaxy\n",
        "* Midnight landscape painting of a city under a starry sky, owl in the shaman forest knowing the ways of magic, warm glow over the buildings\n",
        "* {prefs['prompt_generator']['phrase']}\"\"\"\n",
        "    if prefs['prompt_generator']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "      response = openai_client.completions.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)\n",
        "      #print(response)\n",
        "      result = response.choices[0].text.strip()#[\"choices\"][0][\"text\"].strip()\n",
        "    elif prefs['prompt_generator']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "      response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        temperature=prefs['prompt_generator']['AI_temperature'],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "      )\n",
        "      #print(str(response))\n",
        "      result = response.choices[0].message.content.strip()#[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    elif \"GPT-4\" in prefs['prompt_generator']['AI_engine']:\n",
        "      gpt_model = \"gpt-4-1106-preview\" if \"Turbo\" in prefs['prompt_generator']['AI_engine'] else \"gpt-4\"\n",
        "      response = openai_client.chat.completions.create(\n",
        "        model=gpt_model,\n",
        "        temperature=prefs['prompt_generator']['AI_temperature'],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "      )\n",
        "      result = response.choices[0].message.content.strip()#[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    elif prefs['prompt_generator']['AI_engine'] == \"Google Gemini\":\n",
        "      #print(palm.list_models())\n",
        "      completion = gemini_model.generate_content(prompt, generation_config={\n",
        "          'temperature': prefs['prompt_generator']['AI_temperature'],\n",
        "          'max_output_tokens': 1024\n",
        "      })\n",
        "      #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt, temperature=prefs['prompt_generator']['AI_temperature'], max_output_tokens=1024)\n",
        "      #print(str(completion))\n",
        "      result = completion.text.strip()\n",
        "    #if result[-1] == '.': result = result[:-1]\n",
        "    #print(str(result))\n",
        "    for p in result.split('\\n'):\n",
        "      pr = p.strip()\n",
        "      if not bool(pr): continue\n",
        "      if pr[-1] == '.': pr = pr[:-1]\n",
        "      if pr[0] == '*': pr = pr[1:].strip()\n",
        "      elif '.' in pr: # Sometimes got 1. 2.\n",
        "        pr = pr.partition('.')[2].strip()\n",
        "      if '\"' in pr: pr = pr.replace('\"', '')\n",
        "      if pr.endswith(\".\"):\n",
        "        pr = pr[:(-1)]\n",
        "      if '*' in pr: pr = pr.rpartition('*')[2].strip()\n",
        "      prompt_results.append(pr)\n",
        "  #print(f\"Request mode influence: {request_modes[prefs['prompt_generator']['request_mode']]}\\n\")\n",
        "  page.prompt_generator_list.controls.append(Installing(\"Requesting Prompts from the AI...\"))\n",
        "  page.prompt_generator_list.update()\n",
        "  prompt_gen()\n",
        "  del page.prompt_generator_list.controls[-1]\n",
        "  page.prompt_generator_list.update()\n",
        "  if len(prompt_results) < prefs['prompt_generator']['amount']:\n",
        "    additional = prefs['prompt_generator']['amount'] - len(prompt_results)\n",
        "    print(f\"Didn't make enough prompts.. Needed {additional} more.\")\n",
        "  n=1\n",
        "  for p in prompt_results:\n",
        "    random_artist=[]\n",
        "    for a in range(prefs['prompt_generator']['random_artists']):\n",
        "      random_artist.append(rnd.choice(artists))\n",
        "    #print(list_variations(random_artist))\n",
        "    artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "    random_style = []\n",
        "    for s in range(prefs['prompt_generator']['random_styles']):\n",
        "      random_style.append(rnd.choice(styles))\n",
        "    style = \", \".join(random_style)\n",
        "    if not prefs['prompt_generator']['phrase_as_subject'] and n == 1:\n",
        "      p = prefs['prompt_generator']['phrase'] + \" \" + p\n",
        "    text_prompt = p\n",
        "    #if prefs['prompt_generator']['random_artists'] > 0 or prefs['prompt_generator']['random_styles'] > 0:\n",
        "    prompts_gen.append(text_prompt)\n",
        "    if prefs['prompt_generator']['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "    if prefs['prompt_generator']['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "    #if prefs['prompt_generator']['random_styles'] != 0 and prefs['prompt_generator']['permutate_artists']:\n",
        "    #  prompts_gen.append(text_prompt)\n",
        "    if prefs['prompt_generator']['permutate_artists']:\n",
        "\n",
        "      for a in list_variations(random_artist):\n",
        "        prompt_variation = p + f\", by {and_list(a)}\"\n",
        "        prompts_gen.append(prompt_variation)\n",
        "      if prefs['prompt_generator']['random_styles'] > 0:\n",
        "        prompts_gen.append(p + f\", style of {style}\")\n",
        "    else: prompts_gen.append(text_prompt)\n",
        "    n += 1\n",
        "  for item in prompts_gen:\n",
        "    page.add_to_prompt_generator(item)\n",
        "    #print(f'   \"{item}\",')\n",
        "\n",
        "remixer_request_modes = [\n",
        "      \"visually detailed wording, flowing sentences, extra long descriptions\",\n",
        "      \"that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations\",\n",
        "      \"that is completely rewritten, inspired by, paints a complete picture of an artistic scene\",\n",
        "      \"with detailed colorful interesting artistic scenic visual descriptions, described to a blind person\",\n",
        "      \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "      \"that replaces every noun, adjective, verb, pronoun, with related words\",\n",
        "      \"that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "      \"that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "      \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "      \"that includes more subjects with descriptions, textured color details, expressive\",]\n",
        "      #\"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",\n",
        "\n",
        "def run_prompt_remixer(page):\n",
        "  import random as rnd\n",
        "  global artists, styles, status\n",
        "  if 'GPT' in prefs['prompt_remixer']['AI_engine']:\n",
        "    try:\n",
        "      import openai\n",
        "    except:\n",
        "      run_sp(\"pip install --upgrade openai\")\n",
        "      import openai\n",
        "      pass\n",
        "    try:\n",
        "      #openai.api_key = prefs['OpenAI_api_key']\n",
        "      from openai import OpenAI\n",
        "      openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "    except:\n",
        "      alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "      return\n",
        "    status['installed_OpenAI'] = True\n",
        "  if prefs['prompt_remixer']['AI_engine'] == \"Google Gemini\":\n",
        "    if not bool(prefs['PaLM_api_key']):\n",
        "      alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "      return\n",
        "    try:\n",
        "      import google.generativeai as genai\n",
        "      if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except:\n",
        "      page.prompt_remixer_list.controls.append(Installing(\"Installing Google MakerSuite Library...\"))\n",
        "      page.prompt_remixer_list.update()\n",
        "      run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "      import google.generativeai as genai\n",
        "      del page.prompt_remixer_list.controls[-1]\n",
        "      page.prompt_remixer_list.update()\n",
        "      pass\n",
        "    try:\n",
        "      genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "    except:\n",
        "      alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "      return\n",
        "    gemini_model = genai.GenerativeModel(model_name='models/gemini-pro')\n",
        "  prompts_remix = []\n",
        "  prompt_results = []\n",
        "\n",
        "  if '_' in prefs['prompt_remixer']['seed_prompt']:\n",
        "    seed_prompt = nsp_parse(prefs['prompt_remixer']['seed_prompt'])\n",
        "  else:\n",
        "    seed_prompt = prefs['prompt_remixer']['seed_prompt']\n",
        "  if '_' in prefs['prompt_remixer']['optional_about_influencer']:\n",
        "    optional_about_influencer = nsp_parse(prefs['prompt_remixer']['optional_about_influencer'])\n",
        "  else:\n",
        "    optional_about_influencer = prefs['prompt_remixer']['optional_about_influencer']\n",
        "  about =  f\" about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"\n",
        "  prompt = f'Write a list of {prefs[\"prompt_remixer\"][\"amount\"]} remixed variations from the following image generation prompt{about}, \"{prefs[\"prompt_remixer\"][\"seed_prompt\"]}\", {remixer_request_modes[int(prefs[\"prompt_remixer\"][\"request_mode\"])]}, and unique without repetition:\\n\\n*'\n",
        "  prompt_results = []\n",
        "\n",
        "  def prompt_remix():\n",
        "    if prefs['prompt_remixer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "      response = openai_client.completions.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=2400, temperature=prefs[\"prompt_remixer\"]['AI_temperature'], presence_penalty=1)\n",
        "      #print(response)\n",
        "      result = response.choices[0].text.strip()\n",
        "    elif prefs['prompt_remixer']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "      response = openai_client.chat.completions.create(model=\"gpt-3.5-turbo-16k\", temperature=prefs[\"prompt_remixer\"]['AI_temperature'], messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "      #print(str(response))\n",
        "      result = response.choices[0].message.content.strip()\n",
        "    elif \"GPT-4\" in prefs['prompt_remixer']['AI_engine']:\n",
        "      gpt_model = \"gpt-4-1106-preview\" if \"Turbo\" in prefs['prompt_remixer']['AI_engine'] else \"gpt-4\"\n",
        "      response = openai_client.chat.completions.create(model=gpt_model, temperature=prefs[\"prompt_remixer\"]['AI_temperature'], messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "      result = response.choices[0].message.content.strip()\n",
        "    elif prefs['prompt_remixer']['AI_engine'] == \"Google Gemini\":\n",
        "      completion = gemini_model.generate_content(prompt, generation_config={\n",
        "          'temperature': prefs['prompt_remixer']['AI_temperature'],\n",
        "          'max_output_tokens': 1024\n",
        "      })\n",
        "      #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt, temperature=prefs['prompt_remixer']['AI_temperature'], max_output_tokens=1024)\n",
        "      #print(str(completion.result))\n",
        "      result = completion.text.strip()\n",
        "    #if result[-1] == '.': result = result[:-1]\n",
        "    #print(str(result))\n",
        "    for p in result.split('\\n'):\n",
        "      pr = p.strip()\n",
        "      if not bool(pr): continue\n",
        "      if pr[-1] == '.': pr = pr[:-1]\n",
        "      if pr[0] == '*': pr = pr[1:].strip()\n",
        "      elif '.' in pr: # Sometimes got 1. 2.\n",
        "        pr = pr.partition('.')[2].strip()\n",
        "      if '*' in pr:\n",
        "        pr = pr.replace('*', '').strip()\n",
        "      prompt_results.append(pr)\n",
        "  page.prompt_remixer_list.controls.append(Text(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\") + f\"\\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\"))\n",
        "  page.prompt_remixer_list.update()\n",
        "  #page.add_to_prompt_remixer(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\") + f\"\\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\")\n",
        "  #print(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"))\n",
        "  #print(f\"Request mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\")\n",
        "  page.prompt_remixer_list.controls.append(Installing(\"Requesting Prompt Remixes...\"))\n",
        "  page.prompt_remixer_list.update()\n",
        "  prompt_remix()\n",
        "  del page.prompt_remixer_list.controls[-1]\n",
        "  del page.prompt_remixer_list.controls[-1]\n",
        "  page.prompt_remixer_list.update()\n",
        "\n",
        "  for p in prompt_results:\n",
        "    random_artist=[]\n",
        "    for a in range(prefs['prompt_remixer']['random_artists']):\n",
        "      random_artist.append(rnd.choice(artists))\n",
        "    #print(list_variations(random_artist))\n",
        "    artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "    random_style = []\n",
        "    for s in range(prefs['prompt_remixer']['random_styles']):\n",
        "      random_style.append(rnd.choice(styles))\n",
        "    style = \", \".join(random_style)\n",
        "    text_prompt = p\n",
        "    if prefs['prompt_remixer']['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "    if prefs['prompt_remixer']['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "    if prefs['prompt_remixer']['random_styles'] == 0 and prefs['prompt_remixer']['permutate_artists']:\n",
        "      prompts_remix.append(text_prompt)\n",
        "    if prefs['prompt_remixer']['permutate_artists']:\n",
        "      for a in list_variations(random_artist):\n",
        "        prompt_variation = p + f\", by {and_list(a)}\"\n",
        "        prompts_remix.append(prompt_variation)\n",
        "      if prefs['prompt_remixer']['random_styles'] > 0:\n",
        "        prompts_remix.append(p + f\", style of {style}\")\n",
        "    else: prompts_remix.append(text_prompt)\n",
        "  for item in prompts_remix:\n",
        "    page.add_to_prompt_remixer(item)\n",
        "\n",
        "def get_stable_lm(ai_model=\"StableLM 3b\"):\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    clear_pipes('stable_lm')\n",
        "    if pipe_stable_lm != None:\n",
        "      return pipe_stable_lm\n",
        "    try:\n",
        "      import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install git+https://github.com/huggingface/accelerate.git\", realtime=False)\n",
        "      import accelerate\n",
        "      pass\n",
        "    try:\n",
        "      os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "      import bitsandbytes\n",
        "    except ModuleNotFoundError:\n",
        "      if sys.platform.startswith(\"win\"):\n",
        "          run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "      else:\n",
        "          run_sp(\"pip install bitsandbytes\", realtime=False)\n",
        "      import bitsandbytes\n",
        "      pass\n",
        "    try:\n",
        "      import transformers\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install -q transformers==4.21.3 --upgrade --force-reinstall\", realtime=False)\n",
        "      import transformers\n",
        "      pass\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    if ai_model == \"StableLM 3b\":\n",
        "        model_name = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
        "    else:\n",
        "        model_name = \"stabilityai/stablelm-base-alpha-7b\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        pipe_stable_lm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"float16\",\n",
        "            load_in_8bit=True,\n",
        "            device_map=\"auto\",\n",
        "            offload_folder=\"./offload\",\n",
        "            cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None\n",
        "        )\n",
        "    except Exception as e:\n",
        "      print(str(e))\n",
        "      return None\n",
        "    return pipe_stable_lm\n",
        "\n",
        "def stable_lm_request(input_sentence, temperature=0.5, max_tokens=2048, top_k=0, top_p=0.9, do_sample=True):\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    inputs = tokenizer_stable_lm(input_sentence, return_tensors=\"pt\")\n",
        "    inputs.to(pipe_stable_lm.device)\n",
        "    tokens = pipe_stable_lm.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=max_tokens,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      do_sample=do_sample,\n",
        "      pad_token_id=tokenizer_stable_lm.eos_token_id,\n",
        "    )\n",
        "    completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "brainstorm_request_modes = {\n",
        "    \"Brainstorm\":\"Brainstorm visual ideas for an image prompt about \",\n",
        "    \"Write\":\"Write an interesting visual scene about \",\n",
        "    \"Rewrite\":\"Rewrite new variations of \",\n",
        "    \"Edit\":\"Edit this text to improve details and structure: \",\n",
        "    \"Story\":\"Write an interesting story with visual details and poetic subjects about \",\n",
        "    \"Description\":\"Describe in graphic detail \",\n",
        "    \"Picture\":\"Paint a picture with words about \",\n",
        "    \"Raw Request\":\"\",\n",
        "}\n",
        "\n",
        "def run_prompt_brainstormer(page):\n",
        "    import random as rnd\n",
        "    global artists, styles, brainstorm_request_modes\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    textsynth_engine = \"gptj_6B\" #param [\"gptj_6B\", \"boris_6B\", \"fairseq_gpt_13B\", \"gptneox_20B\", \"m2m100_1_2B\"]\n",
        "    #markdown HuggingFace Bloom AI Settings\n",
        "    max_tokens_length = 128 #param {type:'slider', min:1, max:64, step:1}\n",
        "    seed = int(2222 * prefs['prompt_brainstormer']['AI_temperature']) #param {type:'integer'}\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
        "\n",
        "    good_key = True\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "      try:\n",
        "        if not bool(prefs['TextSynth_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        print(f\"\\33[91mMissing TextSynth_api_key...\\33[0m Define your key up above.\")\n",
        "        return\n",
        "      else:\n",
        "        try:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        except ImportError:\n",
        "          run_sp(\"pip install textsynthpy\", realtime=False)\n",
        "          clear_output()\n",
        "        finally:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        textsynth = TextSynth(prefs['TextSynth_api_key'], engine=textsynth_engine) # Insert your API key in the previous cell\n",
        "    if 'GPT' in prefs['prompt_brainstormer']['AI_engine']:\n",
        "      try:\n",
        "        if not bool(prefs['OpenAI_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        alert_msg(page, f\"Missing OpenAI_api_key... Define your key in Settings.\")\n",
        "        return\n",
        "      else:\n",
        "        try:\n",
        "          import openai\n",
        "        except ModuleNotFoundError:\n",
        "          run_sp(\"pip install --upgrade openai -qq\", realtime=False)\n",
        "          #clear_output()\n",
        "          pass\n",
        "        finally:\n",
        "          import openai\n",
        "        try:\n",
        "          from openai import OpenAI\n",
        "          openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "        except:\n",
        "          alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "          return\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\" or prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5 XXL\":\n",
        "      try:\n",
        "        if not bool(prefs['HuggingFace_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        alert_msg(page, f\"Missing HuggingFace_api_key... Define your key in Settings.\")\n",
        "        return\n",
        "    #ask_OpenAI_instead = False #@param {type:'boolean'}\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"Google Gemini\":\n",
        "      if not bool(prefs['PaLM_api_key']):\n",
        "        alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "        return\n",
        "      try:\n",
        "        import google.generativeai as genai\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "      except:\n",
        "        page.prompt_brainstormer_list.controls.append(Installing(\"Installing Google MakerSuite Library...\"))\n",
        "        page.prompt_brainstormer_list.update()\n",
        "        run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "        import google.generativeai as genai\n",
        "        del page.prompt_brainstormer_list.controls[-1]\n",
        "        page.prompt_brainstormer_list.update()\n",
        "        pass\n",
        "      try:\n",
        "        genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "      except:\n",
        "        alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "        return\n",
        "      gemini_model = genai.GenerativeModel(model_name='models/gemini-pro')\n",
        "    \n",
        "    prompt_request_modes = [\n",
        "        \"visually detailed wording, flowing sentences, extra long descriptions\",\n",
        "        \"that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations\",\n",
        "        \"that is completely rewritten, inspired by, paints a complete picture of an artistic seen\",\n",
        "        \"with detailed colorful interesting artistic scenic visual descriptions, described to a blind person\",\n",
        "        \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "        \"that replaces every noun, adjective, verb, pronoun, with related words\",\n",
        "        \"that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "        \"that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "        \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "        \"that includes more subjects with descriptions, textured color details, expressive\",]\n",
        "        #\"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",\n",
        "\n",
        "    request = f'{brainstorm_request_modes[prefs[\"prompt_brainstormer\"][\"request_mode\"]]}\"{prefs[\"prompt_brainstormer\"][\"about_prompt\"]}\":' if prefs['prompt_brainstormer']['request_mode'] != \"Raw Request\" else prefs['prompt_brainstormer']['about_prompt']\n",
        "\n",
        "    def query(payload):\n",
        "        #print(payload)\n",
        "        response = requests.request(\"POST\", API_URL, json=payload, headers={\"Authorization\": f\"Bearer {prefs['HuggingFace_api_key']}\"})\n",
        "        #print(response.text)\n",
        "        return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "    def bloom_request(input_sentence):\n",
        "        parameters = {\n",
        "            \"max_new_tokens\": max_tokens_length,\n",
        "            \"do_sample\": False,\n",
        "            \"seed\": seed,\n",
        "            \"early_stopping\": False,\n",
        "            \"length_penalty\": 0.0,\n",
        "            \"eos_token_id\": None,}\n",
        "        payload = {\"inputs\": input_sentence, \"parameters\": parameters,\"options\" : {\"use_cache\": False} }\n",
        "        data = query(payload)\n",
        "        if \"error\" in data:\n",
        "            return f\"\\33[31mERROR: {data['error']}\\33[0m\"\n",
        "\n",
        "        generation = data[0][\"generated_text\"].split(input_sentence, 1)[1]\n",
        "        #return data[0][\"generated_text\"]\n",
        "        return generation\n",
        "\n",
        "    def flan_query(payload):\n",
        "        #print(payload)\n",
        "        response = requests.request(\"POST\", \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\", json=payload, headers={\"Authorization\": f\"Bearer {prefs['HuggingFace_api_key']}\"})\n",
        "        #print(response.text)\n",
        "        return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "    def flan_request(input_sentence):\n",
        "        parameters = {\n",
        "            \"max_new_tokens\": max_tokens_length,\n",
        "            \"do_sample\": False,\n",
        "            \"seed\": seed,\n",
        "            \"early_stopping\": False,\n",
        "            \"length_penalty\": 0.0,\n",
        "            \"eos_token_id\": None,}\n",
        "        payload = {\"inputs\": input_sentence, \"parameters\": parameters,\"options\" : {\"use_cache\": False} }\n",
        "        data = flan_query(payload)\n",
        "        if \"error\" in data:\n",
        "            return f\"\\33[31mERROR: {data['error']}\\33[0m\"\n",
        "\n",
        "        generation = data[0][\"generated_text\"].split(input_sentence, 1)[1]\n",
        "        #return data[0][\"generated_text\"]\n",
        "        return generation\n",
        "\n",
        "    def stable_lm_request(input_sentence):\n",
        "        inputs = tokenizer_stable_lm(input_sentence, return_tensors=\"pt\")\n",
        "        inputs.to(pipe_stable_lm.device)\n",
        "        tokens = pipe_stable_lm.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=2048,\n",
        "          temperature=prefs['prompt_brainstormer']['AI_temperature'],\n",
        "          top_k=0,\n",
        "          top_p=0.9,\n",
        "          do_sample=True,\n",
        "          pad_token_id=tokenizer_stable_lm.eos_token_id,\n",
        "        )\n",
        "        completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
        "        completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)\n",
        "        return completion\n",
        "\n",
        "    def prompt_brainstormer():\n",
        "      #(prompt=prompt, temperature=AI_temperature, presence_penalty=1, stop= \"\\n\")\n",
        "      page.prompt_brainstormer_list.controls.append(Installing(\"Storming the AI's Brain...\"))\n",
        "      page.prompt_brainstormer_list.update()\n",
        "\n",
        "      if prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "        response = textsynth.text_complete(prompt=request, max_tokens=200, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)\n",
        "        #print(str(response))\n",
        "        result = response.text.strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "        response = openai_client.completions.create(engine=\"text-davinci-003\", prompt=request, max_tokens=2400, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)\n",
        "        result = response.choices[0].text.strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "        response = openai_client.chat.completions.create(model=\"gpt-3.5-turbo-16k\", temperature=prefs['prompt_brainstormer']['AI_temperature'], messages=[{\"role\": \"user\", \"content\": request}])\n",
        "        result = response.choices[0].message.content.strip()\n",
        "      elif \"GPT-4\" in prefs['prompt_brainstormer']['AI_engine']:\n",
        "        gpt_model = \"gpt-4-1106-preview\" if \"Turbo\" in prefs['prompt_brainstormer']['AI_engine'] else \"gpt-4\"\n",
        "        response = openai_client.chat.completions.create(model=gpt_model, temperature=prefs['prompt_brainstormer']['AI_temperature'], messages=[{\"role\": \"user\", \"content\": request}])\n",
        "        result = response.choices[0].message.content.strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\":\n",
        "        result = bloom_request(request)\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5\":\n",
        "        result = flan_request(request)\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'].startswith(\"Stable\"):\n",
        "        if pipe_stable_lm != None and tokenizer_stable_lm != None:\n",
        "          page.add_to_prompt_brainstormer(Installing(\"Installing StableLM-Alpha Pipeline...\"))\n",
        "          pipe_stable_lm = get_stable_lm(prefs['prompt_brainstormer']['AI_engine'])\n",
        "          del page.prompt_brainstormer_list.controls[-1]\n",
        "          page.prompt_brainstormer_list.update()\n",
        "        result = stable_lm_request(request, temperature=prefs['prompt_brainstormer']['AI_temperature'])\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"Google Gemini\":\n",
        "        completion = gemini_model.generate_content(request, generation_config={\n",
        "            'temperature': prefs['prompt_brainstormer']['AI_temperature'],\n",
        "            'max_output_tokens': 1024\n",
        "        })\n",
        "        #completion = palm.generate_text(model='models/text-bison-001', prompt=request, temperature=prefs['prompt_brainstormer']['AI_temperature'], max_output_tokens=1024)\n",
        "        result = completion.text.strip()\n",
        "      del page.prompt_brainstormer_list.controls[-1]\n",
        "      page.prompt_brainstormer_list.update()\n",
        "      if '*' in result:\n",
        "        result = result.replace('*', '').strip()\n",
        "      page.add_to_prompt_brainstormer(str(result) + '\\n')\n",
        "    #print(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"))\n",
        "    if good_key:\n",
        "      #print(request)\n",
        "      prompt_brainstormer()\n",
        "\n",
        "def run_prompt_writer(page):\n",
        "    '''try:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nsp_parse\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"wget -qq --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py\")\n",
        "        #print(subprocess.run(['wget', '-q', '--show-progress', '--no-cache', '--backups=1', 'https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nsp_parse'''\n",
        "    import random as rnd\n",
        "    global artists, styles\n",
        "    def generate_prompt():\n",
        "      text_prompts = []\n",
        "      global art_Subjects, by_Artists, art_Styles\n",
        "      nsSubjects = nsp_parse(prefs['prompt_writer']['art_Subjects'])\n",
        "      nsArtists = nsp_parse(prefs['prompt_writer']['by_Artists'])\n",
        "      nsStyles = nsp_parse(prefs['prompt_writer']['art_Styles'])\n",
        "      prompt = nsSubjects\n",
        "      random_artist=[]\n",
        "      if nsArtists: random_artist.append(nsArtists)\n",
        "      for a in range(prefs['prompt_writer']['random_artists']):\n",
        "        random_artist.append(rnd.choice(artists))\n",
        "      artist = and_list(random_artist)\n",
        "      #artist = random.choice(artists) + \" and \" + random.choice(artists)\n",
        "      random_style = []\n",
        "      if prefs['prompt_writer']['art_Styles']: random_style.append(nsStyles)\n",
        "      for s in range(prefs['prompt_writer']['random_styles']):\n",
        "        random_style.append(rnd.choice(styles))\n",
        "      style = \", \".join(random_style)\n",
        "      subject_prompt = prompt\n",
        "      if len(artist) > 0: prompt += f\", by {artist}\"\n",
        "      if len(style) > 0: prompt += f\", style of {style}\"\n",
        "      if not prefs['prompt_writer']['permutate_artists']:\n",
        "        return prompt\n",
        "      if prefs['prompt_writer']['random_styles'] > 0 and prefs['prompt_writer']['permutate_artists']:\n",
        "        text_prompts.append(prompt)\n",
        "      if prefs['prompt_writer']['permutate_artists']:\n",
        "        for a in list_variations(random_artist):\n",
        "          prompt_variation = subject_prompt + f\", by {and_list(a)}\"\n",
        "          text_prompts.append(prompt_variation)\n",
        "        if prefs['prompt_writer']['random_styles'] > 0:\n",
        "          text_prompts.append(subject_prompt + f\", style of {style}\")\n",
        "        return text_prompts\n",
        "      #if mod_Custom and mod_Custom.strip(): prompt += mod_Custom)\n",
        "      #return prompt\n",
        "    prompts_writer = []\n",
        "    for p in range(prefs['prompt_writer']['amount']):\n",
        "      prompts_writer.append(generate_prompt())\n",
        "    for item in prompts_writer:\n",
        "      if type(item) is str:\n",
        "        page.add_to_prompt_writer(item)\n",
        "      if type(item) is list:\n",
        "        for i in item:\n",
        "          page.add_to_prompt_writer(i)\n",
        "\n",
        "def run_magic_prompt(page):\n",
        "    #import random as rnd\n",
        "    global artists, styles, magic_prompt_prefs, pipe_gpt2\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.magic_prompt_output.controls.append(line)\n",
        "      page.magic_prompt_output.update()\n",
        "    def clear_last(lines=1):\n",
        "        clear_line(page.magic_prompt_output, lines=lines)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Installing Magic Prompt GPT-2 Pipeline...\"))\n",
        "    try:\n",
        "        import jinja2\n",
        "    except:\n",
        "        run_sp(\"pip install -q jinja2==3.0.3\")\n",
        "        pass\n",
        "    try:\n",
        "        from transformers import pipeline, set_seed\n",
        "    except:\n",
        "        run_sp(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers.git\")\n",
        "        from transformers import pipeline, set_seed\n",
        "        pass\n",
        "    try:\n",
        "        import sentencepiece\n",
        "    except:\n",
        "        run_sp(\"pip install -q sentencepiece\")\n",
        "        pass\n",
        "    ideas = os.path.join(root_dir, \"ideas.txt\")\n",
        "    if not os.path.exists(ideas):\n",
        "        download_file(\"https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion/raw/main/ideas.txt\")\n",
        "    prompts_magic = []\n",
        "    prompt_results = []\n",
        "    if '_' in magic_prompt_prefs['seed_prompt']:\n",
        "        seed_prompt = nsp_parse(magic_prompt_prefs['seed_prompt'])\n",
        "    else:\n",
        "        seed_prompt = magic_prompt_prefs['seed_prompt']\n",
        "    clear_pipes(\"gpt2\")\n",
        "    if pipe_gpt2 == None:\n",
        "        try:\n",
        "            pipe_gpt2 = pipeline('text-generation', model='Gustavosta/MagicPrompt-Stable-Diffusion', tokenizer='gpt2')\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Initializing GPT-2 Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    with open(ideas, \"r\") as f:\n",
        "        line = f.readlines()\n",
        "    clear_last()\n",
        "    prt(\"Generating Magic Prompts from your Text Input...\")\n",
        "    prt(progress)\n",
        "\n",
        "    def generate(starting_text):\n",
        "        random_seed = int(magic_prompt_prefs['seed']) if int(magic_prompt_prefs['seed']) > 0 else rnd.randint(100, 1000000)\n",
        "        set_seed(random_seed)\n",
        "        if starting_text == \"\":\n",
        "            starting_text: str = line[rnd.randrange(0, len(line))].replace(\"\\n\", \"\").lower().capitalize()\n",
        "            starting_text: str = re.sub(r\"[,:\\-‚Äì.!;?_]\", '', starting_text)\n",
        "        response = pipe_gpt2(starting_text, max_length=(len(starting_text) + rnd.randint(60, 90)), num_return_sequences=int(magic_prompt_prefs['amount']))\n",
        "        response_list = []\n",
        "        for x in response:\n",
        "            resp = x['generated_text'].strip()\n",
        "            if resp != starting_text and len(resp) > (len(starting_text) + 4) and resp.endswith((\":\", \"-\", \"‚Äî\")) is False:\n",
        "                response_list.append(resp)\n",
        "        response_end = \"\\n\".join(response_list)\n",
        "        response_end = re.sub('[^ ]+\\.[^ ]+','', response_end)\n",
        "        response_end = response_end.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "        if response_end != \"\":\n",
        "            return response_end.split(\"\\n\")\n",
        "        else:\n",
        "            prt(\"Error Generating Magic Prompt Responses...\")\n",
        "            return []\n",
        "\n",
        "    #txt = grad.Textbox(lines=1, label=\"Initial Text\", placeholder=\"English Text here\")\n",
        "    #out = grad.Textbox(lines=4, label=\"Generated Prompts\")\n",
        "    #examples = []\n",
        "    #for x in range(8):\n",
        "    #    examples.append(line[rnd.randrange(0, len(line))].replace(\"\\n\", \"\").lower().capitalize())\n",
        "    #title = \"Stable Diffusion Prompt Generator\"\n",
        "    #description = 'This is a demo of the model series: \"MagicPrompt\", in this case, aimed at: \"Stable Diffusion\". To use it, simply submit your text or click on one of the examples. To learn more about the model, [click here](https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion).<br>'\n",
        "    prompt_results = generate(seed_prompt)\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for p in prompt_results:\n",
        "        random_artist=[]\n",
        "        for a in range(magic_prompt_prefs['random_artists']):\n",
        "            random_artist.append(rnd.choice(artists))\n",
        "        #print(list_variations(random_artist))\n",
        "        artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "        random_style = []\n",
        "        for s in range(magic_prompt_prefs['random_styles']):\n",
        "            random_style.append(rnd.choice(styles))\n",
        "        style = \", \".join(random_style)\n",
        "        text_prompt = p\n",
        "        prompts_magic.append(text_prompt)\n",
        "        if magic_prompt_prefs['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "        if magic_prompt_prefs['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "        #if magic_prompt_prefs['random_styles'] == 0 and magic_prompt_prefs['permutate_artists']:\n",
        "        #    prompts_magic.append(text_prompt)\n",
        "        if magic_prompt_prefs['permutate_artists']:\n",
        "            for a in list_variations(random_artist):\n",
        "                prompt_variation = p + f\", by {and_list(a)}\"\n",
        "                prompts_magic.append(prompt_variation)\n",
        "            if magic_prompt_prefs['random_styles'] > 0:\n",
        "                prompts_magic.append(p + f\", style of {style}\")\n",
        "        else: prompts_magic.append(text_prompt)\n",
        "    for item in prompts_magic:\n",
        "        page.add_to_magic_prompt(item)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_distil_gpt2(page):\n",
        "    #import random as rnd\n",
        "    global artists, styles, distil_gpt2_prefs, pipe_distil_gpt2\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.distil_gpt2_output.controls.append(line)\n",
        "      page.distil_gpt2_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.distil_gpt2_output, lines=lines)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Installing Distil GPT-2 Pipeline...\"))\n",
        "\n",
        "    try:\n",
        "        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
        "    except:\n",
        "        run_sp(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers.git\")\n",
        "        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
        "        pass\n",
        "\n",
        "    prompts_distil = []\n",
        "    prompt_results = []\n",
        "    if '_' in distil_gpt2_prefs['seed_prompt']:\n",
        "        seed_prompt = nsp_parse(distil_gpt2_prefs['seed_prompt'])\n",
        "    else:\n",
        "        seed_prompt = distil_gpt2_prefs['seed_prompt']\n",
        "    clear_pipes(\"distil_gpt2\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    if pipe_distil_gpt2 == None:\n",
        "        try:\n",
        "            pipe_distil_gpt2 = GPT2LMHeadModel.from_pretrained('FredZhang7/distilgpt2-stable-diffusion-v2')\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Initializing Distil GPT-2 Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "\n",
        "    clear_last()\n",
        "    prt(\"Generating Distil GPT-2 Results from your Text Input...\")\n",
        "    prt(progress)\n",
        "\n",
        "    def generate(starting_text):\n",
        "        random_seed = int(distil_gpt2_prefs['seed']) if int(distil_gpt2_prefs['seed']) > 0 else rnd.randint(100, 1000000)\n",
        "        set_seed(random_seed)\n",
        "        input_ids = tokenizer(starting_text, return_tensors='pt').input_ids\n",
        "        output = pipe_distil_gpt2.generate(input_ids, do_sample=True, temperature=distil_gpt2_prefs['AI_temperature'], top_k=distil_gpt2_prefs['top_k'], max_length=distil_gpt2_prefs['max_length'], num_return_sequences=distil_gpt2_prefs['amount'], repetition_penalty=distil_gpt2_prefs['repetition_penalty'], penalty_alpha=distil_gpt2_prefs['penalty_alpha'], no_repeat_ngram_size=distil_gpt2_prefs['no_repeat_ngram_size'], early_stopping=True)\n",
        "        results = []\n",
        "        for i in range(len(output)):\n",
        "            results.append(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "        return results\n",
        "    prompt_results = generate(seed_prompt)\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for p in prompt_results:\n",
        "        random_artist=[]\n",
        "        for a in range(distil_gpt2_prefs['random_artists']):\n",
        "            random_artist.append(rnd.choice(artists))\n",
        "        #print(list_variations(random_artist))\n",
        "        artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "        random_style = []\n",
        "        for s in range(distil_gpt2_prefs['random_styles']):\n",
        "            random_style.append(rnd.choice(styles))\n",
        "        style = \", \".join(random_style)\n",
        "        text_prompt = p\n",
        "        prompts_distil.append(text_prompt)\n",
        "        if distil_gpt2_prefs['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "        if distil_gpt2_prefs['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "        #if distil_gpt2_prefs['random_styles'] == 0 and distil_gpt2_prefs['permutate_artists']:\n",
        "        #    prompts_distil.append(text_prompt)\n",
        "        if distil_gpt2_prefs['permutate_artists']:\n",
        "            for a in list_variations(random_artist):\n",
        "                prompt_variation = p + f\", by {and_list(a)}\"\n",
        "                prompts_distil.append(prompt_variation)\n",
        "            if distil_gpt2_prefs['random_styles'] > 0:\n",
        "                prompts_distil.append(p + f\", style of {style}\")\n",
        "        else: prompts_distil.append(text_prompt)\n",
        "    for item in prompts_distil:\n",
        "        page.add_to_distil_gpt2(item)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_upscaling(page):\n",
        "    #print(str(ESRGAN_prefs))\n",
        "    if not status['installed_ESRGAN']:\n",
        "      alert_msg(page, \"You must Install Real-ESRGAN first\")\n",
        "      return\n",
        "    from collections import Counter\n",
        "    enlarge_scale = ESRGAN_prefs['enlarge_scale']\n",
        "    face_enhance = ESRGAN_prefs['face_enhance']\n",
        "    image_path = ESRGAN_prefs['image_path']\n",
        "    save_to_GDrive = ESRGAN_prefs['save_to_GDrive']\n",
        "    upload_file = ESRGAN_prefs['upload_file']\n",
        "    download_locally = ESRGAN_prefs['download_locally']\n",
        "    display_image = ESRGAN_prefs['display_image']\n",
        "    dst_image_path = ESRGAN_prefs['dst_image_path']\n",
        "    filename_suffix = ESRGAN_prefs['filename_suffix']\n",
        "    split_image_grid = ESRGAN_prefs['split_image_grid']\n",
        "    rows = ESRGAN_prefs['rows']\n",
        "    cols = ESRGAN_prefs['cols']\n",
        "    def split(im, rows, cols, img_path, should_cleanup=False):\n",
        "        im_width, im_height = im.size\n",
        "        row_width = int(im_width / rows)\n",
        "        row_height = int(im_height / cols)\n",
        "        n = 0\n",
        "        for i in range(0, cols):\n",
        "            for j in range(0, rows):\n",
        "                box = (j * row_width, i * row_height, j * row_width +\n",
        "                      row_width, i * row_height + row_height)\n",
        "                outp = im.crop(box)\n",
        "                name, ext = os.path.splitext(img_path)\n",
        "                outp_path = name + \"-\" + str(n) + ext\n",
        "                #print(\"Exporting image tile: \" + outp_path)\n",
        "                outp.save(outp_path)\n",
        "                n += 1\n",
        "        if should_cleanup:\n",
        "            #print(\"Cleaning up: \" + img_path)\n",
        "            os.remove(img_path)\n",
        "\n",
        "    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    upload_folder = 'upload'\n",
        "    result_folder = 'results'\n",
        "    if os.path.isdir(upload_folder):\n",
        "        shutil.rmtree(upload_folder)\n",
        "    if os.path.isdir(result_folder):\n",
        "        shutil.rmtree(result_folder)\n",
        "    os.mkdir(upload_folder)\n",
        "    os.mkdir(result_folder)\n",
        "\n",
        "    uploaded = None\n",
        "    if not upload_file:\n",
        "      if not image_path:\n",
        "         alert_msg(page, 'Provide path to image, local or url')\n",
        "         return\n",
        "      if '.' in image_path:\n",
        "        if os.path.exists(image_path):\n",
        "          uploaded = {image_path: image_path.rpartition(slash)[2]}\n",
        "        else:\n",
        "          alert_msg(page, 'File does not exist')\n",
        "          return\n",
        "      else:\n",
        "        if os.path.isdir(image_path):\n",
        "          uploaded = {}\n",
        "          for f in os.listdir(image_path):\n",
        "            uploaded[ os.path.join(image_path, f)] = f\n",
        "        else:\n",
        "          alert_msg(page, 'Image Path directory does not exist')\n",
        "          return\n",
        "    else:\n",
        "      uploaded = files.upload()\n",
        "    page.clear_ESRGAN_output(uploaded)\n",
        "    page.add_to_ESRGAN_output(Text(f\"Upscaling {len(uploaded)} images..\"))\n",
        "    for filename in uploaded.keys():\n",
        "      if not os.path.isfile(filename):\n",
        "        #print(\"Skipping \" + filename)\n",
        "        continue\n",
        "      fname = filename.rpartition(slash)[2] if slash in filename else filename\n",
        "      dst_path = os.path.join(upload_folder, fname)\n",
        "      #print(f'Copy {filename} to {dst_path}')\n",
        "      shutil.copy(filename, dst_path)\n",
        "      if split_image_grid:\n",
        "        img = PILImage.open(dst_path)\n",
        "        split(img, rows, cols, dst_path, True)\n",
        "    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    faceenhance = ' --face_enhance' if face_enhance else ''\n",
        "    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i {upload_folder} --outscale {enlarge_scale}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "    os.chdir(root_dir)\n",
        "    if is_Colab:\n",
        "      from google.colab import files\n",
        "    if not bool(dst_image_path.strip()):\n",
        "      if os.path.isdir(image_path):\n",
        "          dst_image_path = image_path\n",
        "      else:\n",
        "          dst_image_path = prefs['image_output'] #image_path.rpartition(slash)[0]\n",
        "    filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))\n",
        "    for fname in filenames:\n",
        "      fparts = fname.rpartition('_out')\n",
        "      fname_clean = fparts[0] + filename_suffix + fparts[2]\n",
        "      #print(f'Copying {fname_clean}')\n",
        "      #if save_to_GDrive:\n",
        "      if not os.path.isdir(dst_image_path):\n",
        "        os.makedirs(dst_image_path)\n",
        "      shutil.copy(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname), os.path.join(dst_image_path, fname_clean))\n",
        "      if download_locally:\n",
        "        files.download(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname))\n",
        "      if display_image:\n",
        "        page.add_to_ESRGAN_output(Image(src=os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname)))\n",
        "      page.add_to_ESRGAN_output(Row([Text(os.path.join(dst_image_path, fname_clean))], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "def run_retrieve(page):\n",
        "    upload_file = retrieve_prefs['upload_file']\n",
        "    image_path = retrieve_prefs['image_path']\n",
        "    display_full_metadata = retrieve_prefs['display_full_metadata']\n",
        "    display_image = retrieve_prefs['display_image']\n",
        "    add_to_prompts = retrieve_prefs['add_to_prompts']\n",
        "\n",
        "    import json\n",
        "    if is_Colab:\n",
        "      from google.colab import files\n",
        "    def meta_dream(meta):\n",
        "      if meta is not None and len(meta) > 1:\n",
        "          print(str(meta))\n",
        "          arg = {}\n",
        "          p = ''\n",
        "          dream = '    Dream('\n",
        "          if meta.get('title'):\n",
        "            dream += f'\"{meta[\"title\"]}\"'\n",
        "            p = meta[\"title\"]\n",
        "          if meta.get('prompt'):\n",
        "            dream += f'\"{meta[\"prompt\"]}\"'\n",
        "            p = meta[\"prompt\"]\n",
        "          if meta.get('config'):\n",
        "            meta = meta['config']\n",
        "          if meta.get('prompt'):\n",
        "            #dream += f'\"{meta[\"prompt\"]}\"'\n",
        "            p = meta[\"prompt\"]\n",
        "          if meta.get('prompt2'):\n",
        "            dream += f', prompt2=\"{meta[\"prompt2\"]}\"'\n",
        "            arg[\"prompt2\"] = meta[\"prompt2\"]\n",
        "          if meta.get('negative_prompt'):\n",
        "            dream += f', negative=\"{meta[\"negative_prompt\"]}\"'\n",
        "            arg[\"negative_prompt\"] = meta[\"negative_prompt\"]\n",
        "          if meta.get('tweens'):\n",
        "            dream += f', tweens={meta[\"tweens\"]}'\n",
        "            arg[\"tweens\"] = meta[\"tweens\"]\n",
        "          if meta.get('width'):\n",
        "            dream += f', width={meta[\"width\"]}'\n",
        "            arg[\"width\"] = meta[\"width\"]\n",
        "          if meta.get('height'):\n",
        "            dream += f', height={meta[\"height\"]}'\n",
        "            arg[\"height\"] = meta[\"height\"]\n",
        "          if meta.get('guidance_scale'):\n",
        "            dream += f', guidance_scale={meta[\"guidance_scale\"]}'\n",
        "            arg[\"guidance_scale\"] = meta[\"guidance_scale\"]\n",
        "          elif meta.get('CGS'):\n",
        "            dream += f', guidance_scale={meta[\"CGS\"]}'\n",
        "            arg[\"guidance_scale\"] = meta[\"CGS\"]\n",
        "          if meta.get('steps'):\n",
        "            dream += f', steps={meta[\"steps\"]}'\n",
        "            arg[\"steps\"] = meta[\"steps\"]\n",
        "          if meta.get('eta'):\n",
        "            dream += f', eta={meta[\"eta\"]}'\n",
        "            arg[\"eta\"] = meta[\"eta\"]\n",
        "          if meta.get('seed'):\n",
        "            dream += f', seed={meta[\"seed\"]}'\n",
        "            arg[\"seed\"] = meta[\"seed\"]\n",
        "          if meta.get('init_image'):\n",
        "            dream += f', init_image=\"{meta[\"init_image\"]}\"'\n",
        "            arg[\"init_image\"] = meta[\"init_image\"]\n",
        "          if meta.get('mask_image'):\n",
        "            dream += f', mask_image=\"{meta[\"mask_image\"]}\"'\n",
        "            arg[\"mask_image\"] = meta[\"mask_image\"]\n",
        "          if meta.get('init_image_strength'):\n",
        "            dream += f', init_image_strength={meta[\"init_image_strength\"]}'\n",
        "            arg[\"init_image_strength\"] = meta[\"init_image_strength\"]\n",
        "          dream += '),'\n",
        "          page.add_to_retrieve_output(Text(dream, selectable=True))\n",
        "          if display_full_metadata:\n",
        "            page.add_to_retrieve_output(Text(str(metadata)))\n",
        "          if add_to_prompts:\n",
        "            page.add_to_prompts(p, arg)\n",
        "      else:\n",
        "          alert_msg(page, 'Problem reading your config json image meta data.')\n",
        "          return\n",
        "    \n",
        "    def meta_comfy(metac):\n",
        "      if metac is not None and len(metac) > 1:\n",
        "          #print(str(meta))\n",
        "          arg = {}\n",
        "          #comfyui={\"3\": {\"inputs\": {\"seed\": 1058652716688649, \"steps\": 50, \"cfg\": 6.0, \"sampler_name\": \"euler_ancestral\", \"scheduler\": \"normal\", \"denoise\": 1.0, \"model\": [\"4\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"5\", 0]}, \"class_type\": \"KSampler\"}, \"4\": {\"inputs\": {\"ckpt_name\": \"v1-5-pruned-emaonly.ckpt\"}, \"class_type\": \"CheckpointLoaderSimple\"}, \"5\": {\"inputs\": {\"width\": 960, \"height\": 512, \"batch_size\": 4}, \"class_type\": \"EmptyLatentImage\"}, \"6\": {\"inputs\": {\"text\": \"Large fancy Armenian Church with modern and gothic architecture in a city, high quality Thomas Kinkade, futuristic and complex\", \"clip\": [\"4\", 1]}, \"class_type\": \"CLIPTextEncode\"}, \"7\": {\"inputs\": {\"text\": \"text, watermark, russian, russia\", \"clip\": [\"4\", 1]}, \"class_type\": \"CLIPTextEncode\"}, \"8\": {\"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}, \"class_type\": \"VAEDecode\"}, \"9\": {\"inputs\": {\"filename_prefix\": \"ComfyUI\", \"images\": [\"8\", 0]}, \"class_type\": \"SaveImage\"}, \"10\": {\"inputs\": {\"strength\": 1, \"noise_augmentation\": 0, \"conditioning\": [\"6\", 0]}, \"class_type\": \"unCLIPConditioning\"}, \"11\": {\"inputs\": {\"text\": \"\"}, \"class_type\": \"CLIPTextEncode\"}, \"15\": {\"inputs\": {\"style_model_name\": null}, \"class_type\": \"StyleModelLoader\"}, \"16\": {\"inputs\": {\"add_noise\": \"enable\", \"noise_seed\": 262554034916863, \"steps\": 20, \"cfg\": 8, \"sampler_name\": \"euler\", \"scheduler\": \"karras\", \"start_at_step\": 0, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\"}, \"class_type\": \"KSamplerAdvanced\"}, \"17\": {\"inputs\": {\"model_path\": null}, \"class_type\": \"DiffusersLoader\"}, \"18\": {\"inputs\": {\"strength\": 1, \"noise_augmentation\": 0}, \"class_type\": \"unCLIPConditioning\"}}\n",
        "          p = ''\n",
        "          dream = '    Comfy('\n",
        "          for k, v in metac.items():\n",
        "            if 'inputs' in v:\n",
        "              #print(f\"{k}: {v['inputs']}\")\n",
        "              meta = v['inputs']\n",
        "              if meta.get('add_moise'):\n",
        "                continue\n",
        "              if meta.get('title'):\n",
        "                dream += f'\"{meta[\"title\"]}\"'\n",
        "                p = meta[\"title\"]\n",
        "              if meta.get('prompt'):\n",
        "                dream += f'\"{meta[\"prompt\"]}\"'\n",
        "                p = meta[\"prompt\"]\n",
        "              if meta.get('config'):\n",
        "                meta = meta['config']\n",
        "              if meta.get('prompt'):\n",
        "                #dream += f'\"{meta[\"prompt\"]}\"'\n",
        "                p = meta[\"prompt\"]\n",
        "              if meta.get('text'):\n",
        "                if len(meta[\"text\"]) > 8:\n",
        "                  if not bool(p):\n",
        "                    dream += f', prompt=\"{meta[\"text\"]}\"'\n",
        "                    p = meta[\"text\"]\n",
        "                  else:\n",
        "                    dream += f', negative=\"{meta[\"text\"]}\"'\n",
        "                    arg[\"negative_prompt\"] = meta[\"text\"]\n",
        "              if meta.get('negative_prompt'):\n",
        "                dream += f', negative=\"{meta[\"negative_prompt\"]}\"'\n",
        "                arg[\"negative_prompt\"] = meta[\"negative_prompt\"]\n",
        "              if meta.get('width'):\n",
        "                dream += f', width={meta[\"width\"]}'\n",
        "                arg[\"width\"] = meta[\"width\"]\n",
        "              if meta.get('height'):\n",
        "                dream += f', height={meta[\"height\"]}'\n",
        "                arg[\"height\"] = meta[\"height\"]\n",
        "              if meta.get('guidance_scale'):\n",
        "                dream += f', guidance_scale={meta[\"guidance_scale\"]}'\n",
        "                arg[\"guidance_scale\"] = meta[\"guidance_scale\"]\n",
        "              elif meta.get('cfg'):\n",
        "                if 'seed' not in meta:\n",
        "                  continue\n",
        "                dream += f', guidance_scale={meta[\"cfg\"]}'\n",
        "                arg[\"guidance_scale\"] = meta[\"cfg\"]\n",
        "              if meta.get('steps'):\n",
        "                if 'seed' not in meta:\n",
        "                  continue\n",
        "                dream += f', steps={meta[\"steps\"]}'\n",
        "                arg[\"steps\"] = meta[\"steps\"]\n",
        "              if meta.get('eta'):\n",
        "                dream += f', eta={meta[\"eta\"]}'\n",
        "                arg[\"eta\"] = meta[\"eta\"]\n",
        "              if meta.get('seed'):\n",
        "                dream += f', seed={meta[\"seed\"]}'\n",
        "                arg[\"seed\"] = meta[\"seed\"]\n",
        "              if meta.get('init_image'):\n",
        "                dream += f', init_image=\"{meta[\"init_image\"]}\"'\n",
        "                arg[\"init_image\"] = meta[\"init_image\"]\n",
        "              if meta.get('mask_image'):\n",
        "                dream += f', mask_image=\"{meta[\"mask_image\"]}\"'\n",
        "                arg[\"mask_image\"] = meta[\"mask_image\"]\n",
        "              if meta.get('init_image_strength'):\n",
        "                dream += f', init_image_strength={meta[\"init_image_strength\"]}'\n",
        "                arg[\"init_image_strength\"] = meta[\"init_image_strength\"]\n",
        "          dream += '),'\n",
        "          page.add_to_retrieve_output(Text(dream, selectable=True))\n",
        "          if display_full_metadata:\n",
        "            page.add_to_retrieve_output(Text(str(metadata)))\n",
        "          if add_to_prompts:\n",
        "            page.add_to_prompts(p, arg)\n",
        "      else:\n",
        "          alert_msg(page, 'Problem reading your ComfyUI json image meta data.')\n",
        "          return\n",
        "    #Todo: Detect Automatic1111 and convert\n",
        "    def meta_auto1111(metadata):\n",
        "      if metadata is not None and len(metadata) > 1:\n",
        "          print(str(meta))\n",
        "          arg = {}\n",
        "          #https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/processing.py#L445-L462\n",
        "          p = ''\n",
        "          if 'parameters' not in metadata: return\n",
        "          print(metadata[\"parameters\"])\n",
        "          p = metadata[\"parameters\"].split(\"\\n\")[0]\n",
        "          arg[\"negative_prompt\"] = metadata[\"parameters\"].split(\"\\n\")[1].split(\": \")[1]\n",
        "          steps_index = metadata[\"parameters\"].index(\"Steps:\")\n",
        "          steps_info = metadata[\"parameters\"][steps_index:].split(\"\\nTemplate\")[0]\n",
        "          steps_list = steps_info.split(\", \")\n",
        "          cfg_index = metadata[\"parameters\"].index(\"CFG scale:\")\n",
        "          cfg_info = metadata[\"parameters\"][cfg_index:].split(\"\\nTemplate\")[0]\n",
        "          size_index = metadata[\"parameters\"].index(\"Size:\")\n",
        "          size_info = metadata[\"parameters\"][size_index:].split(\"\\nTemplate\")[0]\n",
        "          if 'x' in size_info:\n",
        "            size = size_info.split('x')\n",
        "            dream += f', width={size[0]}'\n",
        "            arg[\"width\"] = int(size[0])\n",
        "            dream += f', height={size[2]}'\n",
        "            arg[\"height\"] = int(size[2])\n",
        "          dream += '),'\n",
        "          page.add_to_retrieve_output(Text(dream, selectable=True))\n",
        "          if display_full_metadata:\n",
        "            page.add_to_retrieve_output(Text(str(metadata)))\n",
        "          if add_to_prompts:\n",
        "            page.add_to_prompts(p, arg)\n",
        "    uploaded = {}\n",
        "    if not upload_file:\n",
        "      if not bool(image_path):\n",
        "        alert_msg(page, 'Provide path to image, local or url')\n",
        "        return\n",
        "      if '.' in image_path:\n",
        "        if os.path.exists(image_path):\n",
        "          uploaded = {image_path: image_path.rpartition(slash)[2]}\n",
        "        else:\n",
        "          alert_msg(page, 'File does not exist')\n",
        "          return\n",
        "      else:\n",
        "        if os.path.isdir(image_path):\n",
        "          uploaded = {}\n",
        "          for f in os.listdir(image_path):\n",
        "            uploaded[ os.path.join(image_path, f)] = f\n",
        "        else:\n",
        "          alert_msg(page, 'The image_path directory does not exist')\n",
        "          return\n",
        "    else:\n",
        "      if not is_Colab:\n",
        "        uploaded = files.upload()\n",
        "        alert_msg(page, \"Can't upload an image easily from non-Colab systems\")\n",
        "        return\n",
        "    if len(uploaded) > 1:\n",
        "      page.add_to_retrieve_output(Text(f\"Revealing Dream of {len(uploaded)} images..\\n\"))\n",
        "    for filename in uploaded.keys():\n",
        "      if not os.path.isfile(filename):\n",
        "        #print(\"Skipping subfolder \" + filename)\n",
        "        continue\n",
        "      #print(filename)\n",
        "      if filename.rpartition('.')[2] == 'json':\n",
        "        meta = json.load(filename)\n",
        "        meta_dream(meta)\n",
        "      elif filename.rpartition('.')[2] == 'png':\n",
        "        img = PILImage.open(filename)\n",
        "        metadata = img.info\n",
        "        if display_image:\n",
        "          page.add_to_retrieve_output(Img(src=filename, gapless_playback=True))\n",
        "          #display(img)\n",
        "        if metadata is None or len(metadata) < 1:\n",
        "          alert_msg(page, 'Sorry, image has no exif data.')\n",
        "          return\n",
        "          #print(metadata)\n",
        "        else:\n",
        "          if metadata.get('config_json'):\n",
        "            json_txt = metadata['config_json']\n",
        "            #print(json_txt)\n",
        "            meta = json.loads(json_txt)\n",
        "            meta_dream(meta)\n",
        "          elif metadata.get('config'):\n",
        "            config = metadata['config']\n",
        "            meta = {}\n",
        "            key = \"\"\n",
        "            val = \"\"\n",
        "            if metadata.get('title'):\n",
        "              meta['prompt'] = metadata['title']\n",
        "            for col in config.split(':'):\n",
        "              #print(col.strip())\n",
        "              if ',' not in col:\n",
        "                key = col\n",
        "              else:\n",
        "                parts = col.rpartition(',')\n",
        "                val = parts[0].strip()\n",
        "                if bool(key) and bool(val):\n",
        "                  meta[key] = val\n",
        "                  val = ''\n",
        "                key = parts[2].strip()\n",
        "          elif metadata.get('prompt'):\n",
        "            json_txt = metadata['prompt']\n",
        "            #print(json_txt)\n",
        "            meta = json.loads(json_txt)\n",
        "            meta_comfy(meta)\n",
        "          elif metadata.get('parameters'):\n",
        "            json_txt = metadata['parameters']\n",
        "            #print(json_txt)\n",
        "            meta = json.loads(json_txt)\n",
        "            meta_auto111(meta)\n",
        "          else:\n",
        "            alert_msg(page, \"No AEIONic Diffusion Deluxe config metadata found inside image.\")\n",
        "\n",
        "def run_initfolder(page):\n",
        "    prompt_string = initfolder_prefs['prompt_string']\n",
        "    init_folder = initfolder_prefs['init_folder']\n",
        "    include_strength = initfolder_prefs['include_strength']\n",
        "    image_strength = initfolder_prefs['image_strength']\n",
        "    #init_image='/content/ pic.png', init_image_strength=0.4\n",
        "    if bool(prompt_string):\n",
        "      p_str = f'\"{prompt_string.strip()}\"'\n",
        "      skip_str = f', init_image_strength={image_strength}' if bool(include_strength) else ''\n",
        "      if os.path.isdir(init_folder):\n",
        "        arg = {}\n",
        "        #print(\"prompts = [\")\n",
        "        for f in os.listdir(init_folder):\n",
        "          init_path = os.path.join(init_folder, f)\n",
        "          if os.path.isdir(init_path): continue\n",
        "          if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            page.add_to_initfolder_output(Text(f'    Dream({p_str}, init_image=\"{init_path}\"{skip_str}),'))\n",
        "            if bool(initfolder_prefs['negative_prompt']):\n",
        "              arg['negative_prompt'] = initfolder_prefs['negative_prompt']\n",
        "            arg['init_image'] = init_path\n",
        "            if bool(include_strength):\n",
        "              arg['init_image_strength'] = image_strength\n",
        "            page.add_to_prompts(prompt_string, arg)\n",
        "        if not bool(status['installed_img2img']):\n",
        "          alert_msg(page, 'Make sure you Install the Image2Image module before running Stable Diffusion on prompts...')\n",
        "       # print(\"]\")\n",
        "      else:\n",
        "        alert_msg(page, 'The init_folder directory does not exist.')\n",
        "    else: alert_msg(page, 'Your prompt_string is empty. What do you want to apply to images?')\n",
        "\n",
        "def run_init_video(page):\n",
        "    prompt = init_video_prefs['prompt']\n",
        "    video_file = init_video_prefs['video_file']\n",
        "    file_prefix = init_video_prefs['file_prefix']\n",
        "    try:\n",
        "        start_time = float(init_video_prefs['start_time'])\n",
        "        end_time = float(init_video_prefs['end_time'])\n",
        "        fps = int(init_video_prefs['fps'])\n",
        "    except Exception:\n",
        "        alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "        return\n",
        "    max_size = init_video_prefs['max_size']\n",
        "    show_images = init_video_prefs['show_images']\n",
        "    output_dir = os.path.join(stable_dir, init_video_prefs['batch_folder_name'])\n",
        "    if not bool(prompt):\n",
        "        alert_msg(page, \"Provide a good prompt to apply to All Frames in List.\")\n",
        "        return\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.init_video_output, lines=lines)\n",
        "    page.init_video_output.controls.clear()\n",
        "    page.init_video_output.update()\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    if video_file.startswith(\"http\"):\n",
        "        page.add_to_init_video_output(Text(\"Downloading the Video File...\"))\n",
        "        local = download_file(video_file)\n",
        "        vname = local.rpartition(slash)\n",
        "        vout = os.path.join(root_dir, vname)\n",
        "        shutil.move(local, vout)\n",
        "        video_file = vout\n",
        "        clear_last()\n",
        "    if not bool(video_file):\n",
        "        alert_msg(page, \"Provide a valid Video File to Extract...\")\n",
        "        return\n",
        "    else:\n",
        "        if not os.path.exists(video_file):\n",
        "            alert_msg(page, \"The provided Video File doesn't Exist...\")\n",
        "            return\n",
        "\n",
        "    '''try:\n",
        "        import ffmpeg\n",
        "    except Exception:\n",
        "        run_process(\"pip install -q ffmpeg\", page=page)\n",
        "        import ffmpeg\n",
        "        pass\n",
        "    def convert_video_to_images(video_file, fps, start_time, end_time, output_dir):\n",
        "        \"\"\"\n",
        "        Convert a video file to a sequence of images.\n",
        "        \"\"\"\n",
        "        # Check if the output directory exists, otherwise create it.\n",
        "        command = ['ffmpeg',\n",
        "                '-i', video_file,\n",
        "                #'-r', str(fps),\n",
        "                #'-ss', str(start_time),\n",
        "                #'-t', str(end_time),\n",
        "                '-vf', f'fps={fps}'\n",
        "                #'-vf', \"select='between(t,2,6)+between(t,15,24)'\"\n",
        "                #'-qscale:v', '2',\n",
        "                '-vsync', \"0\"\n",
        "                '-f', 'image2',\n",
        "                os.path.join(output_dir, '%08d.png')]\n",
        "\n",
        "        run_process(command, page, print=True)\n",
        "    try:\n",
        "        import imageio\n",
        "    except Exception:\n",
        "        run_process(\"pip install -q imageio\", page=page)\n",
        "        import imageio\n",
        "        pass\n",
        "    def convert_video_to_images(video_file, fps, start_time, end_time, resolution):\n",
        "        # create video reader object\n",
        "        reader = imageio.get_reader(video_file)\n",
        "        # set the reader parameters\n",
        "        reader.set_fps(fps)\n",
        "        reader.set_start_time(start_time)\n",
        "        reader.set_end_time(end_time)\n",
        "        # get list of frames\n",
        "        frames = reader.get_meta_data()['nframes']\n",
        "        # loop through each frame and save as png with frame number in filename\n",
        "        for frame in range(frames):\n",
        "            frame_img = reader.get_data(frame)\n",
        "            frame_img = frame_img.resize(resolution)\n",
        "            imageio.imwrite(\"frame_{}.png\".format(frame), frame_img)\n",
        "        reader.close()'''\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    page.add_to_init_video_output(Installing(\"Processing Video File...\"))\n",
        "    page.add_to_init_video_output(progress)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "    except Exception as e:\n",
        "        alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "        clear_last()\n",
        "        return\n",
        "    count = 0\n",
        "    files = []\n",
        "    frames = []\n",
        "    w = h = 0\n",
        "    cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "    video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    start_frame = int(start_time * fps)\n",
        "    if end_time == 0 or end_time == 0.0:\n",
        "        end_frame = int(video_length)\n",
        "    else:\n",
        "        end_frame = int(end_time * fps)\n",
        "    total = end_frame - start_frame\n",
        "\n",
        "    #print(f\"Length: {video_length}, start_frame: {start_frame}, end_frame: {end_frame}\")\n",
        "    for i in range(start_frame, end_frame):\n",
        "        # Read frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        success, image = cap.read()\n",
        "\n",
        "        # Save frame as png\n",
        "        if success:\n",
        "            filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "            if w == 0:\n",
        "                shape = image.shape\n",
        "                w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                clear_last()\n",
        "                page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))\n",
        "            image = cv2.resize(image, (w, h), interpolation = cv2.INTER_AREA)\n",
        "            percent = count / total\n",
        "            progress.value = percent\n",
        "            progress.update()\n",
        "            #print(f\"{count / total}% - Saving {filename} - {shape}\")\n",
        "            cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "            files.append(filename)\n",
        "            if show_images:\n",
        "                page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                #page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))\n",
        "            count += 1\n",
        "    cap.release()\n",
        "    if show_images:\n",
        "        progress.value = 0.0\n",
        "        progress.update()\n",
        "    else:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "    '''\n",
        "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
        "    end_time = min(end_time, video_length / fps)\n",
        "    cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "    print(f\"Length: {video_length}, end_time: {end_time}\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
        "        # Break if past end time\n",
        "        if current_time > end_time and (end_time != 0.0 or end_time == 0):\n",
        "            break\n",
        "        frames.append(frame)\n",
        "        print(f\"{count} - ret:{ret}, time: {current_time}\")\n",
        "        count += 1\n",
        "    for i, frame in enumerate(frames):\n",
        "        filename = os.path.join(output_dir, f'{file_prefix}{i}.png')\n",
        "        if w == 0:\n",
        "            shape = frame.shape\n",
        "            w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "            clear_last()\n",
        "            page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))\n",
        "        frame = cv2.resize(frame, (w, h), interpolation = cv2.INTER_AREA)\n",
        "        cv2.imwrite(filename, frame)\n",
        "        files.append(filename)\n",
        "        if show_images:\n",
        "            page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))\n",
        "    cap.release()'''\n",
        "    if not bool(files):\n",
        "        alert_msg(page, \"ERROR Creating Images from Video...\")\n",
        "        return\n",
        "\n",
        "    for f in files:\n",
        "        arg = {}\n",
        "        if bool(init_video_prefs['negative_prompt']):\n",
        "            arg['negative_prompt'] = init_video_prefs['negative_prompt']\n",
        "        arg['init_image'] = f\n",
        "        arg['width'] = w\n",
        "        arg['height'] = h\n",
        "        if bool(init_video_prefs['include_strength']):\n",
        "            arg['init_image_strength'] = init_video_prefs['image_strength']\n",
        "        page.add_to_prompts(prompt, arg)\n",
        "    page.add_to_init_video_output(Text(f'Added {len(files)} Files as Init-Image in Prompts List...', weight=FontWeight.BOLD))\n",
        "    page.add_to_init_video_output(Text(f'Saved to {output_dir}'))\n",
        "    if prefs['enable_sounds']: page.snd_drop.play()\n",
        "\n",
        "def multiple_of_64(x):\n",
        "    return multiple_of(x, 64)\n",
        "def multiple_of_8(x):\n",
        "    return multiple_of(x, 8)\n",
        "def multiple_of(x, num):\n",
        "    return int(round(x/num)*num)\n",
        "def scale_dimensions(width, height, max=1024, multiple=16):\n",
        "  max = int(max)\n",
        "  r_width = width\n",
        "  r_height = height\n",
        "  if width < max and height < max:\n",
        "    if width >= height:\n",
        "      ratio = max / width\n",
        "      r_width = max\n",
        "      r_height = int(height * ratio)\n",
        "    else:\n",
        "      ratio = max / height\n",
        "      r_height = max\n",
        "      r_width = int(width * ratio)\n",
        "    width = r_width\n",
        "    height = r_height\n",
        "  if width >= height:\n",
        "    if width > max:\n",
        "      r_width = max\n",
        "      r_height = int(height * (max/width))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  else:\n",
        "    if height > max:\n",
        "      r_height = max\n",
        "      r_width = int(width * (max/height))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  return multiple_of(r_width, multiple), multiple_of(r_height, multiple)\n",
        "\n",
        "def run_repainter(page):\n",
        "    global repaint_prefs, prefs, status, pipe_repaint\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(repaint_prefs['original_image']) or not bool(repaint_prefs['mask_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.RePainter.controls.append(line)\n",
        "      page.RePainter.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.RePainter, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.RePainter.auto_scroll = scroll\n",
        "      page.RePainter.update()\n",
        "    def clear_list():\n",
        "      page.RePainter.controls = page.RePainter.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing RePaint Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    if repaint_prefs['original_image'].startswith('http'):\n",
        "      #response = requests.get(repaint_prefs['original_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(repaint_prefs['original_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(repaint_prefs['original_image']):\n",
        "        original_img = PILImage.open(repaint_prefs['original_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your original_image {repaint_prefs['original_image']}\")\n",
        "        return\n",
        "    width, height = original_img.size\n",
        "    width, height = scale_dimensions(width, height, repaint_prefs['max_size'])\n",
        "    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    original_img = ImageOps.exif_transpose(original_img).convert(\"RGB\")\n",
        "    mask_img = None\n",
        "    if repaint_prefs['mask_image'].startswith('http'):\n",
        "      #response = requests.get(repaint_prefs['mask_image'])\n",
        "      #mask_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      mask_img = PILImage.open(requests.get(repaint_prefs['mask_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(repaint_prefs['mask_image']):\n",
        "        mask_img = PILImage.open(repaint_prefs['mask_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your mask_image {repaint_prefs['mask_image']}\")\n",
        "        return\n",
        "    #mask_img = mask_img.convert(\"L\")\n",
        "    #mask_img = mask_img.convert(\"1\")\n",
        "    if repaint_prefs['invert_mask']:\n",
        "       mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)\n",
        "    mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    clear_pipes('repaint')\n",
        "    if not status['installed_repaint']:\n",
        "      get_repaint(page)\n",
        "      status['installed_repaint'] = True\n",
        "    if pipe_repaint is None:\n",
        "      pipe_repaint = get_repaint_pipe()\n",
        "    clear_last()\n",
        "    prt(\"Generating Repaint of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(repaint_prefs['seed']) if int(repaint_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "#Sizes of tensors must match except in dimension 1. Expected size 58 but got size 59 for tensor number 1 in the list.\n",
        "    try:\n",
        "      #from IPython.utils.capture import capture_output\n",
        "      #with capture_output() as captured:\n",
        "      image = pipe_repaint(image=original_img, mask_image=mask_img, num_inference_steps=repaint_prefs['num_inference_steps'], eta=repaint_prefs['eta'], jump_length=repaint_prefs['jump_length'], jump_n_sample=repaint_prefs['jump_length'], generator=generator, callback=callback_fnc, callback_steps=1).images[0]\n",
        "      #print(str(captured.stdout))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Repaint your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "      return\n",
        "    fname = repaint_prefs['original_image'].rpartition('.')[0]\n",
        "    fname = fname.rpartition(slash)[2]\n",
        "    if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    image_path = available_file(stable_dir, fname, 1)\n",
        "    image.save(image_path)\n",
        "    out_path = image_path\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    #TODO: ESRGAN, Metadata & PyDrive\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      new_file = available_file(prefs['image_output'], fname, 1)\n",
        "      out_path = new_file\n",
        "      shutil.copy(image_path, new_file)\n",
        "    elif bool(prefs['image_output']):\n",
        "      new_file = available_file(prefs['image_output'], fname, 1)\n",
        "      out_path = new_file\n",
        "      shutil.copy(image_path, new_file)\n",
        "    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_image_variation(page):\n",
        "    global image_variation_prefs, pipe_image_variation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.ImageVariation.controls.append(line)\n",
        "      page.ImageVariation.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.ImageVariation, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.ImageVariation.auto_scroll = scroll\n",
        "      page.ImageVariation.update()\n",
        "    def clear_list():\n",
        "      page.ImageVariation.controls = page.ImageVariation.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = image_variation_prefs['num_inference_steps']#len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    if image_variation_prefs['init_image'].startswith('http'):\n",
        "      init_img = PILImage.open(requests.get(image_variation_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(image_variation_prefs['init_image']):\n",
        "        init_img = PILImage.open(image_variation_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {image_variation_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = init_img.size\n",
        "    width, height = scale_dimensions(width, height, image_variation_prefs['max_size'])\n",
        "    tform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(\n",
        "            (width, height),\n",
        "            interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "            antialias=False,\n",
        "            ),\n",
        "        transforms.Normalize(\n",
        "          [0.48145466, 0.4578275, 0.40821073],\n",
        "          [0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "    init_img = tform(init_img).to(torch_device)\n",
        "    #init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    #init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    clear_pipes('image_variation')\n",
        "    if pipe_image_variation == None:\n",
        "        from diffusers import StableDiffusionImageVariationPipeline\n",
        "        prt(Installing(\"Downloading Image Variation Pipeline\"))\n",
        "        model_id = \"fusing/sd-image-variations-diffusers\"\n",
        "        pipe_image_variation = StableDiffusionImageVariationPipeline.from_pretrained(model_id, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_image_variation.to(torch_device)\n",
        "        pipe_image_variation = pipeline_scheduler(pipe_image_variation)\n",
        "        pipe_image_variation = optimize_pipe(pipe_image_variation)\n",
        "        #pipe_image_variation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if image_variation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating Variation{s} of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(image_variation_prefs['seed']) if int(image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "\n",
        "    try:\n",
        "        images = pipe_image_variation(image=init_img, height=height, width=width, num_inference_steps=image_variation_prefs['num_inference_steps'], guidance_scale=image_variation_prefs['guidance_scale'], eta=image_variation_prefs['eta'], num_images_per_prompt=image_variation_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running pipeline\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    fname = image_variation_prefs['init_image'].rpartition('.')[0]\n",
        "    fname = fname.rpartition(slash)[2]\n",
        "    if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    for image in images:\n",
        "        image_path = available_file(stable_dir, fname, 1)\n",
        "        image.save(image_path)\n",
        "        out_path = image_path\n",
        "        #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #TODO: ESRGAN, Metadata & PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(prefs['image_output'], fname, 1)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(prefs['image_output'], fname, 1)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    page.ImageVariation.auto_scroll = False\n",
        "    page.ImageVariation.update()\n",
        "    autoscroll(True)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_background_remover(page):\n",
        "    global background_remover_prefs, pipe_background_remover\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.BackgroundRemover.controls.append(line)\n",
        "      page.BackgroundRemover.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.BackgroundRemover, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.BackgroundRemover.auto_scroll = scroll\n",
        "      page.BackgroundRemover.update()\n",
        "    def clear_list():\n",
        "      page.BackgroundRemover.controls = page.BackgroundRemover.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = background_remover_prefs['num_inference_steps']#len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    installer = Installing(\"Downloading Background Remover Pipeline\")\n",
        "    prt(installer)\n",
        "    try:\n",
        "        from huggingface_hub import hf_hub_download\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...HuggingFace Hub\")\n",
        "        run_process(\"pip install --upgrade huggingface_hub\", page=page)\n",
        "        from huggingface_hub import hf_hub_download\n",
        "        pass\n",
        "    try:\n",
        "        import onnx\n",
        "    except ImportError as e:\n",
        "        installer.status(\"...installing onnx\")\n",
        "        run_sp(\"pip install -q onnx==1.14.0\", realtime=False)\n",
        "        pass\n",
        "    try:\n",
        "        import onnxruntime\n",
        "    except ImportError as e:\n",
        "        installer.status(\"...installing onnxruntime\")\n",
        "        run_sp(\"pip install -q onnxruntime-gpu==1.15.0\", realtime=False)\n",
        "        pass\n",
        "    if pipe_background_remover == None:\n",
        "        installer.status(\"...downloading model\")\n",
        "        pipe_background_remover = hf_hub_download('nateraw/background-remover-files', 'modnet.onnx', repo_type='dataset')\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    import numpy as np\n",
        "    import onnxruntime\n",
        "    import requests\n",
        "    def get_scale_factor(im_h, im_w, ref_size=512):\n",
        "        if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:\n",
        "            if im_w >= im_h:\n",
        "                im_rh = ref_size\n",
        "                im_rw = int(im_w / im_h * ref_size)\n",
        "            elif im_w < im_h:\n",
        "                im_rw = ref_size\n",
        "                im_rh = int(im_h / im_w * ref_size)\n",
        "        else:\n",
        "            im_rh = im_h\n",
        "            im_rw = im_w\n",
        "        im_rw = im_rw - im_rw % 32\n",
        "        im_rh = im_rh - im_rh % 32\n",
        "        x_scale_factor = im_rw / im_w\n",
        "        y_scale_factor = im_rh / im_h\n",
        "        return x_scale_factor, y_scale_factor\n",
        "    if background_remover_prefs['init_image'].startswith('http'):\n",
        "        installer.status(\"...downloading image\")\n",
        "        init_file = background_remover_prefs['init_image'].rpartition(\"/\")[2].rpartition(\".\")[0]\n",
        "        init_img = PILImage.open(requests.get(background_remover_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "        if os.path.isfile(background_remover_prefs['init_image']):\n",
        "            init_file = background_remover_prefs['init_image'].rpartition(slash)[2].rpartition(\".\")[0]\n",
        "            init_img = PILImage.open(background_remover_prefs['init_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {background_remover_prefs['init_image']}\")\n",
        "            return\n",
        "    if bool(background_remover_prefs['output_name']):\n",
        "        init_file = format_filename(background_remover_prefs['output_name'], force_underscore=True)\n",
        "    batch_output = os.path.join(stable_dir, background_remover_prefs['batch_folder_name'])\n",
        "    init_path = os.path.join(batch_output, f\"{init_file}.jpg\")\n",
        "    image_path = available_file(batch_output, init_file, 1)\n",
        "    mask_path = available_file(batch_output, f\"{init_file}-mask.png\", 1)\n",
        "    mask_file = available_file(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']), f\"{init_file}-mask.png\", 1)\n",
        "    if not os.path.isdir(batch_output):\n",
        "        os.makedirs(batch_output)\n",
        "    if not os.path.isdir(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name'])):\n",
        "        os.makedirs(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']))\n",
        "    init_img.save(init_path)\n",
        "    clear_pipes('background_remover')\n",
        "    clear_last()\n",
        "    prt(f\"Generating Foreground of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    im = cv2.imread(init_path)\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    if len(im.shape) == 2:\n",
        "        im = im[:, :, None]\n",
        "    if im.shape[2] == 1:\n",
        "        im = np.repeat(im, 3, axis=2)\n",
        "    elif im.shape[2] == 4:\n",
        "        im = im[:, :, 0:3]\n",
        "    im = (im - 127.5) / 127.5\n",
        "    im_h, im_w, im_c = im.shape\n",
        "    x, y = get_scale_factor(im_h, im_w)\n",
        "    im = cv2.resize(im, None, fx=x, fy=y, interpolation=cv2.INTER_AREA)\n",
        "    im = np.transpose(im)\n",
        "    im = np.swapaxes(im, 1, 2)\n",
        "    im = np.expand_dims(im, axis=0).astype('float32')\n",
        "    session = onnxruntime.InferenceSession(pipe_background_remover, None, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    output_name = session.get_outputs()[0].name\n",
        "    result = session.run([output_name], {input_name: im})\n",
        "    matte = (np.squeeze(result[0]) * 255).astype('uint8')\n",
        "    matte = cv2.resize(matte, dsize=(im_w, im_h), interpolation=cv2.INTER_AREA)\n",
        "    cv2.imwrite(mask_path, matte)\n",
        "    if background_remover_prefs['save_mask']:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        prt(Row([ImageButton(src=mask_path, width=im_w, height=im_h, data=mask_file, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    image = PILImage.open(init_path)\n",
        "    matte = PILImage.open(mask_path)\n",
        "    image = np.asarray(image)\n",
        "    if len(image.shape) == 2:\n",
        "        image = image[:, :, None]\n",
        "    if image.shape[2] == 1:\n",
        "        image = np.repeat(image, 3, axis=2)\n",
        "    elif image.shape[2] == 4:\n",
        "        image = image[:, :, 0:3]\n",
        "    b, g, r = cv2.split(image)\n",
        "    mask = np.asarray(matte)\n",
        "    a = np.ones(mask.shape, dtype='uint8') * 255\n",
        "    alpha_im = cv2.merge([b, g, r, a], 4)\n",
        "    bg = np.zeros(alpha_im.shape)\n",
        "    new_mask = np.stack([mask, mask, mask, mask], axis=2)\n",
        "    foreground = np.where(new_mask > background_remover_prefs['threshold'], alpha_im, bg).astype(np.uint8)\n",
        "    img = PILImage.fromarray(foreground)\n",
        "    width, height = img.size\n",
        "    if not background_remover_prefs['save_mask']:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "    autoscroll(True)\n",
        "    img.save(image_path)\n",
        "    out_path = image_path\n",
        "    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    #TODO: ESRGAN, Metadata & PyDrive\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "        new_file = available_file(prefs['image_output'], init_file, 1)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "        if background_remover_prefs['save_mask']:\n",
        "            shutil.copy(mask_path, mask_file)\n",
        "    elif bool(prefs['image_output']):\n",
        "        new_file = available_file(prefs['image_output'], init_file, 1)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "        if background_remover_prefs['save_mask']:\n",
        "            shutil.copy(mask_path, mask_file)\n",
        "    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    page.BackgroundRemover.auto_scroll = False\n",
        "    page.BackgroundRemover.update()\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "loaded_blip_diffusion_task = \"\"\n",
        "def run_blip_diffusion(page, from_list=False, with_params=False):\n",
        "    global blip_diffusion_prefs, pipe_blip_diffusion, prefs, loaded_blip_diffusion_task\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if int(status['cpu_memory']) <= 10:\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run BLIP Diffusion right now. Either Change runtime type to High-RAM mode and restart or use other BLIP Diffusion 2.1 in Extras.\")\n",
        "      return\n",
        "    blip_diffusion_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            for x in range(blip_diffusion_prefs['num_images']):\n",
        "                seed = 0 if blip_diffusion_prefs['seed'] <= 0 else blip_diffusion_prefs['seed'] + x\n",
        "                blip_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':blip_diffusion_prefs['init_image'], 'guidance_scale':blip_diffusion_prefs['guidance_scale'], 'steps':blip_diffusion_prefs['steps'], 'width':blip_diffusion_prefs['width'], 'height':blip_diffusion_prefs['height'], 'strength':blip_diffusion_prefs['strength'], 'num_images':blip_diffusion_prefs['num_images'], 'seed':seed})\n",
        "        else:\n",
        "            for x in range(p['num_images']):\n",
        "                seed = 0 if p['seed'] <= 0 else p['seed'] + x\n",
        "                blip_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':seed})\n",
        "    else:\n",
        "      if not bool(blip_diffusion_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      for x in range(blip_diffusion_prefs['num_images']):\n",
        "          seed = 0 if blip_diffusion_prefs['seed'] <= 0 else blip_diffusion_prefs['seed'] + x\n",
        "          blip_diffusion_prompts.append({'prompt': blip_diffusion_prefs['prompt'], 'negative_prompt':blip_diffusion_prefs['negative_prompt'], 'init_image':blip_diffusion_prefs['init_image'], 'guidance_scale':blip_diffusion_prefs['guidance_scale'], 'steps':blip_diffusion_prefs['steps'], 'width':blip_diffusion_prefs['width'], 'height':blip_diffusion_prefs['height'], 'strength':blip_diffusion_prefs['strength'], 'num_images':blip_diffusion_prefs['num_images'], 'seed':seed})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.BLIPDiffusion.controls.append(line)\n",
        "        if update:\n",
        "          page.BLIPDiffusion.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.BLIPDiffusion, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.BLIPDiffusion.auto_scroll = scroll\n",
        "        page.BLIPDiffusion.update()\n",
        "      else:\n",
        "        page.BLIPDiffusion.auto_scroll = scroll\n",
        "        page.BLIPDiffusion.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.BLIPDiffusion.controls = page.BLIPDiffusion.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = blip_diffusion_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing BLIP-Diffusion Engine & Model... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"blip_diffusion\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = False\n",
        "    use_controlnet = blip_diffusion_prefs['controlnet_type'] != \"None\"\n",
        "    if use_controlnet:\n",
        "        try:\n",
        "          from controlnet_aux import CannyDetector\n",
        "        except ModuleNotFoundError:\n",
        "          installer.status(f\"...installing controlnet-aux\")\n",
        "          run_sp(\"pip install --upgrade controlnet-aux\", realtime=False)\n",
        "          pass\n",
        "    task_type = \"controlnet\" if use_controlnet else \"text2img\"\n",
        "    if loaded_blip_diffusion_task != task_type:\n",
        "        clear_pipes()\n",
        "    installer.status(f\"...blip_diffusion {task_type} Pipeline\")\n",
        "    model_id = \"Salesforce/blipdiffusion\"\n",
        "    if pipe_blip_diffusion == None or loaded_blip_diffusion_task != task_type:\n",
        "        try:\n",
        "            if task_type == \"text2img\":\n",
        "                from diffusers.pipelines import BlipDiffusionPipeline\n",
        "                model_id = \"Salesforce/blipdiffusion\"\n",
        "                pipe_blip_diffusion = BlipDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            elif task_type == \"controlnet_canny\":\n",
        "                from diffusers.pipelines import BlipDiffusionControlNetPipeline\n",
        "                from controlnet_aux import CannyDetector\n",
        "                model_id = \"Salesforce/blipdiffusion-controlnet\"\n",
        "                pipe_blip_diffusion = BlipDiffusionControlNetPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling unet\")\n",
        "                pipe_blip_diffusion.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_blip_diffusion.unet = torch.compile(pipe_blip_diffusion.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_blip_diffusion = pipe_blip_diffusion.to(\"cuda\")\n",
        "            elif cpu_offload:\n",
        "                pipe_blip_diffusion.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_blip_diffusion.to(\"cuda\")\n",
        "            loaded_blip_diffusion_task = task_type\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing BLIP Diffusion, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('blip_diffusion')\n",
        "    cldm_cond_image = None\n",
        "    if use_controlnet:\n",
        "        if not bool(blip_diffusion_prefs['control_image']):\n",
        "            alert_msg(page, f\"ERROR: You must provide a ControlNet Image.\")\n",
        "            return\n",
        "        fname = blip_diffusion_prefs['control_image'].rpartition(slash)[2]\n",
        "        if blip_diffusion_prefs['control_image'].startswith('http'):\n",
        "            installer.status(f\"...download control_image\")\n",
        "            control_image = PILImage.open(requests.get(blip_diffusion_prefs['control_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(blip_diffusion_prefs['control_image']):\n",
        "                control_image = PILImage.open(blip_diffusion_prefs['control_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your control_image {pr['control_image']}\")\n",
        "                return\n",
        "        control_image = control_image.resize((blip_diffusion_prefs['width'], blip_diffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "        control_image = ImageOps.exif_transpose(control_image).convert(\"RGB\")\n",
        "        if blip_diffusion_prefs['controlnet_type'] == \"Canny Edge\":\n",
        "            installer.status(f\"...running Canny Edge Detector\")\n",
        "            from controlnet_aux import CannyDetector\n",
        "            canny = CannyDetector()\n",
        "            cldm_cond_image = canny(control_image, 30, 70, output_type=\"pil\")\n",
        "        elif blip_diffusion_prefs['controlnet_type'] == \"HED\":\n",
        "            installer.status(f\"...running HED Edge Detector\")\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            cldm_cond_image = hed(control_image)\n",
        "    clear_last()\n",
        "    for pr in blip_diffusion_prompts:\n",
        "        prt(\"Generating your BLIP-Diffusion Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        init_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        total_steps = pr['steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        cond_subject = [x.strip() for x in blip_diffusion_prefs['source_subject_category'].split(',')]\n",
        "        tgt_subject = [x.strip() for x in blip_diffusion_prefs['target_subject_category'].split(',')]\n",
        "        try:\n",
        "            if task_type == \"text2img\":\n",
        "                images = pipe_blip_diffusion(\n",
        "                    prompt=pr['prompt'], neg_prompt=pr['negative_prompt'],\n",
        "                    source_subject_category=cond_subject, target_subject_category=tgt_subject,\n",
        "                    reference_image=init_img,\n",
        "                    #num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    prompt_strength=pr['strength'],\n",
        "                    prompt_reps=blip_diffusion_prefs['prompt_reps'],\n",
        "                    generator=generator,\n",
        "                    callback=callback_fnc,\n",
        "                ).images\n",
        "            elif task_type == \"controlnet\":\n",
        "                images = pipe_blip_diffusion(\n",
        "                    prompt=pr['prompt'], neg_prompt=pr['negative_prompt'],\n",
        "                    source_subject_category=cond_subject, target_subject_category=tgt_subject,\n",
        "                    reference_image=init_img,\n",
        "                    conditioning_image=cldm_cond_image,\n",
        "                    #num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    prompt_strength=pr['strength'],\n",
        "                    prompt_reps=blip_diffusion_prefs['prompt_reps'],\n",
        "                    generator=generator,\n",
        "                    callback=callback_fnc,\n",
        "                ).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating {task_type} images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(blip_diffusion_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, blip_diffusion_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{blip_diffusion_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not blip_diffusion_prefs['display_upscaled_image'] or not blip_diffusion_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #if save_to_GDrive:\n",
        "            batch_output = os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': blip_diffusion_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            #out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if blip_diffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=blip_diffusion_prefs[\"enlarge_scale\"], face_enhance=blip_diffusion_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                if blip_diffusion_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([Img(src=upscaled_path, width=pr['width'] * float(blip_diffusion_prefs[\"enlarge_scale\"]), height=pr['height'] * float(blip_diffusion_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {blip_diffusion_prefs['enlarge_scale']}x with ESRGAN\" if blip_diffusion_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"BLIP-Diffusion {task_type} {'' if blip_diffusion_prefs['controlnet_type'] == 'None' else blip_diffusion_prefs['controlnet_type']}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    #metadata.add_text(\"title\", blip_diffusion_prefs['file_name'])\n",
        "                    # TODO: Merge Metadata with pr[]\n",
        "                    config_json = blip_diffusion_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_anytext(page, from_list=False, with_params=False):\n",
        "    global anytext_prefs, pipe_anytext, prefs, status\n",
        "    anytext_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            anytext_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':anytext_prefs['guidance_scale'], 'num_inference_steps':anytext_prefs['num_inference_steps'], 'width':anytext_prefs['width'], 'height':anytext_prefs['height'], 'init_image':anytext_prefs['init_image'], 'mask_image':anytext_prefs['mask_image'], 'init_image_strength':anytext_prefs['init_image_strength'], 'num_images':anytext_prefs['num_images'], 'seed':anytext_prefs['seed']})\n",
        "        else:\n",
        "            anytext_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(anytext_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      anytext_prompts.append({'prompt': anytext_prefs['prompt'], 'negative_prompt':anytext_prefs['negative_prompt'], 'guidance_scale':anytext_prefs['guidance_scale'], 'num_inference_steps':anytext_prefs['num_inference_steps'], 'width':anytext_prefs['width'], 'height':anytext_prefs['height'], 'init_image':anytext_prefs['init_image'], 'mask_image':anytext_prefs['mask_image'], 'init_image_strength':anytext_prefs['init_image_strength'], 'num_images':anytext_prefs['num_images'], 'seed':anytext_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.AnyText.controls.append(line)\n",
        "        if update:\n",
        "          page.AnyText.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.AnyText, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.AnyText.auto_scroll = scroll\n",
        "        page.AnyText.update()\n",
        "      else:\n",
        "        page.AnyText.auto_scroll = scroll\n",
        "        page.AnyText.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.AnyText.controls = page.AnyText.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = anytext_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing AnyText Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"anytext\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    anytext_dir = os.path.join(root_dir, \"AnyText\")\n",
        "    anytext_ttf = os.path.join(anytext_dir, 'font', 'horison.ttf')\n",
        "    ttf = os.path.basename(anytext_ttf)\n",
        "    if not os.path.exists(anytext_dir):\n",
        "        installer.status(f\"...cloning tyxsspa/AnyText.git\")\n",
        "        run_sp(\"git clone https://github.com/tyxsspa/AnyText.git\", cwd=root_dir)\n",
        "    sys.path.append(anytext_dir)\n",
        "    if bool(anytext_prefs['font_ttf']):\n",
        "        if '/' in anytext_prefs['font_ttf']:\n",
        "            ttf = anytext_prefs['font_ttf'].rparition('/')[2]\n",
        "        elif '\\\\' in anytext_prefs['font_ttf']:\n",
        "            ttf = anytext_prefs['font_ttf'].rparition('\\\\')[2]\n",
        "        if os.path.isfile(os.path.join(anytext_dir, 'font', ttf)):\n",
        "            anytext_ttf = os.path.join(anytext_dir, 'font', ttf)\n",
        "        else:\n",
        "            if anytext_prefs['font_ttf'].startswith(\"http\"):\n",
        "                ttf_path = download_file(anytext_prefs['font_ttf'], to=os.path.join(anytext_dir, \"fonts\"), ext=\"ttf\")\n",
        "            else:\n",
        "                ttf_path = os.path.join(anytext_prefs['font_ttf'])\n",
        "                if os.path.isfile(ttf_path):\n",
        "                    shutil.copy(ttf_path, os.path.join(anytext_dir, 'font', ttf))\n",
        "                else:\n",
        "                    prt(\"Font Path not found...\")\n",
        "                    return\n",
        "            ttf = os.path.basename(ttf_path)\n",
        "            anytext_ttf = os.path.join(anytext_dir, 'font', ttf)\n",
        "    else:\n",
        "        anytext_ttf = os.path.join(anytext_dir, 'font', 'horison.ttf')\n",
        "        if not os.path.isfile(anytext_ttf):\n",
        "            installer.status(f\"...downloading horison.ttf\")\n",
        "            run_sp(f\"wget https://dl.dafont.com/dl/?f=horison -O {os.path.join(anytext_dir, 'font', 'horison.zip')}\")\n",
        "            run_sp(f\"unzip {os.path.join(anytext_dir, 'font', 'horison.zip')}\", cwd=os.path.join(anytext_dir, 'font'))\n",
        "            os.remove(os.path.join(anytext_dir, 'font', 'horison.zip'))\n",
        "    pip_install(\"modelscope omegaconf pytorch-lightning sentencepiece easydict open-clip-torch|open_clip scikit-image|skimage sacremoses subword_nmt jieba tensorflow fsspec\", installer=installer)\n",
        "    os.chdir(anytext_dir)\n",
        "    try:\n",
        "        import xformers\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing FaceBook's Xformers\")\n",
        "        run_sp(\"pip install -U xformers\", realtime=False)\n",
        "        status['installed_xformers'] = True\n",
        "        pass\n",
        "    from modelscope.pipelines import pipeline\n",
        "    import cv2, re\n",
        "    import numpy as np\n",
        "    def count_lines(prompt):\n",
        "        prompt = prompt.replace('‚Äú', '\"')\n",
        "        prompt = prompt.replace('‚Äù', '\"')\n",
        "        p = '\"(.*?)\"'\n",
        "        strs = re.findall(p, prompt)\n",
        "        if len(strs) == 0:\n",
        "            strs = [' ']\n",
        "        return len(strs)\n",
        "    def generate_rectangles(w, h, n, max_trys=200):\n",
        "        img = np.zeros((h, w, 1), dtype=np.uint8)\n",
        "        rectangles = []\n",
        "        attempts = 0\n",
        "        n_pass = 0\n",
        "        low_edge = int(max(w, h)*0.3 if n <= 3 else max(w, h)*0.2)  # ~150, ~100\n",
        "        while attempts < max_trys:\n",
        "            rect_w = min(np.random.randint(max((w*0.5)//n, low_edge), w), int(w*0.8))\n",
        "            ratio = np.random.uniform(4, 10)\n",
        "            rect_h = max(low_edge, int(rect_w/ratio))\n",
        "            rect_h = min(rect_h, int(h*0.8))\n",
        "            # gen rotate angle\n",
        "            rotation_angle = 0\n",
        "            rand_value = np.random.rand()\n",
        "            if rand_value < 0.7:\n",
        "                pass\n",
        "            elif rand_value < 0.8:\n",
        "                rotation_angle = np.random.randint(0, 40)\n",
        "            elif rand_value < 0.9:\n",
        "                rotation_angle = np.random.randint(140, 180)\n",
        "            else:\n",
        "                rotation_angle = np.random.randint(85, 95)\n",
        "            x = np.random.randint(0, w - rect_w)\n",
        "            y = np.random.randint(0, h - rect_h)\n",
        "            rect_pts = cv2.boxPoints(((rect_w/2, rect_h/2), (rect_w, rect_h), rotation_angle))\n",
        "            rect_pts = np.int32(rect_pts)\n",
        "            rect_pts += (x, y)\n",
        "            if np.any(rect_pts < 0) or np.any(rect_pts[:, 0] >= w) or np.any(rect_pts[:, 1] >= h):\n",
        "                attempts += 1\n",
        "                continue\n",
        "            if any(check_overlap_polygon(rect_pts, rp) for rp in rectangles):\n",
        "                attempts += 1\n",
        "                continue\n",
        "            n_pass += 1\n",
        "            cv2.fillPoly(img, [rect_pts], 255)\n",
        "            rectangles.append(rect_pts)\n",
        "            if n_pass == n:\n",
        "                break\n",
        "        print(\"attempts:\", attempts)\n",
        "        if len(rectangles) != n:\n",
        "            prt(f'Failed in auto generate positions after {attempts} attempts, try again!')\n",
        "        return img\n",
        "    def check_overlap_polygon(rect_pts1, rect_pts2):\n",
        "        poly1 = cv2.convexHull(rect_pts1)\n",
        "        poly2 = cv2.convexHull(rect_pts2)\n",
        "        rect1 = cv2.boundingRect(poly1)\n",
        "        rect2 = cv2.boundingRect(poly2)\n",
        "        if rect1[0] + rect1[2] >= rect2[0] and rect2[0] + rect2[2] >= rect1[0] and rect1[1] + rect1[3] >= rect2[1] and rect2[1] + rect2[3] >= rect1[1]:\n",
        "            return True\n",
        "        return False\n",
        "    anytext_model = 'damo/cv_anytext_text_generation_editing'\n",
        "    if pipe_anytext == None:\n",
        "        installer.status(f\"...initialize AnyText Pipeline\")\n",
        "        try:\n",
        "            pipe_anytext = pipeline('my-anytext-task', model=anytext_model, model_revision='v1.1.1', use_fp16=not prefs['higher_vram_mode'], use_translator=False, font_path= f'font/{ttf}')\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing AnyText...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    s = \"\" if len(anytext_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your AnyText Image{s}...\")\n",
        "    for pr in anytext_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        n_lines = count_lines(pr['prompt'])\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        params = {\n",
        "            \"sort_priority\": anytext_prefs['sort_priority'],\n",
        "            \"show_debug\": True,\n",
        "            \"revise_pos\": anytext_prefs['revise_pos'],\n",
        "            \"image_count\": pr['num_images'],\n",
        "            \"ddim_steps\": pr['num_inference_steps'],\n",
        "            \"image_width\": pr['width'],\n",
        "            \"image_height\": pr['height'],\n",
        "            \"strength\": pr['strength'],\n",
        "            \"cfg_scale\": pr['guidance_scale'],\n",
        "            \"a_prompt\": pr['a_prompt'],\n",
        "            \"n_prompt\": pr['negative_prompt']\n",
        "        }\n",
        "        init_img = None\n",
        "        mask_img = None\n",
        "        width, height = (pr['width'], pr['height'])\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            max_size = max(pr['width'], pr['height'])\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, max_size, multiple=64)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            init_img = np.array(init_img)\n",
        "        if bool(pr['mask_image']):\n",
        "            fname = pr['mask_image'].rpartition(slash)[2]\n",
        "            if pr['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['mask_image']):\n",
        "                    mask_img = PILImage.open(pr['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                    return\n",
        "            max_size = max(pr['width'], pr['height'])\n",
        "            width, height = mask_img.size\n",
        "            width, height = scale_dimensions(width, height, max_size, multiple=64)\n",
        "            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            mask_img = ImageOps.exif_transpose(mask_img).convert(\"L\")\n",
        "            alpha_channel = PILImage.new(\"L\", init_img.size, 0)\n",
        "            alpha_channel.paste(mask_img, (0, 0))\n",
        "            black_areas = PILImage.new(\"L\", mask_img.size, 0)\n",
        "            black_areas.paste(mask_img, (0, 0))\n",
        "            inverted_mask = PILImage.eval(black_areas, lambda x: 255 - x)\n",
        "            result = PILImage.alpha_composite(init_img, PILImage.new(\"RGBA\", init_img.size, (0, 0, 0, 255)))\n",
        "            mask_img = result.paste((0, 0, 0, 0), (0, 0), inverted_mask)\n",
        "            mask_img = np.array(mask_img)\n",
        "        else:\n",
        "            mask_img = generate_rectangles(width, height, n_lines, max_trys=500)\n",
        "            cv2.imwrite(os.path.join(anytext_dir, 'pos_imgs.png'), 255-mask_img[..., ::-1])\n",
        "        input_data = {\n",
        "            \"prompt\": pr['prompt'],\n",
        "            \"seed\": random_seed,\n",
        "            \"draw_pos\": mask_img,\n",
        "        }\n",
        "        if bool(mask_img) and bool(init_img):\n",
        "            mode = 'text-editing'\n",
        "            input_data['ori_image'] = init_img\n",
        "            input_data['draw_pos'] = mask_img\n",
        "        elif bool(init_img):\n",
        "            mode = 'text-editing'\n",
        "            input_data['ori_image'] = init_img\n",
        "        else:\n",
        "            mode = 'text-generation'\n",
        "        try:\n",
        "            images, rtn_code, rtn_warning, debug_info = pipe_anytext(input_data, mode=mode, **params)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            os.chdir(root_dir)\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        if rtn_code == 0:\n",
        "            alert_msg(page, f\"ERROR: {rtn_warning}\", content=Text(str(debug_info)))\n",
        "            os.chdir(root_dir)\n",
        "            return\n",
        "        if rtn_warning:\n",
        "            alert_msg(page, f\"WARNING: {rtn_warning}\", content=Text(str(debug_info)))\n",
        "            os.chdir(root_dir)\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(anytext_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, anytext_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            os.chdir(root_dir)\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{anytext_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            cv2.imwrite(image_path, image[..., ::-1])\n",
        "            #image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not anytext_prefs['display_upscaled_image'] or not anytext_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': anytext_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if anytext_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=anytext_prefs[\"enlarge_scale\"], face_enhance=anytext_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if anytext_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(anytext_prefs[\"enlarge_scale\"]), height=pr['height'] * float(anytext_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {anytext_prefs['enlarge_scale']}x with ESRGAN\" if anytext_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"AnyText {mode}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = anytext_prefs.copy()\n",
        "                    config_json['model_path'] = anytext_model\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    os.chdir(root_dir)\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_ip_adapter(page, from_list=False, with_params=False):\n",
        "    global ip_adapter_prefs, pipe_ip_adapter, prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    ip_adapter_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            ip_adapter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':ip_adapter_prefs['guidance_scale'], 'num_inference_steps':ip_adapter_prefs['num_inference_steps'], 'width':ip_adapter_prefs['width'], 'height':ip_adapter_prefs['height'], 'init_image':ip_adapter_prefs['init_image'], 'mask_image':ip_adapter_prefs['mask_image'], 'init_image_strength':ip_adapter_prefs['init_image_strength'], 'num_images':ip_adapter_prefs['num_images'], 'seed':ip_adapter_prefs['seed']})\n",
        "        else:\n",
        "            ip_adapter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      ip_adapter_prompts.append({'prompt': ip_adapter_prefs['prompt'], 'negative_prompt':ip_adapter_prefs['negative_prompt'], 'guidance_scale':ip_adapter_prefs['guidance_scale'], 'num_inference_steps':ip_adapter_prefs['num_inference_steps'], 'width':ip_adapter_prefs['width'], 'height':ip_adapter_prefs['height'], 'init_image':ip_adapter_prefs['init_image'], 'mask_image':ip_adapter_prefs['mask_image'], 'init_image_strength':ip_adapter_prefs['init_image_strength'], 'num_images':ip_adapter_prefs['num_images'], 'seed':ip_adapter_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.IP_Adapter.controls.append(line)\n",
        "        if update:\n",
        "          page.IP_Adapter.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.IP_Adapter, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.IP_Adapter.auto_scroll = scroll\n",
        "        page.IP_Adapter.update()\n",
        "      else:\n",
        "        page.IP_Adapter.auto_scroll = scroll\n",
        "        page.IP_Adapter.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.IP_Adapter.controls = page.IP_Adapter.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = ip_adapter_prefs['num_inference_steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      nonlocal progress\n",
        "      total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing IP-Adapter Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    use_SDXL = ip_adapter_prefs['use_SDXL']\n",
        "    if use_SDXL:\n",
        "        model = get_SDXL_model(prefs['SDXL_model'])\n",
        "        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == ip_adapter_prefs['ip_adapter_SDXL_model'])\n",
        "    else:\n",
        "        model = get_model(prefs['model_ckpt'])\n",
        "        ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == ip_adapter_prefs['ip_adapter_model'])\n",
        "    model_id = model['path']\n",
        "    variant = {'variant': model['revision']} if 'revision' in model else {}\n",
        "    variant = {'variant': model['variant']} if 'variant' in model else variant\n",
        "    if status['loaded_ip_adapter'] != model_id:\n",
        "        clear_pipes()\n",
        "    else:\n",
        "        clear_pipes(\"ip_adapter\")\n",
        "    if bool(ip_adapter_prefs['init_image']) and bool(ip_adapter_prefs['mask_image']):\n",
        "        mode = \"inpaint\"\n",
        "    elif bool(ip_adapter_prefs['init_image']):\n",
        "        mode = \"img2img\"\n",
        "    else:\n",
        "        mode = \"txt2img\"\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = ip_adapter_prefs['cpu_offload']\n",
        "    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpaint\n",
        "    def change_mode(pipe, mode):\n",
        "        status['loaded_ip_adapter_mode'] = mode\n",
        "        if mode == \"inpaint\":\n",
        "            return AutoPipelineForInpaint.from_pipe(pipe)\n",
        "        elif mode == \"img2img\":\n",
        "            return AutoPipelineForImage2Image.from_pipe(pipe)\n",
        "        else:\n",
        "            return AutoPipelineForText2Image.from_pipe(pipe)\n",
        "    if pipe_ip_adapter == None:\n",
        "        installer.status(f\"...loading {model_id}\")\n",
        "        try:\n",
        "            if mode == \"inpaint\":\n",
        "                pipe_ip_adapter = AutoPipelineForInpaint.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)\n",
        "                status['loaded_ip_adapter_mode'] = \"inpaint\"\n",
        "            elif mode == \"img2img\":\n",
        "                pipe_ip_adapter = AutoPipelineForImage2Image.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)\n",
        "                status['loaded_ip_adapter_mode'] = \"img2img\"\n",
        "            else:\n",
        "                pipe_ip_adapter = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)\n",
        "                status['loaded_ip_adapter_mode'] = \"txt2img\"\n",
        "            #pipe_ip_adapter = pipeline_scheduler(pipe_ip_adapter)\n",
        "            if use_SDXL:\n",
        "                pipe_ip_adapter = optimize_SDXL(pipe_ip_adapter)\n",
        "            else:\n",
        "                pipe_ip_adapter = optimize_pipe(pipe_ip_adapter)\n",
        "            pipe_ip_adapter.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing IP-Adapter...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_ip_adapter'] = model_id\n",
        "    else:\n",
        "        clear_pipes('ip_adapter')\n",
        "        if status['loaded_ip_adapter_mode'] != mode:\n",
        "            try:\n",
        "                pipe_ip_adapter = change_mode(pipe_ip_adapter, mode)\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR in {mode} IP-Adapter AutoPipeline from_pipe...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "        if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "            pipe_ip_adapter = pipeline_scheduler(pipe_ip_adapter)\n",
        "    pipe_ip_adapter.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "    pipe_ip_adapter.set_ip_adapter_scale(ip_adapter_prefs['ip_adapter_strength'])\n",
        "    if ip_adapter_prefs['ip_adapter_image'].startswith('http'):\n",
        "        ip_adapter_image = PILImage.open(requests.get(ip_adapter_prefs['ip_adapter_image'], stream=True).raw)\n",
        "    else:\n",
        "        if os.path.isfile(ip_adapter_prefs['ip_adapter_image']):\n",
        "            ip_adapter_image = PILImage.open(ip_adapter_prefs['ip_adapter_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your ip_adapter_image {ip_adapter_prefs['ip_adapter_image']}\")\n",
        "            return\n",
        "    #init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "    ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert(\"RGB\")\n",
        "    clear_last()\n",
        "    s = \"\" if len(ip_adapter_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your IP-Adapter Image{s}...\")\n",
        "    for pr in ip_adapter_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        init_img = None\n",
        "        mask_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        if bool(pr['mask_image']):\n",
        "            fname = pr['mask_image'].rpartition(slash)[2]\n",
        "            if pr['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['mask_image']):\n",
        "                    mask_img = PILImage.open(pr['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                    return\n",
        "            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "        mode = \"inpaint\" if bool(init_img) and bool(mask_img) else \"img2img\" if bool(init_img) else \"txt2img\"\n",
        "        try:\n",
        "            if status['loaded_ip_adapter_mode'] != mode:\n",
        "                pipe_ip_adapter = change_mode(pipe_ip_adapter, mode)\n",
        "            if mode == \"inpaint\":\n",
        "                images = pipe_ip_adapter(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    #height=pr['height'],\n",
        "                    #width=pr['width'],\n",
        "                    num_inference_steps=pr['num_inference_steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    ip_adapter_image=ip_adapter_image,\n",
        "                    init_image=init_img,\n",
        "                    mask_image=mask_img,\n",
        "                    init_image_strength=pr['init_image_strength'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            elif mode == \"img2img\":\n",
        "                images = pipe_ip_adapter(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    #height=pr['height'],\n",
        "                    #width=pr['width'],\n",
        "                    num_inference_steps=pr['num_inference_steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    ip_adapter_image=ip_adapter_image,\n",
        "                    init_image=init_img,\n",
        "                    init_image_strength=pr['init_image_strength'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            else:\n",
        "                images = pipe_ip_adapter(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['num_inference_steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    ip_adapter_image=ip_adapter_image,\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(ip_adapter_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, ip_adapter_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{ip_adapter_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not ip_adapter_prefs['display_upscaled_image'] or not ip_adapter_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': ip_adapter_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if ip_adapter_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=ip_adapter_prefs[\"enlarge_scale\"], face_enhance=ip_adapter_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                if ip_adapter_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(ip_adapter_prefs[\"enlarge_scale\"]), height=pr['height'] * float(ip_adapter_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {ip_adapter_prefs['enlarge_scale']}x with ESRGAN\" if ip_adapter_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"IP-Adapter\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = ip_adapter_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_reference(page, from_list=False):\n",
        "    global reference_prefs, pipe_reference\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    if not reference_prefs['reference_attn'] and not reference_prefs['reference_adain']:\n",
        "      alert_msg(page, \"Reference Attention and/or Reference Adain must be set to True... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.Reference.controls.append(line)\n",
        "        if update:\n",
        "          page.Reference.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.Reference, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.Reference.controls = page.Reference.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.Reference.auto_scroll = scroll\n",
        "        page.Reference.update()\n",
        "      else:\n",
        "        page.Reference.auto_scroll = scroll\n",
        "        page.Reference.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = reference_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    reference_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        reference = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else reference_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}\n",
        "        reference_prompts.append(reference)\n",
        "    else:\n",
        "        if not bool(reference_prefs['prompt']):\n",
        "            alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "            return\n",
        "        reference = {'prompt':reference_prefs['prompt'], 'negative_prompt': reference_prefs['negative_prompt'], 'ref_image': reference_prefs['ref_image'], 'guidance_scale':reference_prefs['guidance_scale'], 'num_inference_steps': reference_prefs['num_inference_steps'], 'width': reference_prefs['width'], 'height': reference_prefs['height'], 'seed': reference_prefs['seed']}\n",
        "        reference_prompts.append(reference)\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      clear_list()\n",
        "    autoscroll(True)\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if reference_prefs['use_SDXL']:\n",
        "        model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    else:\n",
        "        model = get_model(prefs['model_ckpt'])['path']\n",
        "    if reference_prefs['last_model'] == model:\n",
        "        clear_pipes('reference')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "        reference_prefs['last_model'] = model\n",
        "    #torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_reference == None:\n",
        "        prt(Installing(f\"Installing Reference Pipeline with {model} Model... \"))\n",
        "        diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "        if not os.path.exists(diffusers_dir):\n",
        "          run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "        sys.path.append(os.path.join(diffusers_dir, \"examples\", \"community\"))\n",
        "        try:\n",
        "            if reference_prefs['use_SDXL']:\n",
        "                from stable_diffusion_xl_reference import StableDiffusionXLReferencePipeline\n",
        "                pipe_reference = StableDiffusionXLReferencePipeline.from_pretrained(model, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\", safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                pipe_reference = pipeline_scheduler(pipe_reference)\n",
        "                pipe_reference = optimize_SDXL(pipe_reference, freeu=False)\n",
        "                pipe_reference.set_progress_bar_config(disable=True)\n",
        "            else:\n",
        "                from stable_diffusion_reference import StableDiffusionReferencePipeline\n",
        "                pipe_reference = StableDiffusionReferencePipeline.from_pretrained(model, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                #pipe_reference.to(torch_device)\n",
        "                pipe_reference = pipeline_scheduler(pipe_reference)\n",
        "                pipe_reference = optimize_pipe(pipe_reference, freeu=False)\n",
        "                pipe_reference.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Reference Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_reference.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    else:\n",
        "        pipe_reference = pipeline_scheduler(pipe_reference)\n",
        "    s = \"s\" if reference_prefs['num_images'] > 1 or reference_prefs['batch_size'] > 1 else \"\"\n",
        "    prt(f\"Generating Reference{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, reference_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], reference_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in reference_prompts:\n",
        "        if pr['ref_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(pr['ref_image']):\n",
        "                init_img = PILImage.open(pr['ref_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ref_image {pr['ref_image']}\")\n",
        "                return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, reference_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        width = pr['width']\n",
        "        height = pr['height']\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        for num in range(reference_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_reference(ref_image=init_img, prompt=pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=pr['num_inference_steps'], attention_auto_machine_weight=reference_prefs['attention_auto_machine_weight'], gn_auto_machine_weight=reference_prefs['gn_auto_machine_weight'], style_fidelity=reference_prefs['style_fidelity'], reference_attn=reference_prefs['reference_attn'], reference_adain=reference_prefs['reference_adain'], guidance_scale=pr['guidance_scale'], width=width, height=height, num_images_per_prompt=reference_prefs['batch_size'], callback=callback_fnc, callback_steps=1).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running Reference Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            for image in images:\n",
        "                if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "                #for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, reference_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not reference_prefs['display_upscaled_image'] or not reference_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                    time.sleep(0.6)\n",
        "                if reference_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=reference_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if reference_prefs['display_upscaled_image']:\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(reference_prefs[\"enlarge_scale\"]), height=height * float(reference_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        time.sleep(0.6)\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {reference_prefs['enlarge_scale']}x with ESRGAN\" if reference_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"Reference\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr['prompt'])\n",
        "                      config_json = reference_prefs.copy()\n",
        "                      config_json['model_path'] = model\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images'], config_json['batch_size']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      del config_json['max_size']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet_qr(page, from_list=False):\n",
        "    global controlnet_qr_prefs, pipe_controlnet_qr, pipe_controlnet\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetQR.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNetQR.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.ControlNetQR, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNetQR.controls = page.ControlNetQR.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.ControlNetQR.auto_scroll = scroll\n",
        "        page.ControlNetQR.update()\n",
        "      else:\n",
        "        page.ControlNetQR.auto_scroll = scroll\n",
        "        page.ControlNetQR.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_qr_prefs['num_inference_steps'] * 2\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_qr_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        controlnet_qr = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else controlnet_qr_prefs['init_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['max_size'], 'height': p['max_size'], 'strength':p['init_image_strength'], 'seed': p['seed']}\n",
        "        controlnet_qr_prompts.append(controlnet_qr)\n",
        "    else:\n",
        "        if not bool(controlnet_qr_prefs['prompt']):\n",
        "            alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "            return\n",
        "        controlnet_qr = {'prompt':controlnet_qr_prefs['prompt'], 'negative_prompt': controlnet_qr_prefs['negative_prompt'], 'init_image': controlnet_qr_prefs['init_image'], 'guidance_scale':controlnet_qr_prefs['guidance_scale'], 'num_inference_steps': controlnet_qr_prefs['num_inference_steps'], 'width': controlnet_qr_prefs['max_size'], 'height': controlnet_qr_prefs['max_size'], 'strength': controlnet_qr_prefs['strength'], 'seed': controlnet_qr_prefs['seed']}\n",
        "        controlnet_qr_prompts.append(controlnet_qr)\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      clear_list()\n",
        "    autoscroll(True)\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    try:\n",
        "        import qrcode\n",
        "    except ImportError:\n",
        "        prt(Installing(f\"Installing QRCode Library... \"))\n",
        "        run_sp(\"pip install qrcode\", realtime=False)\n",
        "        import qrcode\n",
        "        clear_last()\n",
        "        pass\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ImportError:\n",
        "        prt(Installing(f\"Installing QRCode Library... \"))\n",
        "        run_sp(\"pip install qrcode\", realtime=False)\n",
        "        import qrcode\n",
        "        clear_last()\n",
        "        pass\n",
        "    use_SDXL = 'SDXL' in controlnet_qr_prefs['controlnet_version']\n",
        "    if not use_SDXL:\n",
        "        if '2.1' in controlnet_qr_prefs['controlnet_version']:\n",
        "            sd_model = \"stabilityai/stable-diffusion-2-1\"\n",
        "        else:\n",
        "            sd_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "        #sd_model = get_model(prefs['model_ckpt'])['path']\n",
        "        controlnet_model = \"DionTimmer/controlnet_qrcode-control_v1p_sd15\" if controlnet_qr_prefs['controlnet_version'].endswith(\"1.5\") else \"DionTimmer/controlnet_qrcode-control_v11p_sd21\"\n",
        "        if 'Monster' in controlnet_qr_prefs['controlnet_version']: #\"monster-labs/control_v1p_sd15_qrcode_monster\"\n",
        "            controlnet_model = \"prantik-s/monster_qrcode_v2\"\n",
        "        elif 'Pattern' in controlnet_qr_prefs['controlnet_version']:\n",
        "            controlnet_model = \"Nacholmo/controlnet-qr-pattern-v2\"\n",
        "            sd_model = \"Nacholmo/Counterfeit-V2.5-vae-swapped\"\n",
        "    else:\n",
        "        sd_model = get_SDXL_model(prefs['SDXL_model'])['path']\n",
        "        if 'Monster' in controlnet_qr_prefs['controlnet_version']:\n",
        "            controlnet_model = \"monster-labs/control_v1p_sdxl_qrcode_monster\"\n",
        "        elif 'Pattern' in controlnet_qr_prefs['controlnet_version']:\n",
        "            if 'LLLite' not in controlnet_qr_prefs['controlnet_version']:\n",
        "                controlnet_model = \"Nacholmo/controlnet-qr-pattern-sdxl\"\n",
        "            else:\n",
        "                controlnet_model = \"Nacholmo/qr-pattern-sdxl-ControlNet-LLLite\"\n",
        "    #monster-labs/control_v1p_sdxl_qrcode_monster\n",
        "    use_ip_adapter = controlnet_qr_prefs['use_ip_adapter']\n",
        "    if use_ip_adapter:\n",
        "        if use_SDXL:\n",
        "            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_qr_prefs['ip_adapter_SDXL_model'])\n",
        "        else:\n",
        "            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == controlnet_qr_prefs['ip_adapter_model'])\n",
        "    else:\n",
        "        ip_adapter_model = None\n",
        "    if controlnet_qr_prefs['last_model'] == sd_model and controlnet_qr_prefs['last_controlnet_model'] == controlnet_model and pipe_controlnet_qr != None:\n",
        "        clear_pipes('controlnet_qr')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "        controlnet_qr_prefs['last_model'] = sd_model\n",
        "        controlnet_qr_prefs['last_controlnet_model'] = controlnet_model\n",
        "    #torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_controlnet_qr == None:\n",
        "        installer = Installing(f\"Installing ControlNet QRCode Pipeline with {controlnet_qr_prefs['controlnet_version']} Model... \")\n",
        "        prt(installer)\n",
        "        try:\n",
        "            if not use_SDXL:\n",
        "                from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel\n",
        "                installer.status(f\"...get {controlnet_model}\")\n",
        "                pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16)\n",
        "                installer.status(f\"...get {sd_model}\")\n",
        "                pipe_controlnet_qr = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(sd_model, controlnet=pipe_controlnet, torch_dtype=torch.float16, variant='fp16', safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                installer.status(f\"...optimizing pipe\")\n",
        "                pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)\n",
        "                pipe_controlnet_qr = optimize_pipe(pipe_controlnet_qr)\n",
        "                pipe_controlnet_qr.set_progress_bar_config(disable=True)\n",
        "            else:\n",
        "                from diffusers import StableDiffusionXLControlNetImg2ImgPipeline, ControlNetModel, AutoencoderKL\n",
        "                installer.status(f\"...get {controlnet_model}\")\n",
        "                pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16)\n",
        "                installer.status(f\"...get sdxl-vae-fp16-fix vae\")\n",
        "                vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "                installer.status(f\"...get {sd_model}\")\n",
        "                pipe_controlnet_qr = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(sd_model, controlnet=pipe_controlnet, vae=vae, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                installer.status(f\"...optimizing pipe\")\n",
        "                pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)\n",
        "                pipe_controlnet_qr = optimize_SDXL(pipe_controlnet_qr)\n",
        "                pipe_controlnet_qr.set_progress_bar_config(disable=True)\n",
        "                \n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing ControlNet QRCode Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "    else:\n",
        "        pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)\n",
        "    if use_ip_adapter:\n",
        "        pipe_controlnet_qr.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "        pipe_controlnet_qr.set_ip_adapter_scale(controlnet_qr_prefs['ip_adapter_strength'])\n",
        "        if controlnet_qr_prefs['ip_adapter_image'].startswith('http'):\n",
        "            ip_adapter_image = PILImage.open(requests.get(controlnet_qr_prefs['ip_adapter_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_qr_prefs['ip_adapter_image']):\n",
        "                ip_adapter_image = PILImage.open(controlnet_qr_prefs['ip_adapter_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ip_adapter_image {controlnet_qr_prefs['ip_adapter_image']}\")\n",
        "                return\n",
        "        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert(\"RGB\")\n",
        "        status['loaded_ip_adapter'] = ip_adapter_model\n",
        "        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}\n",
        "    else:\n",
        "        status['loaded_ip_adapter'] = \"\"\n",
        "        ip_adapter_args = {}\n",
        "    s = \"s\" if controlnet_qr_prefs['num_images'] > 1 or controlnet_qr_prefs['batch_size'] > 1 else \"\"\n",
        "    prt(f\"Generating ControlNet QR{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "        os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "        os.makedirs(batch_output)\n",
        "    batch_size = controlnet_qr_prefs['batch_size']\n",
        "    mode = controlnet_qr_prefs['selected_mode'] if '\"' not in controlnet_qr_prefs['selected_mode'] else controlnet_qr_prefs['selected_mode'].split('\"')[1]\n",
        "    if mode == \"image\":#controlnet_qr_prefs['use_image']:\n",
        "        if not bool(controlnet_qr_prefs['ref_image']):\n",
        "            alert_msg(page, f\"ERROR: If using your own QR image, you must provide it.\")\n",
        "            return\n",
        "        if controlnet_qr_prefs['ref_image'].startswith('http'):\n",
        "            qrcode_image = PILImage.open(requests.get(controlnet_qr_prefs['ref_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(pr['ref_image']):\n",
        "                qrcode_image = PILImage.open(controlnet_qr_prefs['ref_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ref_image {controlnet_qr_prefs['ref_image']}\")\n",
        "                return\n",
        "    else:\n",
        "        if not bool(controlnet_qr_prefs['qr_content']):\n",
        "            alert_msg(page, f\"ERROR: If Generating QR Code, you must provide content like a URL.\")\n",
        "            return\n",
        "        prt(\"Generating QR Code from content...\")\n",
        "        qr = qrcode.QRCode(\n",
        "            version=1,\n",
        "            error_correction=qrcode.constants.ERROR_CORRECT_H,\n",
        "            box_size=10,\n",
        "            border=controlnet_qr_prefs['border_thickness'],\n",
        "        )\n",
        "        qr.add_data(controlnet_qr_prefs['qr_content'])\n",
        "        qr.make(fit=True)\n",
        "        qrcode_image = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
        "        clear_last()\n",
        "    width, height = qrcode_image.size\n",
        "    width, height = scale_dimensions(width, height, controlnet_qr_prefs['max_size'], multiple=32)\n",
        "    qrcode_image = qrcode_image.resize((width, height), resample=PILImage.LANCZOS)\n",
        "\n",
        "    for pr in controlnet_qr_prompts:\n",
        "        if bool(pr['init_image']):\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.LANCZOS)\n",
        "            #width = pr['width']\n",
        "            #height = pr['height']\n",
        "        else:\n",
        "            init_img = qrcode_image\n",
        "            pr['strength'] = 0.99\n",
        "        total_steps = pr['num_inference_steps'] * 2\n",
        "        for num in range(controlnet_qr_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            prt(Container(content=None))\n",
        "            clear_last()\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_controlnet_qr(prompt=[pr['prompt']] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, image=[init_img] * batch_size, control_image=qrcode_image, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], controlnet_conditioning_scale=float(controlnet_qr_prefs['conditioning_scale']), control_guidance_start=controlnet_qr_prefs['control_guidance_start'], control_guidance_end=controlnet_qr_prefs['control_guidance_end'], width=width, height=height, num_images_per_prompt=controlnet_qr_prefs['batch_size'], strength=pr['strength'], generator=generator, callback=callback_fnc, callback_steps=1, **ip_adapter_args).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running ControlNet-QRCode Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            i = 0\n",
        "            for image in images:\n",
        "                if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "                #for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not controlnet_qr_prefs['display_upscaled_image'] or not controlnet_qr_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if controlnet_qr_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=controlnet_qr_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if controlnet_qr_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_qr_prefs[\"enlarge_scale\"]), height=height * float(controlnet_qr_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {controlnet_qr_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_qr_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"ControlNet-QRCode\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr['prompt'])\n",
        "                      config_json = controlnet_qr_prefs.copy()\n",
        "                      config_json['controlnet_model_path'] = controlnet_model\n",
        "                      config_json['model_path'] = sd_model\n",
        "                      config_json['seed'] = random_seed + i\n",
        "                      del config_json['num_images'], config_json['batch_size']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      del config_json['max_size']\n",
        "                      del config_json['last_model']\n",
        "                      del config_json['last_controlnet_model']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "                i += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet_segment(page, from_list=False):\n",
        "    global controlnet_segment_prefs, pipe_controlnet_segment\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetSegmentAnything.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNetSegmentAnything.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.ControlNetSegmentAnything, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNetSegmentAnything.controls = page.ControlNetSegmentAnything.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.ControlNetSegmentAnything.auto_scroll = scroll\n",
        "        page.ControlNetSegmentAnything.update()\n",
        "      else:\n",
        "        page.ControlNetSegmentAnything.auto_scroll = scroll\n",
        "        page.ControlNetSegmentAnything.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_segment_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_segment_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        controlnet_segment = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else controlnet_segment_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}\n",
        "        controlnet_segment_prompts.append(controlnet_segment)\n",
        "    else:\n",
        "        if not bool(controlnet_segment_prefs['prompt']):\n",
        "            alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "            return\n",
        "        controlnet_segment = {'prompt':controlnet_segment_prefs['prompt'], 'negative_prompt': controlnet_segment_prefs['negative_prompt'], 'ref_image': controlnet_segment_prefs['ref_image'], 'guidance_scale':controlnet_segment_prefs['guidance_scale'], 'num_inference_steps': controlnet_segment_prefs['num_inference_steps'], 'width': controlnet_segment_prefs['width'], 'height': controlnet_segment_prefs['height'], 'seed': controlnet_segment_prefs['seed']}\n",
        "        controlnet_segment_prompts.append(controlnet_segment)\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    model = \"runwayml/stable-diffusion-v1-5\"#get_model(prefs['model_ckpt'])['path']\n",
        "    model_name = \"1.5\"#get_model(prefs['model_ckpt'])['name']\n",
        "    controlnet_model = \"mfidabel/controlnet-segment-anything\"\n",
        "    autoscroll(True)\n",
        "    installer = Installing(f\"Installing ControlNet Segment-Anything Pipeline with {model} Model... \")\n",
        "    prt(installer)\n",
        "    clear_pipes('controlnet_segment')\n",
        "    #torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    try:\n",
        "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...facebookresearch/segment-anything\")\n",
        "        run_sp(\"pip install git+https://github.com/facebookresearch/segment-anything.git\", realtime=False)\n",
        "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "        pass\n",
        "    SAM_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "    SAM_dir = os.path.join(root_dir, \"sam_vit_h_4b8939.pth\")\n",
        "    if not os.path.exists(SAM_dir):\n",
        "        installer.status(\"...Downloading SAM Weights\")\n",
        "        r = requests.get(SAM_URL, allow_redirects=True)\n",
        "        installer.status(\"...Writing SAM Weights\")\n",
        "        with open(SAM_dir, \"wb\") as sam_weights:\n",
        "            sam_weights.write(r.content)\n",
        "        del r\n",
        "\n",
        "    gc.collect()\n",
        "    sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_dir).to(torch_device)\n",
        "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "    gc.collect()\n",
        "\n",
        "    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "    #prt(installer)\n",
        "    if pipe_controlnet_segment == None:\n",
        "        try:\n",
        "            installer.status(f\"...ControlNet Model {controlnet_model}\")\n",
        "            pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16).to(torch_device)\n",
        "            installer.status(f\"...SD Model {model_name}\")\n",
        "            pipe_controlnet_segment = StableDiffusionControlNetPipeline.from_pretrained(model, controlnet=pipe_controlnet, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)\n",
        "            pipe_controlnet_segment = optimize_pipe(pipe_controlnet_segment)\n",
        "            pipe_controlnet_segment.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing ControlNet Segment-Anything Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_controlnet_segment.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)\n",
        "\n",
        "    def show_anns(anns):\n",
        "        if len(anns) == 0:\n",
        "            return\n",
        "        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "        h, w =  anns[0]['segmentation'].shape\n",
        "        final_img = PILImage.fromarray(np.zeros((h, w, 3), dtype=np.uint8), mode=\"RGB\")\n",
        "        for ann in sorted_anns:\n",
        "            m = ann['segmentation']\n",
        "            img = np.empty((m.shape[0], m.shape[1], 3), dtype=np.uint8)\n",
        "            for i in range(3):\n",
        "                img[:,:,i] = np.random.randint(255, dtype=np.uint8)\n",
        "            final_img.paste(PILImage.fromarray(img, mode=\"RGB\"), (0, 0), PILImage.fromarray(np.uint8(m*255)))\n",
        "        return final_img\n",
        "\n",
        "    clear_last()\n",
        "    s = \"s\" if controlnet_segment_prefs['num_images'] > 1 or controlnet_segment_prefs['batch_size'] > 1 else \"\"\n",
        "    prt(f\"Generating ControlNet Segment-Anything{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in controlnet_segment_prompts:\n",
        "        if pr['ref_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(pr['ref_image']):\n",
        "                init_img = PILImage.open(pr['ref_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ref_image {pr['ref_image']}\")\n",
        "                return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, controlnet_segment_prefs['max_size'], multiple=32)\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        #width = pr['width']\n",
        "        #height = pr['height']\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        batch_size = controlnet_segment_prefs['batch_size']\n",
        "        fname = format_filename(pr['prompt'])\n",
        "        segmented_image = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'], f\"{fname}-segmented.png\")\n",
        "        for num in range(controlnet_segment_prefs['num_images']):\n",
        "            prt(\"Creating Segmented Mask Image...\")\n",
        "            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            np.random.seed(int(random_seed))\n",
        "            init_img_np = np.asarray(init_img)\n",
        "            masks = mask_generator.generate(init_img_np)\n",
        "            segmented_map = show_anns(masks)\n",
        "            segmented_map.save(segmented_image)\n",
        "            clear_last()#src_base64=pil_to_base64(segmented_map)\n",
        "            prt(Row([Img(src=segmented_image, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            del masks\n",
        "            flush()\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            #segmented_map = segment_image(init_img, random_seed)\n",
        "            #yield segmented_map, [PILImage.fromarray(np.zeros((width, height, 3), dtype=np.uint8))] * batch_size]\n",
        "            try:\n",
        "                images = pipe_controlnet_segment([pr['prompt']] * batch_size, [segmented_map] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], width=width, height=height, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running ControlNet Segment-Anything Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            for image in images:\n",
        "                if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "                #for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not controlnet_segment_prefs['display_upscaled_image'] or not controlnet_segment_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                    time.sleep(0.8)\n",
        "                if controlnet_segment_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=controlnet_segment_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if controlnet_segment_prefs['display_upscaled_image']:\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_segment_prefs[\"enlarge_scale\"]), height=height * float(controlnet_segment_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        time.sleep(0.6)\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {controlnet_segment_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_segment_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"ControlNetSegmentAnything\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr['prompt'])\n",
        "                      config_json = controlnet_segment_prefs.copy()\n",
        "                      config_json['model_path'] = model\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images'], config_json['batch_size']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      del config_json['max_size']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_EDICT(page):\n",
        "    global EDICT_prefs, prefs, status, pipe_EDICT, text_encoder_EDICT\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(EDICT_prefs['init_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(EDICT_prefs['base_prompt']) or not bool(EDICT_prefs['target_prompt']):\n",
        "      alert_msg(page, \"You must provide a base prompt describing image and target prompt to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.EDICT.controls.append(line)\n",
        "      page.EDICT.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.EDICT, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.EDICT.auto_scroll = scroll\n",
        "      page.EDICT.update()\n",
        "    def clear_list():\n",
        "      page.EDICT.controls = page.EDICT.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = EDICT_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def center_crop_resize(im):\n",
        "        width, height = im.size\n",
        "        d = min(width, height)\n",
        "        left = (width - d) / 2\n",
        "        upper = (height - d) / 2\n",
        "        right = (width + d) / 2\n",
        "        lower = (height + d) / 2\n",
        "        return im.crop((left, upper, right, lower)).resize((EDICT_prefs['max_size'], EDICT_prefs['max_size']))\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing EDICT Editor Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if EDICT_prefs['init_image'].startswith('http'):\n",
        "      #response = requests.get(EDICT_prefs['init_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(EDICT_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(EDICT_prefs['init_image']):\n",
        "        original_img = PILImage.open(EDICT_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {EDICT_prefs['init_image']}\")\n",
        "        return\n",
        "    #width, height = original_img.size\n",
        "    #width, height = scale_dimensions(width, height, EDICT_prefs['max_size'])\n",
        "    original_img = center_crop_resize(original_img)\n",
        "    clear_pipes('EDICT')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch_dtype = torch.float16\n",
        "    model_id = get_model(prefs['model_ckpt'])['path']\n",
        "    if pipe_EDICT is None:\n",
        "        from diffusers import DiffusionPipeline, DDIMScheduler\n",
        "        from transformers import CLIPTextModel\n",
        "        try:\n",
        "            scheduler = DDIMScheduler(\n",
        "                num_train_timesteps=1000,\n",
        "                beta_start=0.00085,\n",
        "                beta_end=0.012,\n",
        "                beta_schedule=\"scaled_linear\",\n",
        "                set_alpha_to_one=False,\n",
        "                clip_sample=False,\n",
        "            )\n",
        "            text_encoder_EDICT = CLIPTextModel.from_pretrained(pretrained_model_name_or_path=\"openai/clip-vit-large-patch14\", torch_dtype=torch_dtype)\n",
        "            pipe_EDICT = DiffusionPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path=model_id,\n",
        "                custom_pipeline=\"edict_pipeline\",\n",
        "                revision=\"fp16\",\n",
        "                scheduler=scheduler,\n",
        "                text_encoder=text_encoder_EDICT,\n",
        "                leapfrog_steps=True,\n",
        "                torch_dtype=torch_dtype,\n",
        "                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "            ).to(torch_device)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Initialize EDICT Pipeline for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    prt(\"Generating EDICT Edit of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, EDICT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(EDICT_prefs['seed']) if int(EDICT_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    for i in range(EDICT_prefs['num_images']):\n",
        "        generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_EDICT(base_prompt=EDICT_prefs['base_prompt'], target_prompt=EDICT_prefs['target_prompt'], image=original_img, num_inference_steps=EDICT_prefs['num_inference_steps'], strength=EDICT_prefs['strength'], guidance_scale=EDICT_prefs['guidance_scale'], generator=generator)#.images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't EDICT Edit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = format_filename(EDICT_prefs['target_prompt'])\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, EDICT_prefs['batch_folder_name']), fname, i)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            width, height = image.size\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not EDICT_prefs['display_upscaled_image'] or not EDICT_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if EDICT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=EDICT_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if EDICT_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(EDICT_prefs[\"enlarge_scale\"]), height=height * float(EDICT_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {EDICT_prefs['enlarge_scale']}x with ESRGAN\" if EDICT_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"EDICT Editor\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = EDICT_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['max_size']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "        random_seed += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_DiffEdit(page):\n",
        "    global DiffEdit_prefs, prefs, status, pipe_DiffEdit, text_encoder_DiffEdit\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(DiffEdit_prefs['init_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(DiffEdit_prefs['target_prompt']):\n",
        "      alert_msg(page, \"You must provide a target prompt to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.DiffEdit.controls.append(line)\n",
        "      page.DiffEdit.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.DiffEdit, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DiffEdit.auto_scroll = scroll\n",
        "      page.DiffEdit.update()\n",
        "    def clear_list():\n",
        "      page.DiffEdit.controls = page.DiffEdit.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = DiffEdit_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def center_crop_resize(im):\n",
        "        width, height = im.size\n",
        "        d = min(width, height)\n",
        "        left = (width - d) / 2\n",
        "        upper = (height - d) / 2\n",
        "        right = (width + d) / 2\n",
        "        lower = (height + d) / 2\n",
        "        return im.crop((left, upper, right, lower)).resize((DiffEdit_prefs['max_size'], DiffEdit_prefs['max_size']))\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    source_prompt = DiffEdit_prefs['source_prompt']\n",
        "    target_prompt = DiffEdit_prefs['target_prompt']\n",
        "    prt(Installing(f'Installing DiffEdit Pipeline{\" and Caption Generator\" if not bool(source_prompt) else \"\"}...'))\n",
        "    clear_pipes('DiffEdit')\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if DiffEdit_prefs['init_image'].startswith('http'):\n",
        "      #response = requests.get(DiffEdit_prefs['init_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(DiffEdit_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(DiffEdit_prefs['init_image']):\n",
        "        original_img = PILImage.open(DiffEdit_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {DiffEdit_prefs['init_image']}\")\n",
        "        return\n",
        "    #width, height = original_img.size\n",
        "    #width, height = scale_dimensions(width, height, DiffEdit_prefs['max_size'])\n",
        "    original_img = center_crop_resize(original_img)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_caption(images, caption_generator, caption_processor):\n",
        "        text = \"a photograph of\"\n",
        "        inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n",
        "        caption_generator.to(\"cuda\")\n",
        "        outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n",
        "        # offload caption generator\n",
        "        caption_generator.to(\"cpu\")\n",
        "        caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        print(f\"Caption: {caption}\")\n",
        "        del inputs\n",
        "        del caption_generator\n",
        "        return caption\n",
        "    if not bool(source_prompt):\n",
        "        prt(\"Generating Caption from Image with Blip...\")\n",
        "        from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "        captioner_id = \"Salesforce/blip-image-captioning-base\"\n",
        "        processor = BlipProcessor.from_pretrained(captioner_id)\n",
        "        blip_model = BlipForConditionalGeneration.from_pretrained(captioner_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "        source_prompt = generate_caption(original_img, blip_model, processor)\n",
        "        del processor\n",
        "        del blip_model\n",
        "        clear_last()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch_dtype = torch.float16\n",
        "    model_id = get_model(prefs['model_ckpt'])['path']\n",
        "    if pipe_DiffEdit is None:\n",
        "        from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
        "        try:\n",
        "            pipe_DiffEdit = StableDiffusionDiffEditPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16,\n",
        "                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "                safety_checker=None,\n",
        "            )\n",
        "            pipe_DiffEdit.scheduler = DDIMScheduler.from_config(pipe_DiffEdit.scheduler.config)\n",
        "            #pipe_DiffEdit.scheduler = pipeline_scheduler(pipe_DiffEdit)\n",
        "            pipe_DiffEdit.inverse_scheduler = DDIMInverseScheduler.from_config(pipe_DiffEdit.scheduler.config)\n",
        "            pipe_DiffEdit.enable_model_cpu_offload()\n",
        "            pipe_DiffEdit.enable_vae_slicing()\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Initialize DiffEdit Pipeline for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    prt(\"Generating DiffEdit of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(DiffEdit_prefs['seed']) if int(DiffEdit_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "\n",
        "    for i in range(DiffEdit_prefs['num_images']):\n",
        "        generator = torch.manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "            mask_image = pipe_DiffEdit.generate_mask(\n",
        "                image=original_img,\n",
        "                source_prompt=source_prompt,\n",
        "                target_prompt=target_prompt,\n",
        "                generator=generator,\n",
        "            )\n",
        "            inv_latents = pipe_DiffEdit.invert(prompt=source_prompt, image=original_img, generator=generator).latents\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't generate mask for DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        try:\n",
        "            images = pipe_DiffEdit(prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, negative_prompt=source_prompt, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            #images = pipe_DiffEdit(source_prompt=source_prompt, target_prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't generate DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = format_filename(DiffEdit_prefs['target_prompt'])\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name']), fname, i)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            width, height = image.size\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not DiffEdit_prefs['display_upscaled_image'] or not DiffEdit_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if DiffEdit_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=DiffEdit_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if DiffEdit_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(DiffEdit_prefs[\"enlarge_scale\"]), height=height * float(DiffEdit_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {DiffEdit_prefs['enlarge_scale']}x with ESRGAN\" if DiffEdit_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"DiffEdit\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = DiffEdit_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['max_size']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "        random_seed += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_null_text(page):\n",
        "    global null_text_prefs, prefs, status, pipe_null_text, text_encoder_null_text\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(null_text_prefs['init_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(null_text_prefs['base_prompt']) or not bool(null_text_prefs['target_prompt']):\n",
        "      alert_msg(page, \"You must provide a base prompt describing image and target prompt to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Null_Text.controls.append(line)\n",
        "      page.Null_Text.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Null_Text, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Null_Text.auto_scroll = scroll\n",
        "      page.Null_Text.update()\n",
        "    def clear_list():\n",
        "      page.Null_Text.controls = page.Null_Text.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = null_text_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def center_crop_resize(im):\n",
        "        width, height = im.size\n",
        "        d = min(width, height)\n",
        "        left = (width - d) / 2\n",
        "        upper = (height - d) / 2\n",
        "        right = (width + d) / 2\n",
        "        lower = (height + d) / 2\n",
        "        return im.crop((left, upper, right, lower)).resize((null_text_prefs['max_size'], null_text_prefs['max_size']))\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing Null-Text Inversion Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir) or force_updates:\n",
        "      os.chdir(root_dir)\n",
        "      #installer.status(\"...clone diffusers\")\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    if str(os.path.join(diffusers_dir, \"examples\")) not in sys.path:\n",
        "      sys.path.append(os.path.join(diffusers_dir, \"examples\"))\n",
        "      sys.path.append(os.path.join(diffusers_dir, \"examples\", \"community\"))\n",
        "    if null_text_prefs['init_image'].startswith('http'):\n",
        "      #response = requests.get(null_text_prefs['init_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(null_text_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(null_text_prefs['init_image']):\n",
        "        original_img = PILImage.open(null_text_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {null_text_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = original_img.size\n",
        "    width, height = scale_dimensions(width, height, null_text_prefs['max_size'])\n",
        "    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    #original_img = center_crop_resize(original_img)\n",
        "    clear_pipes('null_text')\n",
        "    torch_dtype = torch.float32\n",
        "    #model_id = get_model(prefs['model_ckpt'])['path']\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    if pipe_null_text is None:\n",
        "        from diffusers.schedulers import DDIMScheduler\n",
        "        from examples.community.pipeline_null_text_inversion import NullTextPipeline\n",
        "        try:\n",
        "            scheduler = DDIMScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.0120, beta_schedule=\"scaled_linear\")\n",
        "            pipe_null_text = NullTextPipeline.from_pretrained(model_id, scheduler = scheduler, torch_dtype=torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None).to(torch_device)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Initialize null_text Pipeline for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    prt(\"Generating Null-Text Inversion of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, null_text_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(null_text_prefs['seed']) if int(null_text_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    steps = null_text_prefs['num_inference_steps']\n",
        "    for i in range(null_text_prefs['num_images']):\n",
        "        generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "            inverted_latent, uncond = pipe_null_text.invert(original_img, null_text_prefs['base_prompt'], num_inner_steps=null_text_prefs['num_inner_steps'], early_stop_epsilon= 1e-5, num_inference_steps = steps)\n",
        "            images = pipe_null_text(null_text_prefs['target_prompt'], uncond, inverted_latent, guidance_scale=null_text_prefs['guidance_scale'], num_inference_steps=steps, generator=generator) #.images[0].save(input_image+\".output.jpg\")\n",
        "            #images = pipe_null_text(base_prompt=null_text_prefs['base_prompt'], target_prompt=null_text_prefs['target_prompt'], image=original_img, num_inference_steps=null_text_prefs['num_inference_steps'], strength=null_text_prefs['strength'], guidance_scale=null_text_prefs['guidance_scale'], generator=generator)#.images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't null_text Edit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = format_filename(null_text_prefs['target_prompt'])\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, null_text_prefs['batch_folder_name']), fname, i)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            width, height = image.size\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not null_text_prefs['display_upscaled_image'] or not null_text_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if null_text_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=null_text_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if null_text_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(null_text_prefs[\"enlarge_scale\"]), height=height * float(null_text_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {null_text_prefs['enlarge_scale']}x with ESRGAN\" if null_text_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"Null-Text Inversion\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = null_text_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['max_size']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "        random_seed += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_CLIPstyler(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.CLIPstyler.controls.append(line)\n",
        "      page.CLIPstyler.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.CLIPstyler, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.CLIPstyler.auto_scroll = scroll\n",
        "      page.CLIPstyler.update()\n",
        "    def clear_list():\n",
        "      page.CLIPstyler.controls = page.CLIPstyler.controls[:1]\n",
        "    clipstyler_dir = os.path.join(root_dir, \"CLIPstyler\")\n",
        "    if not os.path.exists(clipstyler_dir):\n",
        "          os.mkdir(clipstyler_dir)\n",
        "    if CLIPstyler_prefs['original_image'].startswith('http'):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(CLIPstyler_prefs['original_image'])\n",
        "        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])\n",
        "        original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        #width, height = original_img.size\n",
        "        #width, height = scale_dimensions(width, height)\n",
        "        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        CLIPstyler_prefs['image_dir'] = fpath\n",
        "    elif os.path.isfile(CLIPstyler_prefs['original_image']):\n",
        "        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])\n",
        "        original_img = PILImage.open(CLIPstyler_prefs['original_image'])\n",
        "        #width, height = original_img.size\n",
        "        #width, height = scale_dimensions(width, height)\n",
        "        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        CLIPstyler_prefs['image_dir'] = fpath\n",
        "    else:\n",
        "        alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "        return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Downloading CLIP-Styler Packages...\"))\n",
        "    run_process(\"pip install ftfy regex tqdm\", realtime=False, page=page)\n",
        "    run_sp(\"pip install git+https://github.com/openai/CLIP.git\", realtime=False)\n",
        "    #os.chdir(clipstyler_dir)\n",
        "    os.chdir(root_dir)\n",
        "    run_sp(\"pip install git+https://github.com/cyclomon/CLIPstyler.git\", realtime=True)\n",
        "    #!git clone https://github.com/cyclomon/CLIPstyler/\n",
        "    run_sp(f\"git clone https://github.com/cyclomon/CLIPstyler/ {clipstyler_dir}\", realtime=True)\n",
        "    os.chdir(root_dir)\n",
        "    #run_process(f\"git clone https://github.com/paper11667/CLIPstyler/ {clipstyler_dir}\", realtime=False, page=page)\n",
        "    sys.path.append(clipstyler_dir)\n",
        "\n",
        "    import torch.nn\n",
        "    import torch.optim as optim\n",
        "    from torchvision import transforms, models\n",
        "    import StyleNet\n",
        "    import utils\n",
        "    import clip\n",
        "    import torch.nn.functional as F\n",
        "    from template import imagenet_templates\n",
        "    from torchvision import utils as vutils\n",
        "    import argparse\n",
        "    from torchvision.transforms.functional import adjust_contrast\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    VGG = models.vgg19(pretrained=True).features\n",
        "    VGG.to(device)\n",
        "    save_dir = stable_dir\n",
        "    if bool(CLIPstyler_prefs['batch_folder_name']):\n",
        "        save_dir = os.path.join(stable_dir, CLIPstyler_prefs['batch_folder_name'])\n",
        "    new_file = format_filename(CLIPstyler_prefs[\"prompt_text\"])\n",
        "    images = []\n",
        "    for parameter in VGG.parameters():\n",
        "        parameter.requires_grad_(False)\n",
        "\n",
        "    def img_denormalize(image):\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = image*std +mean\n",
        "        return image\n",
        "\n",
        "    def img_normalize(image):\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = (image-mean)/std\n",
        "        return image\n",
        "\n",
        "    def clip_normalize(image,device):\n",
        "        image = F.interpolate(image,size=224,mode='bicubic')\n",
        "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n",
        "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = (image-mean)/std\n",
        "        return image\n",
        "\n",
        "    def get_image_prior_losses(inputs_jit):\n",
        "        diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]\n",
        "        diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]\n",
        "        diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]\n",
        "        diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]\n",
        "        loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        return loss_var_l2\n",
        "\n",
        "    from argparse import Namespace\n",
        "    source = CLIPstyler_prefs['source']\n",
        "\n",
        "    training_args = {\n",
        "        \"lambda_tv\": 2e-3,\n",
        "        \"lambda_patch\": 9000,\n",
        "        \"lambda_dir\": 500,\n",
        "        \"lambda_c\": 150,\n",
        "        \"crop_size\": CLIPstyler_prefs['crop_size'],\n",
        "        \"num_crops\":CLIPstyler_prefs['num_crops'],\n",
        "        \"img_height\":CLIPstyler_prefs['height'],\n",
        "        \"img_width\":CLIPstyler_prefs['width'],\n",
        "        \"max_step\":CLIPstyler_prefs['training_iterations'],\n",
        "        \"lr\":5e-4,\n",
        "        \"thresh\":0.7,\n",
        "        \"content_path\":CLIPstyler_prefs['image_dir'],\n",
        "        \"text\":CLIPstyler_prefs['prompt_text']\n",
        "    }\n",
        "\n",
        "    style_args = Namespace(**training_args)\n",
        "\n",
        "    def compose_text_with_templates(text: str, templates=imagenet_templates) -> list:\n",
        "        return [template.format(text) for template in templates]\n",
        "\n",
        "    content_path = style_args.content_path\n",
        "    content_image = utils.load_image2(content_path, img_height=style_args.img_height,img_width =style_args.img_width)\n",
        "    content_image = content_image.to(device)\n",
        "    content_features = utils.get_features(img_normalize(content_image), VGG)\n",
        "    target = content_image.clone().requires_grad_(True).to(device)\n",
        "    style_net = StyleNet.UNet()\n",
        "    style_net.to(device)\n",
        "\n",
        "    style_weights = {'conv1_1': 0.1,\n",
        "                    'conv2_1': 0.2,\n",
        "                    'conv3_1': 0.4,\n",
        "                    'conv4_1': 0.8,\n",
        "                    'conv5_1': 1.6}\n",
        "    clear_last()\n",
        "    prt(\"Generating Stylized Image from your source... Check console output for progress.\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    content_weight = style_args.lambda_c\n",
        "    show_every = 20\n",
        "    optimizer = optim.Adam(style_net.parameters(), lr=style_args.lr)\n",
        "    s_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "    steps = style_args.max_step\n",
        "    content_loss_epoch = []\n",
        "    style_loss_epoch = []\n",
        "    total_loss_epoch = []\n",
        "    output_image = content_image\n",
        "    m_cont = torch.mean(content_image,dim=(2,3),keepdim=False).squeeze(0)\n",
        "    m_cont = [m_cont[0].item(),m_cont[1].item(),m_cont[2].item()]\n",
        "    cropper = transforms.Compose([transforms.RandomCrop(style_args.crop_size)])\n",
        "    augment = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n",
        "        transforms.Resize(224)\n",
        "    ])\n",
        "\n",
        "    clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "    prompt = style_args.text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        template_text = compose_text_with_templates(prompt, imagenet_templates)\n",
        "        tokens = clip.tokenize(template_text).to(device)\n",
        "        text_features = clip_model.encode_text(tokens).dcrop_sizech()\n",
        "        text_features = text_features.mean(axis=0, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        template_source = compose_text_with_templates(source, imagenet_templates)\n",
        "        tokens_source = clip.tokenize(template_source).to(device)\n",
        "        text_source = clip_model.encode_text(tokens_source).dcrop_sizech()\n",
        "        text_source = text_source.mean(axis=0, keepdim=True)\n",
        "        text_source /= text_source.norm(dim=-1, keepdim=True)\n",
        "        source_features = clip_model.encode_image(clip_normalize(content_image,device))\n",
        "        source_features /= (source_features.clone().norm(dim=-1, keepdim=True))\n",
        "\n",
        "\n",
        "    num_crops = style_args.num_crops\n",
        "    for epoch in range(0, steps+1):\n",
        "        s_scheduler.step()\n",
        "        target = style_net(content_image,use_sigmoid=True).to(device)\n",
        "        target.requires_grad_(True)\n",
        "        target_features = utils.get_features(img_normalize(target), VGG)\n",
        "        content_loss = 0\n",
        "        content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
        "        content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)\n",
        "        loss_patch=0\n",
        "        img_proc =[]\n",
        "        for n in range(num_crops):\n",
        "            target_crop = cropper(target)\n",
        "            target_crop = augment(target_crop)\n",
        "            img_proc.append(target_crop)\n",
        "        img_proc = torch.cat(img_proc,dim=0)\n",
        "        img_aug = img_proc\n",
        "        image_features = clip_model.encode_image(clip_normalize(img_aug,device))\n",
        "        image_features /= (image_features.clone().norm(dim=-1, keepdim=True))\n",
        "        img_direction = (image_features-source_features)\n",
        "        img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)\n",
        "        text_direction = (text_features-text_source).repeat(image_features.size(0),1)\n",
        "        text_direction /= text_direction.norm(dim=-1, keepdim=True)\n",
        "        loss_temp = (1- torch.cosine_similarity(img_direction, text_direction, dim=1))\n",
        "        loss_temp[loss_temp<style_args.thresh] =0\n",
        "        loss_patch+=loss_temp.mean()\n",
        "        glob_features = clip_model.encode_image(clip_normalize(target,device))\n",
        "        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))\n",
        "        glob_direction = (glob_features-source_features)\n",
        "        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)\n",
        "        loss_glob = (1- torch.cosine_similarity(glob_direction, text_direction, dim=1)).mean()\n",
        "        reg_tv = style_args.lambda_tv*get_image_prior_losses(target)\n",
        "        total_loss = style_args.lambda_patch*loss_patch + content_weight * content_loss+ reg_tv+ style_args.lambda_dir*loss_glob\n",
        "        total_loss_epoch.append(total_loss)\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        autoscroll(True)\n",
        "        if epoch % show_every == 0:\n",
        "            prt(\"After %d iters:\" % epoch)\n",
        "            prt('  Total loss: ', total_loss.item())\n",
        "            prt('  Content loss: ', content_loss.item())\n",
        "            prt('  patch loss: ', loss_patch.item())\n",
        "            prt('  dir loss: ', loss_glob.item())\n",
        "            prt('  TV loss: ', reg_tv.item())\n",
        "\n",
        "        if epoch % show_every == 0:\n",
        "            output_image = target.clone()\n",
        "            output_image = torch.clamp(output_image,0,1)\n",
        "            output_image = adjust_contrast(output_image,1.5)\n",
        "            img = utils.im_convert2(output_image)\n",
        "            save_file = available_file(save_dir, new_file, 1)\n",
        "            img.save(save_file)\n",
        "            prt(Row([ImageButton(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], subtitle=save_file, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Img(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Text(save_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            images.append(save_file)\n",
        "            #plt.imshow(utils.im_convert2(output_image))\n",
        "            #plt.show()\n",
        "        progress.value = (epoch) / steps\n",
        "        progress.tooltip = f'[{(epoch)} / {steps}]'\n",
        "        progress.update()\n",
        "    #clear_last()\n",
        "    # TODO: ESRGAN and copy to GDrive and Metadata\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_semantic(page):\n",
        "    global semantic_prefs, prefs, status, pipe_semantic, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.SemanticGuidance.controls.append(line)\n",
        "      page.SemanticGuidance.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.SemanticGuidance, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.SemanticGuidance.auto_scroll = scroll\n",
        "      page.SemanticGuidance.update()\n",
        "    def clear_list():\n",
        "      page.SemanticGuidance.controls = page.SemanticGuidance.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = semantic_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Semantic Guidance Pipeline...\"))\n",
        "\n",
        "    clear_pipes('semantic')\n",
        "    if pipe_semantic is None:\n",
        "        from diffusers import SemanticStableDiffusionPipeline\n",
        "        pipe_semantic = SemanticStableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "        pipe_semantic = pipeline_scheduler(pipe_semantic)\n",
        "        pipe_semantic = optimize_pipe(pipe_semantic, vae_slicing=False, freeu=False)\n",
        "        pipe_semantic.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_semantic = pipeline_scheduler(pipe_semantic)\n",
        "    clear_last()\n",
        "    prt(\"Generating Semantic Guidance of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, semantic_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    random_seed = int(semantic_prefs['seed']) if int(semantic_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = semantic_prefs['width']\n",
        "    height = semantic_prefs['height']\n",
        "    editing_prompt = []\n",
        "    edit_warmup_steps = []\n",
        "    edit_guidance_scale = []\n",
        "    edit_threshold = []\n",
        "    edit_weights = []\n",
        "    reverse_editing_direction = []\n",
        "    for ep in semantic_prefs['editing_prompts']:\n",
        "        editing_prompt.append(ep['editing_prompt'])\n",
        "        edit_warmup_steps.append(int(ep['edit_warmup_steps']))\n",
        "        edit_guidance_scale.append(ep['edit_guidance_scale'])\n",
        "        edit_threshold.append(ep['edit_threshold'])\n",
        "        edit_weights.append(ep['edit_weights'])\n",
        "        reverse_editing_direction.append(ep['reverse_editing_direction'])\n",
        "    try:\n",
        "      #print(f\"prompt={semantic_prefs['prompt']}, negative_prompt={semantic_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={semantic_prefs['edit_momentum_scale']}, edit_mom_beta={semantic_prefs['edit_mom_beta']}, num_inference_steps={semantic_prefs['num_inference_steps']}, eta={semantic_prefs['eta']}, guidance_scale={semantic_prefs['guidance_scale']}\")\n",
        "      images = pipe_semantic(prompt=semantic_prefs['prompt'], negative_prompt=semantic_prefs['negative_prompt'], editing_prompt=editing_prompt, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=semantic_prefs['edit_momentum_scale'], edit_mom_beta=semantic_prefs['edit_mom_beta'], num_inference_steps=semantic_prefs['num_inference_steps'], eta=semantic_prefs['eta'], guidance_scale=semantic_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=semantic_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Semantic Guidance your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(semantic_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    num = 0\n",
        "    for image in images:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(os.path.join(stable_dir, semantic_prefs['batch_folder_name']), fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not semantic_prefs['display_upscaled_image'] or not semantic_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if semantic_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=semantic_prefs[\"enlarge_scale\"])\n",
        "            image_path = upscaled_path\n",
        "            if semantic_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(semantic_prefs[\"enlarge_scale\"]), height=height * float(semantic_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            from PIL.PngImagePlugin import PngInfo\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {semantic_prefs['enlarge_scale']}x with ESRGAN\" if semantic_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Semantic Guidance\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = semantic_prefs.copy()\n",
        "              config_json['model_path'] = model_path\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_images']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_demofusion(page, from_list=False, with_params=False):\n",
        "    global demofusion_prefs, pipe_demofusion, prefs\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    demofusion_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            demofusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':demofusion_prefs['guidance_scale'], 'steps':demofusion_prefs['steps'], 'width':demofusion_prefs['width'], 'height':demofusion_prefs['height'], 'num_images':demofusion_prefs['num_images'], 'seed':demofusion_prefs['seed']})\n",
        "        else:\n",
        "            demofusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(demofusion_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      demofusion_prompts.append({'prompt': demofusion_prefs['prompt'], 'negative_prompt':demofusion_prefs['negative_prompt'], 'guidance_scale':demofusion_prefs['guidance_scale'], 'steps':demofusion_prefs['steps'], 'width':demofusion_prefs['width'], 'height':demofusion_prefs['height'], 'num_images':demofusion_prefs['num_images'], 'seed':demofusion_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.DemoFusion.controls.append(line)\n",
        "        if update:\n",
        "          page.DemoFusion.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.DemoFusion, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.DemoFusion.auto_scroll = scroll\n",
        "        page.DemoFusion.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.DemoFusion.controls = page.DemoFusion.controls[:1]\n",
        "    #progress = ProgressBar(bar_height=8)\n",
        "    \n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing DemoFusion Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"demofusion\")\n",
        "    #from pipeline_demofusion_sdxl import DemoFusionSDXLPipeline\n",
        "    #model_ckpt = get_SDXL_model(prefs['SDXL_model'])['path']\n",
        "    model_ckpt = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    if pipe_demofusion == None:\n",
        "        #clear_pipes('demofusion')\n",
        "        try:\n",
        "            from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "            #vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "            pipe_demofusion = DiffusionPipeline.from_pretrained(model_ckpt, custom_pipeline=\"pipeline_demofusion_sdxl\", custom_revision=\"main\", torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling unet\")\n",
        "                #pipe_demofusion.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_demofusion.unet = torch.compile(pipe_demofusion.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_demofusion = pipe_demofusion.to(\"cuda\")\n",
        "            elif demofusion_prefs['cpu_offload']:\n",
        "                pipe_demofusion.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_demofusion = pipe_demofusion.to(\"cuda\")\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing DemoFusion...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('demofusion')\n",
        "    clear_last()\n",
        "    txt2img_output = os.path.join(stable_dir, demofusion_prefs['batch_folder_name'])\n",
        "    make_dir(txt2img_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], demofusion_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    s = \"\" if len(demofusion_prompts) == 1 or pr['num_images'] == 1 else \"s\"\n",
        "    progress = Progress(f\"Generating your DemoFusion Image{s}...\")\n",
        "    total_steps = demofusion_prefs['steps']\n",
        "    phase = 0\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "        callback_fnc.has_been_called = True\n",
        "        nonlocal progress, total_steps, phase\n",
        "        if step == 0:\n",
        "            phase += 1\n",
        "        stage = f'...Phase {phase} {\"Decoding\" if (step + 1) == total_steps else \"Denoising\"}'\n",
        "        #total_steps = len(latents)\n",
        "        percent = (step +1)/ total_steps\n",
        "        progress.progress.value = percent\n",
        "        progress.progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "        progress.progress.update()\n",
        "        progress.status(stage)\n",
        "    \n",
        "    for pr in demofusion_prompts:\n",
        "        phase = 0\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_demofusion(\n",
        "                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                sigma=demofusion_prefs['sigma'],\n",
        "                stride=demofusion_prefs['stride'],\n",
        "                view_batch_size=demofusion_prefs['view_batch_size'],\n",
        "                cosine_scale_1=demofusion_prefs['cosine_scale_1'],\n",
        "                cosine_scale_2=demofusion_prefs['cosine_scale_2'],\n",
        "                cosine_scale_3=demofusion_prefs['cosine_scale_3'],\n",
        "                multi_decoder=demofusion_prefs['multi_decoder'],\n",
        "                show_image=demofusion_prefs['show_image'],\n",
        "                num_images_per_prompt=pr['num_images'],\n",
        "                height=pr['height'],\n",
        "                width=pr['width'],\n",
        "                num_inference_steps=pr['steps'],\n",
        "                guidance_scale=pr['guidance_scale'],\n",
        "                generator=generator,\n",
        "                callback=callback_fnc,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{demofusion_prefs[\"file_prefix\"]}{fname}{seed_suffix}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not demofusion_prefs['display_upscaled_image'] or not demofusion_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': demofusion_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if demofusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=demofusion_prefs[\"enlarge_scale\"], faceenhance=demofusion_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                if demofusion_prefs['display_upscaled_image']:\n",
        "                    prt(Row([Img(src=upscaled_path, width=pr['width'] * float(demofusion_prefs[\"enlarge_scale\"]), height=pr['height'] * float(demofusion_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                from PIL.PngImagePlugin import PngInfo\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {demofusion_prefs['enlarge_scale']}x with ESRGAN\" if demofusion_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"DemoFusion\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = demofusion_prefs.copy()\n",
        "                    config_json['model_path'] = model_ckpt\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(batch_output, fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(batch_output, fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_image2text(page):\n",
        "    global fuyu_tokenizer, fuyu_model, fuyu_processor\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.image2text_output.controls.append(line)\n",
        "      page.image2text_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.image2text_output, lines=lines)\n",
        "    if image2text_prefs['use_AIHorde'] and not bool(prefs['AIHorde_api_key']):\n",
        "      alert_msg(page, \"To use AIHorde API service, you must provide an API key in settings\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    if len(image2text_prefs['images']) < 1:\n",
        "      alert_msg(page, \"You must add one or more files to interrogate first... \")\n",
        "      return\n",
        "    if image2text_prefs['method'] == \"Fuyu-8B\":\n",
        "        if not status['installed_diffusers']:\n",
        "            alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "            return\n",
        "        installer = Installing(\"Downloading Fuyu-8B Image2Text Model...\")\n",
        "        prt(installer)\n",
        "        clear_pipes(\"fuyu\")\n",
        "        from transformers import AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor, FuyuProcessor\n",
        "        import torch\n",
        "        dtype = torch.bfloat16\n",
        "        model_id = \"adept/fuyu-8b\"\n",
        "        if fuyu_tokenizer == None:\n",
        "            installer.status(\"...adept/fuyu-8b Tokenizer\")\n",
        "            fuyu_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            installer.status(\"...loading model (see console)\")\n",
        "            fuyu_model = FuyuForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            installer.status(\"...FuyuImageProcessor\")\n",
        "            fuyu_processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=fuyu_tokenizer)\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        prompt_mode = \"What is happening in this image?\" if 'Detailed' in image2text_prefs['fuyu_mode'] else \"Generate a coco-style caption with art style.\\n\" if 'Simple' in image2text_prefs['fuyu_mode'] else image2text_prefs['question']\n",
        "        i2t_prompts = []\n",
        "        clear_last()\n",
        "        for file in image2text_prefs['images']:\n",
        "            prt(f\"Interrogating Images to Describe Prompt...\")\n",
        "            prt(progress)\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            try:\n",
        "                model_inputs = fuyu_processor(text=prompt_mode, images=[image])\n",
        "                model_inputs = {k: v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=torch_device) for k,v in model_inputs.items()}\n",
        "                generation_output = fuyu_model.generate(**model_inputs, max_new_tokens=60)\n",
        "                prompt_len = model_inputs[\"input_ids\"].shape[-1]\n",
        "                prompt = fuyu_tokenizer.decode(generation_output[0][prompt_len:], skip_special_tokens=True).strip()\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Couldn't run Fuyu generate for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            i2t_prompts.append(prompt)\n",
        "            page.add_to_image2text(prompt)\n",
        "    elif image2text_prefs['method'] == \"Google Gemini Pro\":\n",
        "        installer = Installing(\"Installing Google MakerSuite Library......\")\n",
        "        prt(installer)\n",
        "        if not bool(prefs['PaLM_api_key']):\n",
        "          alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "          return\n",
        "        try:\n",
        "          import google.generativeai as genai\n",
        "          if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "        except:\n",
        "          run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "          import google.generativeai as genai\n",
        "          pass\n",
        "        try:\n",
        "          genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "        except:\n",
        "          alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "          return\n",
        "        gemini_model = genai.GenerativeModel(model_name='gemini-pro-vision')\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        prompt_mode = \"What is happening in this image? Describe it in visual details, artistic style, related artist names, colors and composition.\" if 'Detailed' in image2text_prefs['gemini_mode'] else \"Generate an image prompt with art styles, Poetic Captions, flowing adjectives, and detailed captions.\" if 'Poetic' in image2text_prefs['gemini_mode'] else \"Generate a technical detailed caption, describing all subjects, adjectives, styles, observations, colors and technical details to recreate.\" if 'Technical' in image2text_prefs['gemini_mode'] else \"Generate a coco-style caption with art style.\\n\" if 'Simple' in image2text_prefs['gemini_mode'] else \"Describe the style of this art, with a list of all the known artists it resembles and artistic styles it uses, then lay out the image composition with nouns and descriptive adjectives.\" if 'Artistic' in image2text_prefs['gemini_mode'] else image2text_prefs['question']\n",
        "        i2t_prompts = []\n",
        "        clear_last()\n",
        "        for file in image2text_prefs['images']:\n",
        "            prt(f\"Interrogating Images to Describe Prompt...\")\n",
        "            prt(progress)\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            try:\n",
        "                response = gemini_model.generate_content([prompt_mode, image], stream=True, generation_config={'max_output_tokens': 1024})\n",
        "                response.resolve()\n",
        "                prompt = response.text\n",
        "                prompt = prompt.replace('*', '').replace('\\n', ' ').strip()\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Couldn't run Google Gemini Pro Vision request for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            i2t_prompts.append(prompt)\n",
        "            page.add_to_image2text(prompt)\n",
        "    elif image2text_prefs['method'] == \"BLIP-Interrogation\":\n",
        "        installer = Installing(\"Downloading Image2Text CLIP-Interrogator Blips...\")\n",
        "        prt(installer)\n",
        "        '''try:\n",
        "            if transformers.__version__ != \"4.21.3\": # Diffusers conflict\n",
        "              run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "        run_process(\"pip install -q transformers==4.21.3 --upgrade --force-reinstall\", realtime=False)'''\n",
        "        #run_sp(\"pip install --upgrade transformers==4.21.2\", realtime=False)\n",
        "        run_process(\"pip install ftfy regex tqdm timm fairscale requests\", realtime=False)\n",
        "        import importlib\n",
        "        importlib.reload(transformers)\n",
        "        run_process(\"pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\", realtime=False)\n",
        "        run_process(\"pip install -e git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "        run_process(\"pip clone https://github.com/pharmapsychotic/clip-interrogator.git\", realtime=False)\n",
        "            #['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],\n",
        "        #    pass\n",
        "        # Have to force downgrade of transformers because error with cache_dir, but should upgrade after run\n",
        "        run_process(\"pip install clip-interrogator\", realtime=False)\n",
        "\n",
        "        '''def setup():\n",
        "            install_cmds = [\n",
        "                ['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],\n",
        "                ['pip', 'install', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "                ['pip', 'install', 'git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip'],\n",
        "                ['git', 'clone', 'https://github.com/pharmapsychotic/clip-interrogator.git'],\n",
        "                ['pip', 'install', 'clip-interrogator'],\n",
        "            ]\n",
        "            for cmd in install_cmds:\n",
        "                print(subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        setup()'''\n",
        "        #run_sp(\"pip install git+https://github.com/openai/CLIP.git\", realtime=False)\n",
        "        import argparse, sys, time\n",
        "        sys.path.append('src/blip')\n",
        "        sys.path.append('src/clip')\n",
        "        sys.path.append('clip-interrogator')\n",
        "        import clip\n",
        "        import torch\n",
        "        from clip_interrogator import Interrogator, Config\n",
        "        clear_last()\n",
        "        prt(\"Interrogating Images to Describe Prompt... Check console output for progress.\")\n",
        "        prt(progress)\n",
        "        try:\n",
        "            ci = Interrogator(Config())\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"ERROR: Problem running Interrogator. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        def inference(image, mode):\n",
        "            nonlocal ci\n",
        "            image = image.convert('RGB')\n",
        "            if mode == 'best':\n",
        "                return ci.interrogate(image)\n",
        "            elif mode == 'classic':\n",
        "                return ci.interrogate_classic(image)\n",
        "            else:\n",
        "                return ci.interrogate_fast(image)\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        mode = image2text_prefs['mode'].lower() #'best' #param [\"best\",\"classic\", \"fast\"]\n",
        "        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "        files = image2text_prefs['images']\n",
        "        clear_last()\n",
        "        i2t_prompts = []\n",
        "        for file in files:\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            prompt = inference(image, mode)\n",
        "            i2t_prompts.append(prompt)\n",
        "            page.add_to_image2text(prompt)\n",
        "            #thumb = image.copy()\n",
        "            #thumb.thumbnail([256, 256])\n",
        "            #display(thumb)\n",
        "            #print(prompt)\n",
        "        if image2text_prefs['save_csv']:\n",
        "            if len(i2t_prompts):\n",
        "                import csv\n",
        "                csv_path = os.path.join(folder_path, 'img2txt_prompts.csv')\n",
        "                with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
        "                    w = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
        "                    w.writerow(['image', 'prompt'])\n",
        "                    for file, prompt in zip(files, i2t_prompts):\n",
        "                        w.writerow([file, prompt])\n",
        "\n",
        "                prt(f\"\\n\\n\\nGenerated {len(i2t_prompts)} and saved to {csv_path}, enjoy!\")\n",
        "            else:\n",
        "                prt(f\"Sorry, we couldn't find any images in {folder_path}\")\n",
        "        run_process(\"pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "        run_process(\"pip uninstall -y clip-interrogator\", realtime=False)\n",
        "        run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "        run_process(\"pip install --upgrade transformers\", realtime=False)\n",
        "        clear_last()\n",
        "    elif image2text_prefs['method'] == \"AIHorde Crowdsourced\":\n",
        "        if not status['installed_AIHorde']:\n",
        "          prt(Installing(\"Installing AIHorde API Library...\"))\n",
        "          get_AIHorde(page)\n",
        "          clear_last()\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "\n",
        "        api_host = 'https://stablehorde.net/api'\n",
        "        api_check_url = f\"{api_host}/v2/generate/check/\"\n",
        "        api_get_result_url = f\"{api_host}/v2/interrogate/status/\"\n",
        "        interrogate_url = f\"{api_host}/v2/interrogate/async\"\n",
        "        mode = image2text_prefs['mode'] #'best' #param [\"best\",\"classic\", \"fast\"]\n",
        "        headers = {\n",
        "            #'Content-Type': 'application/json',\n",
        "            #'Accept': 'application/json',\n",
        "            'apikey': prefs['AIHorde_api_key'],\n",
        "        }\n",
        "        payload = {\n",
        "            \"forms\": [{\"name\": \"caption\"}],\n",
        "            \"slow_workers\": False if mode == \"fast\" else True,\n",
        "        }\n",
        "        def get_captions(filename):\n",
        "          with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "          captions = []\n",
        "          for line in lines:\n",
        "            if 'caption result:' in line:\n",
        "              captions.append(line.split('caption result: ')[1])\n",
        "          return captions\n",
        "        def interrogate_line(j, cat):\n",
        "          l = []\n",
        "          for t in j[cat]:\n",
        "            l.append(f\"{t['text']}:{round(t['confidence'], 1)}\")\n",
        "          return f\"**{cat.capitalize()}:** {', '.join(l)}\\n  \\n  \"\n",
        "        def get_interrogation(j):\n",
        "          attr = \"\"\n",
        "          for t in j.keys():\n",
        "            attr += interrogate_line(j, t)\n",
        "          return attr\n",
        "        def get_interrogation_cat(j, cat):\n",
        "          l = []\n",
        "          for t in j[cat]:\n",
        "            if t['confidence'] > 3:\n",
        "              l.append(t['text'])\n",
        "          if cat == \"artist\" or cat == \"artists\":\n",
        "            return and_list(l)\n",
        "          else:\n",
        "            return ', '.join(l)\n",
        "        def get_interrogation_prompt(j):\n",
        "          attrs = {}\n",
        "          for t in j.keys():\n",
        "            attrs[t] = get_interrogation_cat(j, t)\n",
        "          full = f\", by {attrs['artist' if 'artist' in attrs else 'artists']}, as {attrs['medium' if 'medium' in attrs else 'mediums']}, style of {attrs['flavors' if 'flavors' in attrs else 'flavor']}, {attrs['tags' if 'tags' in attrs else 'tag']}, {attrs['movement' if 'movement' in attrs else 'movements']}, technique of {attrs['techniques' if 'techniques' in attrs else 'technique']}, trending on {attrs['trending' if 'trending' in attrs else 'sites']}\"\n",
        "          return full\n",
        "        import yaml\n",
        "        AI_Horde = os.path.join(dist_dir, \"AI-Horde-CLI\")\n",
        "        cli_response = os.path.join(AI_Horde, \"cliRequests.log\")\n",
        "        alchemy_yml = os.path.join(AI_Horde, \"cliRequestsData_Alchemy.yml\")\n",
        "        interrogation_txt = os.path.join(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation.txt\")\n",
        "        def make_yml(request):\n",
        "            alchemy = f'''\n",
        "filename: \"horde_alchemy\"\n",
        "api_key: \"{prefs[\"AIHorde_api_key\"]}\"\n",
        "submit_dict:\n",
        "    trusted_workers: {image2text_prefs[\"trusted_workers\"]}\n",
        "    slow_workers: {image2text_prefs[\"slow_workers\"]}\n",
        "    forms:\n",
        "        - name: \"{request}\"'''\n",
        "\n",
        "            al = yaml.safe_load(alchemy)\n",
        "\n",
        "            with open(alchemy_yml, 'w') as file:\n",
        "                yaml.dump(al, file)\n",
        "        if image2text_prefs[\"request_mode\"] == \"Caption\" or image2text_prefs[\"request_mode\"] == \"Interrogation\":\n",
        "            make_yml(image2text_prefs[\"request_mode\"].lower())\n",
        "        elif image2text_prefs[\"request_mode\"] == \"Full Prompt\":\n",
        "            make_yml(\"caption\")\n",
        "        #print(open('names.yaml').read())\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        #files = [f for f in image2text_prefs['images'] if f.endswith('.jpg') or  f.endswith('.png')] else []\n",
        "        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "        #print(f\"Files: {len(files)}\")\n",
        "        i2t_prompts = []\n",
        "        for file in image2text_prefs['images']:\n",
        "            prt(f\"Getting {image2text_prefs['request_mode']} with the Horde to Describe Image...\")\n",
        "            prt(progress)\n",
        "            stats = Text(\"Stable Horde API Interrogation \")\n",
        "            #prt(stats)\n",
        "            img_file = os.path.join(folder_path, file)\n",
        "            run_process(f'python cli_request_alchemy.py --api_key {prefs[\"AIHorde_api_key\"]} --file \"{alchemy_yml}\" --source_image \"{img_file}\"', cwd=AI_Horde, realtime=True)\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            if image2text_prefs[\"request_mode\"] == \"Caption\":\n",
        "                captions = get_captions(cli_response)\n",
        "                for r in captions:\n",
        "                  prompt = to_title(r.strip(), sentence=True)\n",
        "                  i2t_prompts.append(prompt)\n",
        "                  page.add_to_image2text(prompt)\n",
        "                new_log = available_file(AI_Horde, \"cliRequests\", 0, ext=\"log\")\n",
        "                shutil.move(cli_response, new_log)\n",
        "            elif image2text_prefs[\"request_mode\"] == \"Interrogation\":\n",
        "                with open(interrogation_txt) as json_file:\n",
        "                  interrogation = json.load(json_file)\n",
        "                prt(Markdown(get_interrogation(interrogation), selectable=True))\n",
        "                new_interrogation = available_file(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation\", 0, ext=\"txt\")\n",
        "                shutil.move(interrogation_txt, new_interrogation)\n",
        "                os.remove(cli_response)\n",
        "            elif image2text_prefs[\"request_mode\"] == \"Full Prompt\":\n",
        "                captions = get_captions(cli_response)\n",
        "                make_yml(\"interrogation\")\n",
        "                prt(f\"Getting Interrogation with the Horde to Describe Image...\")\n",
        "                prt(progress)\n",
        "                run_process(f'python cli_request_alchemy.py --api_key {prefs[\"AIHorde_api_key\"]} --file \"{alchemy_yml}\" --source_image \"{img_file}\"', cwd=AI_Horde, realtime=True)\n",
        "                clear_last()\n",
        "                clear_last()\n",
        "                with open(interrogation_txt) as json_file:\n",
        "                  interrogation = json.load(json_file)\n",
        "                prt(Markdown(get_interrogation(interrogation), selectable=True))\n",
        "                new_interrogation = available_file(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation\", 0, ext=\"txt\")\n",
        "                shutil.move(interrogation_txt, new_interrogation)\n",
        "                styles = get_interrogation_prompt(interrogation)\n",
        "                for r in captions:\n",
        "                  prompt = to_title(r.strip(), sentence=True)\n",
        "                  i2t_prompts.append(f\"{prompt}{styles}\")\n",
        "                  page.add_to_image2text(f\"{prompt}{styles}\")\n",
        "                new_log = available_file(AI_Horde, \"cliRequests\", 0, ext=\"log\")\n",
        "                shutil.move(cli_response, new_log)\n",
        "            ''' API Method, want to go back to if I can upload source_image\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            buff = io.BytesIO()\n",
        "            image.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            payload['source_image'] = img_str.decode()\n",
        "\n",
        "            response = requests.post(interrogate_url, headers=headers, json=json.dumps(payload))\n",
        "            if response != None:\n",
        "              if response.status_code != 202:\n",
        "                if response.status_code == 400:\n",
        "                  alert_msg(page, \"Stable Horde-API ERROR: Validation Error...\", content=Text(str(response.text)))\n",
        "                  return\n",
        "                else:\n",
        "                  prt(Text(f\"Stable Horde-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "                  print(payload)\n",
        "                  continue\n",
        "            artifacts = json.loads(response.content)\n",
        "            q_id = artifacts['id']\n",
        "            print(f\"ID: {q_id}\")\n",
        "            elapsed_seconds = 0\n",
        "            try:\n",
        "              while True:\n",
        "                check_response = requests.get(api_check_url + q_id)\n",
        "                check = json.loads(check_response.content)\n",
        "                div = check['wait_time'] + elapsed_seconds\n",
        "                if div == 0: continue\n",
        "                try:\n",
        "                  percentage = (1 - check['wait_time'] / div)\n",
        "                except Exception:\n",
        "                  continue\n",
        "                pb.value = percentage\n",
        "                pb.update()\n",
        "                status_txt = f\"Stable Horde API Interrogation - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}\"\n",
        "                stats.value = status_txt #str(check)\n",
        "                stats.update()\n",
        "                if bool(check['done']):\n",
        "                  break\n",
        "                time.sleep(1)\n",
        "                elapsed_seconds += 1\n",
        "            except Exception as e:\n",
        "              alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "              return\n",
        "            get_response = requests.get(api_get_result_url + q_id)\n",
        "            final_results = json.loads(get_response.content)\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            prt(str(final_results))\n",
        "            for r in final_results['forms']:\n",
        "              prompt = r['result']['*']\n",
        "              i2t_prompts.append(prompt)\n",
        "              page.add_to_image2text(prompt)'''\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_BLIP2_image2text(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.BLIP2_image2text_output.controls.append(line)\n",
        "      page.BLIP2_image2text_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.BLIP2_image2text_output, lines=lines)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    prt(Installing(\"Installing BLIP2 Image2Text from Salesforce LAVIS...\"))\n",
        "\n",
        "    try:\n",
        "        import lavis\n",
        "        #from lavis.models import load_model_and_preprocess\n",
        "    except ModuleNotFoundError:\n",
        "        try:\n",
        "            #run_process(\"pip install clip-salesforce-lavis\", page=page, show=True)\n",
        "            run_sp(\"pip install -q salesforce-lavis\", realtime=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"ERROR Installing salesforce-lavis. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from lavis.models import load_model_and_preprocess\n",
        "    import requests\n",
        "    device = torch.device(torch_device)\n",
        "    clear_last()\n",
        "    prt(Installing(\"Downloading BLIP2 Image2Text from Salesforce LAVIS... It's huge, see console for progress.\"))\n",
        "    model_name = \"blip2_t5\"\n",
        "    if '.' in BLIP2_image2text_prefs['model_type']:\n",
        "      model_name = \"blip2_opt\"\n",
        "    if BLIP2_image2text_prefs['model_type'] == \"base\":\n",
        "      model_name = \"img2prompt_vqa\"\n",
        "    try:\n",
        "        model, vis_processors, _ = load_model_and_preprocess(\n",
        "            name=model_name, model_type=BLIP2_image2text_prefs['model_type'], is_eval=True, device=device)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"ERROR: Problem running Interrogator. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    vis_keys = vis_processors.keys()\n",
        "    print(str(vis_keys))\n",
        "    prt(\"  Examining Images to Describe Prompt... Check console output for progress, first run is slow.\")\n",
        "    prt(progress)\n",
        "    folder_path = BLIP2_image2text_prefs['folder_path']\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "    clear_last()\n",
        "    BLIP2_i2t_prompts = []\n",
        "    try:\n",
        "        for file in files:\n",
        "            img = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            image = vis_processors[\"eval\"](img).unsqueeze(0).to(device)\n",
        "            if BLIP2_image2text_prefs['num_captions'] == 1:\n",
        "                prompt = model.generate({\"image\": image})\n",
        "            else:\n",
        "                prompt = model.generate({\"image\": image}, use_nucleus_sampling=True, num_captions=BLIP2_image2text_prefs['num_captions'])\n",
        "            print(prompt)\n",
        "            if isinstance(prompt, str):\n",
        "                BLIP2_i2t_prompts.append(prompt)\n",
        "                page.add_to_BLIP2_image2text(prompt)\n",
        "            else:\n",
        "                for p in prompt:\n",
        "                    BLIP2_i2t_prompts.append(p)\n",
        "                    page.add_to_BLIP2_image2text(p)\n",
        "            if bool(BLIP2_image2text_prefs['question_prompt']):\n",
        "                question = f\"Question: {BLIP2_image2text_prefs['question_prompt']} Answer:\"\n",
        "                answer = model.generate({\"image\": image, \"prompt\": question})\n",
        "                a = answer.rpartition(':')[2].strip()\n",
        "                prt(f\"Answer: {a}\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"ERROR: Problem Generating from Model. Probably out of memory...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    clear_last()\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_dance_diffusion(page):\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    global dance_pipe, dance_prefs\n",
        "    if dance_prefs['dance_model'] == 'Community' or dance_prefs['dance_model'] == 'Custom':\n",
        "      alert_msg(page, \"Custom Community Checkpoints are not functional yet, working on it so check back later... \")\n",
        "      return\n",
        "    from diffusers import DanceDiffusionPipeline\n",
        "    import scipy.io.wavfile, random\n",
        "    try:\n",
        "      import gdown\n",
        "    except ImportError:\n",
        "      run_sp(\"pip -q install gdown\")\n",
        "    finally:\n",
        "      import gdown\n",
        "    #import sys\n",
        "    #sys.path.append('drive/gdrive/MyDrive/NotebookDatasets/CMVRLG')\n",
        "    #print(dir(os))\n",
        "    #print(dir(os.path))\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.dance_output.controls.append(line)\n",
        "      page.dance_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.dance_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Downloading Dance Diffusion Models\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    dance_model_file = f\"harmonai/{dance_prefs['dance_model']}\"\n",
        "    if dance_prefs['dance_model'] == 'Community':\n",
        "      models_path = os.path.join(root_dir, 'dancediffusion_models')\n",
        "      os.makedirs(models_path, exist_ok=True)\n",
        "      for c in community_dance_diffusion_models:\n",
        "        if c['name'] == dance_prefs['community_model']:\n",
        "          community = c\n",
        "      model_out = os.path.join(models_path, format_filename(community['name'], use_dash=True))\n",
        "      if not os.path.exists(model_out):\n",
        "        prt(\"Converting Checkpoint to Diffusers...\")\n",
        "        os.makedirs(model_out, exist_ok=True)\n",
        "        if bool(community['download']):\n",
        "          dance_model_file = os.path.join(models_path, community['ckpt'])\n",
        "          if community['download'].startswith('https://drive'):\n",
        "            gdown.download(community['download'], dance_model_file, quiet=True)\n",
        "          elif community['download'].startswith('http'):\n",
        "            local = download_file(community['download'])\n",
        "            print(f\"Download {community['download']} local:{local}\")\n",
        "            shutil.move(local, os.path.join(models_path, community['ckpt']))\n",
        "          else:\n",
        "            dance_model_file = community['download']\n",
        "          sample_generator = os.path.join(root_dir, 'sample-generator')\n",
        "          if not os.path.exists(sample_generator):\n",
        "            run_process(\"git clone https://github.com/harmonai-org/sample-generator\", page=page, cwd=root_dir)\n",
        "          import sys\n",
        "          sys.path.insert(1, os.path.join(sample_generator, 'audio_diffusion'))\n",
        "          run_sp(f\"pip install {sample_generator}\", cwd=sample_generator, realtime=False)\n",
        "          v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')\n",
        "          if not os.path.exists(v_diffusion_pytorch):\n",
        "            run_sp(\"git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch\", cwd=root_dir, realtime=False)\n",
        "          run_sp(f\"pip install {v_diffusion_pytorch}\", cwd=v_diffusion_pytorch, realtime=False)\n",
        "          run_sp(f\"python {os.path.join(diffusers_dir, 'scripts', 'convert_dance_diffusion_to_diffusers.py')} --model_path {dance_model_file} --checkpoint_path {model_out}\", cwd=os.path.join(sample_generator, 'audio_diffusion'))#os.path.join(diffusers_dir, 'scripts'))\n",
        "          clear_last()\n",
        "          dance_model_file = model_out\n",
        "          #run_sp(f'gdown {community['download']} {dance_model_file}')\n",
        "          #run_sp(f\"wget {community['download']} -O {models_path}\")\n",
        "      else:\n",
        "        dance_model_file = model_out\n",
        "    if dance_prefs['dance_model'] == 'Custom':\n",
        "      models_path = os.path.join(root_dir, 'dancediffusion_models')\n",
        "      os.makedirs(models_path, exist_ok=True)\n",
        "      if bool(dance_prefs['custom_model']):\n",
        "        if dance_prefs['custom_model'].startswith('https://drive'):\n",
        "          dance_model_file = os.path.join(models_path, \"custom_dance.ckpt\")\n",
        "          gdown.download(dance_prefs['custom_model'], dance_model_file, quiet=True)\n",
        "        elif dance_prefs['custom_model'].startswith('http'):\n",
        "          fname = dance_prefs['custom_model'].rpartition('/')[2]\n",
        "          local = download_file(dance_prefs['custom_model'])\n",
        "          dance_model_file = os.path.join(models_path, fname)\n",
        "          print(f\"Download {dance_prefs['custom_model']} local:{local}\")\n",
        "          shutil.move(local, dance_model_file)\n",
        "        else:\n",
        "          dance_model_file = dance_prefs['custom_model']\n",
        "    if dance_prefs['train_custom']:\n",
        "      dance_audio = os.path.join(root_dir, 'dance-audio')\n",
        "      sample_generator = os.path.join(root_dir, 'sample-generator')\n",
        "      if not os.path.exists(sample_generator):\n",
        "        run_process(\"git clone https://github.com/harmonai-org/sample-generator\", page=page, cwd=root_dir)\n",
        "      run_process(f\"pip install {sample_generator}\", page=page, cwd=root_dir, show=True)\n",
        "      v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')\n",
        "      if not os.path.exists(v_diffusion_pytorch):\n",
        "        run_sp(\"git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch\", cwd=root_dir, realtime=False)\n",
        "      run_sp(f\"pip install {v_diffusion_pytorch}\", cwd=v_diffusion_pytorch, realtime=False)\n",
        "      run_cmd = \"python \" + os.path.join(sample_generator, 'train_uncond.py')\n",
        "      custom_name = format_filename(dance_prefs['custom_name'], use_dash=True)\n",
        "      output_dir = os.path.join(dance_audio, custom_name)\n",
        "      output_dir = output_dir.replace(f\" \", f\"\\ \")\n",
        "      random_crop_str = f\"--random-crop True\" if dance_prefs['random_crop'] else \"\"\n",
        "      run_cmd += f''' --ckpt-path {dance_model_file}\\\n",
        "          --name {custom_name}\\\n",
        "          --training-dir {dance_audio}\\\n",
        "          --sample-size {dance_prefs['sammple_size']}\\\n",
        "          --accum-batches {dance_prefs['accumulate_batches']}\\\n",
        "          --sample-rate {dance_prefs['sample_rate']}\\\n",
        "          --batch-size {dance_prefs['finetune_batch_size']}\\\n",
        "          --demo-every {dance_prefs['demo_every']}\\\n",
        "          --checkpoint-every {dance_prefs['checkpoint_every']}\\\n",
        "          --num-workers 2\\\n",
        "          --num-gpus 1\\\n",
        "          {random_crop_str}\\\n",
        "          --save-path {output_dir}'''\n",
        "      prt(\" Training with \" + run_cmd)\n",
        "      prt(progress)\n",
        "      try:\n",
        "        run_process(run_cmd, page=page, cwd=sample_generator, show=True)\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: CUDA Out of Memory, or something else. Try changing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "        return\n",
        "      if dance_prefs['save_model']:\n",
        "        print(\"Upload to HuggingFace (todo)\")\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      #dance_model_file = os.path.join(output_dir, custom_name + '.ckpt')\n",
        "    try:\n",
        "      dance_pipe = DanceDiffusionPipeline.from_pretrained(dance_model_file, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "      dance_pipe = dance_pipe.to(torch_device)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Problem getting DanceDiffusion Pipeline. Try changing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      return\n",
        "    dance_pipe.set_progress_bar_config(disable=True)\n",
        "    random_seed = int(dance_prefs['seed']) if int(dance_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    dance_generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    clear_last()\n",
        "    pb.width=(page.width if page.web else page.window_width) - 50\n",
        "    prt(pb)\n",
        "    if prefs['higher_vram_mode']:\n",
        "      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s']))\n",
        "    else:\n",
        "      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s'])) #, torch_dtype=torch.float16\n",
        "    #, callback=callback_fn, callback_steps=1)\n",
        "    audio = output.audios\n",
        "    audio_slice = audio[0, -3:, -3:]\n",
        "    clear_last()\n",
        "    #prt(f'audio: {type(audio[0])}, audio_slice: {type(audio_slice)}, len:{len(audio)}')\n",
        "    #audio_slice.tofile(\"/content/dance-test.wav\")\n",
        "    audio_name = f\"dance-{dance_prefs['dance_model']}\" + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else '')\n",
        "    audio_local = os.path.join(root_dir, \"audio_out\")\n",
        "    audio_out = audio_local\n",
        "    os.makedirs(audio_local, exist_ok=True)\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_out = prefs['image_output'].rpartition(slash)[0] + slash + 'audio_out'\n",
        "      os.makedirs(audio_out, exist_ok=True)\n",
        "    i = 0\n",
        "    for a in audio:\n",
        "      fname = available_file(audio_local, audio_name, i, ext=\"wav\")\n",
        "      scipy.io.wavfile.write(fname, dance_pipe.unet.sample_rate, a.transpose())\n",
        "      os.path.abspath(fname)\n",
        "      #a_out = Audio(src=fname, autoplay=False)\n",
        "      #page.overlay.append(a_out)\n",
        "      #page.update()\n",
        "      display_name = fname\n",
        "      #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "      if storage_type == \"Colab Google Drive\":\n",
        "        audio_save = available_file(audio_out, audio_name, i, ext='wav')\n",
        "        shutil.copy(fname, audio_save)\n",
        "        display_name = audio_save\n",
        "      prt(AudioPlayer(src=fname, display=display_name, data=audio_save, page=page))\n",
        "      #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "      i += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_audio_diffusion(page):\n",
        "    global audio_diffusion_prefs, pipe_audio_diffusion, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.audio_diffusion_output.controls.append(line)\n",
        "      page.audio_diffusion_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.audio_diffusion_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    #if not bool(audio_diffusion_prefs['text']):\n",
        "    #  alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "    #  return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    total_steps = audio_diffusion_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Audio Diffusion Pipeline...\")\n",
        "    prt(installer)\n",
        "    audio_diffusion_dir = os.path.join(root_dir, \"audio_diffusion\")\n",
        "\n",
        "    try:\n",
        "        import mel\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing mel\")\n",
        "        try:\n",
        "            run_process(\"pip install -q mel\", page=page, show=True, print=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing AudioDiffusion requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        import mel\n",
        "    import scipy.io.wavfile\n",
        "    from diffusers import DiffusionPipeline, DDIMScheduler\n",
        "    model_id = audio_diffusion_prefs['audio_model']\n",
        "    if audio_diffusion_prefs['loaded_model'] != model_id:\n",
        "      clear_pipes()\n",
        "    else:\n",
        "      clear_pipes('audio_diffusion')\n",
        "    if pipe_audio_diffusion == None:\n",
        "      installer.status(\"...initializing audio_diffusion pipe\")\n",
        "      try:\n",
        "          # TODO: Switch DDPM\n",
        "        a_scheduler = DDIMScheduler()\n",
        "        pipe_audio_diffusion = DiffusionPipeline.from_pretrained(model_id, scheduler=a_scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        pipe_audio_diffusion = pipe_audio_diffusion.to(torch_device)\n",
        "        pipe_audio_diffusion.set_progress_bar_config(disable=True)\n",
        "        audio_diffusion_prefs['loaded_model'] = model_id\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error setting up Audio Diffusion Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    init = audio_diffusion_prefs['audio_file']\n",
        "    if init.startswith('http'):\n",
        "        installer.status(\"...downloading audio file\")\n",
        "        init_audio = download_file(init)\n",
        "    else:\n",
        "        if os.path.isfile(init):\n",
        "            init_audio = init\n",
        "        else:\n",
        "            init_audio = None\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Audio Diffusion Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(audio_diffusion_prefs['seed']) if int(audio_diffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    try:\n",
        "        output = pipe_audio_diffusion(audio_file=init_audio, slice=audio_diffusion_prefs['slice'], steps=audio_diffusion_prefs['steps'], start_step=audio_diffusion_prefs['start_step'], mask_start_secs=audio_diffusion_prefs['mask_start_secs'], mask_end_secs=audio_diffusion_prefs['mask_end_secs'], eta=audio_diffusion_prefs['eta'], batch_size=int(audio_diffusion_prefs['batch_size']), generator=generator)#, callback=callback_fnc)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error Generating Audio Diffusion Output\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    images = output.images\n",
        "    audios = output.audios\n",
        "    sample_rate = pipe_audio_diffusion.mel.get_sample_rate()\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', audio_diffusion_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(audio_diffusion_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, audio_diffusion_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audio_diffusion-tts\", 'audio_diffusion', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    a_name = audio_diffusion_prefs['audio_name']\n",
        "    if not bool(a_name):\n",
        "        if bool(init_audio):\n",
        "            a_name = init_audio.rpartition(slash)[2].rpartition('.')[0]\n",
        "        else:\n",
        "            a_name = f\"audio_diffusion-{model_id.rpartition('/')[2]}\"\n",
        "    fname = format_filename(a_name, force_underscore=True)\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = audio_diffusion_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    for image in images:\n",
        "        iname = available_file(save_dir, audio_name, 0)\n",
        "        image.save(iname)\n",
        "        out_path = iname\n",
        "        prt(Row([Img(src=iname, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(prefs['image_output'], fname, 0)\n",
        "            out_path = new_file\n",
        "            shutil.copy(iname, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(prefs['image_output'], fname, 0)\n",
        "            out_path = new_file\n",
        "            shutil.copy(iname, new_file)\n",
        "        #prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    for a in audios:\n",
        "        aname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        #for i in range(waveform.shape[0]):\n",
        "        #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)\n",
        "        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "        #IPython.display.Audio('generated.wav')\n",
        "        scipy.io.wavfile.write(aname, sample_rate, a.transpose())\n",
        "        #a_out = Audio(src=aname, autoplay=False)\n",
        "        #page.overlay.append(a_out)\n",
        "        #page.update()\n",
        "        display_name = aname\n",
        "        #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "            shutil.copy(aname, audio_save)\n",
        "        elif bool(prefs['image_output']):\n",
        "            audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "            shutil.copy(aname, audio_save)\n",
        "        else: audio_save = aname\n",
        "        display_name = audio_save\n",
        "        prt(AudioPlayer(src=aname, display=audio_save, data=audio_save))\n",
        "        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_music_gen(page):\n",
        "    global music_gen_prefs, pipe_music_gen, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.music_gen_output.controls.append(line)\n",
        "      page.music_gen_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.music_gen_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(music_gen_prefs['prompt']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    '''total_steps = music_gen_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()'''\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading MusicGen Pipeline...\")\n",
        "    prt(installer)\n",
        "    music_gen_dir = os.path.join(root_dir, \"music_gen\")\n",
        "    pip_install(\"sentencepiece ffmpeg librosa torchlibrosa\", installer=installer)\n",
        "    try:\n",
        "        import audiocraft\n",
        "        #from audiocraft.models import musicgen\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...facebookresearch/audiocraft\")\n",
        "        #run_sp(\"pip install -U audiocraft\", realtime=True)\n",
        "        #run_sp(\"pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft\", realtime=False)\n",
        "        run_sp(\"pip install -U git+https://github.com/Oncorporation/audiocraft#egg=audiocraft\", realtime=False)\n",
        "        pass\n",
        "    finally:\n",
        "        from audiocraft.models import musicgen\n",
        "        from audiocraft.data.audio import audio_write\n",
        "        from audiocraft.data.audio_utils import apply_fade, apply_tafade\n",
        "        from audiocraft.utils.extend import generate_music_segments, add_settings_to_image, INTERRUPTING\n",
        "    text = music_gen_prefs['prompt']\n",
        "    init = music_gen_prefs['audio_file']\n",
        "    model_id = music_gen_prefs['audio_model'] if not bool(init) else \"melody\"\n",
        "    if music_gen_prefs['loaded_model'] != model_id:\n",
        "        installer.status(\"...clear_pipes\")\n",
        "        clear_pipes()\n",
        "    else:\n",
        "      clear_pipes('music_gen')\n",
        "    if pipe_music_gen == None:\n",
        "        installer.status(f\"...MusicGen pretrained {model_id}\")\n",
        "        try:\n",
        "            pipe_music_gen = musicgen.MusicGen.get_pretrained(model_id, device='cuda')\n",
        "            music_gen_prefs['loaded_model'] = model_id\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error setting up MusicGen Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    def get_melody(melody_filepath):\n",
        "        audio_data= list(librosa.load(melody_filepath, sr=None))\n",
        "        audio_data[0], audio_data[1] = audio_data[1], audio_data[0]\n",
        "        melody = tuple(audio_data)\n",
        "        return melody\n",
        "    for num in range(music_gen_prefs['num_samples']):\n",
        "        if init.startswith('http'):\n",
        "            installer.status(\"...download audio_file\")\n",
        "            melody = download_file(init)\n",
        "        else:\n",
        "            if os.path.isfile(init):\n",
        "                melody = init\n",
        "            else:\n",
        "                melody = None\n",
        "        if melody is not None:\n",
        "            melody = get_melody(melody)\n",
        "        duration = music_gen_prefs['duration']\n",
        "        overlap = music_gen_prefs['overlap']\n",
        "        dimension = music_gen_prefs['dimension']\n",
        "        seed = music_gen_prefs['seed']\n",
        "        output = None\n",
        "        first_chunk = None\n",
        "        total_samples = duration * 50 + 3\n",
        "        segment_duration = duration\n",
        "        initial_duration = duration\n",
        "        output_segments = []\n",
        "        gen_status = Text(\"  Generating MusicGen Sounds...\", weight=FontWeight.BOLD)\n",
        "        prt(gen_status)\n",
        "        prt(progress)\n",
        "        if seed <= 0:\n",
        "            random_seed = rnd.randint(0, 0xffff_ffff_ffff)\n",
        "        else:\n",
        "            random_seed = seed + num\n",
        "        torch.manual_seed(random_seed)\n",
        "        #if melody != None and duration > 30:\n",
        "        #    duration = 30\n",
        "        chunk = 0\n",
        "        while duration > 0:\n",
        "            chunk += 1\n",
        "            if not output_segments: # first pass of long or short song\n",
        "                if segment_duration > pipe_music_gen.lm.cfg.dataset.segment_duration:\n",
        "                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration\n",
        "                else:\n",
        "                    segment_duration = duration\n",
        "            else: # next pass of long song\n",
        "                if duration + overlap < pipe_music_gen.lm.cfg.dataset.segment_duration:\n",
        "                    segment_duration = duration + overlap\n",
        "                else:\n",
        "                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration\n",
        "            gen_status.value = f\"  Generating MusicGen... Segment duration: {segment_duration}, Remaining duration: {duration}, overlap: {overlap}, chunk: {chunk}\"\n",
        "            page.music_gen_output.update()\n",
        "            #print(f'Segment duration: {segment_duration}, duration: {duration}, overlap: {overlap}')\n",
        "            pipe_music_gen.set_generation_params(\n",
        "                use_sampling=music_gen_prefs['use_sampling'],\n",
        "                two_step_cfg=music_gen_prefs['two_step_cfg'],\n",
        "                top_k=music_gen_prefs['top_k'],\n",
        "                top_p=music_gen_prefs['top_p'],\n",
        "                temperature=music_gen_prefs['temperature'],\n",
        "                cfg_coef=music_gen_prefs['guidance'],\n",
        "                duration=segment_duration,\n",
        "                rep_penalty=0.5\n",
        "            )\n",
        "            try:\n",
        "                if melody:\n",
        "                    if duration > pipe_music_gen.lm.cfg.dataset.segment_duration:\n",
        "                        output_segments, duration = generate_music_segments(text, melody, random_seed, pipe_music_gen, duration, overlap, pipe_music_gen.lm.cfg.dataset.segment_duration, prompt_index, harmony_only=music_gen_prefs['harmony_only'])\n",
        "                    else:\n",
        "                        # pure original code\n",
        "                        sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)\n",
        "                        print(melody.shape)\n",
        "                        if melody.dim() == 2:\n",
        "                            melody = melody[None]\n",
        "                        melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]\n",
        "                        output = pipe_music_gen.generate_with_chroma(\n",
        "                            descriptions=[text],\n",
        "                            melody_wavs=melody,\n",
        "                            melody_sample_rate=sr,\n",
        "                            progress=False\n",
        "                        )\n",
        "                    # All output_segments are populated, so we can break the loop or set duration to 0\n",
        "                    break\n",
        "                    '''import torchaudio\n",
        "                    sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)\n",
        "                    melody, sr = torchaudio.load(init_audio)\n",
        "                    #sr = 32000\n",
        "                    #melody = torch.from_numpy(init_audio).to(pipe_music_gen.device).float().t().unsqueeze(0)\n",
        "                    #if melody.dim() == 2:\n",
        "                    #    melody = melody[None]\n",
        "                    melody = melody[None].expand(3, -1, -1)\n",
        "                    #melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]\n",
        "                    output = pipe_music_gen.generate_with_chroma(\n",
        "                        descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']],\n",
        "                        melody_wavs=melody,\n",
        "                        melody_sample_rate=sr,\n",
        "                        progress=True\n",
        "                    )\n",
        "                    duration -= segment_duration'''\n",
        "                else:\n",
        "                    if not output_segments:\n",
        "                        next_segment = pipe_music_gen.generate(descriptions=[text], progress=False)\n",
        "                        duration -= segment_duration\n",
        "                    else:\n",
        "                        last_chunk = output_segments[-1][:, :, -overlap*pipe_music_gen.sample_rate:]\n",
        "                        next_segment = pipe_music_gen.generate_continuation(last_chunk, pipe_music_gen.sample_rate, descriptions=[text], progress=True)\n",
        "                        duration -= segment_duration - overlap\n",
        "                    output_segments.append(next_segment)\n",
        "                    '''if output is None:\n",
        "                        next_segment = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)\n",
        "                                                      #progress=updateProgress)\n",
        "                        duration -= segment_duration\n",
        "                    else:\n",
        "                        if first_chunk is None and pipe_music_gen.name == \"melody\" and music_gen_prefs['recondition']:\n",
        "                            first_chunk = output[:, :, :pipe_music_gen.lm.cfg.dataset.segment_duration*pipe_music_gen.sample_rate]\n",
        "                        last_chunk = output[:, :, -overlap*pipe_music_gen.sample_rate:]\n",
        "                        next_segment = pipe_music_gen.generate_continuation(last_chunk,\n",
        "                            pipe_music_gen.sample_rate, descriptions=[[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']]], progress=True) #, melody_wavs=(first_chunk), resample=False\n",
        "                        duration -= segment_duration - overlap'''\n",
        "                    #output = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)\n",
        "                #if output is None:\n",
        "                #    output = next_segment\n",
        "                #else:\n",
        "                #    output = torch.cat([output[:, :, :-overlap*pipe_music_gen.sample_rate], next_segment], 2)\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error Generating Music Output\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            #output = output.detach().cpu().float()[0]\n",
        "        if output_segments:\n",
        "            try:\n",
        "                output = output_segments[0]\n",
        "                for i in range(1, len(output_segments)):\n",
        "                    if overlap > 0:\n",
        "                        overlap_samples = overlap * pipe_music_gen.sample_rate\n",
        "                        overlapping_output_fadeout = output[:, :, -overlap_samples:]\n",
        "                        #overlapping_output_fadeout = apply_fade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True, curve_end=0.0, current_device=pipe_music_gen.device)\n",
        "                        overlapping_output_fadeout = apply_tafade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True,shape=\"linear\")\n",
        "                        overlapping_output_fadein = output_segments[i][:, :, :overlap_samples]\n",
        "                        #overlapping_output_fadein = apply_fade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, curve_start=0.0, current_device=pipe_music_gen.device)\n",
        "                        overlapping_output_fadein = apply_tafade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, shape=\"linear\")\n",
        "                        overlapping_output = torch.cat([overlapping_output_fadeout[:, :, :-(overlap_samples // 2)], overlapping_output_fadein],dim=2)\n",
        "                        gen_status.value = f\"  Saving MusicGen... Overlap size Fade:{overlapping_output.size()}\\n output: {output.size()}\\n segment: {output_segments[i].size()}\"\n",
        "                        page.music_gen_output.update()\n",
        "                        #print(f\" Overlap size Fade:{overlapping_output.size()}\\n output: {output.size()}\\n segment: {output_segments[i].size()}\")\n",
        "                        output = torch.cat([output[:, :, :-overlap_samples], overlapping_output, output_segments[i][:, :, overlap_samples:]], dim=dimension)\n",
        "                    else:\n",
        "                        output = torch.cat([output, output_segments[i]], dim=dimension)\n",
        "                output = output.detach().cpu().float()[0]\n",
        "            except Exception as e:\n",
        "                alert_msg(page, f\"Error combining segments: {e}. Using the first segment only.\")\n",
        "                output = output_segments[0].detach().cpu().float()[0]\n",
        "        else:\n",
        "            try:\n",
        "                output = output.detach().cpu().float()[0]\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error Saving Music Output\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "\n",
        "        #sample_rate = pipe_music_gen.mel.get_sample_rate()\n",
        "        save_dir = os.path.join(root_dir, 'audio_out', music_gen_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "        if bool(music_gen_prefs['batch_folder_name']):\n",
        "            audio_out = os.path.join(audio_out, music_gen_prefs['batch_folder_name'])\n",
        "        os.makedirs(audio_out, exist_ok=True)\n",
        "        #voice_dirs = os.listdir(os.path.join(root_dir, \"music_gen-tts\", 'music_gen', 'voices'))\n",
        "        #print(str(voice_dirs))\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        a_name = music_gen_prefs['audio_name']\n",
        "        if not bool(a_name):\n",
        "            a_name = music_gen_prefs['prompt']\n",
        "        fname = format_filename(a_name, force_underscore=True)\n",
        "        if fname[-1] == '.': fname = fname[:-1]\n",
        "        audio_name = f'{music_gen_prefs[\"file_prefix\"]}{fname}'\n",
        "        audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "        #audios = output.audios\n",
        "        idx = 0\n",
        "        for wav in output:\n",
        "            aname = available_file(save_dir, audio_name, num + idx, ext=\"wav\")\n",
        "            with open(aname, \"wb\") as file:\n",
        "                audio_write(file.name, wav.cpu(), pipe_music_gen.sample_rate, strategy=\"loudness\", loudness_compressor=True, loudness_headroom_db=18, add_suffix=False, channels=2)\n",
        "                #waveform_video = gr.make_waveform(file.name)\n",
        "            #for i in range(waveform.shape[0]):\n",
        "            #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)\n",
        "            #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "            #IPython.display.Audio('generated.wav')\n",
        "            #scipy.io.wavfile.write(aname, sample_rate, a.transpose())\n",
        "            #a_out = Audio(src=aname, autoplay=False)\n",
        "            #page.overlay.append(a_out)\n",
        "            #page.update()\n",
        "            display_name = aname\n",
        "            #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')\n",
        "                shutil.copy(aname, audio_save)\n",
        "            elif bool(prefs['image_output']):\n",
        "                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')\n",
        "                shutil.copy(aname, audio_save)\n",
        "            else: audio_save = aname\n",
        "            display_name = audio_save\n",
        "            prt(AudioPlayer(src=aname, display=audio_save, data=audio_out, page=page))\n",
        "            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "            idx += 1\n",
        "        output = None\n",
        "        first_chunk = None\n",
        "        output_segments = []\n",
        "    flush()\n",
        "    torch.cuda.ipc_collect()\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "# https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb\n",
        "\n",
        "def run_dreambooth(page):\n",
        "    global dreambooth_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.dreambooth_output.controls.append(line)\n",
        "      page.dreambooth_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.dreambooth_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.db_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    prt(Installing(\"Downloading DreamBooth Conceptualizers\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    os.chdir(diffusers_dir)\n",
        "    #run_process('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    run_process('pip install \"git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]\"', cwd=root_dir, realtime=False)\n",
        "    dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    os.chdir(dreambooth_dir)\n",
        "    run_process(\"pip install -r requirements.txt\", cwd=dreambooth_dir, realtime=False)\n",
        "    try:\n",
        "      os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "      import bitsandbytes\n",
        "    except ModuleNotFoundError:\n",
        "      if sys.platform.startswith(\"win\"):\n",
        "          run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "      else:\n",
        "          run_sp(\"pip install bitsandbytes\", realtime=False)\n",
        "      import bitsandbytes\n",
        "      pass\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    #save_path = \"./my_concept\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    num_new_images = None\n",
        "\n",
        "    from argparse import Namespace\n",
        "    dreambooth_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        resolution=dreambooth_prefs['max_size'],\n",
        "        center_crop=True,\n",
        "        instance_data_dir=save_path,\n",
        "        instance_prompt=dreambooth_prefs['instance_prompt'].strip(),\n",
        "        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,\n",
        "        max_train_steps=dreambooth_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=dreambooth_prefs['train_batch_size'],\n",
        "        gradient_accumulation_steps=2,\n",
        "        max_grad_norm=1.0,\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=dreambooth_prefs['seed'],#3434554,\n",
        "        with_prior_preservation=dreambooth_prefs['prior_preservation'],\n",
        "        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],\n",
        "        sample_batch_size=dreambooth_prefs['sample_batch_size'],\n",
        "        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'],\n",
        "        class_prompt=dreambooth_prefs['prior_preservation_class_prompt'],\n",
        "        num_class_images=dreambooth_prefs['num_class_images'],\n",
        "        output_dir=os.path.join(root_dir, \"dreambooth-concept\"),\n",
        "    )\n",
        "    if not os.path.exists(dreambooth_args.output_dir): os.mkdir(dreambooth_args.output_dir)\n",
        "    arg_str = \"accelerate launch ./train_dreambooth.py\"\n",
        "    for k, v in vars(dreambooth_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(\"***** Running training *****\")\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {dreambooth_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    try:\n",
        "      run_process(arg_str, page=page, cwd=dreambooth_dir)\n",
        "      #run_sp(arg_str, cwd=dreambooth_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    name_of_your_concept = dreambooth_prefs['name_of_your_concept']\n",
        "    if(dreambooth_prefs['save_concept']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import create_repo\n",
        "      from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        dreambooth_args.output_dir,\n",
        "        torch_dtype=torch.float16,\n",
        "      ).to(\"cuda\")\n",
        "      os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      dreambooth_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      if(dreambooth_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "        repo_id = f\"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        #Join the Concepts Library organization if you aren't part of it already\n",
        "        run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\", realtime=False)\n",
        "        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n",
        "      else:\n",
        "        repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "      output_dir = dreambooth_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "\n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "'''\n",
        "      description = dreambooth_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = dreambooth_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_concept} on Stable Diffusion via Dreambooth using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n",
        "It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**\n",
        "\n",
        "{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "Here are the images used for training this concept:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      readme_file = open(\"README.md\", \"w\")\n",
        "      readme_file.write(readme_text)\n",
        "      readme_file.close()\n",
        "      #Save the token identifier to a file\n",
        "      text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      text_file.write(dreambooth_prefs['instance_prompt'])\n",
        "      text_file.close()\n",
        "      operations = [\n",
        "        CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      ]\n",
        "      print(repo_id)\n",
        "      print(readme_text)\n",
        "      try:\n",
        "        create_repo(repo_id, private=True, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",token=hf_token)\n",
        "      api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      api.upload_folder(folder_path=save_path, path_in_repo=\"concept_images\", repo_id=repo_id, token=hf_token)\n",
        "      prefs['custom_model'] = repo_id\n",
        "      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "      page.custom_model.value = repo_id\n",
        "      try:\n",
        "        page.custom_model.update()\n",
        "      except Exception: pass\n",
        "      prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_textualinversion(page):\n",
        "    global textualinversion_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.textualinversion_output.controls.append(line)\n",
        "      page.textualinversion_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.textualinversion_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.ti_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    page.textualinversion_output.controls.clear()\n",
        "    page.textualinversion_output.update()\n",
        "    prt(Installing(\"Downloading Textual-Inversion Training Models\"))\n",
        "\n",
        "    placeholder_token = textualinversion_prefs['placeholder_token'].strip()\n",
        "    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token\n",
        "    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'\n",
        "    initializer_token = textualinversion_prefs['initializer_token'].strip()\n",
        "    if bool(initializer_token):\n",
        "      if not initializer_token.startswith('<'): initializer_token = '<' + initializer_token\n",
        "      if not initializer_token.endswith('>'): initializer_token = initializer_token + '>'\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    textualinversion_dir = os.path.join(diffusers_dir, \"examples\", \"textual_inversion\")\n",
        "    #textualinversion_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(textualinversion_dir)\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=textualinversion_dir, realtime=False)\n",
        "    run_process(\"pip install -qq bitsandbytes\", page=page)\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(textualinversion_prefs['seed']) if int(textualinversion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = textualinversion_prefs['name_of_your_model']\n",
        "    from argparse import Namespace\n",
        "    textualinversion_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path if not textualinversion_prefs['use_SDXL'] else \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        resolution=textualinversion_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        #train_data_dir=save_path,\n",
        "        #caption_column=textualinversion_prefs['instance_prompt'].strip(),\n",
        "        train_data_dir=save_path,\n",
        "        validation_prompt=textualinversion_prefs['validation_prompt'].strip(),\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=initializer_token,\n",
        "        learnable_property=textualinversion_prefs['what_to_teach'],\n",
        "        learning_rate=textualinversion_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=textualinversion_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=textualinversion_prefs['lr_warmup_steps'],\n",
        "        scale_lr=textualinversion_prefs['scale_lr'],\n",
        "        max_train_steps=textualinversion_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=textualinversion_prefs['train_batch_size'],\n",
        "        checkpointing_steps=textualinversion_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=textualinversion_prefs['gradient_accumulation_steps'],\n",
        "        validation_steps=textualinversion_prefs['validation_steps'],\n",
        "        num_vectors=textualinversion_prefs['num_vectors'],\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        #with_prior_preservation=textualinversion_prefs['prior_preservation'],\n",
        "        #prior_loss_weight=textualinversion_prefs['prior_loss_weight'],\n",
        "        #sample_batch_size=textualinversion_prefs['sample_batch_size'],\n",
        "        #class_data_dir=textualinversion_prefs['class_data_dir'],\n",
        "        #class_prompt=textualinversion_prefs['class_prompt'],\n",
        "        num_class_images=textualinversion_prefs['num_class_images'],\n",
        "        output_dir=os.path.join(save_path, format_filename(textualinversion_prefs['name_of_your_concept'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = textualinversion_args.output_dir\n",
        "    if textualinversion_prefs['use_SDXL']:\n",
        "        arg_str = \"accelerate launch textual_inversion_sdxl.py\"\n",
        "    else:\n",
        "        arg_str = \"accelerate launch textual_inversion.py\"\n",
        "    #arg_str = 'accelerate --mixed_precision=\"fp16\" launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(textualinversion_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running Training *** See Console for Progress\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {textualinversion_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {textualinversion_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {textualinversion_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(textualinversion_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import Repository, create_repo, whoami\n",
        "      #from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      #textualinversion_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      #  textualinversion_args.output_dir,\n",
        "      #  torch_dtype=torch.float16,\n",
        "      #).to(\"cuda\")\n",
        "      #os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      #textualinversion_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = False if textualinversion_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "      output_dir = textualinversion_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      os.chdir(textualinversion_dir)\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.system(arg_str)\n",
        "      #run_sp(arg_str, cwd=textualinversion_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=textualinversion_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "\n",
        "    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?\n",
        "    save_concept_to_public_library = textualinversion_prefs['save_concept']\n",
        "    name_of_your_concept = textualinversion_prefs['name_of_your_concept']\n",
        "    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n",
        "    hf_token_write = prefs['HuggingFace_api_key']\n",
        "\n",
        "    if(save_concept_to_public_library):\n",
        "        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "        from huggingface_hub import create_repo\n",
        "        api = HfApi()\n",
        "        your_username = api.whoami()[\"name\"]\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        #output_dir = textualinversion_prefs[\"output_dir\"]\n",
        "        if(not hf_token_write):\n",
        "            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "        else:\n",
        "            hf_token = hf_token_write\n",
        "        if(textualinversion_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "            #Join the Concepts Library organization if you aren't part of it already\n",
        "            run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\", realtime=False)\n",
        "            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\n",
        "        else:\n",
        "            repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        images_upload = os.listdir(save_path)\n",
        "        image_string = \"\"\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        for i, image in enumerate(images_upload):\n",
        "            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "        '''\n",
        "        if(textualinversion_prefs['what_to_teach'] == \"style\"):\n",
        "            what_to_teach_article = f\"a `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        else:\n",
        "            what_to_teach_article = f\"an `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        description = textualinversion_prefs['readme_description']\n",
        "        if bool(description.strip()):\n",
        "            description = textualinversion_prefs['readme_description'] + '\\n\\n'\n",
        "        readme_text = f'''---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path if not textualinversion_prefs['use_SDXL'] else \"stabilityai/stable-diffusion-xl-base-1.0\"}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- textual_inversion\n",
        "inference: true\n",
        "---\n",
        "# Textual inversion text2image fine-tuning - {repo_id}\n",
        "### {name_of_your_concept} by {your_username} using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).\n",
        "\n",
        "{description}Here is the new concept you will be able to use as {what_to_teach_article}:\n",
        "{image_string}\n",
        "'''\n",
        "        #Save the readme to a file\n",
        "        readme_file = open(\"README.md\", \"w\")\n",
        "        readme_file.write(readme_text)\n",
        "        readme_file.close()\n",
        "        #Save the token identifier to a file\n",
        "        text_file = open(\"token_identifier.txt\", \"w\")\n",
        "        text_file.write(placeholder_token)\n",
        "        text_file.close()\n",
        "        #Save the type of teached thing to a file\n",
        "        type_file = open(\"type_of_concept.txt\",\"w\")\n",
        "        type_file.write(textualinversion_prefs['what_to_teach'])\n",
        "        type_file.close()\n",
        "        operations = [\n",
        "            CommitOperationAdd(path_in_repo=\"learned_embeds.bin\", path_or_fileobj=f\"{output_dir}/learned_embeds.bin\"),\n",
        "            CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"type_of_concept.txt\", path_or_fileobj=\"type_of_concept.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "        ]\n",
        "        try:\n",
        "          create_repo(repo_id, private=True, token=hf_token)\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "        api = HfApi()\n",
        "        api.create_commit(\n",
        "            repo_id=repo_id,\n",
        "            operations=operations,\n",
        "            commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        api.upload_folder(\n",
        "            folder_path=save_path,\n",
        "            path_in_repo=\"concept_images\",\n",
        "            repo_id=repo_id,\n",
        "            token=hf_token\n",
        "        )\n",
        "        prefs['custom_model'] = repo_id\n",
        "        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "        page.custom_model.value = repo_id\n",
        "        try:\n",
        "          page.custom_model.update()\n",
        "        except Exception: pass\n",
        "        prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_LoRA_dreambooth(page):\n",
        "    global LoRA_dreambooth_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.LoRA_dreambooth_output.controls.append(line)\n",
        "      page.LoRA_dreambooth_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.LoRA_dreambooth_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_model\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.lora_dreambooth_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.LoRA_dreambooth_output.controls.clear()\n",
        "    page.LoRA_dreambooth_output.update()\n",
        "    prt(Installing(\"Downloading LoRA DreamBooth Conceptualizers\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    LoRA_dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    #LoRA_dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(LoRA_dreambooth_dir)\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=LoRA_dreambooth_dir, realtime=False)\n",
        "    run_process(\"pip install -qq bitsandbytes\", page=page)\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    #save_path = \"./my_model\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(LoRA_dreambooth_prefs['seed']) if int(LoRA_dreambooth_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = LoRA_dreambooth_prefs['name_of_your_model']\n",
        "    from argparse import Namespace\n",
        "    LoRA_dreambooth_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path if not LoRA_dreambooth_prefs['use_SDXL'] else \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        resolution=LoRA_dreambooth_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        mixed_precision=\"fp16\",\n",
        "        #train_data_dir=save_path,\n",
        "        #caption_column=LoRA_dreambooth_prefs['instance_prompt'].strip(),\n",
        "        instance_data_dir=save_path,\n",
        "        instance_prompt=LoRA_dreambooth_prefs['instance_prompt'].strip(),\n",
        "        learning_rate=LoRA_dreambooth_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=LoRA_dreambooth_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=LoRA_dreambooth_prefs['lr_warmup_steps'],\n",
        "        lr_num_cycles=LoRA_dreambooth_prefs['lr_num_cycles'],\n",
        "        lr_power=LoRA_dreambooth_prefs['lr_power'],\n",
        "        scale_lr=LoRA_dreambooth_prefs['scale_lr'],\n",
        "        max_train_steps=LoRA_dreambooth_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=LoRA_dreambooth_prefs['train_batch_size'],\n",
        "        checkpointing_steps=LoRA_dreambooth_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=LoRA_dreambooth_prefs['gradient_accumulation_steps'],\n",
        "        max_grad_norm=1.0,\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        with_prior_preservation=LoRA_dreambooth_prefs['prior_preservation'],\n",
        "        prior_loss_weight=LoRA_dreambooth_prefs['prior_loss_weight'],\n",
        "        rank=4,#LoRA_dreambooth_prefs['rank'], TODO: \"The dimension of the LoRA update matrices.\"\n",
        "        sample_batch_size=LoRA_dreambooth_prefs['sample_batch_size'],\n",
        "        #class_data_dir=LoRA_dreambooth_prefs['class_data_dir'],\n",
        "        #class_prompt=LoRA_dreambooth_prefs['class_prompt'],\n",
        "        num_class_images=LoRA_dreambooth_prefs['num_class_images'],\n",
        "        output_dir=os.path.join(root_dir, \"LoRA-model\", format_filename(LoRA_dreambooth_prefs['name_of_your_model'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = LoRA_dreambooth_args.output_dir\n",
        "    if not os.path.exists(os.path.join(root_dir, \"LoRA-model\")): os.makedirs(os.path.join(root_dir, \"LoRA-model\"))\n",
        "    if LoRA_dreambooth_prefs['use_SDXL']:\n",
        "      arg_str = \"launch train_dreambooth_lora_sdxl.py\"\n",
        "    else:\n",
        "      arg_str = \"launch train_dreambooth_lora.py\"\n",
        "    #arg_str = 'accelerate --mixed_precision=\"fp16\" launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(LoRA_dreambooth_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running training ***\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {LoRA_dreambooth_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {LoRA_dreambooth_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {LoRA_dreambooth_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(LoRA_dreambooth_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import Repository, create_repo, whoami\n",
        "      #from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      #LoRA_dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      #  LoRA_dreambooth_args.output_dir,\n",
        "      #  torch_dtype=torch.float16,\n",
        "      #).to(\"cuda\")\n",
        "      #os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      #LoRA_dreambooth_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = False if LoRA_dreambooth_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "      output_dir = LoRA_dreambooth_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      #%cd $LoRA_dreambooth_dir # type: ignore\n",
        "      os.chdir(LoRA_dreambooth_dir)\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.system(\"accelerate\" + arg_str)\n",
        "      #run_sp(arg_str, cwd=LoRA_dreambooth_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=LoRA_dreambooth_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    if(LoRA_dreambooth_prefs['save_model']):\n",
        "      model_images = os.path.join(output_dir, 'model_images')\n",
        "      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)\n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-LoRA-library/{slugify(name_of_your_model)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          img_name = f\"image_{i}.png\"\n",
        "          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))\n",
        "          #image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
        "          #img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
        "          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})\n",
        "'''\n",
        "      description = LoRA_dreambooth_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = LoRA_dreambooth_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This is a model fine-tuned on the {model_path} model taught to Stable Diffusion with LoRA.\n",
        "It can be used by modifying the `instance_prompt`: **{LoRA_dreambooth_prefs['instance_prompt']}**\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)\n",
        "\n",
        "Here are the images used for training this model:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      #readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      #readme_file.write(readme_text)\n",
        "      #readme_file.close()\n",
        "      yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "inference: true\n",
        "---\n",
        "      \"\"\"\n",
        "      model_card = f\"\"\"\n",
        "# LoRA DreamBooth - {name_of_your_model}\n",
        "These are LoRA adaption weights for {name_of_your_model}. The weights were trained on {LoRA_dreambooth_args.instance_prompt} using [DreamBooth](https://dreambooth.github.io/).\\n\n",
        "### {repo_id} on Stable Diffusion via LoRA Dreambooth using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {api.whoami()[\"name\"]}\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "\n",
        "Images used for training this model:\n",
        "{image_string}\n",
        "\"\"\"\n",
        "      readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      readme_file.write(yaml + model_card)#(readme_text)\n",
        "      readme_file.close()\n",
        "      #with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
        "      #    f.write(yaml + model_card)\n",
        "      #Save the token identifier to a file\n",
        "      #text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      #text_file.write(LoRA_dreambooth_prefs['instance_prompt'])\n",
        "      #text_file.close()\n",
        "      #operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        #CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      #]\n",
        "      print(repo_id)\n",
        "      print(model_card)\n",
        "\n",
        "\n",
        "      with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "        if \"step_*\" not in gitignore:\n",
        "            gitignore.write(\"step_*\\n\")\n",
        "        if \"epoch_*\" not in gitignore:\n",
        "            gitignore.write(\"epoch_*\\n\")\n",
        "      try:\n",
        "        #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "        #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "        repo.push_to_hub(commit_message=f\"Upload the LoRA model {name_of_your_model} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      prefs['LoRA_dreambooth_model'] = name_of_your_model\n",
        "      prefs['custom_models'].append({'name': name_of_your_model, 'path':repo_id})\n",
        "      page.LoRA_dreambooth_model.options.insert(0, dropdown.Option(name_of_your_model))\n",
        "      page.LoRA_dreambooth_model.value = name_of_your_model\n",
        "      page.LoRA_dreambooth_model.update()\n",
        "      save_settings_file(page)\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_LoRA(page):\n",
        "    global LoRA_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.LoRA_output.controls.append(line)\n",
        "      page.LoRA_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.LoRA_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_model\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.lora_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.LoRA_output.controls.clear()\n",
        "    page.LoRA_output.update()\n",
        "    installer = Installing(\"Downloading LoRA Conceptualizers\")\n",
        "    prt(installer)\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      installer.status(\"...clone diffusers\")\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    installer.status(\"...install training\")\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    #LoRA_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    LoRA_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(LoRA_dir)\n",
        "    installer.status(\"...installing requirements\")\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=LoRA_dir, realtime=False)\n",
        "    pip_install(\"bitsandbytes\", installer=installer)\n",
        "    installer.status(\"...accelerate config\")\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "    from huggingface_hub import Repository, create_repo, whoami\n",
        "    #from diffusers import StableDiffusionPipeline\n",
        "    api = HfApi()\n",
        "    your_username = api.whoami()[\"name\"]\n",
        "    hf_token = prefs['HuggingFace_api_key']\n",
        "    metadata_jsonl = []\n",
        "    for fl in page.lora_file_list.controls:\n",
        "        f = fl.title.value\n",
        "        fn = f.rpartition(slash)[2]\n",
        "        text = fl.subtitle.value\n",
        "        metadata_jsonl.append({'file_name':fn, 'text':text})\n",
        "    with open(os.path.join(save_path, \"metadata.jsonl\"), \"w\") as f:\n",
        "        for meta in metadata_jsonl:\n",
        "          print(json.dumps(meta), file=f)\n",
        "        #json.dump(metadata_jsonl, f, ensure_ascii=False, indent=4)\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(LoRA_prefs['seed']) if int(LoRA_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = LoRA_prefs['name_of_your_model']\n",
        "    repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "    from argparse import Namespace\n",
        "    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100\n",
        "    LoRA_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        #dataset_name=repo_id,\n",
        "        train_data_dir=save_path,\n",
        "        resolution=LoRA_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        image_column=\"image\",\n",
        "        caption_column=\"text\",\n",
        "        #caption_column=LoRA_prefs['instance_prompt'].strip(),\n",
        "        #instance_data_dir=save_path,\n",
        "        validation_prompt=LoRA_prefs['validation_prompt'].strip(),\n",
        "        num_validation_images = LoRA_prefs['num_validation_images'],\n",
        "        validation_epochs=LoRA_prefs['validation_epochs'],\n",
        "        learning_rate=LoRA_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=LoRA_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=LoRA_prefs['lr_warmup_steps'],\n",
        "        #lr_num_cycles=LoRA_prefs['lr_num_cycles'],\n",
        "        #lr_power=LoRA_prefs['lr_power'],\n",
        "        scale_lr=LoRA_prefs['scale_lr'],\n",
        "        max_train_steps=LoRA_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=LoRA_prefs['train_batch_size'],\n",
        "        checkpointing_steps=LoRA_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=LoRA_prefs['gradient_accumulation_steps'],\n",
        "        max_grad_norm=1.0,\n",
        "        mixed_precision=\"fp16\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=LoRA_prefs['gradient_checkpointing'], # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        with_prior_preservation=LoRA_prefs['prior_preservation'],\n",
        "        #prior_loss_weight=LoRA_prefs['prior_loss_weight'],\n",
        "        #sample_batch_size=LoRA_prefs['sample_batch_size'],\n",
        "        #class_data_dir=LoRA_prefs['class_data_dir'],\n",
        "        #class_prompt=LoRA_prefs['class_prompt'],\n",
        "        #num_class_images=LoRA_prefs['num_class_images'],\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        hub_model_id=repo_id,\n",
        "        output_dir=os.path.join(root_dir, \"LoRA-model\", format_filename(LoRA_prefs['name_of_your_model'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = LoRA_args.output_dir\n",
        "    if not os.path.exists(os.path.join(root_dir, \"LoRA-model\")): os.makedirs(os.path.join(root_dir, \"LoRA-model\"))\n",
        "    #arg_str = \"accelerate launch train_dreambooth_lora.py\"\n",
        "    arg_str = 'launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(LoRA_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool) or v == None:\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running training ***\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {LoRA_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {LoRA_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {LoRA_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(LoRA_prefs['save_model']):\n",
        "      private = False if LoRA_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      output_dir = LoRA_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      #run_sp(\"accelerate \" + arg_str, cwd=LoRA_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=LoRA_dir)\n",
        "      #%cd $LoRA_dir # type: ignore\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.chdir(LoRA_dir)\n",
        "      os.system(\"accelerate\" + arg_str)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    if(LoRA_prefs['save_model']):\n",
        "      model_images = os.path.join(output_dir, 'model_images')\n",
        "      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)\n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-LoRA-library/{slugify(name_of_your_model)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          if image.endswith(\"jsonl\"): continue\n",
        "          img_name = f\"image_{i}.png\"\n",
        "          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))\n",
        "          #image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
        "          #img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
        "          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})\n",
        "'''\n",
        "      shutil.copy(os.path.join(save_path, \"metadata.jsonl\"), os.path.join(model_images, \"metadata.jsonl\"))\n",
        "      description = LoRA_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = LoRA_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_model} model taught to Stable Diffusion with LoRA.\n",
        "It can be used by modifying the `validation_prompt`: **{LoRA_prefs['validation_prompt']}**\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)\n",
        "\n",
        "Here are the images used for training this model:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      #readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      #readme_file.write(readme_text)\n",
        "      #readme_file.close()\n",
        "      yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- lora\n",
        "inference: true\n",
        "---\n",
        "      \"\"\"\n",
        "      model_card = f\"\"\"\n",
        "# LoRA Model - {name_of_your_model}\n",
        "These are LoRA adaption weights for {model_path}. The weights were validated with {LoRA_args.validation_prompt} using [DreamBooth](https://dreambooth.github.io/).\\n\n",
        "### {repo_id} on Stable Diffusion via LoRA using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {your_username}\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "\n",
        "Images used for training this model:\n",
        "{image_string}\n",
        "\"\"\"\n",
        "      readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      readme_file.write(yaml + model_card)#(readme_text)\n",
        "      readme_file.close()\n",
        "      #with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
        "      #    f.write(yaml + model_card)\n",
        "      #Save the token identifier to a file\n",
        "      #text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      #text_file.write(LoRA_prefs['instance_prompt'])\n",
        "      #text_file.close()\n",
        "      #operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        #CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      #]\n",
        "      print(repo_id)\n",
        "      print(model_card)\n",
        "\n",
        "      with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "        if \"step_*\" not in gitignore:\n",
        "            gitignore.write(\"step_*\\n\")\n",
        "        if \"epoch_*\" not in gitignore:\n",
        "            gitignore.write(\"epoch_*\\n\")\n",
        "      try:\n",
        "        #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "        #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "        repo.push_to_hub(commit_message=f\"Upload the LoRA model {name_of_your_model} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      prefs['LoRA_model'] = name_of_your_model\n",
        "      prefs['custom_LoRA_models'].append({'name': name_of_your_model, 'path':repo_id})\n",
        "      page.LoRA_model.options.insert(0, dropdown.Option(name_of_your_model))\n",
        "      page.LoRA_model.value = name_of_your_model\n",
        "      page.LoRA_model.update()\n",
        "      save_settings_file(page)\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_converter(page):\n",
        "    global converter_prefs, prefs\n",
        "    #https://colab.research.google.com/github/camenduru/converter-colab/blob/main/converter_colab.ipynb\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.converter_output.controls.append(line)\n",
        "      page.converter_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.converter_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    model_name = converter_prefs['model_name']\n",
        "    model_path = converter_prefs['model_path']\n",
        "    if not bool(converter_prefs['model_name']):\n",
        "      alert_msg(page, \"Provide a name to call the converted custom model\")\n",
        "      return\n",
        "    if not bool(model_path):\n",
        "      alert_msg(page, \"Provide the path to the custom model to convert\")\n",
        "      return\n",
        "    model_file = format_filename(model_name, use_dash=True)\n",
        "    if converter_prefs['from_format'] == 'ckpt':\n",
        "      model_file += '.ckpt'\n",
        "    page.converter_output.controls.clear()\n",
        "    installer = Installing(\"Initializing Converter Tools...\")\n",
        "    prt(installer)\n",
        "    custom_models = os.path.join(root_dir, 'custom_models',)\n",
        "    custom_path = os.path.join(custom_models, format_filename(model_name, use_dash=True))\n",
        "    checkpoint_file = os.path.join(custom_models, model_file)\n",
        "    make_dir(custom_path)\n",
        "    pip_install(\"omegaconf gdown\", installer=installer)\n",
        "    installer.status(\"...get Diffusers\")\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install \"git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]\"', cwd=root_dir, realtime=False)\n",
        "    scripts_dir = os.path.join(diffusers_dir, \"scripts\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "\n",
        "    installer.status(\"...downloading\")\n",
        "    if model_path.startswith('https://drive'):\n",
        "      import gdown\n",
        "      gdown.download(model_path, checkpoint_file, quiet=True)\n",
        "    elif model_path.startswith('http'):\n",
        "      local = download_file(model_path)\n",
        "      print(f\"Download {model_path} local:{local}\")\n",
        "      shutil.move(local, checkpoint_file)\n",
        "    elif os.path.isfile(model_path):\n",
        "      shutil.copy(model_path, checkpoint_file)\n",
        "    elif os.path.isdir(model_path):\n",
        "      if os.path.exists(custom_path):\n",
        "        shutil.rmtree(custom_path)\n",
        "      shutil.copytree(model_path, custom_path, dirs_exist_ok=True)\n",
        "      checkpoint_file = custom_path\n",
        "    elif '/' in model_path and not model_path.startswith('/'):\n",
        "      checkpoint_file = model_path # hopefully Huggingface\n",
        "    else:\n",
        "      alert_msg(page, f\"Couldn't recognize source model file path {model_path}.\")\n",
        "      return\n",
        "    clear_last()\n",
        "    prt(Text(f'Converting {model_file} to {converter_prefs[\"to_format\"]}...', weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "\n",
        "    if converter_prefs['from_format'] == \"lora_safetensors\":\n",
        "      run_cmd = f\"python {os.path.join(scripts_dir, 'convert_lora_safetensor_to_diffusers.py')}\"\n",
        "    else:\n",
        "      run_cmd = f\"python {os.path.join(scripts_dir, 'convert_original_stable_diffusion_to_diffusers.py')}\"\n",
        "    if converter_prefs['from_format'] == \"safetensors\":\n",
        "      run_cmd += f' --from_safetensors'\n",
        "    if converter_prefs['from_format'] == \"controlnet\":\n",
        "      run_cmd += f' --controlnet'\n",
        "    if converter_prefs['from_format'] == \"lora_safetensors\" and bool(converter_prefs['base_model']):\n",
        "      run_cmd += f' --base_model {converter_prefs[\"base_model\"]}'\n",
        "    if converter_prefs['from_format'] == \"ckpt\" or converter_prefs['from_format'] == \"safetensors\" or converter_prefs['from_format'] == \"lora_safetensors\":\n",
        "      run_cmd += f' --checkpoint_path {checkpoint_file}'\n",
        "    if converter_prefs['to_format'] == \"safetensors\":\n",
        "      run_cmd += f' --to_safetensors'\n",
        "    #if status['installed_xformers']:\n",
        "    if converter_prefs['model_type'] == \"SD v1.x text2image\" and converter_prefs['from_format'] != \"lora_safetensors\":\n",
        "      run_cmd += f' --image_size 512'\n",
        "    elif converter_prefs['model_type'] == \"SD v2.x text2image\" and converter_prefs['from_format'] != \"lora_safetensors\":\n",
        "      run_cmd += f' --image_size 768'\n",
        "      run_cmd += f' --upcast_attention'\n",
        "      run_cmd += f' --prediction_type v_prediction'\n",
        "    run_cmd += f' --scheduler_type {converter_prefs[\"scheduler_type\"]}'\n",
        "    if converter_prefs['half_percision']:\n",
        "      run_cmd += f' --half'\n",
        "    run_cmd += f' --dump_path {custom_path}'\n",
        "    #TODO: Add this to UI to use\n",
        "    #if bool(converter_prefs['vae_path']): #\"Set to a path, hub id to an already converted vae to not convert it again.\"\n",
        "    #  run_cmd += f' --vae_path {converter_prefs[\"vae_path\"]}'\n",
        "    if converter_prefs['from_format'] == converter_prefs[\"to_format\"]:\n",
        "      out_file = os.path.join(custom_path, f\"{format_filename(model_name, use_dash=True)}.{converter_prefs['to_format']}\")\n",
        "      print(f\"From and To Formats are the same. Using file without conversion to {out_file}\")\n",
        "      shutil.move(checkpoint_file, out_file)\n",
        "      custom_path = out_file\n",
        "    else:\n",
        "      prt(f\"Running {run_cmd}\")\n",
        "      try:\n",
        "        run_sp(run_cmd, cwd=scripts_dir, realtime=True)\n",
        "        #run_process(run_cmd, page=page, cwd=scripts_dir, show=True)\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error Running convert_original_stable_diffusion_to_diffusers\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "      if len(os.listdir(custom_path)) == 0:\n",
        "        prt(f\"Problem converting your model. Check console and source checkpoint and try again...\")\n",
        "        os.rmdir(custom_path)\n",
        "        return\n",
        "      clear_last()\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    prt(f\"Done Converting... Saved locally at {custom_path}\")\n",
        "    if converter_prefs['load_custom_model']:\n",
        "      prefs['custom_model'] = custom_path\n",
        "      prefs['custom_models'].append({'name': model_name, 'path':custom_path})\n",
        "    if(converter_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import model_info, create_repo, create_branch, upload_folder\n",
        "      from huggingface_hub.utils import RepositoryNotFoundError, revisionNotFoundError\n",
        "      from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      '''dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        custom_path,\n",
        "        torch_dtype=torch.float16,\n",
        "      ).to(\"cuda\")\n",
        "      os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      dreambooth_pipe.save_pretrained(\"fp16_model\")'''\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = True\n",
        "      if(converter_prefs['where_to_save_model'] == \"Public Library\"):\n",
        "        private = False\n",
        "      #  repo_id = f\"sd-dreambooth-library/{slugify(model_name)}\"\n",
        "      if '/' in model_path and not model_path.startswith('/'):\n",
        "        repo_id = model_path\n",
        "      else:\n",
        "        repo_id = f\"{your_username}/{format_filename(model_name, use_dash=True)}\"\n",
        "      #output_dir = dreambooth_args.output_dir\n",
        "      if(not bool(prefs['HuggingFace_api_key'])):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "\n",
        "      description = converter_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = converter_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {model_name} model on Stable Diffusion using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "\n",
        "{description}\n",
        "You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "    '''\n",
        "      #Save the readme to a file\n",
        "      readme_file = open(\"README.md\", \"w\")\n",
        "      readme_file.write(readme_text)\n",
        "      readme_file.close()\n",
        "      #Save the token identifier to a file\n",
        "      '''text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      text_file.write(dreambooth_prefs['instance_prompt'])\n",
        "      text_file.close()'''\n",
        "      operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      ]\n",
        "      print(repo_id)\n",
        "      print(readme_text)\n",
        "      try:\n",
        "          repo_exists = True\n",
        "          r_info = model_info(repo_id, token=hf_token)\n",
        "      except RepositoryNotFoundError:\n",
        "          repo_exists = False\n",
        "          pass\n",
        "      if not repo_exists:\n",
        "        try:\n",
        "          create_repo(repo_id, private=private, token=hf_token)\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "      branch = f\"{converter_prefs['from_format']}-to-{converter_prefs['to_format']}\"\n",
        "      try:\n",
        "          branch_exists = True\n",
        "          b_info = model_info(repo_id, revision=branch, token=hf_token)\n",
        "      except revisionNotFoundError:\n",
        "          branch_exists = False\n",
        "      finally:\n",
        "          if branch_exists:\n",
        "              print(b_info)\n",
        "          else:\n",
        "              create_branch(repo_id, branch=branch, token=hf_token)\n",
        "      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the converted model {model_name} embeds\",token=hf_token)\n",
        "      api.upload_folder(folder_path=custom_path, path_in_repo=\"\", revision=branch, repo_id=repo_id, commit_message=f\"Upload the converted model {model_name} embeds\", token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      #api.upload_folder(folder_path=save_path, path_in_repo=\"concept_images\", repo_id=repo_id, token=hf_token)\n",
        "      prefs['custom_model'] = repo_id\n",
        "      prefs['custom_models'].append({'name': model_name, 'path':repo_id})\n",
        "      page.custom_model.value = repo_id\n",
        "      try:\n",
        "        page.custom_model.update()\n",
        "      except Exception: pass\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "\n",
        "    def push_ckpt(model_to, token, branch):\n",
        "      try:\n",
        "          repo_exists = True\n",
        "          r_info = model_info(model_to, token=token)\n",
        "      except RepositoryNotFoundError:\n",
        "          repo_exists = False\n",
        "      finally:\n",
        "          if repo_exists:\n",
        "              print(r_info)\n",
        "          else:\n",
        "              create_repo(model_to, private=True, token=token)\n",
        "      try:\n",
        "          branch_exists = True\n",
        "          b_info = model_info(model_to, revision=branch, token=token)\n",
        "      except revisionNotFoundError:\n",
        "          branch_exists = False\n",
        "      finally:\n",
        "          if branch_exists:\n",
        "              print(b_info)\n",
        "          else:\n",
        "              create_branch(model_to, branch=branch, token=token)\n",
        "      upload_folder(folder_path=\"ckpt\", path_in_repo=\"\", revision=branch, repo_id=model_to, commit_message=f\"ckpt\", token=token)\n",
        "      return \"push ckpt done!\"\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_checkpoint_merger(page):\n",
        "    global checkpoint_merger_prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.checkpoint_merger_output.controls.append(line)\n",
        "      page.checkpoint_merger_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.checkpoint_merger_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "        alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "        return\n",
        "    if len(checkpoint_merger_prefs['pretrained_models']) < 2:\n",
        "        alert_msg(page, \"Select 2 or more compatible checkpoint models to the list before running...\")\n",
        "        return\n",
        "    prt(Installing(\"Downloading Required Models and Merging...\"))\n",
        "    try:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        model_path = checkpoint_merger_prefs['pretrained_models'][0]\n",
        "        checkpoint_merger_pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"checkpoint_merger.py\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        merged_pipe = checkpoint_merger_pipe.merge(checkpoint_merger_prefs['pretrained_models'], interp = checkpoint_merger_prefs['interp'] if checkpoint_merger_prefs['interp'] != \"weighted_sum\" else None, alpha = checkpoint_merger_prefs['alpha'], force = checkpoint_merger_prefs['force'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        merged_pipe.to(torch_device)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Problem running Merger. Check parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "            torch.cuda.empty_cache()\n",
        "        return\n",
        "    model_name = format_filename(checkpoint_merger_prefs['name_of_your_model'], force_underscores=True)\n",
        "    output_dir = os.path.join(root_dir, 'my_models', model_name)\n",
        "    repo_id = f\"{prefs['HuggingFace_username']}/{model_name}\"\n",
        "    if bool(checkpoint_merger_prefs['validation_prompt']):\n",
        "        prt(\"Generating Test Validation Image...\")\n",
        "        try:\n",
        "            image = merged_pipe(checkpoint_merger_prefs['validation_prompt']).images[0]\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Problem creating image with merged_pipe...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            with torch.no_grad():\n",
        "                torch.cuda.empty_cache()\n",
        "            return\n",
        "        fname = format_filename(checkpoint_merger_prefs['validation_prompt'])\n",
        "        fpath = available_file(stable_dir, fname, 0)\n",
        "        image.save(fpath)\n",
        "        clear_last()\n",
        "        prt(Img(src=fpath))\n",
        "    if checkpoint_merger_prefs['save_model']:\n",
        "        private = False if checkpoint_merger_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "        from huggingface_hub import HfFolder, create_repo, Repository\n",
        "        if(not prefs['HuggingFace_api_key']):\n",
        "            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "        else:\n",
        "            hf_token = prefs['HuggingFace_api_key']\n",
        "        try:\n",
        "            create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "            repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "    else:\n",
        "        if os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "    try:\n",
        "        merged_pipe.save_pretrained(output_dir)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Issue saving pretrained.  Check parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "            torch.cuda.empty_cache()\n",
        "        return\n",
        "    del checkpoint_merger_pipe\n",
        "    del merged_pipe\n",
        "    if checkpoint_merger_prefs['save_model']:\n",
        "        description = checkpoint_merger_prefs['readme_description']\n",
        "        if bool(description.strip()):\n",
        "            description = checkpoint_merger_prefs['readme_description'] + '\\n\\n'\n",
        "        models_string = \"\"\n",
        "        for m in checkpoint_merger_prefs['pretrained_models']:\n",
        "            models_string += f\"* [{m}](https://huggingface.co/{m})\\n\"\n",
        "        yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- merge\n",
        "inference: true\n",
        "---\n",
        "\"\"\"\n",
        "        model_card = f\"\"\"\n",
        "# Merged Checkpoint Model - {checkpoint_merger_prefs['name_of_your_model']}\n",
        "These are fine-tuned combined weights of {' + '.join(checkpoint_merger_prefs['pretrained_models'])}.\\n\n",
        "### {repo_id} on Stable Diffusion via Custom Checkpoint using [AEIONic Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AEIONic/blob/main/AEIONic_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {prefs['HuggingFace_username']}\n",
        "{description}\n",
        "\n",
        "Checkpoints used for training this model:\n",
        "{models_string}\n",
        "Alpha Interpolation: {checkpoint_merger_prefs['alpha']}\n",
        "Interpolation Method: {checkpoint_merger_prefs['interp']}\n",
        "\"\"\"\n",
        "        readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "        readme_file.write(yaml + model_card)#(readme_text)\n",
        "        readme_file.close()\n",
        "        print(repo_id)\n",
        "        print(model_card)\n",
        "        with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "            if \"step_*\" not in gitignore:\n",
        "                gitignore.write(\"step_*\\n\")\n",
        "            if \"epoch_*\" not in gitignore:\n",
        "                gitignore.write(\"epoch_*\\n\")\n",
        "        try:\n",
        "            #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "            #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "            #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "            repo.push_to_hub(commit_message=f\"Upload the Merged model {model_name} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, f\"ERROR Pushing {model_name} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Installers->Diffusers Custom Model_ dropdown.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_tortoise_tts(page):\n",
        "    #https://github.com/neonbjb/tortoise-tts\n",
        "    global tortoise_prefs, pipe_tortoise_tts, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.tortoise_output.controls.append(line)\n",
        "      page.tortoise_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.tortoise_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if tortoise_prefs['train_custom'] and not bool(tortoise_prefs['custom_voice_name']):\n",
        "      alert_msg(page, \"Provide a Custom Voice Name when training your audio files.\")\n",
        "      return\n",
        "    if not bool(tortoise_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI voice to read...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Tortoise-TTS Packages...\")\n",
        "    prt(installer)\n",
        "    tortoise_dir = os.path.join(root_dir, \"tortoise-tts\")\n",
        "    voice_dir = os.path.join(tortoise_dir, 'tortoise', 'voices')\n",
        "    if not os.path.isdir(tortoise_dir):\n",
        "      installer.status(\"...cloning jnorberg/toroise-tts\")\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/jnordberg/tortoise-tts.git\", page=page)\n",
        "    os.chdir(tortoise_dir)\n",
        "    pip_install(\"ffmpeg pydub\", installer=installer)\n",
        "    try:\n",
        "      from tortoise.api import TextToSpeech\n",
        "    except Exception:\n",
        "      installer.status(\"...installing all requirements\")\n",
        "      try:\n",
        "        run_process(\"pip install -r requirements.txt\", page=page, cwd=tortoise_dir)\n",
        "        run_process(\"python setup.py install\", page=page, cwd=tortoise_dir)\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error Installing Tortoise TextToSpeech requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "      pass\n",
        "    import torch\n",
        "    import torchaudio\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from tortoise.api import TextToSpeech\n",
        "    from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "    clear_pipes('tortoise_tts')\n",
        "    # This will download all the models used by Tortoise from the HuggingFace hub.\n",
        "    if pipe_tortoise_tts == None:\n",
        "      installer.status(\"...initialize TextToSpeech pipe\")\n",
        "      try:\n",
        "        pipe_tortoise_tts = TextToSpeech()\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Tortoise TextToSpeech package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Tortoise Text-to-Speech... (slow, but wins the race)\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', tortoise_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    custom_voices = os.path.join(audio_out, 'custom_voices')\n",
        "    if bool(tortoise_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, tortoise_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"tortoise-tts\", 'tortoise', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(tortoise_prefs['text'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = tortoise_prefs['file_prefix']\n",
        "    tortoise_custom_voices = prefs['tortoise_custom_voices']\n",
        "    tortoise_prefs['voice'] = []\n",
        "    if tortoise_prefs['train_custom']:\n",
        "        if len(tortoise_prefs['custom_wavs']) <2:\n",
        "          alert_msg(page, \"To train a custom voice, provide at least 2 audio files to mimic.\")\n",
        "          return\n",
        "        CUSTOM_VOICE_NAME = format_filename(tortoise_prefs['custom_voice_name'])\n",
        "        custom_voice_folder = os.path.join(root_dir, \"tortoise-tts\", 'tortoise', 'voices', CUSTOM_VOICE_NAME)\n",
        "        os.makedirs(custom_voice_folder, exist_ok=True)\n",
        "        for i, f in enumerate(tortoise_prefs['custom_wavs']):\n",
        "            if f.lower().endswith('mp3'):\n",
        "              sound = pydub.AudioSpegment.from_mp3(f)\n",
        "              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format=\"wav\", bitrate=\"22050\")\n",
        "            elif f.lower().endswith('wav'):\n",
        "              sound = pydub.AudioSpegment.from_wav(f)\n",
        "              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format=\"wav\", bitrate=\"22050\")\n",
        "            else:\n",
        "              alert_msg(f\"Unknown file type {f.rpartition('.')[2]}... Use only .wav and .mp3 audio clips.\")\n",
        "              return\n",
        "              #shutil.copy(f, os.path.join(custom_voice_folder, f'{i+1}.wav'))\n",
        "        #for i, file_data in enumerate(files.upload().values()):\n",
        "        #    with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:\n",
        "        #        f.write(file_data)\n",
        "        if not CUSTOM_VOICE_NAME in tortoise_prefs['voice']:\n",
        "          #tortoise_prefs['voice'].append(CUSTOM_VOICE_NAME)\n",
        "          page.tortoise_voices.controls.append(Checkbox(label=CUSTOM_VOICE_NAME, value=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "          page.tortoise_voices.update()\n",
        "          output_voice_folder = os.path.join(custom_voices, CUSTOM_VOICE_NAME)\n",
        "          if os.path.exists(output_voice_folder):\n",
        "            shutil.rmtree(output_voice_folder)\n",
        "            print(f'Output Voice Folder already existed: {output_voice_folder}... Deleting to remake.')\n",
        "          #os.makedirs(output_voice_folder, exist_ok=False)\n",
        "          shutil.copytree(custom_voice_folder, output_voice_folder)\n",
        "          prefs['tortoise_custom_voices'].append({'name':CUSTOM_VOICE_NAME, 'folder': output_voice_folder})\n",
        "          save_settings_file(page)\n",
        "    for v in page.tortoise_voices.controls:\n",
        "        if v.value == True:\n",
        "          tortoise_prefs['voice'].append(v.label)\n",
        "          if not os.path.exists(os.path.join(voice_dir, v.label)):\n",
        "            for custom in tortoise_custom_voices:\n",
        "              if custom['name'] == v.label:\n",
        "                if os.path.exists(custom['folder']):\n",
        "                  #os.makedirs(os.path.join(voice_dir, custom['name'], exist_ok=True))\n",
        "                  shutil.copytree(custom['folder'], os.path.join(voice_dir, custom['name']))\n",
        "                else:\n",
        "                  print(f\"Couldn't find custom folder {custom['folder']}\")\n",
        "    # Load it and send it through Tortoise.\n",
        "    voice = tortoise_prefs['voice']\n",
        "    v_str = voice if isinstance(voice, str) else '+'.join(voice) if isinstance(voice, list) else ''\n",
        "    if len(voice) == 0: v_str = 'random'\n",
        "    audio_name = f'{file_prefix}{v_str}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "    #print(str(voice))\n",
        "    #print(fname)\n",
        "    if len(voice) == 0:\n",
        "        voice_samples = conditioning_latents = None\n",
        "    elif len(voice) == 1:\n",
        "        voice_samples, conditioning_latents = load_voice(voice[0])\n",
        "    else:\n",
        "        voice_samples, conditioning_latents = load_voices(voice)\n",
        "    gen = pipe_tortoise_tts.tts_with_preset(tortoise_prefs['text'], voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=tortoise_prefs['preset'])\n",
        "    torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "    #IPython.display.Audio('generated.wav')\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    #a_out = Audio(src=fname, autoplay=False)\n",
        "    #page.overlay.append(a_out)\n",
        "    #page.update()\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    prt(AudioPlayer(src=fname, display=display_name, data=display_name))\n",
        "    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_audio_ldm(page):\n",
        "    global audioLDM_prefs, pipe_audio_ldm, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.audioLDM_output.controls.append(line)\n",
        "      page.audioLDM_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.audioLDM_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(audioLDM_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Audio LDM Packages...\", )\n",
        "    prt(installer)\n",
        "    audioLDM_dir = os.path.join(root_dir, \"audioldm-text-to-audio-generation\")\n",
        "    #voice_dir = os.path.join(audioLDM_dir, 'audioldm', 'voices')\n",
        "    if not os.path.isdir(audioLDM_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation\", page=page)\n",
        "    os.chdir(audioLDM_dir)\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(audioLDM_dir, 'audioldm'))\n",
        "    try:\n",
        "        from audioldm import text_to_audio, build_model\n",
        "    except Exception:\n",
        "        pip_install(\"einops pyyaml soundfile librosa torchlibrosa pandas gradio\", installer=installer, q=True)\n",
        "    finally:\n",
        "        from audioldm import text_to_audio, build_model\n",
        "    import soundfile as sf\n",
        "    model_id=\"cvssp/audioldm-s-full-v2\"\n",
        "    clear_pipes('audio_ldm')\n",
        "    # This will download all the models used by Audio LDM from the HuggingFace hub.\n",
        "    if pipe_audio_ldm == None:\n",
        "      try:\n",
        "        pipe_audio_ldm = build_model()\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Audio LDM package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating AudioLDM Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(audioLDM_prefs['seed']) if int(audioLDM_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    try:\n",
        "      waveform = text_to_audio(pipe_audio_ldm, audioLDM_prefs['text'], random_seed, duration=audioLDM_prefs['duration'], guidance_scale=audioLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM_prefs['n_candidates']))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating text_to_audio waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', audioLDM_prefs['batch_folder_name'])\n",
        "    make_dir(save_dir)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(audioLDM_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, audioLDM_prefs['batch_folder_name'])\n",
        "    make_dir(audio_out)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audioldm-tts\", 'audioldm', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(audioLDM_prefs['text'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = audioLDM_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "    for i in range(waveform.shape[0]):\n",
        "        sf.write(fname, waveform[i, 0], samplerate=16000)\n",
        "    #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "    #IPython.display.Audio('generated.wav')\n",
        "    clear_last(2)\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_audio_ldm2(page):\n",
        "    global audioLDM2_prefs, pipe_audio_ldm2, prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.audioLDM2_output.controls.append(line)\n",
        "      page.audioLDM2_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.audioLDM2_output, lines=lines)\n",
        "    if not bool(audioLDM2_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = audioLDM2_prefs['steps']# * audioLDM2_prefs['batch_size']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    installer = Installing(\"Loading Audio LDM-2 Packages...\")\n",
        "    prt(installer)\n",
        "    pip_install(\"soundfile pandas scipy\", installer=installer)\n",
        "    import soundfile as sf\n",
        "    import scipy\n",
        "    if audioLDM2_prefs['save_mp3']:\n",
        "        pip_install(\"ffmpeg pydub\", q=True, installer=installer)\n",
        "        import ffmpeg, pydub\n",
        "    from diffusers import AudioLDM2Pipeline\n",
        "    model_id = audioLDM2_prefs['model_name']\n",
        "    if 'loaded_ldm2' not in status:\n",
        "        status['loaded_ldm2'] = \"\"\n",
        "    if status['loaded_ldm2'] == model_id:\n",
        "        clear_pipes('audio_ldm2')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    # This will download all the models used by Audio LDM from the HuggingFace hub.\n",
        "    if pipe_audio_ldm2 == None:\n",
        "      try:\n",
        "        installer.status(\"...loading pipeline\")\n",
        "        pipe_audio_ldm2 = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)#build_model(model_name=model_id)\n",
        "        pipe_audio_ldm2 = pipeline_scheduler(pipe_audio_ldm2)\n",
        "        status['loaded_scheduler'] = prefs['scheduler_mode']\n",
        "        #pipe_audio_ldm2.scheduler = LMSDiscreteScheduler.from_config(pipe_audio_ldm2.scheduler.config)\n",
        "        if prefs['enable_torch_compile']:\n",
        "            installer.status(\"...torch compile\")\n",
        "            pipe_audio_ldm2 = torch.compile(pipe_audio_ldm2)\n",
        "            #torch.set_float32_matmul_precision(\"high\")\n",
        "        pipe_audio_ldm2 = pipe_audio_ldm2.to(torch_device)\n",
        "        pipe_audio_ldm2.set_progress_bar_config(disable=True)\n",
        "        status['loaded_ldm2'] = model_id\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Audio LDM 2 model\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        installer.status(f\"...scheduler {prefs['scheduler_mode']}\")\n",
        "        pipe_audio_ldm2 = pipeline_scheduler(pipe_audio_ldm2)\n",
        "      \n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating AudioLDM-2 Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(audioLDM2_prefs['seed']) if int(audioLDM2_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(random_seed)\n",
        "    try:\n",
        "      audios = pipe_audio_ldm2(audioLDM2_prefs['text'],\n",
        "          negative_prompt=audioLDM2_prefs['negative_prompt'],\n",
        "          num_inference_steps=audioLDM2_prefs['steps'],\n",
        "          guidance_scale=audioLDM2_prefs['guidance_scale'],\n",
        "          audio_length_in_s=audioLDM2_prefs['duration'],\n",
        "          num_waveforms_per_prompt=audioLDM2_prefs['batch_size'],\n",
        "          generator=generator,\n",
        "          callback=callback_fnc,\n",
        "      ).audios\n",
        "      #waveform = text_to_audio(pipe_audio_ldm2, audioLDM2_prefs['text'], random_seed, duration=10, guidance_scale=audioLDM2_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM2_prefs['n_candidates']), batchsize=int(audioLDM2_prefs['batch_size']), transcript=audioLDM2_prefs['transcription'] if 'speech' in model_id else \"\")\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating AudioLDM2 waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', audioLDM2_prefs['batch_folder_name'])\n",
        "    make_dir(save_dir)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(audioLDM2_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, audioLDM2_prefs['batch_folder_name'])\n",
        "    make_dir(audio_out)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audioldm2-tts\", 'audioldm2', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    # waveform = [(16000, np.random.randn(16000)), (16000, np.random.randn(16000))]\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    num = 0\n",
        "    for audio in audios:\n",
        "        fname = format_filename(audioLDM2_prefs['text'])\n",
        "        if fname[-1] == '.': fname = fname[:-1]\n",
        "        file_prefix = audioLDM2_prefs['file_prefix']\n",
        "        audio_name = f'{file_prefix}-{fname}'\n",
        "        audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "        audio_metadata = {\n",
        "          \"sample_rate\": 16000,\n",
        "          \"artist\": prefs['meta_ArtistName'],\n",
        "          \"copyright\": prefs['meta_Copyright'],\n",
        "          \"software\": \"AEIONic Diffusion Deluxe\",\n",
        "        }\n",
        "        if prefs['save_config_in_metadata']:\n",
        "          config_json = audioLDM2_prefs.copy()\n",
        "          del config_json['batch_size']\n",
        "          del config_json['n_candidates']\n",
        "          del config_json['file_prefix']\n",
        "          config_json['seed'] = random_seed + num\n",
        "          config_json['sceduler'] = status['loaded_scheduler']\n",
        "          audio_metadata[\"config\"] = config_json\n",
        "        fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        scipy.io.wavfile.write(fname, data=audio, rate=16000)#, audio_metadata\n",
        "        #for i in range(waveform.shape[0]):\n",
        "        #    sf.write(fname, waveform[i, 0], samplerate=16000)\n",
        "        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "        if audioLDM2_prefs['save_mp3']:\n",
        "          wav_file = pydub.AudioSegment.from_wav(fname)\n",
        "          #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})\n",
        "          mp3_name = available_file(audio_out, audio_name, 0, ext=\"mp3\")\n",
        "          mp3_file = wav_file.export(mp3_name, format=\"mp3\", tags=audio_metadata)\n",
        "          os.remove(fname)\n",
        "          fname = mp3_name\n",
        "          display_name = fname\n",
        "        else:\n",
        "        #if storage_type == \"Colab Google Drive\":\n",
        "          audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "          shutil.move(fname, audio_save)\n",
        "          fname = audio_save\n",
        "          display_name = audio_save\n",
        "        #display_name = fname\n",
        "        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))\n",
        "        num += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_music_ldm(page):\n",
        "    global musicLDM_prefs, pipe_music_ldm, prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.musicLDM_output.controls.append(line)\n",
        "      page.musicLDM_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.musicLDM_output, lines=lines)\n",
        "    if not bool(musicLDM_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the music of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = musicLDM_prefs['steps']# * musicLDM_prefs['batch_size']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    installer = Installing(\"Loading Music LDM Packages...\")\n",
        "    prt(installer)\n",
        "    pip_install(\"soundfile pandas scipy\", q=True, installer=installer)\n",
        "    if musicLDM_prefs['save_mp3']:\n",
        "        pip_install(\"ffmpeg pydub\", q=True, installer=installer)\n",
        "        import pydub, ffmpeg\n",
        "    from diffusers import MusicLDMPipeline, PNDMScheduler\n",
        "    model_id = musicLDM_prefs['model_name']\n",
        "    if 'loaded_ldm' not in status:\n",
        "        status['loaded_ldm'] = \"\"\n",
        "    if status['loaded_ldm'] == model_id:\n",
        "        clear_pipes('music_ldm')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    if pipe_music_ldm == None:\n",
        "      try:\n",
        "        installer.status(\"...loading pipeline\")\n",
        "        scheduler = PNDMScheduler(skip_prk_steps=True)\n",
        "        pipe_music_ldm = MusicLDMPipeline.from_pretrained(model_id, torch_dtype=torch.float16, scheduler=scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)#build_model(model_name=model_id)\n",
        "        #pipe_music_ldm = pipeline_scheduler(pipe_music_ldm, big_3=True)\n",
        "        #status['loaded_scheduler'] = prefs['scheduler_mode']\n",
        "        #pipe_music_ldm.scheduler = LMSDiscreteScheduler.from_config(pipe_music_ldm.scheduler.config)\n",
        "        if prefs['enable_torch_compile']:\n",
        "            installer.status(\"...torch compile\")\n",
        "            pipe_music_ldm = torch.compile(pipe_music_ldm)\n",
        "            #torch.set_float32_matmul_precision(\"high\")\n",
        "        pipe_music_ldm = pipe_music_ldm.to(torch_device)\n",
        "        pipe_music_ldm.set_progress_bar_config(disable=True)\n",
        "        status['loaded_ldm'] = model_id\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading MusicLDM model\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    #elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "    #    installer.status(f\"...scheduler {prefs['scheduler_mode']}\")\n",
        "    #    pipe_music_ldm = pipeline_scheduler(pipe_music_ldm)\n",
        "      \n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating MusicLDM Songs...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(musicLDM_prefs['seed']) if int(musicLDM_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(random_seed)\n",
        "    try:\n",
        "      audios = pipe_music_ldm(musicLDM_prefs['text'],\n",
        "          negative_prompt=musicLDM_prefs['negative_prompt'],\n",
        "          num_inference_steps=musicLDM_prefs['steps'],\n",
        "          guidance_scale=musicLDM_prefs['guidance_scale'],\n",
        "          audio_length_in_s=musicLDM_prefs['duration'],\n",
        "          num_waveforms_per_prompt=musicLDM_prefs['batch_size'],\n",
        "          generator=generator,\n",
        "          callback=callback_fnc,\n",
        "      ).audios\n",
        "      #waveform = text_to_audio(pipe_music_ldm, musicLDM_prefs['text'], random_seed, duration=10, guidance_scale=musicLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(musicLDM_prefs['n_candidates']), batchsize=int(musicLDM_prefs['batch_size']), transcript=musicLDM_prefs['transcription'] if 'speech' in model_id else \"\")\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating MusicLDM waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', musicLDM_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(musicLDM_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, musicLDM_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"musicldm-tts\", 'musicldm', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    # waveform = [(16000, np.random.randn(16000)), (16000, np.random.randn(16000))]\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    num = 0\n",
        "    for audio in audios:\n",
        "        fname = format_filename(musicLDM_prefs['text'])\n",
        "        if fname[-1] == '.': fname = fname[:-1]\n",
        "        file_prefix = musicLDM_prefs['file_prefix']\n",
        "        audio_name = f'{file_prefix}-{fname}'\n",
        "        audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "        audio_metadata = {\n",
        "          \"sample_rate\": 16000,\n",
        "          \"artist\": prefs['meta_ArtistName'],\n",
        "          \"copyright\": prefs['meta_Copyright'],\n",
        "          \"software\": \"AEIONic Diffusion Deluxe\",\n",
        "        }\n",
        "        if prefs['save_config_in_metadata']:\n",
        "          config_json = musicLDM_prefs.copy()\n",
        "          del config_json['batch_size']\n",
        "          del config_json['n_candidates']\n",
        "          del config_json['file_prefix']\n",
        "          config_json['seed'] = random_seed + num\n",
        "          config_json['sceduler'] = status['loaded_scheduler']\n",
        "          audio_metadata[\"config\"] = config_json\n",
        "        fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        scipy.io.wavfile.write(fname, data=audio, rate=16000)#, audio_metadata\n",
        "        #for i in range(waveform.shape[0]):\n",
        "        #    sf.write(fname, waveform[i, 0], samplerate=16000)\n",
        "        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "        if musicLDM_prefs['save_mp3']:\n",
        "          wav_file = pydub.AudioSegment.from_wav(fname)\n",
        "          #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})\n",
        "          mp3_name = available_file(audio_out, audio_name, 0, ext=\"mp3\")\n",
        "          mp3_file = wav_file.export(mp3_name, format=\"mp3\", tags=audio_metadata)\n",
        "          os.remove(fname)\n",
        "          fname = mp3_name\n",
        "          display_name = fname\n",
        "        else:\n",
        "        #if storage_type == \"Colab Google Drive\":\n",
        "          audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "          shutil.move(fname, audio_save)\n",
        "          fname = audio_save\n",
        "          display_name = audio_save\n",
        "        #display_name = fname\n",
        "        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))\n",
        "        num += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_bark(page):\n",
        "    global bark_prefs, pipe_bark, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.bark_output.controls.append(line)\n",
        "      page.bark_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.bark_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(bark_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Bark Packages...\")\n",
        "    prt(installer)\n",
        "    pip_install(\"scipy\", installer=installer)\n",
        "    from scipy.io.wavfile import write as write_wav\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(root_dir, 'audioldm'))\n",
        "    try:\n",
        "        from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "        if force_updates: raise ImportError(\"Forcing update\")\n",
        "    except Exception:\n",
        "        installer.status(\"...installing suno-ai/bark\")\n",
        "        try:\n",
        "            run_process(\"pip install git+https://github.com/suno-ai/bark.git\", page=page)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Bark requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "    if bark_prefs['use_bettertransformer']:\n",
        "      #https://colab.research.google.com/drive/1XO0RhINg4ZZCdJJmPeJ9lOQs98skJ8h_\n",
        "        try:\n",
        "            from optimum.bettertransformer import BetterTransformer\n",
        "        except ModuleNotFoundError:\n",
        "            installer.status(\"...installing Optimum BetterTransformer\")\n",
        "            run_process(\"pip install --upgrade git+https://github.com/huggingface/optimum.git\", page=page)\n",
        "            from optimum.bettertransformer import BetterTransformer\n",
        "            pass\n",
        "        from transformers import BarkModel, set_seed, AutoProcessor\n",
        "        installer.status(\"...initializing suno/bark model\")\n",
        "        bark_model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(torch_device)\n",
        "        installer.status(\"...initializing suno/bark processor\")\n",
        "        processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
        "        installer.status(\"...initializing BetterTransformer model\")\n",
        "        bark_model = BetterTransformer.transform(bark_model, keep_original_model=False)\n",
        "    import soundfile as sf\n",
        "    clear_pipes()\n",
        "    if not bark_prefs['use_bettertransformer']: preload_models()\n",
        "    clear_last()\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(bark_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, bark_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    history_prompt = bark_prefs['acoustic_prompt']\n",
        "    if history_prompt == \"Unconditional\":\n",
        "        history_prompt = None\n",
        "    if history_prompt == \"Anouncer\":\n",
        "        history_prompt = \"announcer\"\n",
        "    for i in range(bark_prefs['n_iterations']):\n",
        "        prt(Text(\"  Generating Bark Audio...\", weight=FontWeight.BOLD))\n",
        "        prt(progress)\n",
        "        #random_seed = int(bark_prefs['seed']) if int(bark_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        try:\n",
        "            if not bark_prefs['use_bettertransformer']:\n",
        "                audio_array = generate_audio(bark_prefs['text'], history_prompt=history_prompt, text_temp=bark_prefs['text_temp'], waveform_temp=bark_prefs['waveform_temp'])\n",
        "            else:\n",
        "                set_seed(0)\n",
        "                inputs = processor(bark_prefs['text'], voice_preset=history_prompt).to(torch_device)\n",
        "                audio_array = bark_model.generate(**inputs, do_sample = True, fine_temperature=bark_prefs['text_temp'], coarse_temperature=bark_prefs['waveform_temp'])\n",
        "                audio_array = audio_array.cpu().numpy().squeeze()\n",
        "                sample_rate = bark_model.generation_config.sample_rate\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error generating Bark waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "        fname = format_filename(bark_prefs['text'])\n",
        "        if fname[-1] == '.': fname = fname[:-1]\n",
        "        file_prefix = bark_prefs['file_prefix']\n",
        "        audio_name = f'{file_prefix}-{fname}'\n",
        "        audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "        fname = available_file(audio_out, audio_name, i, ext=\"wav\")\n",
        "        write_wav(fname, SAMPLE_RATE if not bark_prefs['use_bettertransformer'] else sample_rate, audio_array)\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        #a_out = Audio(src=fname, autoplay=False)\n",
        "        #page.overlay.append(a_out)\n",
        "        #page.update()\n",
        "        display_name = fname\n",
        "        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))\n",
        "        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_riffusion(page):\n",
        "    global riffusion_prefs, pipe_riffusion, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.riffusion_output.controls.append(line)\n",
        "      page.riffusion_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.riffusion_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(riffusion_prefs['prompt']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    \n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Riffusion Packages...\")\n",
        "    prt(installer)\n",
        "    riffusion_dir = os.path.join(root_dir, \"riffusion-inference\")\n",
        "    if not os.path.isdir(riffusion_dir):\n",
        "      installer.status(\"...cloning riffusion-inference\")\n",
        "      os.chdir(root_dir) # -b v0.3.0\n",
        "      run_process(\"git clone https://github.com/hmartiro/riffusion-inference\", page=page)\n",
        "    os.chdir(riffusion_dir)\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(riffusion_dir, 'riffusion'))\n",
        "    try:\n",
        "        from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "    except Exception:\n",
        "        try:\n",
        "            installer.status(\"...installing requirements\")\n",
        "            run_process(\"pip install -r requirements.txt\", page=page, cwd=riffusion_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Riffusion requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "        from riffusion.spectrogram_params import SpectrogramParams\n",
        "        try:\n",
        "            from riffusion.audio import spectrogram_from_waveform\n",
        "        except Exception:\n",
        "            print(\"Still can't find riffusion.audio import spectrogram_from_waveform\")\n",
        "            pass\n",
        "        #from IPython.display import Audio\n",
        "        from scipy.io import wavfile\n",
        "    def image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> PILImage.Image:\n",
        "        data = np.power(spectrogram, power_for_image)\n",
        "        data = data * 255 / max_volume\n",
        "        data = 255 - data\n",
        "        image = PILImage.fromarray(data.astype(np.uint8))\n",
        "        image = image.transpose(PILImage.FLIP_TOP_BOTTOM)\n",
        "        image = image.convert(\"RGB\")\n",
        "        return image\n",
        "    model_id=\"riffusion/riffusion-model-v1\"\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', riffusion_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(riffusion_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, riffusion_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audioldm-tts\", 'audioldm', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(riffusion_prefs['prompt'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = riffusion_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "\n",
        "    init = riffusion_prefs['audio_file']\n",
        "    if bool(init):\n",
        "        if init.startswith('http'):\n",
        "            init_audio = download_file(init)\n",
        "        else:\n",
        "            if os.path.isfile(init):\n",
        "                init_audio = init\n",
        "            else:\n",
        "                init_audio = None\n",
        "    else:\n",
        "        init_audio = None\n",
        "\n",
        "    if pipe_riffusion == None or (init_audio == None and riffusion_prefs['loaded_pipe'] == \"image\") or (init_audio != None and riffusion_prefs['loaded_pipe'] == \"text\"):\n",
        "      clear_pipes()\n",
        "      installer.status(\"...initializing pipeline\")\n",
        "      try:\n",
        "        if init_audio == None:\n",
        "            from diffusers import DiffusionPipeline #, custom_pipeline=\"AlanB/lpw_stable_diffusion_mod\"\n",
        "            pipe_riffusion = DiffusionPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_riffusion = optimize_pipe(pipe_riffusion)\n",
        "            riffusion_prefs['loaded_pipe'] = \"text\"\n",
        "        else:\n",
        "            from diffusers import StableDiffusionImg2ImgPipeline\n",
        "            pipe_riffusion = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_riffusion = pipe_riffusion.to(torch_device)\n",
        "            riffusion_prefs['loaded_pipe'] = \"image\"\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Riffusion package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    else:\n",
        "        clear_pipes('riffusion')\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Riffusion Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(riffusion_prefs['seed']) if int(riffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    try:\n",
        "        if init_audio == None:\n",
        "            params = SpectrogramParams()\n",
        "            converter = SpectrogramImageConverter(params)\n",
        "            specs = pipe_riffusion(\n",
        "                riffusion_prefs['prompt'],\n",
        "                negative_prompt=riffusion_prefs['negative_prompt'],\n",
        "                guidance_scale=riffusion_prefs['guidance_scale'],\n",
        "                steps=riffusion_prefs['steps'],\n",
        "                width=riffusion_prefs['max_size'],\n",
        "                height=riffusion_prefs['max_size'],\n",
        "                batch_size=riffusion_prefs['batch_size'],\n",
        "                generator=generator,\n",
        "            ).images\n",
        "        else:\n",
        "            rate, data = wavfile.read(init_audio)\n",
        "            data = np.mean(data, axis=1)\n",
        "            data = data.astype(np.float32)\n",
        "            data = data[rate*7:rate*14]\n",
        "            spectrogram = spectrogram_from_waveform(waveform=data, sample_rate=rate, n_fft=8192, hop_length=512, win_length=8192)\n",
        "            spec = image_from_spectrogram(spectrogram)\n",
        "            specs = pipe_riffusion(\n",
        "                prompt=riffusion_prefs['prompt'],\n",
        "                negative_prompt=riffusion_prefs['negative_prompt'],\n",
        "                image=spec,\n",
        "                strength=riffusion_prefs['strength'],\n",
        "                guidance_scale=riffusion_prefs['guidance_scale'],\n",
        "                steps=riffusion_prefs['steps'],\n",
        "                width=riffusion_prefs['max_size'],\n",
        "                height=riffusion_prefs['max_size'],\n",
        "                batch_size=riffusion_prefs['batch_size'],\n",
        "                generator=generator,\n",
        "            ).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating Spectrogram Image Converter from Diffusion...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for spec in specs:\n",
        "        audio_file = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        image_file = available_file(save_dir, audio_name, 0)\n",
        "        wav = converter.audio_from_spectrogram_image(image=spec)\n",
        "        wav.export(audio_file, format='wav')\n",
        "        spec.save(image_file)\n",
        "        #a_out = Audio(src=audio_file, autoplay=False)\n",
        "        #page.overlay.append(a_out)\n",
        "        #page.update()\n",
        "        display_name = audio_file\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "          audio_save = available_file(audio_out, fname, 0, ext='wav')\n",
        "          image_save = available_file(audio_out, fname, 0)\n",
        "          shutil.copy(audio_file, audio_save)\n",
        "          shutil.copy(image_file, image_save)\n",
        "          display_name = audio_save\n",
        "        prt(AudioPlayer(src=audio_file, display=display_name, data=display_name, page=page))\n",
        "        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_mubert(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.tortoise_output.controls.append(line)\n",
        "      page.tortoise_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.tortoise_output, lines=lines)\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Downloading Mubert Packages...\", )\n",
        "    prt(installer)\n",
        "    mubert_dir = os.path.join(root_dir, \"mubert-songs\")\n",
        "    if bool(mubert_prefs['batch_folder_name']):\n",
        "        mubert_dir = os.path.join(mubert_dir, mubert_prefs['batch_folder_name'])\n",
        "    make_dir(mubert_dir)\n",
        "    import time\n",
        "    pip_install(\"sentence_transformers httpx\", installer=installer)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import httpx\n",
        "    \n",
        "    MUBERT_TAGS_STRING = 'tribal,action,kids,neo-classic,run 130,pumped,jazz / funk,ethnic,dubtechno,reggae,acid jazz,liquidfunk,funk,witch house,tech house,underground,artists,mystical,disco,sensorium,r&b,agender,psychedelic trance / psytrance,peaceful,run 140,piano,run 160,setting,meditation,christmas,ambient,horror,cinematic,electro house,idm,bass,minimal,underscore,drums,glitchy,beautiful,technology,tribal house,country pop,jazz & funk,documentary,space,classical,valentines,chillstep,experimental,trap,new jack swing,drama,post-rock,tense,corporate,neutral,happy,analog,funky,spiritual,sberzvuk special,chill hop,dramatic,catchy,holidays,fitness 90,optimistic,orchestra,acid techno,energizing,romantic,minimal house,breaks,hyper pop,warm up,dreamy,dark,urban,microfunk,dub,nu disco,vogue,keys,hardcore,aggressive,indie,electro funk,beauty,relaxing,trance,pop,hiphop,soft,acoustic,chillrave / ethno-house,deep techno,angry,dance,fun,dubstep,tropical,latin pop,heroic,world music,inspirational,uplifting,atmosphere,art,epic,advertising,chillout,scary,spooky,slow ballad,saxophone,summer,erotic,jazzy,energy 100,kara mar,xmas,atmospheric,indie pop,hip-hop,yoga,reggaeton,lounge,travel,running,folk,chillrave & ethno-house,detective,darkambient,chill,fantasy,minimal techno,special,night,tropical house,downtempo,lullaby,meditative,upbeat,glitch hop,fitness,neurofunk,sexual,indie rock,future pop,jazz,cyberpunk,melancholic,happy hardcore,family / kids,synths,electric guitar,comedy,psychedelic trance & psytrance,edm,psychedelic rock,calm,zen,bells,podcast,melodic house,ethnic percussion,nature,heavy,bassline,indie dance,techno,drumnbass,synth pop,vaporwave,sad,8-bit,chillgressive,deep,orchestral,futuristic,hardtechno,nostalgic,big room,sci-fi,tutorial,joyful,pads,minimal 170,drill,ethnic 108,amusing,sleepy ambient,psychill,italo disco,lofi,house,acoustic guitar,bassline house,rock,k-pop,synthwave,deep house,electronica,gabber,nightlife,sport & fitness,road trip,celebration,electro,disco house,electronic'\n",
        "    MUBERT_TAGS = np.array(MUBERT_TAGS_STRING.split(','))\n",
        "    MUBERT_LICENSE = \"ttmmubertlicense#f0acYBenRcfeFpNT4wpYGaTQIyDI4mJGv5MfIhBFz97NXDwDNFHmMRsBSzmGsJwbTpP1A6i07AXcIeAHo5\"\n",
        "    MUBERT_MODE = \"loop\"\n",
        "    MUBERT_TOKEN = \"4951f6428e83172a4f39de05d5b3ab10d58560b8\"\n",
        "\n",
        "    def get_mubert_tags_embeddings(w2v_model):\n",
        "        return w2v_model.encode(MUBERT_TAGS)\n",
        "\n",
        "    def get_pat(email: str):\n",
        "        r = httpx.post('https://api-b2b.mubert.com/v2/GetServiceAccess', json={\"method\": \"GetServiceAccess\", \"params\": {\"email\": email, \"license\": MUBERT_LICENSE, \"token\": MUBERT_TOKEN, \"mode\": MUBERT_MODE}})\n",
        "        rdata = json.loads(r.text)\n",
        "        if rdata['status'] != 1:\n",
        "            alert_msg(page, \"ERROR Requesting Mubert Service. Probably incorrect e-mail...\")\n",
        "        pat = rdata['data']['pat']\n",
        "        return pat\n",
        "\n",
        "    def find_similar(em, embeddings, method='cosine'):\n",
        "        scores = []\n",
        "        for ref in embeddings:\n",
        "            if method == 'cosine':\n",
        "                scores.append(1 - np.dot(ref, em) / (np.linalg.norm(ref) * np.linalg.norm(em)))\n",
        "            if method == 'norm':\n",
        "                scores.append(np.linalg.norm(ref - em))\n",
        "        return np.array(scores), np.argsort(scores)\n",
        "\n",
        "    def get_tags_for_prompts(w2v_model, mubert_tags_embeddings, prompts, top_n=3, debug=False):\n",
        "        prompts_embeddings = w2v_model.encode(prompts)\n",
        "        ret = []\n",
        "        for i, pe in enumerate(prompts_embeddings):\n",
        "            scores, idxs = find_similar(pe, mubert_tags_embeddings)\n",
        "            top_tags = MUBERT_TAGS[idxs[:top_n]]\n",
        "            top_prob = 1 - scores[idxs[:top_n]]\n",
        "            if debug:\n",
        "                prt(f\"Prompt: {prompts[i]}\\nTags: {', '.join(top_tags)}\\nScores: {top_prob}\\n\\n\\n\")\n",
        "            ret.append((prompts[i], list(top_tags)))\n",
        "        return ret\n",
        "    minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    mubert_tags_embeddings = get_mubert_tags_embeddings(minilm)\n",
        "\n",
        "    def get_track_by_tags(tags, pat, duration, maxit=20, loop=False):\n",
        "        if loop:\n",
        "            mode = \"loop\"\n",
        "        else:\n",
        "            mode = \"track\"\n",
        "        r = httpx.post('https://api-b2b.mubert.com/v2/RecordTrackTTM', json={\"method\": \"RecordTrackTTM\", \"params\": { \"pat\": pat,\"duration\": duration, \"tags\": tags, \"mode\": mode}})\n",
        "        rdata = json.loads(r.text)\n",
        "        assert rdata['status'] == 1, rdata['error']['text']\n",
        "        trackurl = rdata['data']['tasks'][0]['download_link']\n",
        "        prt('Generating your Mubert track... ')\n",
        "        for i in range(maxit):\n",
        "            r = httpx.get(trackurl)\n",
        "            if r.status_code == 200:\n",
        "                return trackurl\n",
        "            time.sleep(1)\n",
        "\n",
        "    def generate_track_by_prompt(email, prompt, duration, loop=False):\n",
        "        try:\n",
        "            pat = get_pat(email)\n",
        "            _, tags = get_tags_for_prompts(minilm, mubert_tags_embeddings, [prompt, ])[0]\n",
        "            return get_track_by_tags(tags, pat, int(duration), loop=loop), \"Success\", \", \".join(tags)\n",
        "        except Exception as e:\n",
        "            return None, str(e), \"\"\n",
        "    #btn.click(fn=generate_track_by_prompt, inputs=[email, prompt, duration, is_loop], outputs=[out, result_msg, tags])\n",
        "    clear_last()\n",
        "    out, result_msg, tags = generate_track_by_prompt(mubert_prefs['email'], mubert_prefs['prompt'], mubert_prefs['duration'], loop=mubert_prefs['is_loop'])\n",
        "    if out == None:\n",
        "      alert_msg(page, \"Error generating track by prompt. The API Key problably reached montly limit...\",  content=Text(result_msg))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    mubert_songs = os.path.join(audio_out, 'mubert_songs')\n",
        "    audio_name = format_filename(mubert_prefs['prompt'])\n",
        "    audio_name = f\"{mubert_prefs['file_prefix']}{audio_name}\"\n",
        "    if bool(mubert_prefs['batch_folder_name']):\n",
        "      mubert_songs = os.path.join(audio_out, mubert_prefs['batch_folder_name'])\n",
        "    os.makedirs(mubert_songs, exist_ok=True)\n",
        "    fname = available_file(mubert_songs, audio_name, 0, ext=\"mp3\")\n",
        "    audio_file = download_file(out)\n",
        "    shutil.copy(audio_file, fname)\n",
        "    #a_out = Audio(src=fname, autoplay=False)\n",
        "    #page.overlay.append(a_out)\n",
        "    #page.update()\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='mp3')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))\n",
        "    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Column([Text(display_name), Text(tags, style=TextThemeStyle.DISPLAY_SMALL)])]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_whisper(page):\n",
        "    global whisper_prefs, whisper_requests, pipe_whisper, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Whisper.controls.append(line)\n",
        "      page.Whisper.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Whisper, lines=lines)\n",
        "    def clear_list():\n",
        "      page.Whisper.controls = page.Whisper.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Whisper.auto_scroll = scroll\n",
        "      page.Whisper.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Installing OpenAI Whisper Speech-to-Text Packages...\")\n",
        "    prt(installer)\n",
        "    try:\n",
        "        import whisper\n",
        "    except Exception as e:\n",
        "        installer.status(\"...openai/whisper.git\")\n",
        "        run_sp(\"pip install git+https://github.com/openai/whisper.git -q\", realtime=False)\n",
        "        import whisper\n",
        "        pass\n",
        "    if 'loaded_whisper' not in status:\n",
        "        status['loaded_whisper'] = whisper_prefs['model_size']\n",
        "    if status['loaded_whisper'] == whisper_prefs['model_size']:\n",
        "        clear_pipes(\"whisper\")\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    newer = '-v' in whisper_prefs['model_size']\n",
        "    if newer:\n",
        "        pip_install(\"git+https://github.com/huggingface/transformers.git|transformers accelerate\", installer=installer)\n",
        "        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "        from transformers.pipelines.audio_utils import ffmpeg_read\n",
        "    if pipe_whisper is None:\n",
        "        if not newer:\n",
        "            installer.status(f\"...load_model {whisper_prefs['model_size']}\")\n",
        "            pipe_whisper = whisper.load_model(whisper_prefs['model_size'])\n",
        "            status['loaded_whisper'] = whisper_prefs['model_size']\n",
        "        else:\n",
        "            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "            model_id = f\"openai/whisper-{whisper_prefs['model_size']}\"\n",
        "            pipe_whisper = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "                model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        "            ).to(torch_device)\n",
        "            processor = AutoProcessor.from_pretrained(model_id)\n",
        "            pipe_whisper = pipeline(\n",
        "                \"automatic-speech-recognition\",\n",
        "                model=model,\n",
        "                tokenizer=processor.tokenizer,\n",
        "                feature_extractor=processor.feature_extractor,\n",
        "                max_new_tokens=128,\n",
        "                chunk_length_s=30,\n",
        "                batch_size=8,\n",
        "                return_timestamps=True,\n",
        "                torch_dtype=torch_dtype,\n",
        "                device=torch_device,\n",
        "            )\n",
        "    from_language = \"\"\n",
        "    def transcribe(audio):\n",
        "        nonlocal installer, from_language\n",
        "        if not newer:\n",
        "            installer.status(\"...load_audio\")\n",
        "            audio = whisper.load_audio(audio)\n",
        "            if whisper_prefs['trim_audio']:\n",
        "                installer.status(\"...pad_or_trim audio\")\n",
        "                audio = whisper.pad_or_trim(audio)\n",
        "            if whisper_prefs['simple_transcribe']:\n",
        "                installer.status(\"...Whisper transcribe\")\n",
        "                options = {\n",
        "                    \"task\": \"transcribe\"\n",
        "                }\n",
        "                result = whisper.transcribe(pipe_whisper, audio, **options)\n",
        "            else:\n",
        "                installer.status(\"...log_mel_spectrogram\")\n",
        "                mel = whisper.log_mel_spectrogram(audio).to(pipe_whisper.device)\n",
        "                if whisper_prefs['detect_language'] or whisper_prefs['translate']:\n",
        "                    installer.status(\"...detect_language\")\n",
        "                    _, probs = pipe_whisper.detect_language(mel)\n",
        "                    from_language = max(probs, key=probs.get)\n",
        "                    prt(f\"Detected language: {from_language}\")\n",
        "                installer.status(\"...DecodingOptions\")\n",
        "                options = whisper.DecodingOptions(fp16 = False)\n",
        "                result = whisper.decode(pipe_whisper, mel, options)\n",
        "            return result.text\n",
        "        else:\n",
        "            installer.status(\"...load audio\")\n",
        "            inputs = ffmpeg_read(audio, pipe_whisper.feature_extractor.sampling_rate)\n",
        "            inputs = {\"array\": inputs, \"sampling_rate\": pipe.feature_extractor.sampling_rate}\n",
        "            generate_kwargs={\"language\": f\"<|en|>\"}#{\"task\": \"translate\"})\n",
        "            installer.status(\"...Whisper transcribe\")\n",
        "            result = pipe_whisper(inputs)\n",
        "            return result['text']\n",
        "    audio_path = whisper_prefs['audio_file'].strip()\n",
        "    local_path = \"\"\n",
        "    if audio_path.startswith(\"http\"):\n",
        "        if audio_path.startswith(\"https://youtu\") or 'youtube' in audio_path:\n",
        "            try:\n",
        "                import ffmpeg\n",
        "            except ImportError as e:\n",
        "                installer.status(\"...installing ffmpeg\")\n",
        "                run_sp(\"pip install -q ffmpeg\", realtime=False)\n",
        "                pass\n",
        "            try:\n",
        "                import yt_dlp\n",
        "            except ImportError as e:\n",
        "                installer.status(\"...installing yt_dlp\")\n",
        "                run_sp(\"pip install yt_dlp\", realtime=False)\n",
        "                import yt_dlp\n",
        "                pass\n",
        "            f_name = \"\"\n",
        "            def cb(d):\n",
        "                nonlocal f_name\n",
        "                if d[\"status\"] == \"downloading\" and \"total_bytes_estimate\" in d:# and d[\"total_bytes\"] > 0:\n",
        "                    if d[\"total_bytes_estimate\"] > 0:\n",
        "                        dl_progress = float(d[\"downloaded_bytes\"]) / float(d[\"total_bytes_estimate\"])\n",
        "                        progress.value = dl_progress\n",
        "                        progress.update()\n",
        "                else:\n",
        "                    f_name = d['filename']\n",
        "            installer.status(\"...getting YouTube file\")\n",
        "            prt(progress)\n",
        "            ydl_opts = {\n",
        "                'format': 'm4a/bestaudio/best',\n",
        "                \"progress_hooks\": [cb],\n",
        "                'verbose': False,\n",
        "                'quiet': True,\n",
        "                'postprocessors': [{\n",
        "                    'key': 'FFmpegExtractAudio',\n",
        "                    'preferredcodec': 'mp3',\n",
        "                }]\n",
        "            }\n",
        "            try:\n",
        "                with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                    error_code = ydl.download(audio_path)\n",
        "            except Exception as e:\n",
        "                alert_msg(page, f\"ERROR: Couldn't download YouTube video for some reason...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            clear_last()\n",
        "            #print(error_code)\n",
        "            if error_code != 0:\n",
        "                alert_msg(page, f\"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...\")\n",
        "                return\n",
        "            local_path = f\"{f_name.rpartition('.')[0]}.mp3\"\n",
        "            #file_object = yt.formats.filter(only_audio=True)[0].download()\n",
        "            #f_name = yt.safe_filename()\n",
        "            #.download(convert='mp3')\n",
        "            #file_object = yt.download() #b variable stores the filename and meta(if available) as object of Output class.\n",
        "            #installer.status(\"...converting to mp3\")\n",
        "            #local_path = AudioSegment.from_file(file_object).export(f\"{f_name}.mp3\", format=\"mp3\")\n",
        "            #extras.Convert(file_object,'mp3',add_meta=True)\n",
        "            #local_path = file_object.file_path\n",
        "        else:\n",
        "            installer.status(\"...downloading file\")\n",
        "            local_path = download_file(audio_path)\n",
        "    else:\n",
        "        local_path = audio_path\n",
        "    \n",
        "    if (local_path.endswith(\"mp4\") or local_path.endswith(\"avi\")):\n",
        "        pip_install(\"ffmpeg\", installer=installer)\n",
        "        import ffmpeg\n",
        "        installer.status(\"...converting to mp3\")\n",
        "        video = ffmpeg.input(local_path)\n",
        "        audio = video.audio\n",
        "        filename, extension = os.path.splitext(os.path.basename(local_path))\n",
        "        local_path = os.path.join(os.path.dirname(local_path), f\"{filename}.mp3\")\n",
        "        audio.output(local_path)\n",
        "    elif not (local_path.endswith(\"mp3\") or local_path.endswith(\"wav\")):\n",
        "        alert_msg(page, f\"ERROR: File path must be an mp3, wav, mp4 or avi file...\")\n",
        "        return\n",
        "    if not os.path.exists(local_path):\n",
        "        alert_msg(page, f\"ERROR: File not found...\")\n",
        "        return\n",
        "    installer.status(\"\")\n",
        "    installer.show_progress(False)\n",
        "    installer.set_message(\"Running Whisper-AI on your Dialog...\")\n",
        "    prt(progress)\n",
        "    try:\n",
        "        #TODO: Split long files into smaller chunks to transcribe queue\n",
        "        #ffmpeg -hide_banner -i <YOUR AUDIO FILE> -c copy -map 0 -segment_time 1:0:0 -f segment <OUTPUT DIR>/segment_%03d\n",
        "        transcription = transcribe(local_path)\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"ERROR: Couldn't Transcribe audio for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    clear_last()\n",
        "    installer.status(\"\")\n",
        "    installer.set_message(\"Generated Whisper-AI Transcription:\")\n",
        "    prt(Text(transcription, size=20, selectable=True))\n",
        "    use_ai = whisper_prefs['reformat'] or whisper_prefs['rewrite'] or whisper_prefs['summarize'] or whisper_prefs['describe'] or whisper_prefs['article'] or whisper_prefs['keypoints'] or whisper_prefs['keywords']\n",
        "    if use_ai:\n",
        "        good_key = True\n",
        "        if 'GPT' in whisper_prefs['AI_engine']:\n",
        "            try:\n",
        "                if not bool(prefs['OpenAI_api_key']): good_key = False\n",
        "            except NameError: good_key = False\n",
        "            if not good_key:\n",
        "                alert_msg(page, f\"Missing OpenAI_api_key... Define your key in Settings.\")\n",
        "                return\n",
        "            else:\n",
        "                try:\n",
        "                    import openai\n",
        "                except ModuleNotFoundError:\n",
        "                    installer.status(\"...installing openai\")\n",
        "                    run_sp(\"pip install --upgrade openai -qq\", realtime=False)\n",
        "                    pass\n",
        "                finally:\n",
        "                    import openai\n",
        "                try:\n",
        "                    from openai import OpenAI\n",
        "                    openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "                except:\n",
        "                    alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "                    return\n",
        "                installer.status(\"\")\n",
        "        elif whisper_prefs['AI_engine'] == \"Google Gemini\":\n",
        "            if not bool(prefs['PaLM_api_key']):\n",
        "                alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "                return\n",
        "            try:\n",
        "                import google.generativeai as genai\n",
        "                if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "            except:\n",
        "                installer.status(\"Installing Google MakerSuite Library...\")\n",
        "                run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "                import google.generativeai as genai\n",
        "                pass\n",
        "            try:\n",
        "                genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "            except:\n",
        "                alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "                return\n",
        "            gemini_model = genai.GenerativeModel(model_name='models/gemini-pro')\n",
        "            installer.status(\"\")\n",
        "    def question(request):\n",
        "        if whisper_prefs['AI_engine'] == \"OpenAI GPT-3\":\n",
        "            response = openai_client.completions.create(engine=\"text-davinci-003\", prompt=request, max_tokens=2400, temperature=whisper_prefs['AI_temperature'], presence_penalty=1)\n",
        "            result = response.choices[0].text.strip()\n",
        "        elif whisper_prefs['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "            response = openai_client.chat.completions.create(model=\"gpt-3.5-turbo-16k\", temperature=whisper_prefs['AI_temperature'], messages=[{\"role\": \"user\", \"content\": request}])\n",
        "            result = response.choices[0].message.content.strip()\n",
        "        elif \"GPT-4\" in whisper_prefs['AI_engine']:\n",
        "            gpt_model = \"gpt-4-1106-preview\" if \"Turbo\" in whisper_prefs['AI_engine'] else \"gpt-4\"\n",
        "            response = openai_client.chat.completions.create(model=gpt_model, temperature=whisper_prefs['AI_temperature'], messages=[{\"role\": \"user\", \"content\": request}])\n",
        "            result = response.choices[0].message.content.strip()\n",
        "        elif whisper_prefs['AI_engine'] == \"Google Gemini\":\n",
        "            response = gemini_model.generate_content(request, generation_config={\n",
        "                'temperature': whisper_prefs['AI_temperature'],\n",
        "                'max_output_tokens': 1024\n",
        "            })\n",
        "            #response = palm.generate_text(model='models/text-bison-001', prompt=request, temperature=whisper_prefs['AI_temperature'], max_output_tokens=1024)\n",
        "            result = response.text.strip()\n",
        "        if '*' in result:\n",
        "            result = result.replace('*', '').strip()\n",
        "        return result\n",
        "    if whisper_prefs['reformat']:\n",
        "        prt(\"Reformat of transcript:\")\n",
        "        response = question(f\"{whisper_requests['reformat']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['rewrite']:\n",
        "        prt(\"Rewrite edit of transcript:\")\n",
        "        response = question(f\"{whisper_requests['rewrite']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['summarize']:\n",
        "        prt(\"Summary of transcript:\")\n",
        "        response = question(f\"{whisper_requests['summarize']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['describe']:\n",
        "        prt(\"Description of transcript:\")\n",
        "        response = question(f\"{whisper_requests['describe']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['article']:\n",
        "        prt(\"Article of transcript:\")\n",
        "        response = question(f\"{whisper_requests['article']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['keypoints']:\n",
        "        prt(\"Key Points of transcript:\")\n",
        "        response = question(f\"{whisper_requests['keypoints']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['keywords']:\n",
        "        prt(\"SEO Keywords of transcript:\")\n",
        "        response = question(f\"{whisper_requests['keywords']}\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    if whisper_prefs['translate']:\n",
        "        prt(\"Translation of transcript:\")\n",
        "        response = question(f\"Translate the following text from {from_language} into {whisper_prefs['to_language']}.\\n{transcription}\")\n",
        "        prt(Text(response, size=18, selectable=True))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_voice_fixer(page):\n",
        "    global voice_fixer_prefs, pipe_voice_fixer\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.VoiceFixer.controls.append(line)\n",
        "      page.VoiceFixer.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.VoiceFixer, lines=lines)\n",
        "    def clear_list():\n",
        "      page.VoiceFixer.controls = page.VoiceFixer.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.VoiceFixer.auto_scroll = scroll\n",
        "      page.VoiceFixer.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not (voice_fixer_prefs['mode_0'] or voice_fixer_prefs['mode_1'] or voice_fixer_prefs['mode_2']):\n",
        "        alert_msg(page, f\"You must select at least one Fixer Mode to restore...\")\n",
        "        return\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Installing VoiceFixer Packages...\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"voice_fixer\")\n",
        "    try:\n",
        "        from voicefixer import VoiceFixer\n",
        "    except ModuleNotFoundError as e:\n",
        "        installer.status(\"...installing voicefixer (takes a while)\")\n",
        "        run_sp(\"pip install voicefixer --upgrade\", realtime=False)\n",
        "        from voicefixer import VoiceFixer\n",
        "        pass\n",
        "    if pipe_voice_fixer == None:\n",
        "        installer.status(\"...initializing voicefixer model\")\n",
        "        pipe_voice_fixer = VoiceFixer()\n",
        "    audio_path = voice_fixer_prefs['audio_file'].strip()\n",
        "    local_path = \"\"\n",
        "    if audio_path.startswith(\"http\"):\n",
        "        if audio_path.startswith(\"https://youtu\") or 'youtube' in audio_path:\n",
        "            pip_install(\"ffmpeg yt_dlp\", installer=installer)\n",
        "            import yt_dlp\n",
        "            f_name = \"\"\n",
        "            def cb(d):\n",
        "                nonlocal f_name\n",
        "                if d[\"status\"] == \"downloading\" and \"total_bytes_estimate\" in d:# and d[\"total_bytes\"] > 0:\n",
        "                    if d[\"total_bytes_estimate\"] > 0:\n",
        "                        dl_progress = float(d[\"downloaded_bytes\"]) / float(d[\"total_bytes_estimate\"])\n",
        "                        progress.value = dl_progress\n",
        "                        progress.update()\n",
        "                else:\n",
        "                    f_name = d['filename']\n",
        "            installer.status(\"...getting YouTube file\")\n",
        "            prt(progress)\n",
        "            ydl_opts = {\n",
        "                'format': 'm4a/bestaudio/best',\n",
        "                \"progress_hooks\": [cb],\n",
        "                'verbose': False,\n",
        "                'quiet': True,\n",
        "                'postprocessors': [{\n",
        "                    'key': 'FFmpegExtractAudio',\n",
        "                    'preferredcodec': 'mp3',\n",
        "                }]\n",
        "            }\n",
        "            try:\n",
        "                with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                    error_code = ydl.download(audio_path)\n",
        "            except Exception as e:\n",
        "                alert_msg(page, f\"ERROR: Couldn't download YouTube video for some reason...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            clear_last()\n",
        "            #print(error_code)\n",
        "            if error_code != 0:\n",
        "                alert_msg(page, f\"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...\")\n",
        "                return\n",
        "            local_path = f\"{f_name.rpartition('.')[0]}.mp3\"\n",
        "        else:\n",
        "            installer.status(\"...downloading file\")\n",
        "            local_path = download_file(audio_path)\n",
        "    else:\n",
        "        local_path = audio_path\n",
        "    if not (local_path.endswith(\"mp3\") or local_path.endswith(\"wav\") or local_path.endswith(\"flac\")):\n",
        "        alert_msg(page, f\"ERROR: File path must be a wav, mp3 or flac file...\")\n",
        "        return\n",
        "    if not os.path.exists(local_path):\n",
        "        alert_msg(page, f\"ERROR: Audio File not found...\")\n",
        "        return\n",
        "    if local_path.endswith(\"mp3\"):\n",
        "        try:\n",
        "            import ffmpeg\n",
        "        except ImportError as e:\n",
        "            installer.status(\"...installing ffmpeg\")\n",
        "            run_sp(\"pip install -q ffmpeg\", realtime=False)\n",
        "            pass\n",
        "        try:\n",
        "            import pydub\n",
        "        except ImportError:\n",
        "            installer.status(\"...installing pydub\")\n",
        "            run_sp(\"pip install -q pydub\", realtime=False)\n",
        "            import pydub\n",
        "            pass\n",
        "        from pydub import AudioSegment\n",
        "        installer.status(\"...converting from mp3\")\n",
        "        sound = AudioSegment.from_mp3(local_path)\n",
        "        wav_file = local_path.rpartition(slash)[2].rpartition(\".\")[0]\n",
        "        local_path = available_file(root_dir, wav_file, 0, ext='wav')\n",
        "        sound = sound.set_frame_rate(44100)\n",
        "        installer.status(\"...converting to wav\")\n",
        "        sound.export(local_path, format=\"wav\")\n",
        "    #save_dir = os.path.join(root_dir, 'audio_out', voice_fixer_prefs['batch_folder_name'])\n",
        "    #if not os.path.exists(save_dir):\n",
        "    #    os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(voice_fixer_prefs['batch_folder_name']):\n",
        "        audio_out = os.path.join(audio_out, voice_fixer_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    if bool(voice_fixer_prefs['audio_name']):\n",
        "        fname = format_filename(voice_fixer_prefs['audio_name'], force_underscore=True)\n",
        "    else: fname = \"output\"\n",
        "    fname = f\"{voice_fixer_prefs['file_prefix']}{fname}\"\n",
        "    output_file = available_file(audio_out, fname, 0, ext='wav')\n",
        "    modes = []\n",
        "    if voice_fixer_prefs['mode_0']: modes.append(0)\n",
        "    if voice_fixer_prefs['mode_1']: modes.append(1)\n",
        "    if voice_fixer_prefs['mode_2']: modes.append(2)\n",
        "    clear_last()\n",
        "    try:\n",
        "        for mode in modes:\n",
        "            prt(f\"Running VoiceFixer mode {mode} on your Recording...\")\n",
        "            prt(progress)\n",
        "            output_file = available_file(audio_out, f\"{fname}-mode{mode}\", 0, ext='wav', no_num=True)\n",
        "            pipe_voice_fixer.restore(input=local_path, output=output_file, cuda=torch_device == \"cuda\", mode=mode)\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            prt(AudioPlayer(src=output_file, display=output_file, page=page))\n",
        "            #a_out = Audio(src=output_file, autoplay=False)\n",
        "            #page.overlay.append(a_out)\n",
        "            #page.update()\n",
        "            #display_name = output_file\n",
        "            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"ERROR: Couldn't Restore audio for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        clear_last()\n",
        "        return\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "loaded_StableUnCLIP = None\n",
        "def run_unCLIP(page, from_list=False):\n",
        "    global unCLIP_prefs, pipe_unCLIP, loaded_StableUnCLIP\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.unCLIP.controls.append(line)\n",
        "        if update:\n",
        "          page.unCLIP.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.unCLIP, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.unCLIP.controls = page.unCLIP.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.unCLIP.auto_scroll = scroll\n",
        "        page.unCLIP.update()\n",
        "      else:\n",
        "        page.unCLIP.auto_scroll = scroll\n",
        "        page.unCLIP.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "      total_steps = unCLIP_prefs['prior_num_inference_steps']\n",
        "    else:\n",
        "      total_steps = unCLIP_prefs['prior_num_inference_steps'] + unCLIP_prefs['decoder_num_inference_steps'] + unCLIP_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        unCLIP_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(unCLIP_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      unCLIP_prompts.append(unCLIP_prefs['prompt'])\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes('unCLIP')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    model_id = \"kakaobrain/karlo-v1-alpha\"\n",
        "    stable = \"Stable \" if unCLIP_prefs['use_StableUnCLIP_pipeline'] else \"\"\n",
        "    if pipe_unCLIP != None and ((loaded_StableUnCLIP == True and not unCLIP_prefs['use_StableUnCLIP_pipeline']) or (loaded_StableUnCLIP == False and unCLIP_prefs['use_StableUnCLIP_pipeline'])):\n",
        "        del pipe_unCLIP\n",
        "        flush()\n",
        "        pipe_unCLIP = None\n",
        "    if pipe_unCLIP == None:\n",
        "        prt(Installing(f\"  Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "              from diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline\n",
        "              from diffusers.models import PriorTransformer\n",
        "              from transformers import CLIPTokenizer, CLIPTextModelWithProjection\n",
        "              prior = PriorTransformer.from_pretrained(model_id, subfolder=\"prior\", torch_dtype=torch.float16)\n",
        "              prior_text_model_id = \"openai/clip-vit-large-patch14\"\n",
        "              prior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)\n",
        "              prior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=torch.float16)\n",
        "              prior_scheduler = UnCLIPScheduler.from_pretrained(model_id, subfolder=\"prior_scheduler\")\n",
        "              #prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)\n",
        "              prior_scheduler = pipeline_scheduler(prior_scheduler, from_scheduler=False)\n",
        "              stable_unclip_model_id = \"stabilityai/stable-diffusion-2-1-unclip-small\"\n",
        "              pipe_unCLIP = StableUnCLIPPipeline.from_pretrained(\n",
        "                  stable_unclip_model_id,\n",
        "                  torch_dtype=torch.float16,\n",
        "                  variant=\"fp16\",\n",
        "                  prior_tokenizer=prior_tokenizer,\n",
        "                  prior_text_encoder=prior_text_model,\n",
        "                  prior=prior,\n",
        "                  prior_scheduler=prior_scheduler,\n",
        "              )\n",
        "              pipe_unCLIP.to(torch_device)\n",
        "              pipe_unCLIP.enable_attention_slicing()\n",
        "              pipe_unCLIP.enable_sequential_cpu_offload()\n",
        "              #from diffusers import DiffusionPipeline\n",
        "              #pipe_unCLIP = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"stable_unclip\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, decoder_pipe_kwargs=dict(image_encoder=None))\n",
        "              #pipe_unCLIP.to(torch_device)\n",
        "              #pipe_unCLIP = optimize_pipe(pipe_unCLIP)\n",
        "              loaded_StableUnCLIP = True\n",
        "            else:\n",
        "              from diffusers import UnCLIPPipeline\n",
        "              pipe_unCLIP = UnCLIPPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "              #pipe_unCLIP.to(torch_device)\n",
        "              pipe_unCLIP = optimize_pipe(pipe_unCLIP, freeu=False)\n",
        "              loaded_StableUnCLIP = False\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Downloading {stable}unCLIP Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        pipe_unCLIP.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\" Generating {stable}unCLIP{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in unCLIP_prompts:\n",
        "        for num in range(unCLIP_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_prefs['seed']) + num) if int(unCLIP_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                if unCLIP_prefs['use_StableUnCLIP_pipeline']:#decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'],\n",
        "                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], num_images_per_prompt=1, width=512, height=512, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                else:\n",
        "                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"Error running {stable}unCLIP Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_prefs['display_upscaled_image'] or not unCLIP_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, width=512, height=512, data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    w = int(unCLIP_prefs['width'] * unCLIP_prefs[\"enlarge_scale\"])\n",
        "                    h = int(unCLIP_prefs['height'] * unCLIP_prefs[\"enlarge_scale\"])\n",
        "                    prt(Row([Text(f'Enlarging {unCLIP_prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "                    upscale_image(image_path, upscaled_path, scale=unCLIP_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    clear_last()\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {unCLIP_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"{stable}unCLIP\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr)\n",
        "                      config_json = unCLIP_prefs.copy()\n",
        "                      config_json['model_path'] = model_id\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                if unCLIP_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_image_variation(page, from_list=False):\n",
        "    global unCLIP_image_variation_prefs, pipe_unCLIP_image_variation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.controls.append(line)\n",
        "        if update:\n",
        "          page.UnCLIP_ImageVariation.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.UnCLIP_ImageVariation, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.controls = page.UnCLIP_ImageVariation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.UnCLIP_ImageVariation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageVariation.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageVariation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_image_variation_prefs['decoder_num_inference_steps'] + unCLIP_image_variation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_image_variation_inits = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if bool(p['init_image']):\n",
        "          unCLIP_image_variation_inits.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(unCLIP_image_variation_prefs['init_image']):\n",
        "        alert_msg(page, \"You need to add a Initial Image first... \")\n",
        "        return\n",
        "      unCLIP_image_variation_inits.append(unCLIP_image_variation_prefs['init_image'])\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_pipes('unCLIP_image_variation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_unCLIP_image_variation == None:\n",
        "        from diffusers import UnCLIP_ImageVariationPipeline\n",
        "        prt(Installing(\" Downloading unCLIP Image Variation Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            pipe_unCLIP_image_variation = UnCLIP_ImageVariationPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_unCLIP_image_variation.to(torch_device)\n",
        "            pipe_unCLIP_image_variation = optimize_pipe(pipe_unCLIP_image_variation, freeu=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading unCLIP Image Variation Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_unCLIP_image_variation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_image_variation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating unCLIP Image Variation{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for init in unCLIP_image_variation_inits:\n",
        "        if init.startswith('http'):\n",
        "          init_img = PILImage.open(requests.get(init, stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(init):\n",
        "            init_img = PILImage.open(init)\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {init}\")\n",
        "            return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_variation_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        for num in range(unCLIP_image_variation_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_image_variation_prefs['seed']) + num) if int(unCLIP_image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_unCLIP_image_variation(image=init_img, decoder_num_inference_steps=unCLIP_image_variation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_variation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_variation_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running unCLIP Image Variation Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            #fname = format_filename(unCLIP_image_variation_prefs['file_name'])\n",
        "            fname = init.rpartition(slash)[2].rpartition('.')[0]\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_image_variation_prefs['display_upscaled_image'] or not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=unCLIP_image_variation_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if unCLIP_image_variation_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_variation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_image_variation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {unCLIP_image_variation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"unCLIP_image_variation\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      #metadata.add_text(\"title\", unCLIP_image_variation_prefs['file_name'])\n",
        "                      config_json = unCLIP_image_variation_prefs.copy()\n",
        "                      config_json['model_path'] = \"fusing/karlo-image-variations-diffusers\"\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      del config_json['file_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_interpolation(page, from_list=False):\n",
        "    global unCLIP_interpolation_prefs, pipe_unCLIP_interpolation, loaded_StableUnCLIP\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.unCLIP_interpolation_output.controls.append(line)\n",
        "        if update:\n",
        "          page.unCLIP_interpolation_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.unCLIP_interpolation_output, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.unCLIP_Interpolation.controls = page.unCLIP_Interpolation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.unCLIP_Interpolation.auto_scroll = scroll\n",
        "        page.unCLIP_Interpolation.update()\n",
        "      else:\n",
        "        page.unCLIP_Interpolation.auto_scroll = scroll\n",
        "        page.unCLIP_Interpolation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_interpolation_prefs['prior_num_inference_steps'] + unCLIP_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_interpolation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_interpolation_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        unCLIP_interpolation_prompts.append({'start_prompt': p.prompt, 'end_prompt':p['prompt2']})\n",
        "    else:\n",
        "      if not bool(unCLIP_interpolation_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      unCLIP_interpolation_prompts.append({'start_prompt': unCLIP_interpolation_prefs['prompt'], 'end_prompt':unCLIP_interpolation_prefs['end_prompt']})\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes()#'unCLIP_interpolation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    model_id = \"kakaobrain/karlo-v1-alpha\"\n",
        "    stable = \"Stable \"\n",
        "    if pipe_unCLIP_interpolation != None:\n",
        "        del pipe_unCLIP_interpolation\n",
        "        flush()\n",
        "        pipe_unCLIP_interpolation = None\n",
        "    if pipe_unCLIP_interpolation == None:\n",
        "        prt(Installing(f\"Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            from diffusers import DiffusionPipeline\n",
        "            pipe_unCLIP_interpolation = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"AlanB/unclip_text_interpolation_mod\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, decoder_pipe_kwargs=dict(image_encoder=None)\n",
        "            pipe_unCLIP_interpolation.to(torch_device)\n",
        "            pipe_unCLIP_interpolation.enable_attention_slicing()\n",
        "            pipe_unCLIP_interpolation.enable_sequential_cpu_offload()\n",
        "            loaded_StableUnCLIP = True\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Downloading {stable}unCLIP Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_unCLIP_interpolation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_interpolation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\" Generating {stable}unCLIP{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in unCLIP_interpolation_prompts:\n",
        "        for num in range(unCLIP_interpolation_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_interpolation_prefs['seed']) + num) if int(unCLIP_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_unCLIP_interpolation(start_prompt=pr['start_prompt'], end_prompt=pr['end_prompt'], steps=unCLIP_interpolation_prefs['steps'], prior_num_inference_steps=unCLIP_interpolation_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_interpolation_prefs['decoder_num_inference_steps'], super_res_inference_steps=unCLIP_interpolation_prefs['super_res_inference_steps'], prior_guidance_scale=unCLIP_interpolation_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_interpolation_prefs['decoder_guidance_scale'], callback=callback_fnc, generator=generator).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"Error running {stable}unCLIP Interpolation Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            file_max_length = int(prefs['file_max_length']) / 2\n",
        "            fname = f\"{format_filename(pr['start_prompt'], max_length=file_max_length)}-to-{format_filename(pr['end_prompt'], max_length=file_max_length)}\"\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_interpolation_prefs['display_upscaled_image'] or not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=unCLIP_interpolation_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {unCLIP_interpolation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"{stable}unCLIP Text Interpolation\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", f\"{pr['start_prompt']}-to-{pr['end_prompt']}\")\n",
        "                      config_json = unCLIP_interpolation_prefs.copy()\n",
        "                      config_json['model_path'] = model_id\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                if unCLIP_interpolation_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_interpolation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_interpolation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_image_interpolation(page, from_list=False):\n",
        "    global unCLIP_image_interpolation_prefs, pipe_unCLIP_image_interpolation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.controls.append(line)\n",
        "        if update:\n",
        "          page.UnCLIP_ImageInterpolation.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.UnCLIP_ImageInterpolation, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.controls = page.UnCLIP_ImageInterpolation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.UnCLIP_ImageInterpolation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageInterpolation.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageInterpolation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_image_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_image_interpolation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_image_interpolation_inits = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if bool(p['init_image']) and bool(p['mask_image']):\n",
        "          unCLIP_image_interpolation_inits.append({'init_image':p['init_image'], 'end_image':p['mask_image']})\n",
        "    else:\n",
        "      if not bool(unCLIP_image_interpolation_prefs['init_image']) or not bool(unCLIP_image_interpolation_prefs['end_image']):\n",
        "        alert_msg(page, \"You need to add a Initial and Ending Image first... \")\n",
        "        return\n",
        "      unCLIP_image_interpolation_inits.append({'init_image':unCLIP_image_interpolation_prefs['init_image'], 'end_image':unCLIP_image_interpolation_prefs['end_image']})\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_pipes('unCLIP_image_interpolation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    dtype = torch.float16 if not prefs['higher_vram_mode'] else torch.float32 if torch.cuda.is_available() else torch.bfloat16\n",
        "    if pipe_unCLIP_image_interpolation == None:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        prt(Installing(\"Downloading unCLIP Image Interpolation Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            pipe_unCLIP_image_interpolation = DiffusionPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", custom_pipeline=\"unclip_image_interpolation\", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_unCLIP_image_interpolation.to(torch_device)\n",
        "            pipe_unCLIP_image_interpolation = optimize_pipe(pipe_unCLIP_image_interpolation, freeu=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading unCLIP Image Interpolation Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_unCLIP_image_interpolation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_image_interpolation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating unCLIP Image Interpolation{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for interpolation_images in unCLIP_image_interpolation_inits:\n",
        "        if interpolation_images['init_image'].startswith('http'):\n",
        "          init_img = PILImage.open(requests.get(interpolation_images['init_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(interpolation_images['init_image']):\n",
        "            init_img = PILImage.open(interpolation_images['init_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {interpolation_images['init_image']}\")\n",
        "            return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "\n",
        "        if interpolation_images['end_image'].startswith('http'):\n",
        "          end_img = PILImage.open(requests.get(interpolation_images['end_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(interpolation_images['end_image']):\n",
        "            end_img = PILImage.open(interpolation_images['end_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_end_imageimage {interpolation_images['end_image']}\")\n",
        "            return\n",
        "        width, height = end_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])\n",
        "        end_img = end_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        end_img = ImageOps.exif_transpose(end_img).convert(\"RGB\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        num = 0\n",
        "        random_seed = (int(unCLIP_image_interpolation_prefs['seed']) + num) if int(unCLIP_image_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_unCLIP_image_interpolation(image=[init_img, end_img], steps=unCLIP_image_interpolation_prefs['interpolation_steps'], decoder_num_inference_steps=unCLIP_image_interpolation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_interpolation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_interpolation_prefs['decoder_guidance_scale'], num_images_per_prompt=unCLIP_image_interpolation_prefs['num_images'], generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error running unCLIP Image Interpolation Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        autoscroll(True)\n",
        "        clear_last()\n",
        "        #fname = format_filename(unCLIP_image_interpolation_prefs['file_name'])\n",
        "        fname_init = interpolation_images['init_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        fname_end = interpolation_images['end_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        fname = f\"{fname_init[:int(prefs['file_max_length']/2)]}-to-{fname_end[:int(prefs['file_max_length']/2)]}\"\n",
        "        if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            image_path = available_file(os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not unCLIP_image_interpolation_prefs['display_upscaled_image'] or not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=unCLIP_image_interpolation_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if unCLIP_image_interpolation_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_interpolation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_image_interpolation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {unCLIP_image_interpolation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"unCLIP Image Interpolation\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    #metadata.add_text(\"title\", unCLIP_image_interpolation_prefs['file_name'])\n",
        "                    config_json = unCLIP_image_interpolation_prefs.copy()\n",
        "                    config_json['model_path'] = \"kakaobrain/karlo-v1-alpha-image-variations\"\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    del config_json['file_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num +=1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_magic_mix(page, from_list=False):\n",
        "    global magic_mix_prefs, pipe_magic_mix\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.MagicMix.controls.append(line)\n",
        "        if update:\n",
        "          page.MagicMix.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.MagicMix, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.MagicMix.controls = page.MagicMix.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.MagicMix.auto_scroll = scroll\n",
        "        page.MagicMix.update()\n",
        "      else:\n",
        "        page.MagicMix.auto_scroll = scroll\n",
        "        page.MagicMix.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = magic_mix_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    magic_mix_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        magic_mix_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(magic_mix_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      magic_mix_prompts.append(magic_mix_prefs['prompt'])\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      clear_list()\n",
        "    autoscroll(True)\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if magic_mix_prefs['init_image'].startswith('http'):\n",
        "      init_img = PILImage.open(requests.get(magic_mix_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(magic_mix_prefs['init_image']):\n",
        "        init_img = PILImage.open(magic_mix_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {magic_mix_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = init_img.size\n",
        "    width, height = scale_dimensions(width, height, magic_mix_prefs['max_size'])\n",
        "    init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "    init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    '''tform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(\n",
        "            (width, height),\n",
        "            interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "            antialias=False,\n",
        "            ),\n",
        "        transforms.Normalize(\n",
        "          [0.48145466, 0.4578275, 0.40821073],\n",
        "          [0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "    init_img = tform(init_img).to(torch_device)'''\n",
        "    clear_pipes('magic_mix')\n",
        "    #torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    model = get_model(prefs['model_ckpt'])['path']\n",
        "    scheduler_mode = magic_mix_prefs['scheduler_mode']\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      schedule = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      schedule = PNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      schedule = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    if pipe_magic_mix == None or magic_mix_prefs['scheduler_mode'] != magic_mix_prefs['scheduler_last']:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        prt(Installing(\"Downloading MagicMix Pipeline... \"))\n",
        "        try:\n",
        "            pipe_magic_mix = DiffusionPipeline.from_pretrained(model, custom_pipeline=\"AlanB/magic_mix_mod\", scheduler=schedule, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_magic_mix.to(torch_device)\n",
        "            pipe_magic_mix = optimize_pipe(pipe_magic_mix, freeu=False)\n",
        "            magic_mix_prefs['scheduler_last'] = magic_mix_prefs['scheduler_mode']\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading MagicMix Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_magic_mix.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"es\" if magic_mix_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating MagicMix{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, magic_mix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in magic_mix_prompts:\n",
        "        for num in range(magic_mix_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(magic_mix_prefs['seed']) + num) if int(magic_mix_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                image = pipe_magic_mix(img=init_img, prompt=pr, steps=magic_mix_prefs['num_inference_steps'], kmin=magic_mix_prefs['kmin'], kmax=magic_mix_prefs['kmax'], mix_factor=magic_mix_prefs['mix_factor'], guidance_scale=magic_mix_prefs['guidance_scale'], seed=random_seed, callback=callback_fnc, callback_steps=1)#.images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running MagicMix Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            #for image in images:\n",
        "            image_path = available_file(os.path.join(stable_dir, magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not magic_mix_prefs['display_upscaled_image'] or not magic_mix_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if magic_mix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=magic_mix_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if magic_mix_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(magic_mix_prefs[\"enlarge_scale\"]), height=height * float(magic_mix_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {magic_mix_prefs['enlarge_scale']}x with ESRGAN\" if magic_mix_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"magic_mix\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  metadata.add_text(\"title\", pr)\n",
        "                  config_json = magic_mix_prefs.copy()\n",
        "                  config_json['model_path'] = model\n",
        "                  config_json['seed'] = random_seed\n",
        "                  del config_json['num_images']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  del config_json['file_name']\n",
        "                  del config_json[\"scheduler_last\"]\n",
        "                  del config_json['max_size']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_paint_by_example(page):\n",
        "    global paint_by_example_prefs, prefs, status, pipe_paint_by_example\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(paint_by_example_prefs['original_image']) or (not bool(paint_by_example_prefs['alpha_mask']) and not bool(paint_by_example_prefs['mask_image'])):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(paint_by_example_prefs['example_image']):\n",
        "      alert_msg(page, \"You must provide an Example Image to Transfer Subject from...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.PaintByExample.controls.append(line)\n",
        "      page.PaintByExample.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.PaintByExample, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.PaintByExample.auto_scroll = scroll\n",
        "      page.PaintByExample.update()\n",
        "    def clear_list():\n",
        "      page.PaintByExample.controls = page.PaintByExample.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = paint_by_example_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing Paint-by-Example Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if paint_by_example_prefs['original_image'].startswith('http'):\n",
        "      #response = requests.get(paint_by_example_prefs['original_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(paint_by_example_prefs['original_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(paint_by_example_prefs['original_image']):\n",
        "        original_img = PILImage.open(paint_by_example_prefs['original_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your original_image {paint_by_example_prefs['original_image']}\")\n",
        "        return\n",
        "    width, height = original_img.size\n",
        "    width, height = scale_dimensions(width, height, paint_by_example_prefs['max_size'])\n",
        "    if bool(paint_by_example_prefs['alpha_mask']):\n",
        "      original_img = ImageOps.exif_transpose(original_img).convert(\"RGBA\")\n",
        "    else:\n",
        "      original_img = ImageOps.exif_transpose(original_img).convert(\"RGB\")\n",
        "    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    mask_img = None\n",
        "    if not bool(paint_by_example_prefs['mask_image']) and bool(paint_by_example_prefs['alpha_mask']):\n",
        "      red, green, blue, alpha = PILImage.Image.split(original_img)\n",
        "      #mask_img = ImageOps.invert(alpha.convert('RGB'))\n",
        "      mask_img = alpha.convert('L')\n",
        "    else:\n",
        "      if paint_by_example_prefs['mask_image'].startswith('http'):\n",
        "        mask_img = PILImage.open(requests.get(paint_by_example_prefs['mask_image'], stream=True).raw)\n",
        "      else:\n",
        "        if os.path.isfile(paint_by_example_prefs['mask_image']):\n",
        "          mask_img = PILImage.open(paint_by_example_prefs['mask_image'])\n",
        "        else:\n",
        "          alert_msg(page, f\"ERROR: Couldn't find your mask_image {paint_by_example_prefs['mask_image']}\")\n",
        "          return\n",
        "      if paint_by_example_prefs['invert_mask']:\n",
        "        mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    #mask_img = mask_img.convert(\"L\")\n",
        "    #mask_img = mask_img.convert(\"1\")\n",
        "    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)\n",
        "    mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    if paint_by_example_prefs['example_image'].startswith('http'):\n",
        "      example_img = PILImage.open(requests.get(paint_by_example_prefs['example_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(paint_by_example_prefs['example_image']):\n",
        "        example_img = PILImage.open(paint_by_example_prefs['example_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your Example Image {paint_by_example_prefs['example_image']}\")\n",
        "        return\n",
        "\n",
        "    clear_pipes('paint_by_example')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    model_id = \"Fantasy-Studio/Paint-by-Example\"\n",
        "    if pipe_paint_by_example is None:\n",
        "      from diffusers import PaintByExamplePipeline, PNDMScheduler\n",
        "      PNDMscheduler = PNDMScheduler(skip_prk_steps=True)\n",
        "      pipe_paint_by_example = PaintByExamplePipeline.from_pretrained(model_id, scheduler=PNDMscheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "      pipe_paint_by_example = pipe_paint_by_example.to(\"cpu\")\n",
        "    clear_last()\n",
        "    prt(\"Generating Paint-by-Example of your Image... (slow because it uses CPU instead of GPU)\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(paint_by_example_prefs['seed']) if int(paint_by_example_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    try:\n",
        "      images = pipe_paint_by_example(image=original_img, mask_image=mask_img, example_image=example_img, num_inference_steps=paint_by_example_prefs['num_inference_steps'], eta=paint_by_example_prefs['eta'], guidance_scale=paint_by_example_prefs['guidance_scale'], num_images_per_prompt=paint_by_example_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Paint-by-Example your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    filename = paint_by_example_prefs['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "    #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    num = 0\n",
        "    for image in images:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not paint_by_example_prefs['display_upscaled_image'] or not paint_by_example_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if paint_by_example_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=paint_by_example_prefs[\"enlarge_scale\"])\n",
        "            image_path = upscaled_path\n",
        "            if paint_by_example_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(paint_by_example_prefs[\"enlarge_scale\"]), height=height * float(paint_by_example_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {paint_by_example_prefs['enlarge_scale']}x with ESRGAN\" if paint_by_example_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Paint-by-Example\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = paint_by_example_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_images']\n",
        "              del config_json['max_size']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['invert_mask']\n",
        "              del config_json['alpha_mask']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        time.sleep(0.2)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(True)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_instruct_pix2pix(page, from_list=False):\n",
        "    global instruct_pix2pix_prefs, prefs, status, pipe_instruct_pix2pix\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['original_image']) and not instruct_pix2pix_prefs['use_init_video']:\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['init_video']) and instruct_pix2pix_prefs['use_init_video']:\n",
        "      alert_msg(page, \"You must provide the Input Initial Video Clip to process...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a Instructional Image Editing Prompt...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.InstructPix2Pix.controls.append(line)\n",
        "        if update:\n",
        "          page.InstructPix2Pix.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.InstructPix2Pix, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.InstructPix2Pix.auto_scroll = scroll\n",
        "        page.InstructPix2Pix.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = instruct_pix2pix_prefs['num_inference_steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.InstructPix2Pix.controls = page.InstructPix2Pix.controls[:1]\n",
        "    instruct_pix2pix_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        instruct = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else instruct_pix2pix_prefs['original_image'], 'seed': p['seed']}\n",
        "        instruct_pix2pix_prompts.append(instruct)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      if not bool(instruct_pix2pix_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      instruct = {'prompt':instruct_pix2pix_prefs['prompt'], 'negative_prompt': instruct_pix2pix_prefs['negative_prompt'], 'original_image': instruct_pix2pix_prefs['original_image'], 'seed': instruct_pix2pix_prefs['seed']}\n",
        "      instruct_pix2pix_prompts.append(instruct)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    installer = Installing(f\"Installing Instruct-Pix2Pix{' SDXL' if instruct_pix2pix_prefs['use_SDXL'] else ''} Pipeline...\")\n",
        "    prt(installer)\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    model_id = \"timbrooks/instruct-pix2pix\"\n",
        "    model_id_SDXL = \"diffusers/sdxl-instructpix2pix-768\"\n",
        "    if 'loaded_instructpix2pix' not in status:\n",
        "      status['loaded_instructpix2pix'] = ''\n",
        "    if status['loaded_instructpix2pix'] != (model_id_SDXL if instruct_pix2pix_prefs['use_SDXL'] else model_id):\n",
        "      clear_pipes()\n",
        "    else:\n",
        "      clear_pipes('instruct_pix2pix')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_instruct_pix2pix is None:\n",
        "      if instruct_pix2pix_prefs['use_SDXL']:\n",
        "        from diffusers import StableDiffusionXLInstructPix2PixPipeline\n",
        "        pipe_instruct_pix2pix = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(model_id_SDXL, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "        pipe_instruct_pix2pix = optimize_SDXL(pipe_instruct_pix2pix)\n",
        "        status['loaded_instructpix2pix'] = model_id_SDXL\n",
        "      else:\n",
        "        from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "        pipe_instruct_pix2pix = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "        pipe_instruct_pix2pix = optimize_pipe(pipe_instruct_pix2pix)\n",
        "        #pipe_instruct_pix2pix = pipe_instruct_pix2pix.to(torch_device)\n",
        "        status['loaded_instructpix2pix'] = model_id\n",
        "    pipeline_scheduler(pipe_instruct_pix2pix)\n",
        "    ip_adapter_arg = {}\n",
        "    if not instruct_pix2pix_prefs['use_SDXL']:\n",
        "      if instruct_pix2pix_prefs['use_ip_adapter']:\n",
        "        installer.status(f\"...initialize IP-Adapter\")\n",
        "        ip_adapter_img = None\n",
        "        if instruct_pix2pix_prefs['ip_adapter_image'].startswith('http'):\n",
        "          i_response = requests.get(instruct_pix2pix_prefs['ip_adapter_image'])\n",
        "          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "        else:\n",
        "          if os.path.isfile(instruct_pix2pix_prefs['ip_adapter_image']):\n",
        "            ip_adapter_img = PILImage.open(instruct_pix2pix_prefs['ip_adapter_image'])\n",
        "          else:\n",
        "            clear_last()\n",
        "            prt(f\"ERROR: Couldn't find your ip_adapter_image {instruct_pix2pix_prefs['ip_adapter_image']}\")\n",
        "        if bool(ip_adapter_img):\n",
        "          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img\n",
        "        if bool(ip_adapter_arg):\n",
        "            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == instruct_pix2pix_prefs['ip_adapter_model'])\n",
        "            pipe_instruct_pix2pix.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "            pipe_instruct_pix2pix.set_ip_adapter_scale(instruct_pix2pix_prefs['ip_adapter_strength'])\n",
        "    clear_last()\n",
        "    prt(\"Generating Instruct-Pix2Pix of your Image...\")\n",
        "    prt(progress)\n",
        "    max_size = instruct_pix2pix_prefs['max_size']\n",
        "    batch_output = os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name'])\n",
        "    output_dir = batch_output\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    if instruct_pix2pix_prefs['use_init_video']:\n",
        "        init_vid = instruct_pix2pix_prefs['init_video']\n",
        "        try:\n",
        "            start_time = float(instruct_pix2pix_prefs['start_time'])\n",
        "            end_time = float(instruct_pix2pix_prefs['end_time'])\n",
        "            fps = int(instruct_pix2pix_prefs['fps'])\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        if init_vid.startswith('http'):\n",
        "            init_vid = download_file(init_vid, output_dir)\n",
        "        else:\n",
        "            if not os.path.isfile(init_vid):\n",
        "              alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "              return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                video.append(PILImage.fromarray(image))\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        #reader = imageio.get_reader(instruct_pix2pix_prefs['init_video'], \"ffmpeg\")\n",
        "        #frame_count = instruct_pix2pix_prefs['fps'] #TODO: This isn't frame count, do it right\n",
        "        #video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n",
        "    else: video = None\n",
        "    for pr in instruct_pix2pix_prompts:\n",
        "      if not instruct_pix2pix_prefs['use_init_video']:\n",
        "        if pr['original_image'].startswith('http'):\n",
        "          #response = requests.get(instruct_pix2pix_prefs['original_image'])\n",
        "          #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          original_img = PILImage.open(requests.get(pr['original_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(pr['original_image']):\n",
        "            original_img = PILImage.open(pr['original_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your original_image {pr['original_image']}\")\n",
        "            return\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, instruct_pix2pix_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "      for num in range(instruct_pix2pix_prefs['num_images']):\n",
        "        prt(progress)\n",
        "        random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "          if instruct_pix2pix_prefs['use_init_video']:\n",
        "            from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "            pipe_instruct_pix2pix.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n",
        "            images = pipe_instruct_pix2pix([pr['prompt']] * len(video), image=video, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_arg).images\n",
        "          else:\n",
        "            images = pipe_instruct_pix2pix(pr['prompt'], image=original_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_arg).images\n",
        "        except Exception as e:\n",
        "          clear_last()\n",
        "          alert_msg(page, f\"ERROR: Couldn't run Instruct-Pix2Pix on your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "          return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        filename = f\"-{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        #num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not instruct_pix2pix_prefs['display_upscaled_image'] or not instruct_pix2pix_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=instruct_pix2pix_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "                if instruct_pix2pix_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(instruct_pix2pix_prefs[\"enlarge_scale\"]), height=height * float(instruct_pix2pix_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {instruct_pix2pix_prefs['enlarge_scale']}x with ESRGAN\" if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"Instruct-Pix2Pix{' SDXL' if instruct_pix2pix_prefs['use_SDXL'] else ''}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = instruct_pix2pix_prefs.copy()\n",
        "                  config_json['model_path'] = model_id_SDXL if instruct_pix2pix_prefs['use_SDXL'] else model_id\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['num_images']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            #num += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet(page, from_list=False):\n",
        "    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(controlnet_prefs['original_image']) and len(controlnet_prefs['multi_controlnets']) == 0:\n",
        "      alert_msg(page, \"You must provide the Original Image to process...\")\n",
        "      return\n",
        "    if not bool(controlnet_prefs['prompt']) and not from_list:\n",
        "      alert_msg(page, \"You must provide a Prompt to paint in your image...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNet.controls.append(line)\n",
        "        #page.controlnet_output.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNet.update()\n",
        "          #page.controlnet_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.ControlNet, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNet.controls = page.ControlNet.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNet.auto_scroll = scroll\n",
        "        page.ControlNet.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'seed': p['seed']}\n",
        "        controlnet_prompts.append(control)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      #page.controlnet_output.controls.clear()\n",
        "    else:\n",
        "      if not bool(controlnet_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      original = controlnet_prefs['original_image']\n",
        "      conditioning_scale = controlnet_prefs['conditioning_scale']\n",
        "      control_guidance_start = controlnet_prefs['control_guidance_start']\n",
        "      control_guidance_end = controlnet_prefs['control_guidance_end']\n",
        "      if len(controlnet_prefs['multi_controlnets']) > 0:\n",
        "        original = []\n",
        "        conditioning_scale = []\n",
        "        control_guidance_start = []\n",
        "        control_guidance_end = []\n",
        "        for c in controlnet_prefs['multi_controlnets']:\n",
        "          original.append(c['original_image'])\n",
        "          conditioning_scale.append(c['conditioning_scale'])\n",
        "          control_guidance_start.append(c['control_guidance_start'])\n",
        "          control_guidance_end.append(c['control_guidance_end'])\n",
        "      control = {'prompt':controlnet_prefs['prompt'], 'negative_prompt': controlnet_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_prefs['seed']}\n",
        "      if controlnet_prefs['use_init_video']:\n",
        "        control['init_video'] = controlnet_prefs['init_video']\n",
        "        control['start_time'] = controlnet_prefs['start_time']\n",
        "        control['end_time'] = controlnet_prefs['end_time']\n",
        "        control['fps'] = controlnet_prefs['fps']\n",
        "      controlnet_prompts.append(control)\n",
        "      #page.controlnet_output.controls.clear()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    installer = Installing(\"Installing ControlNet Packages...\")\n",
        "    prt(installer)\n",
        "    if status['loaded_controlnet'] == controlnet_prefs[\"control_task\"]:\n",
        "        clear_pipes('controlnet')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    try:\n",
        "        try:\n",
        "          from controlnet_aux import MLSDdetector\n",
        "        except ModuleNotFoundError:\n",
        "          installer.status(\"...installing controlnet-aux\")\n",
        "          run_sp(\"pip install --upgrade controlnet-aux\", realtime=False)\n",
        "          #run_sp(\"pip install git+https://github.com/patrickvonplaten/controlnet_aux.git\")\n",
        "          pass\n",
        "        from controlnet_aux import MLSDdetector\n",
        "        from controlnet_aux import OpenposeDetector\n",
        "        from diffusers import StableDiffusionControlNetPipeline, StableDiffusionXLControlNetPipeline, ControlNetModel\n",
        "        #run_sp(\"pip install scikit-image\", realtime=False)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Required Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        flush()\n",
        "        return\n",
        "    canny_checkpoint = \"lllyasviel/control_v11p_sd15_canny\"\n",
        "    scribble_checkpoint = \"lllyasviel/control_v11p_sd15_scribble\"\n",
        "    openpose_checkpoint = \"lllyasviel/control_v11p_sd15_openpose\"\n",
        "    depth_checkpoint = \"lllyasviel/control_v11p_sd15_depth\"\n",
        "    HED_checkpoint = \"lllyasviel/control_v11p_sd15_softedge\"\n",
        "    mlsd_checkpoint = \"lllyasviel/control_v11p_sd15_mlsd\"\n",
        "    normal_checkpoint = \"lllyasviel/control_v11p_sd15_normalbae\"\n",
        "    seg_checkpoint = \"lllyasviel/control_v11p_sd15_seg\"\n",
        "    lineart_checkpoint = \"lllyasviel/control_v11p_sd15_lineart\"\n",
        "    ip2p_checkpoint = \"lllyasviel/control_v11e_sd15_ip2p\"\n",
        "    shuffle_checkpoint = \"lllyasviel/control_v11e_sd15_shuffle\"\n",
        "    tile_checkpoint = \"lllyasviel/control_v11f1e_sd15_tile\"\n",
        "    brightness_checkpoint = \"ioclab/control_v1p_sd15_brightness\"\n",
        "    mediapipe_face_checkpoint = \"CrucibleAI/ControlNetMediaPipeFace\"\n",
        "    hed = None\n",
        "    openpose = None\n",
        "    depth_estimator = None\n",
        "    mlsd = None\n",
        "    image_processor = None\n",
        "    image_segmentor = None\n",
        "    normal = None\n",
        "    lineart = None\n",
        "    shuffle = None\n",
        "    face_detector = None\n",
        "    def get_controlnet(task):\n",
        "        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector\n",
        "        if controlnet_models[task] != None:\n",
        "            return controlnet_models[task]\n",
        "        if task == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "            task = \"Canny Map Edge\"\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(canny_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Scribble\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "            task = \"OpenPose\"\n",
        "            from controlnet_aux import OpenposeDetector\n",
        "            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Depth\":\n",
        "            from transformers import pipeline\n",
        "            depth_estimator = pipeline('depth-estimation')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Kandinsky Depth\":\n",
        "            from transformers import pipeline\n",
        "            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n",
        "            depth_estimator = pipeline('depth-estimation')\n",
        "            #controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "            controlnet_models[task] = KandinskyV22ControlnetPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"HED\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"M-LSD\":\n",
        "            from controlnet_aux import MLSDdetector\n",
        "            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Normal Map\":\n",
        "            #from transformers import pipeline\n",
        "            #depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\")\n",
        "            from controlnet_aux import NormalBaeDetector\n",
        "            normal = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Segmented\":\n",
        "            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "            from controlnet_utils import ade_palette\n",
        "            image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"LineArt\":\n",
        "            from controlnet_aux import LineartDetector\n",
        "            lineart = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Shuffle\":\n",
        "            from controlnet_aux import ContentShuffleDetector\n",
        "            shuffle = ContentShuffleDetector()\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Tile\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Brightness\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)\n",
        "        elif task == \"Mediapipe Face\":\n",
        "            from controlnet_aux import MediapipeFaceDetector\n",
        "            face_detector = MediapipeFaceDetector()\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(mediapipe_face_checkpoint, subfolder=\"diffusion_sd15\", torch_dtype=torch.float16, use_safetensors=True)\n",
        "        elif task == \"QR Code Monster\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained('monster-labs/control_v1p_sd15_qrcode_monster', subfolder='v2', torch_dtype=torch.float16, use_safetensors=True)\n",
        "        elif task == \"Instruct Pix2Pix\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "\n",
        "        return controlnet_models[task]\n",
        "    width, height = 0, 0\n",
        "    def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        W, H = input_image.size\n",
        "        k = float(resolution) / min(H, W)\n",
        "        H *= k\n",
        "        W *= k\n",
        "        H = int(round(H / 64.0)) * 64\n",
        "        W = int(round(W / 64.0)) * 64\n",
        "        img = input_image.resize((W, H), resample=PILImage.LANCZOS)\n",
        "        return img\n",
        "    def prep_image(task, img):\n",
        "        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector\n",
        "        nonlocal width, height\n",
        "        if isinstance(img, str):\n",
        "          if img.startswith('http'):\n",
        "              #response = requests.get(controlnet_prefs['original_image'])\n",
        "              #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              original_img = PILImage.open(requests.get(img, stream=True).raw)\n",
        "          else:\n",
        "              if os.path.isfile(img):\n",
        "                  original_img = PILImage.open(img)\n",
        "              else:\n",
        "                  alert_msg(page, f\"ERROR: Couldn't find your original_image {img}\")\n",
        "                  return\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])\n",
        "          #print(f\"Size: {width}x{height}\")\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        #return original_img\n",
        "        try:\n",
        "            if task == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = cv2.Canny(input_image, controlnet_prefs['low_threshold'], controlnet_prefs['high_threshold'])\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"Scribble\":\n",
        "                original_img = hed(original_img, scribble=True)\n",
        "            elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "                original_img = openpose(original_img, hand_and_face=True)\n",
        "            elif task == \"Depth\":\n",
        "                original_img = depth_estimator(original_img)['depth']\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"Kandinsky Depth\":\n",
        "                original_img = depth_estimator(original_img)['depth']\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                detected_map = torch.from_numpy(input_image).float() / 255.0\n",
        "                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to(\"cuda\")\n",
        "                #original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"HED\":\n",
        "                original_img = hed(original_img, safe=True)\n",
        "            elif task == \"M-LSD\":\n",
        "                original_img = mlsd(original_img)\n",
        "            elif task == \"Normal Map\":\n",
        "                #depth_estimator = pipeline(\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\" )\n",
        "                '''original_img = depth_estimator(original_img)['predicted_depth'][0]\n",
        "                input_image = original_img.numpy()\n",
        "                image_depth = input_image.copy()\n",
        "                image_depth -= np.min(image_depth)\n",
        "                image_depth /= np.max(image_depth)\n",
        "                bg_threhold = 0.4\n",
        "                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "                x[image_depth < bg_threhold] = 0\n",
        "                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "                y[image_depth < bg_threhold] = 0\n",
        "                z = np.ones_like(x) * np.pi * 2.0\n",
        "                input_image = np.stack([x, y, z], axis=2)\n",
        "                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
        "                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(input_image)'''\n",
        "                original_img = normal(original_img)\n",
        "            elif task == \"Segmented\":\n",
        "                from controlnet_utils import ade_palette\n",
        "                pixel_values = image_processor(original_img, return_tensors=\"pt\").pixel_values\n",
        "                with torch.no_grad():\n",
        "                  outputs = image_segmentor(pixel_values)\n",
        "                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]\n",
        "                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "                palette = np.array(ade_palette())\n",
        "                for label, color in enumerate(palette):\n",
        "                    color_seg[seg == label, :] = color\n",
        "                color_seg = color_seg.astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(color_seg)\n",
        "            elif task == \"LineArt\":\n",
        "                original_img = lineart(original_img)\n",
        "            elif task == \"Shuffle\":\n",
        "                original_img = shuffle(original_img)\n",
        "            elif task == \"Tile\":\n",
        "                original_img = resize_for_condition_image(original_img, 1024)\n",
        "            elif task == \"Brightness\":\n",
        "                original_img = PILImage.fromarray(original_img).convert('L')\n",
        "            elif task == \"Mediapipe Face\":\n",
        "                original_img = face_detector(original_img, scribble=True)\n",
        "                '''pip_install(\"mediapipe\")\n",
        "                import mediapipe as mp\n",
        "                mp_drawing = mp.solutions.drawing_utils\n",
        "                mp_drawing_styles = mp.solutions.drawing_styles\n",
        "                mp_face_mesh = mp.solutions.face_mesh\n",
        "                input_image = np.array(original_img)\n",
        "                with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=5, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:\n",
        "                    results = face_mesh.process(input_image)\n",
        "                res = input_image[:, :, ::-1].copy()\n",
        "                show_tesselation = show_contours = show_irises = True\n",
        "                if results.multi_face_landmarks is not None:\n",
        "                    for face_landmarks in results.multi_face_landmarks:\n",
        "                        if show_tesselation:\n",
        "                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_TESSELATION, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
        "                        if show_contours:\n",
        "                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_CONTOURS, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
        "                        if show_irises:\n",
        "                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_IRISES, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
        "                original_img = PILImage.fromarray(res[:, :, ::-1])'''\n",
        "            return original_img\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Preparing ControlNet {controlnet_prefs['control_task']} Input Image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "    def prep_video(vid):\n",
        "        nonlocal width, height\n",
        "        if vid.startswith('http'):\n",
        "            init_vid = download_file(vid, stable_dir)\n",
        "        else:\n",
        "            if os.path.isfile(vid):\n",
        "                init_vid = vid\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_video {vid}\")\n",
        "                return\n",
        "        try:\n",
        "            start_time = float(controlnet_prefs['start_time'])\n",
        "            end_time = float(controlnet_prefs['end_time'])\n",
        "            fps = int(controlnet_prefs['fps'])\n",
        "            max_size = controlnet_prefs['max_size']\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))\n",
        "                video.append(image)\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        return video\n",
        "    loaded_controlnet = None\n",
        "    if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:\n",
        "        controlnet = []\n",
        "        loaded_controlnet = []\n",
        "        for c in controlnet_prefs['multi_controlnets']:\n",
        "            controlnet.append(get_controlnet(c['control_task']))\n",
        "            loaded_controlnet.append(c['control_task'])\n",
        "    else:\n",
        "        controlnet = get_controlnet(controlnet_prefs['control_task'])\n",
        "        loaded_controlnet = controlnet_prefs['control_task']\n",
        "    for k, v in controlnet_models.items():\n",
        "      if v != None and k in loaded_controlnet:\n",
        "        del v\n",
        "        controlnet_models[k] = None\n",
        "    controlnet_type = \"text2image\"\n",
        "    if controlnet_prefs['use_image2image']:\n",
        "        if bool(controlnet_prefs['init_image']):\n",
        "            if bool(controlnet_prefs['mask_image']) or controlnet_prefs['alpha_mask']:\n",
        "                controlnet_type = \"inpaint\"\n",
        "            else:\n",
        "                controlnet_type = \"image2image\"\n",
        "    use_ip_adapter = controlnet_prefs['use_ip_adapter']\n",
        "    if use_ip_adapter:\n",
        "        ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == controlnet_prefs['ip_adapter_model'])\n",
        "    else:\n",
        "        ip_adapter_model = None\n",
        "    if controlnet_type != status['loaded_controlnet_type']:\n",
        "        clear_pipes()\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    model_path = model['path']\n",
        "    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_prefs[\"control_task\"]:\n",
        "        #if controlnet_prefs[\"use_SDXL\"]:\n",
        "        #TODO: pipe_controlnet = StableDiffusionXLControlNetPipeline\n",
        "        if controlnet_type == \"text2image\":\n",
        "            pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        elif controlnet_type == \"image2image\":\n",
        "            from diffusers import StableDiffusionControlNetImg2ImgPipeline\n",
        "            pipe_controlnet = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        elif controlnet_type == \"inpaint\":\n",
        "            from diffusers import StableDiffusionControlNetInpaintPipeline\n",
        "            pipe_controlnet = StableDiffusionControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_controlnet.enable_model_cpu_offload()\n",
        "        pipe_controlnet = optimize_pipe(pipe_controlnet, vae_slicing=True, vae_tiling=True)\n",
        "        status['loaded_controlnet'] = loaded_controlnet #controlnet_prefs[\"control_task\"]\n",
        "        status['loaded_controlnet_type'] = controlnet_type\n",
        "    #else:\n",
        "        #pipe_controlnet.controlnet=controlnet\n",
        "    pipe_controlnet = pipeline_scheduler(pipe_controlnet)\n",
        "    if controlnet_prefs['use_init_video']:\n",
        "        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "    init_img = None\n",
        "    mask_img = None\n",
        "    if bool(controlnet_prefs['init_image'] and controlnet_prefs['use_image2image']):\n",
        "        if controlnet_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(controlnet_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_prefs['init_image']):\n",
        "                init_img = PILImage.open(controlnet_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {controlnet_prefs['init_image']}\")\n",
        "                return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])\n",
        "        if bool(controlnet_prefs['alpha_mask']):\n",
        "            init_img = init_img.convert(\"RGBA\")\n",
        "        else:\n",
        "            init_img = init_img.convert(\"RGB\")\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if not bool(controlnet_prefs['mask_image']) and bool(controlnet_prefs['alpha_mask']):\n",
        "            mask_img = init_img.convert('RGBA')\n",
        "            red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "            mask_img = alpha.convert('L')\n",
        "        elif bool(controlnet_prefs['mask_image']):\n",
        "            if controlnet_prefs['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(controlnet_prefs['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(controlnet_prefs['mask_image']):\n",
        "                    mask_img = PILImage.open(controlnet_prefs['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {controlnet_prefs['mask_image']}\")\n",
        "                    return\n",
        "            width, height = mask_img.size\n",
        "            width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])\n",
        "            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if controlnet_prefs['invert_mask'] and not controlnet_prefs['alpha_mask']:\n",
        "            from PIL import ImageOps\n",
        "            mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    if use_ip_adapter:\n",
        "        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "        pipe_controlnet.set_ip_adapter_scale(controlnet_prefs['ip_adapter_strength'])\n",
        "        if controlnet_prefs['ip_adapter_image'].startswith('http'):\n",
        "            ip_adapter_image = PILImage.open(requests.get(controlnet_prefs['ip_adapter_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_prefs['ip_adapter_image']):\n",
        "                ip_adapter_image = PILImage.open(controlnet_prefs['ip_adapter_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ip_adapter_image {controlnet_prefs['ip_adapter_image']}\")\n",
        "                return\n",
        "        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert(\"RGB\")\n",
        "        status['loaded_ip_adapter'] = ip_adapter_model\n",
        "        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}\n",
        "    else:\n",
        "        ip_adapter_args = {}\n",
        "    clear_last()\n",
        "    prt(f\"Generating ControlNet {controlnet_prefs['control_task']} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    for pr in controlnet_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        filename = f\"{controlnet_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:\n",
        "            original_img = []\n",
        "            for c in controlnet_prefs['multi_controlnets']:\n",
        "                original_img.append(prep_image(c['control_task'], c['original_image']))\n",
        "                if controlnet_prefs['show_processed_image']:\n",
        "                    processed_img = available_file(batch_output, f\"{filename}-{c['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                    w, h = original_img[-1].size\n",
        "                    original_img[-1].save(processed_img)\n",
        "                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        elif not controlnet_prefs['use_init_video']:\n",
        "            original_img = prep_image(controlnet_prefs['control_task'], pr['original_image'])\n",
        "            if controlnet_prefs['show_processed_image']:\n",
        "                processed_img = available_file(batch_output, f\"{filename}-{controlnet_prefs['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                w, h = original_img.size\n",
        "                original_img.save(processed_img)\n",
        "                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            video_img = prep_video(pr['original_image'])\n",
        "            latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)\n",
        "        try:\n",
        "            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            if controlnet_type == \"text2image\":\n",
        "                if not controlnet_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"image2image\":\n",
        "                if not controlnet_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"inpaint\":\n",
        "                if not controlnet_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            '''\n",
        "            if not controlnet_prefs['use_init_video']:\n",
        "                images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            else:\n",
        "                images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images'''\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Generating ControlNet {controlnet_prefs['control_task']}...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "        clear_pipes('controlnet')\n",
        "        clear_last()\n",
        "        #clear_last()\n",
        "        autoscroll(True)\n",
        "        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, controlnet_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            #PILImage.fromarray(image).save(image_path)\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            new_file = available_file(batch_output, fname, num)\n",
        "            if not controlnet_prefs['display_upscaled_image'] or not controlnet_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=controlnet_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "            if prefs['save_image_metadata']:\n",
        "                task = and_list(controlnet_prefs['control_task']) if isinstance(controlnet_prefs['control_task'], list) else controlnet_prefs['control_task']\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {controlnet_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"ControlNet \" + task)\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = controlnet_prefs.copy()\n",
        "                  config_json['model_path'] = model_path\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['batch_size']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            if controlnet_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_prefs[\"enlarge_scale\"]), height=height * float(controlnet_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "    autoscroll(False)\n",
        "    del hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet_xl(page, from_list=False):\n",
        "    global controlnet_xl_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_xl_models\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(controlnet_xl_prefs['original_image']) and len(controlnet_xl_prefs['multi_controlnets']) == 0:\n",
        "      alert_msg(page, \"You must provide the Original Image to process...\")\n",
        "      return\n",
        "    if not bool(controlnet_xl_prefs['prompt']) and not from_list:\n",
        "      alert_msg(page, \"You must provide a Prompt to paint in your image...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetXL.controls.append(line)\n",
        "        #page.controlnet_xl_output.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNetXL.update()\n",
        "          #page.controlnet_xl_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.ControlNetXL, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNetXL.controls = page.ControlNetXL.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetXL.auto_scroll = scroll\n",
        "        page.ControlNetXL.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_xl_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_xl_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_xl_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'seed': p['seed']}\n",
        "        controlnet_xl_prompts.append(control)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      #page.controlnet_xl_output.controls.clear()\n",
        "    else:\n",
        "      if not bool(controlnet_xl_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      original = controlnet_xl_prefs['original_image']\n",
        "      conditioning_scale = controlnet_xl_prefs['conditioning_scale']\n",
        "      control_guidance_start = controlnet_xl_prefs['control_guidance_start']\n",
        "      control_guidance_end = controlnet_xl_prefs['control_guidance_end']\n",
        "      if len(controlnet_xl_prefs['multi_controlnets']) > 0:\n",
        "        original = []\n",
        "        conditioning_scale = []\n",
        "        control_guidance_start = []\n",
        "        control_guidance_end = []\n",
        "        for c in controlnet_xl_prefs['multi_controlnets']:\n",
        "          original.append(c['original_image'])\n",
        "          conditioning_scale.append(c['conditioning_scale'])\n",
        "          control_guidance_start.append(c['control_guidance_start'])\n",
        "          control_guidance_end.append(c['control_guidance_end'])\n",
        "      control = {'prompt':controlnet_xl_prefs['prompt'], 'negative_prompt': controlnet_xl_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_xl_prefs['seed']}\n",
        "      if controlnet_xl_prefs['use_init_video']:\n",
        "        control['init_video'] = controlnet_xl_prefs['init_video']\n",
        "        control['start_time'] = controlnet_xl_prefs['start_time']\n",
        "        control['end_time'] = controlnet_xl_prefs['end_time']\n",
        "        control['fps'] = controlnet_xl_prefs['fps']\n",
        "      controlnet_xl_prompts.append(control)\n",
        "      #page.controlnet_xl_output.controls.clear()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    installer = Installing(\"Installing ControlNetXL Packages...\")\n",
        "    prt(installer)\n",
        "    if status['loaded_controlnet'] == controlnet_xl_prefs[\"control_task\"]:\n",
        "        clear_pipes('controlnet')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    try:\n",
        "        try:\n",
        "          from controlnet_aux import MLSDdetector\n",
        "        except ModuleNotFoundError:\n",
        "          installer.status(\"...installing controlnet-aux\")\n",
        "          run_sp(\"pip install --upgrade controlnet-aux\", realtime=False)\n",
        "          #run_sp(\"pip install git+https://github.com/patrickvonplaten/controlnet_aux.git\")\n",
        "          pass\n",
        "        from controlnet_aux import MLSDdetector\n",
        "        from controlnet_aux import OpenposeDetector\n",
        "        from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\n",
        "        #run_sp(\"pip install scikit-image\", realtime=False)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Required Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        flush()\n",
        "        return\n",
        "    canny_checkpoint = \"diffusers/controlnet-canny-sdxl-1.0\"\n",
        "    canny_small_checkpoint = \"diffusers/controlnet-canny-sdxl-1.0-small\"\n",
        "    canny_mid_checkpoint = \"diffusers/controlnet-canny-sdxl-1.0-mid\"\n",
        "    depth_checkpoint = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
        "    depth_small_checkpoint = \"diffusers/controlnet-depth-sdxl-1.0-small\"\n",
        "    depth_mid_checkpoint = \"diffusers/controlnet-depth-sdxl-1.0-mid\"\n",
        "    seg_checkpoint = \"SargeZT/sdxl-controlnet-seg\"\n",
        "    softedge_checkpoint = \"SargeZT/controlnet-sd-xl-1.0-softedge-dexined\"#\"SargeZT/sdxl-controlnet-softedge\"\n",
        "    lineart_checkpoint = \"zbulrush/controlnet-sd-xl-1.0-lineart\"\n",
        "    openpose_checkpoint = \"thibaud/controlnet-openpose-sdxl-1.0\"\n",
        "    scribble_checkpoint = \"lllyasviel/control_v11p_sd15_scribble\"\n",
        "    HED_checkpoint = \"lllyasviel/control_v11p_sd15_softedge\"\n",
        "    mlsd_checkpoint = \"lllyasviel/control_v11p_sd15_mlsd\"\n",
        "    normal_checkpoint = \"lllyasviel/control_v11p_sd15_normalbae\"\n",
        "    ip2p_checkpoint = \"lllyasviel/control_v11e_sd15_ip2p\"\n",
        "    shuffle_checkpoint = \"lllyasviel/control_v11e_sd15_shuffle\"\n",
        "    tile_checkpoint = \"lllyasviel/control_v11f1e_sd15_tile\"\n",
        "    brightness_checkpoint = \"ioclab/control_v1p_sd15_brightness\"\n",
        "    hed = None\n",
        "    openpose = None\n",
        "    depth_estimator = None\n",
        "    feature_extractor = None\n",
        "    mlsd = None\n",
        "    image_processor = None\n",
        "    image_segmentor = None\n",
        "    normal = None\n",
        "    lineart = None\n",
        "    shuffle = None\n",
        "    original_img = None\n",
        "    def get_controlnet(task):\n",
        "        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        if controlnet_xl_models[task] != None:\n",
        "            return controlnet_xl_models[task]\n",
        "        if \"Canny Map\" in task or task == \"Video Canny Edge\":\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(canny_mid_checkpoint if 'mid' in task else canny_small_checkpoint if 'small' in task else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "            task = \"Canny Map Edge\"\n",
        "        elif task == \"Scribble\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "            task = \"OpenPose\"\n",
        "            from controlnet_aux import OpenposeDetector\n",
        "            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Kandinsky Depth\":\n",
        "            from transformers import pipeline\n",
        "            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n",
        "            depth_estimator = pipeline('depth-estimation')\n",
        "            #controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "            controlnet_xl_models[task] = KandinskyV22ControlnetPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16).to(torch_device)\n",
        "        elif \"Depth\" in task:\n",
        "            from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
        "            depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
        "            feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_mid_checkpoint if 'mid' in task else depth_small_checkpoint if 'small' in task else depth_checkpoint, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Softedge\":\n",
        "            from controlnet_aux import HEDdetector, PidiNetDetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            hed = PidiNetDetector.from_pretrained('lllyasviel/Annotators') #pidi_net\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(softedge_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"HED\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"M-LSD\":\n",
        "            from controlnet_aux import MLSDdetector\n",
        "            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNetXL')\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Normal Map\":\n",
        "            #from transformers import pipeline\n",
        "            #depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\")\n",
        "            from controlnet_aux import NormalBaeDetector\n",
        "            normal = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Segmented\":\n",
        "            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "            from controlnet_utils import ade_palette\n",
        "            image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"LineArt\":\n",
        "            from controlnet_aux import LineartDetector\n",
        "            lineart = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Shuffle\":\n",
        "            from controlnet_aux import ContentShuffleDetector\n",
        "            shuffle = ContentShuffleDetector()\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Tile\":\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Brightness\":\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)\n",
        "        elif task == \"Instruct Pix2Pix\":\n",
        "            controlnet_xl_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "\n",
        "        return controlnet_xl_models[task]\n",
        "    width, height = 0, 0\n",
        "    def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        W, H = input_image.size\n",
        "        k = float(resolution) / min(H, W)\n",
        "        H *= k\n",
        "        W *= k\n",
        "        H = int(round(H / 64.0)) * 64\n",
        "        W = int(round(W / 64.0)) * 64\n",
        "        img = input_image.resize((W, H), resample=PILImage.LANCZOS)\n",
        "        return img\n",
        "    def prep_image(task, img):\n",
        "        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        nonlocal width, height\n",
        "        if isinstance(img, str):\n",
        "          if img.startswith('http'):\n",
        "              #response = requests.get(controlnet_xl_prefs['original_image'])\n",
        "              #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              original_img = PILImage.open(requests.get(img, stream=True).raw)\n",
        "          else:\n",
        "              if os.path.isfile(img):\n",
        "                  original_img = PILImage.open(img)\n",
        "              else:\n",
        "                  alert_msg(page, f\"ERROR: Couldn't find your original_image {img}\")\n",
        "                  return\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])\n",
        "          #print(f\"Size: {width}x{height}\")\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        #return original_img\n",
        "        try:\n",
        "            if 'Canny' in task: # == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = cv2.Canny(input_image, controlnet_xl_prefs['low_threshold'], controlnet_xl_prefs['high_threshold'])\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"Scribble\":\n",
        "                original_img = hed(original_img, scribble=True)\n",
        "            elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "                original_img = openpose(original_img, hand_and_face=True)\n",
        "            elif task == \"Kandinsky Depth\":\n",
        "                original_img = depth_estimator(original_img)['depth']\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                detected_map = torch.from_numpy(input_image).float() / 255.0\n",
        "                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to(\"cuda\")\n",
        "                #original_img = PILImage.fromarray(input_image)\n",
        "            elif \"Depth\" in task:\n",
        "                original_img = feature_extractor(images=original_img, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
        "                with torch.no_grad(), torch.autocast(\"cuda\"):\n",
        "                    depth_map = depth_estimator(original_img).predicted_depth\n",
        "                depth_map = torch.nn.functional.interpolate(\n",
        "                    depth_map.unsqueeze(1),\n",
        "                    size=(1024, 1024),\n",
        "                    mode=\"bicubic\",\n",
        "                    align_corners=False,\n",
        "                )\n",
        "                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
        "                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
        "                depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
        "                original_img = torch.cat([depth_map] * 3, dim=1)\n",
        "                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
        "                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))\n",
        "            elif task == \"Softedge\":\n",
        "                original_img = hed(original_img, safe=True)\n",
        "            elif task == \"HED\":\n",
        "                original_img = hed(original_img, safe=True)\n",
        "            elif task == \"M-LSD\":\n",
        "                original_img = mlsd(original_img)\n",
        "            elif task == \"Normal Map\":\n",
        "                #depth_estimator = pipeline(\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\" )\n",
        "                '''original_img = depth_estimator(original_img)['predicted_depth'][0]\n",
        "                input_image = original_img.numpy()\n",
        "                image_depth = input_image.copy()\n",
        "                image_depth -= np.min(image_depth)\n",
        "                image_depth /= np.max(image_depth)\n",
        "                bg_threhold = 0.4\n",
        "                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "                x[image_depth < bg_threhold] = 0\n",
        "                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "                y[image_depth < bg_threhold] = 0\n",
        "                z = np.ones_like(x) * np.pi * 2.0\n",
        "                input_image = np.stack([x, y, z], axis=2)\n",
        "                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
        "                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(input_image)'''\n",
        "                original_img = normal(original_img)\n",
        "            elif task == \"Segmented\":\n",
        "                from controlnet_utils import ade_palette\n",
        "                pixel_values = image_processor(original_img, return_tensors=\"pt\").pixel_values\n",
        "                with torch.no_grad():\n",
        "                  outputs = image_segmentor(pixel_values)\n",
        "                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]\n",
        "                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "                palette = np.array(ade_palette())\n",
        "                for label, color in enumerate(palette):\n",
        "                    color_seg[seg == label, :] = color\n",
        "                color_seg = color_seg.astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(color_seg)\n",
        "            elif task == \"LineArt\":\n",
        "                original_img = lineart(original_img)\n",
        "            elif task == \"Shuffle\":\n",
        "                original_img = shuffle(original_img)\n",
        "            elif task == \"Tile\":\n",
        "                original_img = resize_for_condition_image(original_img, 1024)\n",
        "            elif task == \"Brightness\":\n",
        "                original_img = PILImage.fromarray(original_img).convert('L')\n",
        "            return original_img\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Preparing ControlNet-XL {controlnet_xl_prefs['control_task']} Input Image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "    def prep_video(vid):\n",
        "        nonlocal width, height\n",
        "        if vid.startswith('http'):\n",
        "            init_vid = download_file(vid, stable_dir)\n",
        "        else:\n",
        "            if os.path.isfile(vid):\n",
        "                init_vid = vid\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_video {vid}\")\n",
        "                return\n",
        "        try:\n",
        "            start_time = float(controlnet_xl_prefs['start_time'])\n",
        "            end_time = float(controlnet_xl_prefs['end_time'])\n",
        "            fps = int(controlnet_xl_prefs['fps'])\n",
        "            max_size = controlnet_xl_prefs['max_size']\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                image = prep_image(controlnet_xl_prefs['control_task'], PILImage.fromarray(image))\n",
        "                video.append(image)\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        return video\n",
        "    loaded_controlnet = None\n",
        "    if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:\n",
        "        controlnet = []\n",
        "        loaded_controlnet = []\n",
        "        for c in controlnet_xl_prefs['multi_controlnets']:\n",
        "            controlnet.append(get_controlnet(c['control_task']))\n",
        "            loaded_controlnet.append(c['control_task'])\n",
        "        if len(controlnet) == 1:\n",
        "            controlnet = controlnet[0]\n",
        "            loaded_controlnet = loaded_controlnet[0]\n",
        "    else:\n",
        "        controlnet = get_controlnet(controlnet_xl_prefs['control_task'])\n",
        "        loaded_controlnet = controlnet_xl_prefs['control_task']\n",
        "    for k, v in controlnet_xl_models.items():\n",
        "      if v != None and k in loaded_controlnet:\n",
        "        del v\n",
        "        controlnet_xl_models[k] = None\n",
        "    controlnet_type = \"text2image\"\n",
        "    if controlnet_xl_prefs['use_image2image']:\n",
        "        if bool(controlnet_xl_prefs['init_image']):\n",
        "            if bool(controlnet_xl_prefs['mask_image']) or controlnet_xl_prefs['alpha_mask']:\n",
        "                controlnet_type = \"inpaint\"\n",
        "            else:\n",
        "                controlnet_type = \"image2image\"\n",
        "    use_ip_adapter = controlnet_xl_prefs['use_ip_adapter']\n",
        "    if use_ip_adapter:\n",
        "        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_xl_prefs['ip_adapter_SDXL_model'])\n",
        "    else:\n",
        "        ip_adapter_model = None\n",
        "    if controlnet_type != status['loaded_controlnet_type']:\n",
        "        clear_pipes()\n",
        "    #model = get_model(prefs['model_ckpt'])\n",
        "    model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_xl_prefs[\"control_task\"]:\n",
        "        vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "        if controlnet_type == \"text2image\":\n",
        "            pipe_controlnet = StableDiffusionXLControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        elif controlnet_type == \"image2image\":\n",
        "            from diffusers import StableDiffusionXLControlNetImg2ImgPipeline\n",
        "            pipe_controlnet = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        elif controlnet_type == \"inpaint\":\n",
        "            from diffusers import StableDiffusionXLControlNetInpaintPipeline\n",
        "            pipe_controlnet = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_controlnet.enable_model_cpu_offload()\n",
        "        pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)\n",
        "        status['loaded_controlnet'] = loaded_controlnet #controlnet_xl_prefs[\"control_task\"]\n",
        "        status['loaded_controlnet_type'] = controlnet_type\n",
        "    pipe_controlnet = pipeline_scheduler(pipe_controlnet)\n",
        "    if controlnet_xl_prefs['use_init_video']:\n",
        "        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "    init_img = None\n",
        "    mask_img = None\n",
        "    if bool(controlnet_xl_prefs['init_image'] and controlnet_xl_prefs['use_image2image']):\n",
        "        if controlnet_xl_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(controlnet_xl_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_xl_prefs['init_image']):\n",
        "                init_img = PILImage.open(controlnet_xl_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {controlnet_xl_prefs['init_image']}\")\n",
        "                return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])\n",
        "        if bool(controlnet_xl_prefs['alpha_mask']):\n",
        "            init_img = init_img.convert(\"RGBA\")\n",
        "        else:\n",
        "            init_img = init_img.convert(\"RGB\")\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if not bool(controlnet_xl_prefs['mask_image']) and bool(controlnet_xl_prefs['alpha_mask']):\n",
        "            mask_img = init_img.convert('RGBA')\n",
        "            red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "            mask_img = alpha.convert('L')\n",
        "        elif bool(controlnet_xl_prefs['mask_image']):\n",
        "            if controlnet_xl_prefs['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(controlnet_xl_prefs['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(controlnet_xl_prefs['mask_image']):\n",
        "                    mask_img = PILImage.open(controlnet_xl_prefs['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {controlnet_xl_prefs['mask_image']}\")\n",
        "                    return\n",
        "            width, height = mask_img.size\n",
        "            width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])\n",
        "            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if controlnet_xl_prefs['invert_mask'] and not controlnet_xl_prefs['alpha_mask']:\n",
        "            from PIL import ImageOps\n",
        "            mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    if use_ip_adapter:\n",
        "        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "        pipe_controlnet.set_ip_adapter_scale(controlnet_xl_prefs['ip_adapter_strength'])\n",
        "        if controlnet_xl_prefs['ip_adapter_image'].startswith('http'):\n",
        "            ip_adapter_image = PILImage.open(requests.get(controlnet_xl_prefs['ip_adapter_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_xl_prefs['ip_adapter_image']):\n",
        "                ip_adapter_image = PILImage.open(controlnet_xl_prefs['ip_adapter_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ip_adapter_image {controlnet_xl_prefs['ip_adapter_image']}\")\n",
        "                return\n",
        "        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert(\"RGB\")\n",
        "        status['loaded_ip_adapter'] = ip_adapter_model\n",
        "        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}\n",
        "    else:\n",
        "        ip_adapter_args = {}\n",
        "    clear_last()\n",
        "    prt(f\"Generating ControlNet-XL {controlnet_xl_prefs['control_task']} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_xl_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in controlnet_xl_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        filename = f\"{controlnet_xl_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:\n",
        "            original_img = []\n",
        "            for c in controlnet_xl_prefs['multi_controlnets']:\n",
        "                original_img.append(prep_image(c['control_task'], c['original_image']))\n",
        "                if controlnet_xl_prefs['show_processed_image']:\n",
        "                    processed_img = available_file(batch_output, f\"{filename}-{c['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                    w, h = original_img[-1].size\n",
        "                    original_img[-1].save(processed_img)\n",
        "                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        elif not controlnet_xl_prefs['use_init_video']:\n",
        "            original_img = prep_image(controlnet_xl_prefs['control_task'], pr['original_image'])\n",
        "            if controlnet_xl_prefs['show_processed_image']:\n",
        "                processed_img = available_file(batch_output, f\"{filename}-{controlnet_xl_prefs['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                w, h = original_img.size\n",
        "                original_img.save(processed_img)\n",
        "                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            video_img = prep_video(pr['original_image'])\n",
        "            latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)\n",
        "   \n",
        "        try:\n",
        "            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            if controlnet_type == \"text2image\":\n",
        "                if not controlnet_xl_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"image2image\":\n",
        "                if not controlnet_xl_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"inpaint\":\n",
        "                if not controlnet_xl_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Generating ControlNet-XL {controlnet_xl_prefs['control_task']}...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "        clear_pipes('controlnet')\n",
        "        clear_last()\n",
        "        #clear_last()\n",
        "        autoscroll(True)\n",
        "        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        \n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            #PILImage.fromarray(image).save(image_path)\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            new_file = available_file(batch_output, fname, num)\n",
        "            if not controlnet_xl_prefs['display_upscaled_image'] or not controlnet_xl_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if controlnet_xl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=controlnet_xl_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "            if prefs['save_image_metadata']:\n",
        "                task = and_list(controlnet_xl_prefs['control_task']) if isinstance(controlnet_xl_prefs['control_task'], list) else controlnet_xl_prefs['control_task']\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {controlnet_xl_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_xl_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"ControlNetXL \" + task)\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = controlnet_xl_prefs.copy()\n",
        "                  config_json['model_path'] = model_path\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['batch_size']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            if controlnet_xl_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_xl_prefs[\"enlarge_scale\"]), height=height * float(controlnet_xl_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "    autoscroll(False)\n",
        "    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet_xs(page, from_list=False):\n",
        "    global controlnet_xs_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_xs_models\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(controlnet_xs_prefs['original_image']) and len(controlnet_xs_prefs['multi_controlnets']) == 0:\n",
        "      alert_msg(page, \"You must provide the Original Image to process...\")\n",
        "      return\n",
        "    if not bool(controlnet_xs_prefs['prompt']) and not from_list:\n",
        "      alert_msg(page, \"You must provide a Prompt to paint in your image...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetXS.controls.append(line)\n",
        "        #page.controlnet_xs_output.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNetXS.update()\n",
        "          #page.controlnet_xs_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.ControlNetXS, lines=lines)\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNetXS.controls = page.ControlNetXS.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNetXS.auto_scroll = scroll\n",
        "        page.ControlNetXS.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_xs_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None: #(pipe, step, timestep, callback_kwargs):\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_xs_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_xs_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_xs_prefs['original_image'], 'conditioning_scale': controlnet_xs_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xs_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xs_prefs['control_guidance_end'], 'seed': p['seed']}\n",
        "        controlnet_xs_prompts.append(control)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      #page.controlnet_xs_output.controls.clear()\n",
        "    else:\n",
        "      if not bool(controlnet_xs_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      original = controlnet_xs_prefs['original_image']\n",
        "      conditioning_scale = controlnet_xs_prefs['conditioning_scale']\n",
        "      control_guidance_start = controlnet_xs_prefs['control_guidance_start']\n",
        "      control_guidance_end = controlnet_xs_prefs['control_guidance_end']\n",
        "      if len(controlnet_xs_prefs['multi_controlnets']) > 0:\n",
        "        original = []\n",
        "        conditioning_scale = []\n",
        "        control_guidance_start = []\n",
        "        control_guidance_end = []\n",
        "        for c in controlnet_xs_prefs['multi_controlnets']:\n",
        "          original.append(c['original_image'])\n",
        "          conditioning_scale.append(c['conditioning_scale'])\n",
        "          control_guidance_start.append(c['control_guidance_start'])\n",
        "          control_guidance_end.append(c['control_guidance_end'])\n",
        "      control = {'prompt':controlnet_xs_prefs['prompt'], 'negative_prompt': controlnet_xs_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_xs_prefs['seed']}\n",
        "      if controlnet_xs_prefs['use_init_video']:\n",
        "        control['init_video'] = controlnet_xs_prefs['init_video']\n",
        "        control['start_time'] = controlnet_xs_prefs['start_time']\n",
        "        control['end_time'] = controlnet_xs_prefs['end_time']\n",
        "        control['fps'] = controlnet_xs_prefs['fps']\n",
        "      controlnet_xs_prompts.append(control)\n",
        "      #page.controlnet_xs_output.controls.clear()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    installer = Installing(\"Installing ControlNet-XS Packages...\")\n",
        "    prt(installer)\n",
        "    if status['loaded_controlnet'] == controlnet_xs_prefs[\"control_task\"]:\n",
        "        clear_pipes('controlnet')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    try:\n",
        "        try:\n",
        "          from controlnet_aux import MLSDdetector\n",
        "        except ModuleNotFoundError:\n",
        "          installer.status(\"...installing controlnet-aux\")\n",
        "          run_sp(\"pip install --upgrade controlnet-aux\", realtime=False)\n",
        "          #run_sp(\"pip install git+https://github.com/patrickvonplaten/controlnet_aux.git\")\n",
        "          pass\n",
        "        from controlnet_aux import MLSDdetector\n",
        "        from controlnet_aux import OpenposeDetector\n",
        "        from diffusers import StableDiffusionXLControlNetXSPipeline, StableDiffusionControlNetXSPipeline, ControlNetXSModel, ControlNetModel, AutoencoderKL\n",
        "        #run_sp(\"pip install scikit-image\", realtime=False)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Required Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        flush()\n",
        "        return\n",
        "    canny_checkpoint = \"UmerHA/ConrolNetXS-SD2.1-canny\"\n",
        "    canny_SDXL_checkpoint = \"UmerHA/ConrolNetXS-SDXL-canny\"\n",
        "    depth_checkpoint = \"UmerHA/ConrolNetXS-SD2.1-depth\"\n",
        "    depth_SDXL_checkpoint = \"UmerHA/ConrolNetXS-SDXL-depth\"\n",
        "    hed = None\n",
        "    openpose = None\n",
        "    depth_estimator = None\n",
        "    feature_extractor = None\n",
        "    mlsd = None\n",
        "    image_processor = None\n",
        "    image_segmentor = None\n",
        "    normal = None\n",
        "    lineart = None\n",
        "    shuffle = None\n",
        "    original_img = None\n",
        "    def get_controlnet(task):\n",
        "        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        if controlnet_xs_models[task] != None:\n",
        "            return controlnet_xs_models[task]\n",
        "        if \"Canny Map\" in task or task == \"Video Canny Edge\":\n",
        "            controlnet_xs_models[task] = ControlNetXSModel.from_pretrained(canny_SDXL_checkpoint if controlnet_xs_prefs['use_SDXL'] else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "            task = \"Canny Map Edge\"\n",
        "        elif \"Depth\" in task:\n",
        "            from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
        "            depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
        "            feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
        "            controlnet_xs_models[task] = ControlNetXSModel.from_pretrained(depth_SDXL_checkpoint if controlnet_xs_prefs['use_SDXL'] else depth_checkpoint, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)\n",
        "        return controlnet_xs_models[task]\n",
        "    width, height = 0, 0\n",
        "    def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        W, H = input_image.size\n",
        "        k = float(resolution) / min(H, W)\n",
        "        H *= k\n",
        "        W *= k\n",
        "        H = int(round(H / 64.0)) * 64\n",
        "        W = int(round(W / 64.0)) * 64\n",
        "        img = input_image.resize((W, H), resample=PILImage.LANCZOS)\n",
        "        return img\n",
        "    def prep_image(task, img):\n",
        "        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        nonlocal width, height\n",
        "        if isinstance(img, str):\n",
        "          if img.startswith('http'):\n",
        "              #response = requests.get(controlnet_xs_prefs['original_image'])\n",
        "              #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              original_img = PILImage.open(requests.get(img, stream=True).raw)\n",
        "          else:\n",
        "              if os.path.isfile(img):\n",
        "                  original_img = PILImage.open(img)\n",
        "              else:\n",
        "                  alert_msg(page, f\"ERROR: Couldn't find your original_image {img}\")\n",
        "                  return\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])\n",
        "          #print(f\"Size: {width}x{height}\")\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        #return original_img\n",
        "        try:\n",
        "            if 'Canny' in task: # == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = cv2.Canny(input_image, controlnet_xs_prefs['low_threshold'], controlnet_xs_prefs['high_threshold'])\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif \"Depth\" in task:\n",
        "                original_img = feature_extractor(images=original_img, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
        "                with torch.no_grad(), torch.autocast(\"cuda\"):\n",
        "                    depth_map = depth_estimator(original_img).predicted_depth\n",
        "                depth_map = torch.nn.functional.interpolate(\n",
        "                    depth_map.unsqueeze(1),\n",
        "                    size=(1024, 1024),\n",
        "                    mode=\"bicubic\",\n",
        "                    align_corners=False,\n",
        "                )\n",
        "                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
        "                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
        "                depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
        "                original_img = torch.cat([depth_map] * 3, dim=1)\n",
        "                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
        "                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))\n",
        "            return original_img\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Preparing ControlNet-XL {controlnet_xs_prefs['control_task']} Input Image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "    def prep_video(vid):\n",
        "        nonlocal width, height\n",
        "        if vid.startswith('http'):\n",
        "            init_vid = download_file(vid, stable_dir)\n",
        "        else:\n",
        "            if os.path.isfile(vid):\n",
        "                init_vid = vid\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_video {vid}\")\n",
        "                return\n",
        "        try:\n",
        "            start_time = float(controlnet_xs_prefs['start_time'])\n",
        "            end_time = float(controlnet_xs_prefs['end_time'])\n",
        "            fps = int(controlnet_xs_prefs['fps'])\n",
        "            max_size = controlnet_xs_prefs['max_size']\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                image = prep_image(controlnet_xs_prefs['control_task'], PILImage.fromarray(image))\n",
        "                video.append(image)\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        return video\n",
        "    loaded_controlnet = None\n",
        "    if len(controlnet_xs_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xs_prefs['use_init_video']:\n",
        "        controlnet = []\n",
        "        loaded_controlnet = []\n",
        "        for c in controlnet_xs_prefs['multi_controlnets']:\n",
        "            controlnet.append(get_controlnet(c['control_task']))\n",
        "            loaded_controlnet.append(c['control_task'])\n",
        "        if len(controlnet) == 1:\n",
        "            controlnet = controlnet[0]\n",
        "            loaded_controlnet = loaded_controlnet[0]\n",
        "    else:\n",
        "        controlnet = get_controlnet(controlnet_xs_prefs['control_task'])\n",
        "        loaded_controlnet = controlnet_xs_prefs['control_task']\n",
        "    for k, v in controlnet_xs_models.items():\n",
        "      if v != None and k in loaded_controlnet:\n",
        "        del v\n",
        "        controlnet_xs_models[k] = None\n",
        "    controlnet_type = \"text2image\"\n",
        "    if controlnet_xs_prefs['use_image2image']:\n",
        "        if bool(controlnet_xs_prefs['init_image']):\n",
        "            if bool(controlnet_xs_prefs['mask_image']) or controlnet_xs_prefs['alpha_mask']:\n",
        "                controlnet_type = \"inpaint\"\n",
        "            else:\n",
        "                controlnet_type = \"image2image\"\n",
        "    use_ip_adapter = controlnet_xs_prefs['use_ip_adapter']\n",
        "    if use_ip_adapter:\n",
        "        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_xs_prefs['ip_adapter_SDXL_model'])\n",
        "    else:\n",
        "        ip_adapter_model = None\n",
        "    \n",
        "    #model = get_model(prefs['model_ckpt'])\n",
        "    model_path = \"stabilityai/stable-diffusion-xl-base-1.0\" if controlnet_xs_prefs['use_SDXL'] else \"stabilityai/stable-diffusion-2-1\" \n",
        "    if controlnet_type != status['loaded_controlnet_type'] or model_path != status['loaded_model']:\n",
        "        clear_pipes()\n",
        "    \n",
        "    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_xs_prefs[\"control_task\"]:\n",
        "        if controlnet_xs_prefs['use_SDXL']:\n",
        "            vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "            if controlnet_type == \"text2image\":\n",
        "                pipe_controlnet = StableDiffusionXLControlNetXSPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            elif controlnet_type == \"image2image\":\n",
        "                from diffusers import StableDiffusionXLControlNetImg2ImgPipeline\n",
        "                pipe_controlnet = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            elif controlnet_type == \"inpaint\":\n",
        "                from diffusers import StableDiffusionXLControlNetInpaintPipeline\n",
        "                pipe_controlnet = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_controlnet.enable_model_cpu_offload()\n",
        "            #pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)\n",
        "        else:\n",
        "            if controlnet_type == \"text2image\":\n",
        "                pipe_controlnet = StableDiffusionControlNetXSPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            elif controlnet_type == \"image2image\":\n",
        "                from diffusers import StableDiffusionControlNetImg2ImgPipeline\n",
        "                pipe_controlnet = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            elif controlnet_type == \"inpaint\":\n",
        "                from diffusers import StableDiffusionControlNetInpaintPipeline\n",
        "                pipe_controlnet = StableDiffusionControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_controlnet.enable_model_cpu_offload()\n",
        "            #pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)\n",
        "        if prefs['enable_torch_compile']:\n",
        "            installer.status(f\"...Torch compiling unet\")\n",
        "            pipe_controlnet.to(memory_format=torch.channels_last)\n",
        "            pipe_controlnet.unet.to(memory_format=torch.channels_last)\n",
        "            pipe_controlnet.unet = torch.compile(pipe_controlnet.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "            pipe_controlnet.controlnet.to(memory_format=torch.channels_last)\n",
        "            pipe_controlnet.controlnet = torch.compile(pipe_controlnet.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "        elif controlnet_xs_prefs['cpu_offload']:\n",
        "            pipe_controlnet.enable_model_cpu_offload()\n",
        "        else:\n",
        "            pipe_controlnet.to(\"cuda\")\n",
        "        status['loaded_controlnet'] = loaded_controlnet #controlnet_xs_prefs[\"control_task\"]\n",
        "        status['loaded_controlnet_type'] = controlnet_type\n",
        "        status['loaded_model'] = model_path\n",
        "    pipe_controlnet = pipeline_scheduler(pipe_controlnet)\n",
        "    if controlnet_xs_prefs['use_init_video']:\n",
        "        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "    init_img = None\n",
        "    mask_img = None\n",
        "    if bool(controlnet_xs_prefs['init_image'] and controlnet_xs_prefs['use_image2image']):\n",
        "        if controlnet_xs_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(controlnet_xs_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_xs_prefs['init_image']):\n",
        "                init_img = PILImage.open(controlnet_xs_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {controlnet_xs_prefs['init_image']}\")\n",
        "                return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])\n",
        "        if bool(controlnet_xs_prefs['alpha_mask']):\n",
        "            init_img = init_img.convert(\"RGBA\")\n",
        "        else:\n",
        "            init_img = init_img.convert(\"RGB\")\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if not bool(controlnet_xs_prefs['mask_image']) and bool(controlnet_xs_prefs['alpha_mask']):\n",
        "            mask_img = init_img.convert('RGBA')\n",
        "            red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "            mask_img = alpha.convert('L')\n",
        "        elif bool(controlnet_xs_prefs['mask_image']):\n",
        "            if controlnet_xs_prefs['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(controlnet_xs_prefs['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(controlnet_xs_prefs['mask_image']):\n",
        "                    mask_img = PILImage.open(controlnet_xs_prefs['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {controlnet_xs_prefs['mask_image']}\")\n",
        "                    return\n",
        "            width, height = mask_img.size\n",
        "            width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])\n",
        "            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if controlnet_xs_prefs['invert_mask'] and not controlnet_xs_prefs['alpha_mask']:\n",
        "            from PIL import ImageOps\n",
        "            mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    if use_ip_adapter:\n",
        "        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "        pipe_controlnet.set_ip_adapter_scale(controlnet_xs_prefs['ip_adapter_strength'])\n",
        "        if controlnet_xs_prefs['ip_adapter_image'].startswith('http'):\n",
        "            ip_adapter_image = PILImage.open(requests.get(controlnet_xs_prefs['ip_adapter_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(controlnet_xs_prefs['ip_adapter_image']):\n",
        "                ip_adapter_image = PILImage.open(controlnet_xs_prefs['ip_adapter_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your ip_adapter_image {controlnet_xs_prefs['ip_adapter_image']}\")\n",
        "                return\n",
        "        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert(\"RGB\")\n",
        "        status['loaded_ip_adapter'] = ip_adapter_model\n",
        "        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}\n",
        "    else:\n",
        "        ip_adapter_args = {}\n",
        "    clear_last()\n",
        "    prt(f\"Generating ControlNet-XS {controlnet_xs_prefs['control_task']} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_xs_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_xs_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in controlnet_xs_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        filename = f\"{controlnet_xs_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        if len(controlnet_xs_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xs_prefs['use_init_video']:\n",
        "            original_img = []\n",
        "            for c in controlnet_xs_prefs['multi_controlnets']:\n",
        "                original_img.append(prep_image(c['control_task'], c['original_image']))\n",
        "                if controlnet_xs_prefs['show_processed_image']:\n",
        "                    processed_img = available_file(batch_output, f\"{filename}-{c['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                    w, h = original_img[-1].size\n",
        "                    original_img[-1].save(processed_img)\n",
        "                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        elif not controlnet_xs_prefs['use_init_video']:\n",
        "            original_img = prep_image(controlnet_xs_prefs['control_task'], pr['original_image'])\n",
        "            if controlnet_xs_prefs['show_processed_image']:\n",
        "                processed_img = available_file(batch_output, f\"{filename}-{controlnet_xs_prefs['control_task'].partition(' ')[0]}\", 0, no_num=True)\n",
        "                w, h = original_img.size\n",
        "                original_img.save(processed_img)\n",
        "                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            video_img = prep_video(pr['original_image'])\n",
        "            latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)\n",
        "   \n",
        "        try:\n",
        "            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "            if controlnet_type == \"text2image\":\n",
        "                if not controlnet_xs_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"image2image\":\n",
        "                if not controlnet_xs_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "            elif controlnet_type == \"inpaint\":\n",
        "                if not controlnet_xs_prefs['use_init_video']:\n",
        "                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "                else:\n",
        "                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Generating ControlNet-XL {controlnet_xs_prefs['control_task']}...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            flush()\n",
        "            return\n",
        "        clear_pipes('controlnet')\n",
        "        clear_last()\n",
        "        #clear_last()\n",
        "        autoscroll(True)\n",
        "        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        \n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, controlnet_xs_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            #PILImage.fromarray(image).save(image_path)\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            new_file = available_file(batch_output, fname, num)\n",
        "            if not controlnet_xs_prefs['display_upscaled_image'] or not controlnet_xs_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if controlnet_xs_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=controlnet_xs_prefs[\"enlarge_scale\"])\n",
        "                image_path = upscaled_path\n",
        "            if prefs['save_image_metadata']:\n",
        "                task = and_list(controlnet_xs_prefs['control_task']) if isinstance(controlnet_xs_prefs['control_task'], list) else controlnet_xs_prefs['control_task']\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {controlnet_xs_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_xs_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"ControlNet-XS \" + task)\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = controlnet_xs_prefs.copy()\n",
        "                  config_json['model_path'] = model_path\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['batch_size']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            if controlnet_xs_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_xs_prefs[\"enlarge_scale\"]), height=height * float(controlnet_xs_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "    autoscroll(False)\n",
        "    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_controlnet_tile_upscale(page, source_image, prompt=\"best quality\", scale_factor=2.5):\n",
        "    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using ControlNet Tile Upscale...\")\n",
        "      return\n",
        "    from diffusers import ControlNetModel, DiffusionPipeline\n",
        "\n",
        "    def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        W, H = input_image.size\n",
        "        k = float(resolution) / min(H, W)\n",
        "        H *= k\n",
        "        W *= k\n",
        "        H = int(round(H / 64.0)) * 64\n",
        "        W = int(round(W / 64.0)) * 64\n",
        "        img = input_image.resize((W, H), resample=Image.LANCZOS)\n",
        "        return img\n",
        "\n",
        "    controlnet = ControlNetModel.from_pretrained('lllyasviel/control_v11f1e_sd15_tile',\n",
        "                                                torch_dtype=torch.float16)\n",
        "    pipe_controlnet = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n",
        "                                            custom_pipeline=\"stable_diffusion_controlnet_img2img\",\n",
        "                                            controlnet=controlnet,\n",
        "                                            torch_dtype=torch.float16).to('cuda')\n",
        "    pipe_controlnet.enable_xformers_memory_efficient_attention()\n",
        "    if isinstance(source_image, str):\n",
        "        from diffusers.utils import load_image\n",
        "        source_image = load_image(source_image)\n",
        "\n",
        "    condition_image = resize_for_condition_image(source_image, 1024)\n",
        "    image = pipe_controlnet(prompt=prompt,\n",
        "                negative_prompt=\"blur, lowres, bad anatomy, bad hands, cropped, worst quality\",\n",
        "                image=condition_image,\n",
        "                controlnet_conditioning_image=condition_image,\n",
        "                width=condition_image.size[0],\n",
        "                height=condition_image.size[1],\n",
        "                strength=1.0,\n",
        "                generator=torch.manual_seed(0),\n",
        "                num_inference_steps=32,\n",
        "                ).images[0]\n",
        "    return image\n",
        "\n",
        "def run_controlnet_video2video(page):\n",
        "    global controlnet_video2video_prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.ControlNet_Video2Video.controls.append(line)\n",
        "      page.ControlNet_Video2Video.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.ControlNet_Video2Video, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "        page.ControlNet_Video2Video.auto_scroll = scroll\n",
        "        page.ControlNet_Video2Video.update()\n",
        "    if not bool(controlnet_video2video_prefs['init_video']):\n",
        "        alert_msg(page, \"You must provide a target init video...\")\n",
        "        return\n",
        "    if not bool(controlnet_video2video_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide an interesting prompt to guide the video...\")\n",
        "        return\n",
        "    page.ControlNet_Video2Video.controls = page.ControlNet_Video2Video.controls[:1]\n",
        "    autoscroll()\n",
        "    installer = Installing(\"Installing ControlNet Video2Video Libraries...\")\n",
        "    prt(installer)\n",
        "    controlnet_video2video_dir = os.path.join(root_dir, \"controlnetvideo\")\n",
        "    if not os.path.exists(controlnet_video2video_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning un1tz3r0/controlnetvideo.git\")\n",
        "            run_sp(\"git clone https://github.com/un1tz3r0/controlnetvideo.git\", cwd=root_dir, realtime=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing ControlNet Video Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    pip_install(\"ffmpeg opencv-python|cv2 click moviepy mediapipe controlnet-aux|controlnet_aux watchdog\", q=True, installer=installer)\n",
        "    try:\n",
        "        import xformers\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing FaceBook's Xformers\")\n",
        "        #run_sp(\"pip install --pre -U triton\", realtime=False)\n",
        "        run_sp(\"pip install -U xformers\", realtime=False)\n",
        "        status['installed_xformers'] = True\n",
        "        pass\n",
        "    clear_pipes()\n",
        "\n",
        "    from PIL import ImageOps\n",
        "    if bool(controlnet_video2video_prefs['output_name']):\n",
        "        fname = format_filename(controlnet_video2video_prefs['output_name'], force_underscore=True)\n",
        "    elif bool(controlnet_video2video_prefs['prompt']):\n",
        "        fname = format_filename(controlnet_video2video_prefs['prompt'], force_underscore=True)\n",
        "    elif bool(controlnet_video2video_prefs['batch_folder_name']):\n",
        "        fname = format_filename(controlnet_video2video_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else: fname = \"output\"\n",
        "    if bool(controlnet_video2video_prefs['file_prefix']):\n",
        "        fname = f\"{controlnet_video2video_prefs['file_prefix']}{fname}\"\n",
        "    if bool(controlnet_video2video_prefs['batch_folder_name']):\n",
        "        batch_output = os.path.join(stable_dir, controlnet_video2video_prefs['batch_folder_name'])\n",
        "    else: batch_output = stable_dir\n",
        "    make_dir(batch_output)\n",
        "    output_path = os.path.join(prefs['image_output'], controlnet_video2video_prefs['batch_folder_name'])\n",
        "    make_dir(output_path)\n",
        "    init_vid = controlnet_video2video_prefs['init_video']\n",
        "    if init_vid.startswith('http'):\n",
        "        init_vid = download_file(init_vid, batch_output)\n",
        "    else:\n",
        "        if not os.path.isfile(init_vid):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "            return\n",
        "    clear_last()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(f\"Generating your ControlNet Video...\")\n",
        "    #prt(progress)\n",
        "    autoscroll(False)\n",
        "    total_steps = controlnet_video2video_prefs['steps']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)\n",
        "    #output_file = os.path.join(output_path, f\"{fname}{'.mp4' if is_video else '.png'}\")\n",
        "    cmd = f'python controlnetvideo.py \"{init_vid}\" --controlnet {controlnet_video2video_prefs[\"control_task\"].lower()}'\n",
        "    cmd += f' --prompt \"{controlnet_video2video_prefs[\"prompt\"]}\"'\n",
        "    if bool(controlnet_video2video_prefs[\"negative_prompt\"]):\n",
        "      cmd += f' --negative-prompt \"{controlnet_video2video_prefs[\"negative_prompt\"]}\"'\n",
        "    cmd += f\" --prompt-strength {controlnet_video2video_prefs['prompt_strength']}\"\n",
        "    cmd += f\" --num-inference-steps {controlnet_video2video_prefs['steps']}\"\n",
        "    cmd += f\" --init-image-strength {controlnet_video2video_prefs['init_image_strength']}\"\n",
        "    cmd += f\" --controlnet-strength {controlnet_video2video_prefs['controlnet_strength']}\"\n",
        "    cmd += f\" --feedthrough-strength {controlnet_video2video_prefs['feedthrough_strength']}\"\n",
        "    cmd += f\" --motion-alpha {controlnet_video2video_prefs['motion_alpha']} --motion-sigma {controlnet_video2video_prefs['motion_sigma']}\"\n",
        "    cmd += f\" --color-amount {controlnet_video2video_prefs['color_amount']}\"\n",
        "    cmd += f\" --color-fix {controlnet_video2video_prefs['color_fix'].lower()}\"\n",
        "    cmd += f\" --start-time {controlnet_video2video_prefs['start_time']}\"\n",
        "    if controlnet_video2video_prefs[\"end_time\"] != 0.0:\n",
        "      cmd += f\" --end-time {controlnet_video2video_prefs['end_time']}\"\n",
        "    if controlnet_video2video_prefs[\"duration\"] != 0.0:\n",
        "      cmd += f\" --duration {controlnet_video2video_prefs['duration']}\"\n",
        "    if not controlnet_video2video_prefs[\"fix_orientation\"]:\n",
        "      cmd += f\" --no-fix-orientation\"\n",
        "    cmd += f\" --canny-low-thr {controlnet_video2video_prefs['low_threshold']} --canny-high-thr {controlnet_video2video_prefs['high_threshold']}\"\n",
        "    cmd += f\" --mlsd-score-thr {controlnet_video2video_prefs['mlsd_score_thr']} --mlsd-dist-thr {controlnet_video2video_prefs['mlsd_dist_thr']}\"\n",
        "    cmd += f\" --max-dimension {controlnet_video2video_prefs['max_dimension']} --min-dimension {controlnet_video2video_prefs['min_dimension']}\"\n",
        "    cmd += f\" --round-dims-to {controlnet_video2video_prefs['round_dims_to']}\"\n",
        "    if controlnet_video2video_prefs['no_audio']: cmd += \" --no-audio\"\n",
        "    if controlnet_video2video_prefs['skip_dumped_frames']: cmd += \" --skip-dumped-frames\"\n",
        "    frames = os.path.join(batch_output, '{n:08d}.png') #TODO Add fname\n",
        "    cmd += f\" --dump-frames '{frames}'\"\n",
        "    cmd += f' \"{output_file}\"'\n",
        "    w = 0\n",
        "    h = 0\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import LoggingEventHandler, FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self, event):\n",
        "        nonlocal img_idx, w, h\n",
        "        if event.is_directory:\n",
        "          return None\n",
        "        elif event.event_type == 'created' and event.src_path.endswith(\"png\"):\n",
        "          autoscroll(True)\n",
        "          if w == 0:\n",
        "            time.sleep(0.8)\n",
        "            try:\n",
        "              frame = PILImage.open(event.src_path)\n",
        "              w, h = frame.size\n",
        "              clear_last()\n",
        "            except Exception:\n",
        "              pass\n",
        "          clear_last()\n",
        "          if controlnet_video2video_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          else:\n",
        "            fpath = event.src_path\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          prt(progress)\n",
        "          if controlnet_video2video_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "            shutil.copy(event.src_path, fpath)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "          img_idx += 1\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, batch_output, recursive=True)\n",
        "    observer.start()\n",
        "    prt(f\"Running {cmd}\")\n",
        "    prt(progress)\n",
        "    try:\n",
        "        #os.system(f'cd {controlnet_video2video_dir};{cmd}')\n",
        "        #if is_Colab:\n",
        "        #  os.chdir(controlnet_video2video_dir)\n",
        "        #  $cmd\n",
        "        #  os.chdir(root_dir)\n",
        "        #else:\n",
        "        #TODO: Parse output to get percent current for progress callback_fnc\n",
        "        run_sp(cmd, cwd=controlnet_video2video_dir, realtime=controlnet_video2video_prefs['show_console'])\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        observer.stop()\n",
        "        alert_msg(page, \"Error running controlnetvideo.py!\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    observer.stop()\n",
        "    #clear_last()\n",
        "    autoscroll(True)\n",
        "    #TODO: Upscale Image\n",
        "    if os.path.isfile(output_file):\n",
        "        prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))\n",
        "    else:\n",
        "        prt(\"Error Generating Output File! Maybe NSFW Image detected?\")\n",
        "    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_deepfloyd(page, from_list=False):\n",
        "    global deepfloyd_prefs, prefs, status, pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3\n",
        "    import torch\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "    #  return\n",
        "    #if not bool(deepfloyd_prefs['init_image']):\n",
        "    #  alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "    #  return\n",
        "    if not bool(deepfloyd_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a Text-to-Image Prompt...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.DeepFloyd.controls.append(line)\n",
        "        if update:\n",
        "          page.DeepFloyd.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.DeepFloyd, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.DeepFloyd.auto_scroll = scroll\n",
        "        page.DeepFloyd.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = deepfloyd_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.DeepFloyd.controls = page.DeepFloyd.controls[:1]\n",
        "    deepfloyd_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        df_prompt = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else deepfloyd_prefs['init_image'], 'mask_image': p['mask_image'] if bool(p['mask_image']) else deepfloyd_prefs['mask_image'], 'image_strength': p['init_image_strength'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'seed': p['seed']}\n",
        "        deepfloyd_prompts.append(df_prompt)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      if not bool(deepfloyd_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      df_prompt = {'prompt':deepfloyd_prefs['prompt'], 'negative_prompt': deepfloyd_prefs['negative_prompt'], 'init_image': deepfloyd_prefs['init_image'], 'mask_image': deepfloyd_prefs['mask_image'], 'image_strength': deepfloyd_prefs['image_strength'], 'guidance_scale':deepfloyd_prefs['guidance_scale'], 'num_inference_steps': deepfloyd_prefs['num_inference_steps'], 'seed': deepfloyd_prefs['seed']}\n",
        "      deepfloyd_prompts.append(df_prompt)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    #if not status['installed_diffusers']:\n",
        "    installer = Installing(\"Installing Diffusers & Required Packages...\")\n",
        "    prt(installer)\n",
        "    try:\n",
        "        import accelerate\n",
        "        #TODO: Uninstall other version first\n",
        "    except ImportError:\n",
        "        installer.status(\"...Accelerate\")\n",
        "        run_process(\"pip install git+https://github.com/huggingface/accelerate.git\", page=page)\n",
        "        pass\n",
        "    #if deepfloyd_prefs['low_memory']:\n",
        "    #    pip_install(\"bitsandbytes\", upgrade=True, installer=installer)\n",
        "    try:\n",
        "        import diffusers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...HuggingFace Diffusers\")\n",
        "        #run_process(\"pip install --upgrade diffusers~=0.16\", page=page)\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git\", page=page)\n",
        "        run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import transformers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...Transformers\")\n",
        "        run_process(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers\", page=page)\n",
        "        #run_process(\"pip install --upgrade transformers~=4.28\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...SafeTensors v0.3\")\n",
        "        run_process(\"pip install --upgrade safetensors~=0.3\", page=page)\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "        pass\n",
        "    try:\n",
        "        import sentencepiece\n",
        "    except ImportError:\n",
        "        installer.status(\"...SentencePiece\")\n",
        "        run_sp(\"pip install --upgrade sentencepiece\", realtime=True) #~=0.1\n",
        "        import sentencepiece\n",
        "        pass\n",
        "    \n",
        "    installer.set_message(\"Installing DeepFloyd IF Required Packages...\")\n",
        "    if deepfloyd_prefs['low_memory']:\n",
        "        #clear_last(update=False)\n",
        "        #prt(Installing(\"Installing DeepFloyd IF Required Packages...\"))\n",
        "        try:\n",
        "          import bitsandbytes\n",
        "        except ImportError:\n",
        "          installer.status(\"...BitsandBytes\")\n",
        "          os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "          #run_sp(\"export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\", realtime=False)\n",
        "          if sys.platform.startswith(\"win\"):\n",
        "              run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "          else:\n",
        "              run_sp(\"pip install --upgrade bitsandbytes\", realtime=True) #~=0.38\n",
        "          import bitsandbytes\n",
        "          pass\n",
        "    try:\n",
        "        import torch\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...Torch v2.0\")\n",
        "        run_process(\"pip install --upgrade torch~=2.0\", page=page)\n",
        "        import torch\n",
        "        pass\n",
        "    try:\n",
        "        from huggingface_hub import notebook_login, HfFolder, login\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...HuggingFace Hub\")\n",
        "        run_process(\"pip install huggingface_hub --upgrade\", page=page)\n",
        "        from huggingface_hub import notebook_login, HfFolder, login\n",
        "        pass\n",
        "    if not os.path.exists(HfFolder.path_token):\n",
        "        try:\n",
        "          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)\n",
        "        except Exception:\n",
        "          alert_msg(page, \"ERROR Logging into HuggingFace... Check your API Key or Internet conenction.\")\n",
        "          return\n",
        "\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers.utils import pt_to_pil\n",
        "    from diffusers import IFPipeline, IFImg2ImgPipeline, IFInpaintingPipeline, IFSuperResolutionPipeline\n",
        "    #from diffusers.pipelines.deepfloyd_if.safety_checker IFSafetyChecker\n",
        "    from transformers import T5EncoderModel, T5Tokenizer\n",
        "    #run_sp(\"accelerate config default\", realtime=False)\n",
        "    clear_pipes('deepfloyd')\n",
        "    torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    if deepfloyd_prefs['model_size'].startswith(\"X\"):\n",
        "        model_id = \"DeepFloyd/IF-I-XL-v1.0\"\n",
        "        model_id_II = \"DeepFloyd/IF-II-XL-v1.0\"\n",
        "    elif deepfloyd_prefs['model_size'].startswith(\"L\"):\n",
        "        model_id = \"DeepFloyd/IF-I-L-v1.0\"\n",
        "        model_id_II = \"DeepFloyd/IF-II-L-v1.0\"\n",
        "    elif deepfloyd_prefs['model_size'].startswith(\"M\"):\n",
        "        model_id = \"DeepFloyd/IF-I-M-v1.0\"\n",
        "        model_id_II = \"DeepFloyd/IF-II-M-v1.0\"\n",
        "    clear_last(update=False)\n",
        "    if 'last_deepfloyd_mode' not in status:\n",
        "      status['last_deepfloyd_mode'] = \"\"\n",
        "    max_size = deepfloyd_prefs['max_size']\n",
        "    batch_output = os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name'])\n",
        "    output_dir = batch_output\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "\n",
        "    for pr in deepfloyd_prompts:\n",
        "        init_img = None\n",
        "        mask_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])\n",
        "            if bool(deepfloyd_prefs['alpha_mask']):\n",
        "                init_img = init_img.convert(\"RGBA\")\n",
        "            else:\n",
        "                init_img = init_img.convert(\"RGB\")\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            if not bool(pr['mask_image']) and bool(deepfloyd_prefs['alpha_mask']):\n",
        "                mask_img = init_img.convert('RGBA')\n",
        "                red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "                mask_img = alpha.convert('L')\n",
        "            elif bool(pr['mask_image']):\n",
        "                if pr['mask_image'].startswith('http'):\n",
        "                    #response = requests.get(deepfloyd_prefs['init_image'])\n",
        "                    #init_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                    mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "                else:\n",
        "                    if os.path.isfile(pr['mask_image']):\n",
        "                        mask_img = PILImage.open(pr['mask_image'])\n",
        "                    else:\n",
        "                        alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                        return\n",
        "                width, height = mask_img.size\n",
        "                width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])\n",
        "                mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if deepfloyd_prefs['invert_mask'] and not deepfloyd_prefs['alpha_mask']:\n",
        "            from PIL import ImageOps\n",
        "            mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "        for num in range(deepfloyd_prefs['num_images']):\n",
        "            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator().manual_seed(random_seed)\n",
        "            try:\n",
        "                installer = Installing(\"Running DeepFloyd-IF Text Encoder...\")\n",
        "                prt(installer)\n",
        "                if deepfloyd_prefs['low_memory']:\n",
        "                    #, load_in_8bit=True\n",
        "                    installer.status(\"...text_encoder T5EncoderModel\") #, variant=\"8bit\"\n",
        "                    text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", device_map=\"auto\", load_in_8bit=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    installer.status(\"...DiffusionPipeline\")\n",
        "                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, text_encoder=text_encoder, unet=None, use_safetensors=True, device_map=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    # Still getting errors here! WTF?\n",
        "                    #images = pipe_deepfloyd(pr['prompt'], image=init_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=deepfloyd_prefs['num_inference_steps'], eta=deepfloyd_prefs['eta'], image_guidance_scale=deepfloyd_prefs['guidance_scale'], num_images_per_prompt=deepfloyd_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                    installer.status(\"...encode_prompts\")\n",
        "                    prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                    del text_encoder\n",
        "                    del pipe_deepfloyd\n",
        "                    pipe_deepfloyd = None\n",
        "                installer.status(\"...clearing pipes\")\n",
        "                flush()\n",
        "                clear_last(update=False)\n",
        "                safety_modules = {}\n",
        "                if init_img == None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Pipeline...\"))\n",
        "                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device)\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    #clear_last()\n",
        "                    if deepfloyd_prefs['low_memory']:\n",
        "                        pipe_deepfloyd = IFPipeline.from_pretrained(model_id, text_encoder=None, device_map=\"sequential\", use_safetensors=True, variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                        #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                        #pipe_deepfloyd.unet.to(memory_format=torch.channels_last)\n",
        "                        #pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                    else:\n",
        "                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != \"text2image\"):\n",
        "                            #install.status(\"...DiffusionPipeline\")\n",
        "                            pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                            pipe_deepfloyd.to(torch_device)\n",
        "                            #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                            if prefs['enable_torch_compile']:\n",
        "                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)\n",
        "                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                        #install.status(\"...encode_prompts\")\n",
        "                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    if not deepfloyd_prefs['keep_pipelines']:\n",
        "                        del pipe_deepfloyd\n",
        "                        flush()\n",
        "                        pipe_deepfloyd = None\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Super Resolution Pipeline...\"))\n",
        "                    #IFSuperResolutionPipeline\n",
        "                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != \"text2image\":\n",
        "                        pipe_deepfloyd2 = DiffusionPipeline.from_pretrained(model_id_II, text_encoder=None, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16)\n",
        "                        if not deepfloyd_prefs['low_memory']:\n",
        "                            pipe_deepfloyd2.enable_model_cpu_offload()\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd2(\n",
        "                        image=images,\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    status['last_deepfloyd_mode'] = \"text2image\"\n",
        "                elif init_img != None and mask_img == None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Image2Image Pipeline...\"))\n",
        "                    if deepfloyd_prefs['low_memory']:\n",
        "                        pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                        #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    else:\n",
        "                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != \"image2image\"):\n",
        "                            #install.status(\"...DiffusionPipeline\")\n",
        "                            pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                            pipe_deepfloyd.to(torch_device)\n",
        "                            #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                            if prefs['enable_torch_compile']:\n",
        "                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)\n",
        "                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                        #install.status(\"...encode_prompts\")\n",
        "                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=init_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    if not deepfloyd_prefs['keep_pipelines']:\n",
        "                        del pipe_deepfloyd\n",
        "                        flush()\n",
        "                        pipe_deepfloyd = None\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Img2Img Super Resolution Pipeline...\"))\n",
        "                    from diffusers import IFImg2ImgSuperResolutionPipeline\n",
        "                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != \"image2image\":\n",
        "                        pipe_deepfloyd2 = IFImg2ImgSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "                        if not deepfloyd_prefs['low_memory']:\n",
        "                            pipe_deepfloyd2.enable_model_cpu_offload()\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd2(\n",
        "                        image=images,\n",
        "                        origional_image=init_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    status['last_deepfloyd_mode'] = \"image2image\"\n",
        "                elif init_img != None and mask_img != None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Inpainting Pipeline...\"))\n",
        "                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device)\n",
        "                    if deepfloyd_prefs['low_memory']:\n",
        "                        pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                        pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    else:\n",
        "                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != \"inpainting\"):\n",
        "                            #install.status(\"...DiffusionPipeline\")\n",
        "                            pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)\n",
        "                            pipe_deepfloyd.to(torch_device)\n",
        "                            #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                            if prefs['enable_torch_compile']:\n",
        "                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)\n",
        "                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                        #install.status(\"...encode_prompts\")\n",
        "                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=init_img,\n",
        "                        mask_image=mask_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    if not deepfloyd_prefs['keep_pipelines']:\n",
        "                        del pipe_deepfloyd\n",
        "                        flush()\n",
        "                        pipe_deepfloyd = None\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Inpainting Super Resolution Pipeline...\"))\n",
        "                    from diffusers import IFInpaintingSuperResolutionPipeline\n",
        "                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != \"inpainting\":\n",
        "                        pipe_deepfloyd2 = IFInpaintingSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, use_safetensors=True, variant=\"fp16\", torch_dtype=torch.float16, device_map=None)\n",
        "                        if not deepfloyd_prefs['low_memory']:\n",
        "                            pipe_deepfloyd2.enable_model_cpu_offload()\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd2(\n",
        "                        image=images,\n",
        "                        origional_image=init_img,\n",
        "                        mask_image=mask_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    status['last_deepfloyd_mode'] = \"inpainting\"\n",
        "                if not deepfloyd_prefs['keep_pipelines']:\n",
        "                    del pipe_deepfloyd2\n",
        "                    flush()\n",
        "                    pipe_deepfloyd2 = None\n",
        "                prt(Installing(\"Stage 3: Installing Stable Diffusion X4 Upscaler Pipeline...\"))\n",
        "                if pipe_deepfloyd3 == None:\n",
        "                    pipe_deepfloyd3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, use_safetensors=True, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    pipe_deepfloyd3.enable_model_cpu_offload()\n",
        "                total_steps = deepfloyd_prefs['upscale_num_inference_steps']\n",
        "                clear_last()\n",
        "                prt(progress)\n",
        "                images = pipe_deepfloyd3(prompt=pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, image=images, noise_level=100, num_inference_steps=deepfloyd_prefs['upscale_num_inference_steps'], guidance_scale=deepfloyd_prefs['upscale_guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                if deepfloyd_prefs['apply_watermark']:\n",
        "                    from diffusers.pipelines.deepfloyd_if import IFWatermarker\n",
        "                    watermarker = IFWatermarker.from_pretrained(model_id, subfolder=\"watermarker\")\n",
        "                    watermarker.apply_watermark(images, pipe_deepfloyd3.unet.config.sample_size)\n",
        "                    del watermarker\n",
        "                if not deepfloyd_prefs['keep_pipelines']:\n",
        "                    del pipe_deepfloyd3\n",
        "                    flush()\n",
        "                    pipe_deepfloyd3 = None\n",
        "\n",
        "            except EnvironmentError as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: You must accept the license on the DeepFloyd model card first.\", content=Text(str(e)))\n",
        "                #del pipe_deepfloyd\n",
        "                flush()\n",
        "                return\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Couldn't run IF-DeepFloyd on your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                clear_pipes()\n",
        "                return\n",
        "\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            filename = f\"{deepfloyd_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "            #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            #num = 0\n",
        "            for image in images:\n",
        "                random_seed += num\n",
        "                fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "                image_path = available_file(os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image = pt_to_pil(image)\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not deepfloyd_prefs['display_upscaled_image'] or not deepfloyd_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if deepfloyd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    #w = int(arg['width'] * prefs[\"enlarge_scale\"])\n",
        "                    #h = int(arg['height'] * prefs[\"enlarge_scale\"])\n",
        "                    #prt(Row([Text(f'Enlarging {prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "                    prt(Row([Text(f'Enlarging Real-ESRGAN {prefs[\"enlarge_scale\"]}X')], alignment=MainAxisAlignment.CENTER))\n",
        "                    upscale_image(image_path, upscaled_path, scale=deepfloyd_prefs[\"enlarge_scale\"])\n",
        "                    image_path = upscaled_path\n",
        "                    clear_last()\n",
        "                    if deepfloyd_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(deepfloyd_prefs[\"enlarge_scale\"]), height=height * float(deepfloyd_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                else:\n",
        "                    time.sleep(0.8)\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {deepfloyd_prefs['enlarge_scale']}x with ESRGAN\" if deepfloyd_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"DeepFloyd-IF\") #TODO: Img2Img or Inpainting\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                        config_json = deepfloyd_prefs.copy()\n",
        "                        config_json['model_path'] = model_id\n",
        "                        config_json['seed'] = random_seed\n",
        "                        config_json['prompt'] = pr['prompt']\n",
        "                        config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                        del config_json['num_images']\n",
        "                        del config_json['max_size']\n",
        "                        del config_json['display_upscaled_image']\n",
        "                        del config_json['batch_folder_name']\n",
        "                        if not config_json['apply_ESRGAN_upscale']:\n",
        "                            del config_json['enlarge_scale']\n",
        "                            del config_json['apply_ESRGAN_upscale']\n",
        "                        metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "                #num += 1\n",
        "    if not deepfloyd_prefs['keep_pipelines']:\n",
        "        clear_pipes()\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_amused(page, from_list=False, with_params=False):\n",
        "    global amused_prefs, pipe_amused, prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    amused_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            amused_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':amused_prefs['guidance_scale'], 'num_inference_steps':amused_prefs['num_inference_steps'], 'width':amused_prefs['width'], 'height':amused_prefs['height'], 'init_image':amused_prefs['init_image'], 'mask_image':amused_prefs['mask_image'], 'init_image_strength':amused_prefs['init_image_strength'], 'num_images':amused_prefs['num_images'], 'seed':amused_prefs['seed']})\n",
        "        else:\n",
        "            amused_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(amused_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      amused_prompts.append({'prompt': amused_prefs['prompt'], 'negative_prompt':amused_prefs['negative_prompt'], 'guidance_scale':amused_prefs['guidance_scale'], 'num_inference_steps':amused_prefs['num_inference_steps'], 'width':amused_prefs['width'], 'height':amused_prefs['height'], 'init_image':amused_prefs['init_image'], 'mask_image':amused_prefs['mask_image'], 'init_image_strength':amused_prefs['init_image_strength'], 'num_images':amused_prefs['num_images'], 'seed':amused_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.Amused.controls.append(line)\n",
        "        if update:\n",
        "          page.Amused.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.Amused, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.Amused.auto_scroll = scroll\n",
        "        page.Amused.update()\n",
        "      else:\n",
        "        page.Amused.auto_scroll = scroll\n",
        "        page.Amused.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.Amused.controls = page.Amused.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = amused_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Amused Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"amused\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = amused_prefs['cpu_offload']\n",
        "    amused_model = \"amused/amused-512\" if amused_prefs['amused_model'] == \"amused-512\" else \"amused/amused-256\" if amused_prefs['amused_model'] == \"amused-256\" else amused_prefs['amused_custom_model']\n",
        "    if 'loaded_amused' not in status: status['loaded_amused'] = \"\"\n",
        "    if 'loaded_amused_mode' not in status: status['loaded_amused_mode'] = \"\"\n",
        "    if amused_model != status['loaded_amused']:\n",
        "        clear_pipes()\n",
        "    #from optimum.intel import OVLatentConsistencyModelPipeline\n",
        "    #pipe = OVLatentConsistencyModelPipeline.from_pretrained(\"rupeshs/Amused-dreamshaper-v7-openvino-int8\", ov_config={\"CACHE_DIR\": \"\"})\n",
        "    mem_kwargs = {} if prefs['higher_vram_mode'] else {'variant': \"fp16\", 'torch_dtype': torch.float16}\n",
        "    from diffusers import AmusedPipeline, AmusedImg2ImgPipeline, AmusedInpaintPipeline\n",
        "    def get_amused_pipe(mode=\"Text2Image\"):\n",
        "        global status\n",
        "        try:\n",
        "            if mode == \"Inpaint\":\n",
        "                pipe_amused = AmusedInpaintPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "            elif mode == \"Image2Image\":\n",
        "                pipe_amused = AmusedImg2ImgPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "            else:\n",
        "                pipe_amused = AmusedPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "            pipe_amused.to(torch_device)\n",
        "            pipe_amused.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Amused...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_amused'] = amused_model\n",
        "        status['loaded_amused_mode'] = mode\n",
        "        return pipe_amused\n",
        "    \n",
        "    if pipe_amused == None:\n",
        "        installer.status(f\"...initialize Amused Pipeline\")\n",
        "        try:\n",
        "            if bool(amused_prefs['init_image']) and bool(amused_prefs['mask_image']):\n",
        "                pipe_amused = AmusedInpaintPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "                status['loaded_amused_mode'] = \"Inpaint\"\n",
        "            elif bool(amused_prefs['init_image']):\n",
        "                pipe_amused = AmusedImg2ImgPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "                status['loaded_amused_mode'] = \"Image2Image\"\n",
        "            else:\n",
        "                pipe_amused = AmusedPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)\n",
        "                status['loaded_amused_mode'] = \"Text2Image\"\n",
        "            #if prefs['enable_torch_compile']:\n",
        "            #    installer.status(f\"...Torch compiling transformer\")\n",
        "            #    pipe_amused.transformer = torch.compile(pipe_amused.transformer, mode=\"reduce-overhead\", fullgraph=True)\n",
        "            #    pipe_amused = pipe_amused.to(torch_device)\n",
        "            #elif cpu_offload:\n",
        "            #    pipe_amused.enable_model_cpu_offload()\n",
        "            #else:\n",
        "            pipe_amused.to(torch_device)\n",
        "            pipe_amused.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Amused...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_amused'] = amused_model\n",
        "    else:\n",
        "        clear_pipes('amused')\n",
        "    \n",
        "    clear_last()\n",
        "    s = \"\" if len(amused_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your Amused Image{s}...\")\n",
        "    for pr in amused_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator().manual_seed(random_seed)\n",
        "        init_img = None\n",
        "        mask_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            max_size = max(pr['width'], pr['height'])\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, max_size, multiple=32)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        if bool(pr['mask_image']):\n",
        "            fname = pr['mask_image'].rpartition(slash)[2]\n",
        "            if pr['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['mask_image']):\n",
        "                    mask_img = PILImage.open(pr['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                    return\n",
        "            max_size = max(pr['width'], pr['height'])\n",
        "            width, height = mask_img.size\n",
        "            width, height = scale_dimensions(width, height, max_size, multiple=32)\n",
        "            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            mask_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        if bool(mask_img) and bool(init_img):\n",
        "            mode = \"Inpainting\"\n",
        "            mods = {'image': init_img, 'mask': mask_img, 'strength': pr['init_image_strength']}\n",
        "        elif bool(init_img):\n",
        "            mode = \"Image2Image\"\n",
        "            mods = {'image': init_img, 'strength': pr['init_image_strength']}\n",
        "        else:\n",
        "            mode = \"Text2Image\"\n",
        "            mods = {'height': pr['height'], 'width': pr['width']}\n",
        "        if mode != status['loaded_amused_mode']:\n",
        "            prt(Installing(f\"Initializing Amused {mode} Pipeline...\"))\n",
        "            clear_pipes()\n",
        "            pipe_amused = get_amused_pipe(mode)\n",
        "            clear_last()\n",
        "        try:\n",
        "            images = pipe_amused(\n",
        "                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                num_images_per_prompt=pr['num_images'],\n",
        "                num_inference_steps=pr['num_inference_steps'],\n",
        "                guidance_scale=pr['guidance_scale'],\n",
        "                generator=generator,\n",
        "                callback=callback_fnc,\n",
        "                callback_steps = 1,\n",
        "                **mods,\n",
        "            ).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(amused_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, amused_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{amused_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not amused_prefs['display_upscaled_image'] or not amused_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], amused_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': amused_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if amused_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=amused_prefs[\"enlarge_scale\"], face_enhance=amused_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if amused_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(amused_prefs[\"enlarge_scale\"]), height=pr['height'] * float(amused_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {amused_prefs['enlarge_scale']}x with ESRGAN\" if amused_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"Amused {mode}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = amused_prefs.copy()\n",
        "                    config_json['model_path'] = amused_model\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], amused_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], amused_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_wuerstchen(page, from_list=False, with_params=False):\n",
        "    global wuerstchen_prefs, pipe_wuerstchen, prefs\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    wuerstchen_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            wuerstchen_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':wuerstchen_prefs['guidance_scale'], 'steps':wuerstchen_prefs['steps'], 'width':wuerstchen_prefs['width'], 'height':wuerstchen_prefs['height'], 'num_images':wuerstchen_prefs['num_images'], 'seed':wuerstchen_prefs['seed']})\n",
        "        else:\n",
        "            wuerstchen_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(wuerstchen_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      wuerstchen_prompts.append({'prompt': wuerstchen_prefs['prompt'], 'negative_prompt':wuerstchen_prefs['negative_prompt'], 'guidance_scale':wuerstchen_prefs['guidance_scale'], 'steps':wuerstchen_prefs['steps'], 'width':wuerstchen_prefs['width'], 'height':wuerstchen_prefs['height'], 'num_images':wuerstchen_prefs['num_images'], 'seed':wuerstchen_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.Wuerstchen.controls.append(line)\n",
        "        if update:\n",
        "          page.Wuerstchen.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.Wuerstchen, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.Wuerstchen.auto_scroll = scroll\n",
        "        page.Wuerstchen.update()\n",
        "      else:\n",
        "        page.Wuerstchen.auto_scroll = scroll\n",
        "        page.Wuerstchen.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.Wuerstchen.controls = page.Wuerstchen.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prior_steps = wuerstchen_prefs['prior_steps']\n",
        "    total_steps = wuerstchen_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    def prior_callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      prior_callback_fnc.has_been_called = True\n",
        "      nonlocal progress, prior_steps\n",
        "      percent = (step +1)/ prior_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {prior_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing W√ºrstchen Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"wuerstchen\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n",
        "    cpu_offload = False\n",
        "    if pipe_wuerstchen == None:\n",
        "        #clear_pipes('wuerstchen')\n",
        "        try:\n",
        "            from diffusers import WuerstchenCombinedPipeline\n",
        "            pipe_wuerstchen = WuerstchenCombinedPipeline.from_pretrained(\"warp-ai/wuerstchen\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling unet\")\n",
        "                #pipe_wuerstchen.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_wuerstchen.unet = torch.compile(pipe_wuerstchen.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_wuerstchen = pipe_wuerstchen.to(\"cuda\")\n",
        "            elif cpu_offload:\n",
        "                pipe_wuerstchen.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_wuerstchen.to(\"cuda\")\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing W√ºrstchen, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('wuerstchen')\n",
        "    clear_last()\n",
        "    s = \"\" if len(wuerstchen_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your W√ºrstchen Image{s}...\")\n",
        "    for pr in wuerstchen_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['steps']\n",
        "        for n in range(pr['num_images']):\n",
        "            random_seed = (int(pr['seed']) + n) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_wuerstchen(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    prior_guidance_scale=wuerstchen_prefs['prior_guidance_scale'],\n",
        "                    prior_num_inference_steps=wuerstchen_prefs['prior_steps'],\n",
        "                    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n",
        "                    #num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    decoder_guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    prior_callback_on_step_end=prior_callback_fnc,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            txt2img_output = stable_dir\n",
        "            batch_output = prefs['image_output']\n",
        "            txt2img_output = stable_dir\n",
        "            if bool(wuerstchen_prefs['batch_folder_name']):\n",
        "                txt2img_output = os.path.join(stable_dir, wuerstchen_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(txt2img_output):\n",
        "                os.makedirs(txt2img_output)\n",
        "            if images is None:\n",
        "                prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "                return\n",
        "            idx = 0\n",
        "            for image in images:\n",
        "                fname = format_filename(pr['prompt'])\n",
        "                #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "                fname = f'{wuerstchen_prefs[\"file_prefix\"]}{fname}'\n",
        "                image_path = available_file(txt2img_output, fname, 1)\n",
        "                image.save(image_path)\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                if not wuerstchen_prefs['display_upscaled_image'] or not wuerstchen_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                batch_output = os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name'])\n",
        "                if not os.path.exists(batch_output):\n",
        "                    os.makedirs(batch_output)\n",
        "                if storage_type == \"PyDrive Google Drive\":\n",
        "                    newFolder = gdrive.CreateFile({'title': wuerstchen_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                    newFolder.Upload()\n",
        "                    batch_output = newFolder\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "                if wuerstchen_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=wuerstchen_prefs[\"enlarge_scale\"], faceenhance=wuerstchen_prefs[\"face_enhance\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if wuerstchen_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([Img(src=upscaled_path, width=pr['width'] * float(wuerstchen_prefs[\"enlarge_scale\"]), height=pr['height'] * float(wuerstchen_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {wuerstchen_prefs['enlarge_scale']}x with ESRGAN\" if wuerstchen_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"W√ºrstchen\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                        config_json = wuerstchen_prefs.copy()\n",
        "                        config_json['model_path'] = \"wuerstchen-community/wuerstchen-2-2-decoder\"\n",
        "                        config_json['seed'] = random_seed\n",
        "                        del config_json['num_images']\n",
        "                        del config_json['display_upscaled_image']\n",
        "                        del config_json['batch_folder_name']\n",
        "                        if not config_json['apply_ESRGAN_upscale']:\n",
        "                            del config_json['enlarge_scale']\n",
        "                            del config_json['apply_ESRGAN_upscale']\n",
        "                        metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name']), fname, 0)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name']), fname, 0)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_pixart_alpha(page, from_list=False, with_params=False):\n",
        "    global pixart_alpha_prefs, pipe_pixart_alpha, pipe_pixart_alpha_encoder, prefs\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    pixart_alpha_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            pixart_alpha_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':pixart_alpha_prefs['guidance_scale'], 'num_inference_steps':pixart_alpha_prefs['num_inference_steps'], 'width':pixart_alpha_prefs['width'], 'height':pixart_alpha_prefs['height'], 'num_images':pixart_alpha_prefs['num_images'], 'seed':pixart_alpha_prefs['seed']})\n",
        "        else:\n",
        "            pixart_alpha_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(pixart_alpha_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      pixart_alpha_prompts.append({'prompt': pixart_alpha_prefs['prompt'], 'negative_prompt':pixart_alpha_prefs['negative_prompt'], 'guidance_scale':pixart_alpha_prefs['guidance_scale'], 'num_inference_steps':pixart_alpha_prefs['num_inference_steps'], 'width':pixart_alpha_prefs['width'], 'height':pixart_alpha_prefs['height'], 'num_images':pixart_alpha_prefs['num_images'], 'seed':pixart_alpha_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.PixArtAlpha.controls.append(line)\n",
        "        if update:\n",
        "          page.PixArtAlpha.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.PixArtAlpha, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.PixArtAlpha.auto_scroll = scroll\n",
        "        page.PixArtAlpha.update()\n",
        "      else:\n",
        "        page.PixArtAlpha.auto_scroll = scroll\n",
        "        page.PixArtAlpha.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.PixArtAlpha.controls = page.PixArtAlpha.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = pixart_alpha_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing PixArt-Œ± Engine & Models... See console for progress, may take a while.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"pixart_alpha\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    pip_install(\"sentencepiece\", installer=installer, upgrade=True)\n",
        "    use_8bit = pixart_alpha_prefs['use_8bit']\n",
        "    if use_8bit:\n",
        "        try:\n",
        "          os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "          import bitsandbytes\n",
        "        except ModuleNotFoundError:\n",
        "          if sys.platform.startswith(\"win\"):\n",
        "              run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "          else:\n",
        "              run_sp(\"pip install bitsandbytes\", realtime=False, upgrade=True)\n",
        "          import bitsandbytes\n",
        "          pass\n",
        "        #pip_install(\"bitsandbytes\", q=True, installer=installer)\n",
        "    if pixart_alpha_prefs['clean_caption']:\n",
        "        pip_install(\"beautifulsoup4|bs4 ftfy\", installer=installer)\n",
        "    text_encoder = None\n",
        "    cpu_offload = pixart_alpha_prefs['cpu_offload']\n",
        "    pixart_model = \"PixArt-alpha/PixArt-XL-2-1024-MS\" if pixart_alpha_prefs['pixart_model'] == \"PixArt-XL-2-1024-MS\" else \"PixArt-alpha/PixArt-XL-2-512x512\" if pixart_alpha_prefs['pixart_model'] == \"PixArt-XL-2-512x512\" else \"PixArt-alpha/PixArt-LCM-XL-2-1024-MS\" if pixart_alpha_prefs['pixart_model'] == \"PixArt-LCM-XL-2-1024-MS\" else pixart_alpha_prefs['pixart_custom_model']\n",
        "    if 'loaded_pixart_8bit' not in status: status['loaded_pixart_8bit'] = use_8bit\n",
        "    if 'loaded_pixart' not in status: status['loaded_pixart'] = \"\"\n",
        "    if pixart_model != status['loaded_pixart'] or use_8bit != status['loaded_pixart_8bit']:\n",
        "        clear_pipes()\n",
        "    scheduler = {'scheduler': 'LCM'} if 'LCM' in pixart_model else {}\n",
        "    if pipe_pixart_alpha == None:\n",
        "        installer.status(f\"...initialize PixArtAlpha Pipeline\")\n",
        "        try:\n",
        "            from diffusers import PixArtAlphaPipeline\n",
        "            if not use_8bit:\n",
        "                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)\n",
        "                if prefs['enable_torch_compile']:\n",
        "                    installer.status(f\"...Torch compiling transformer\")\n",
        "                    pipe_pixart_alpha.transformer = torch.compile(pipe_pixart_alpha.transformer, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                    pipe_pixart_alpha = pipe_pixart_alpha.to(torch_device)\n",
        "                elif cpu_offload:\n",
        "                    pipe_pixart_alpha.enable_model_cpu_offload()\n",
        "                else:\n",
        "                    pipe_pixart_alpha.to(torch_device)\n",
        "            else:\n",
        "                from transformers import T5EncoderModel\n",
        "                installer.status(f\"...loading text encoder\")\n",
        "                text_encoder = T5EncoderModel.from_pretrained(pixart_model, subfolder=\"text_encoder\", load_in_8bit=True, device_map=\"auto\")\n",
        "                pipe_pixart_alpha_encoder = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=text_encoder, transformer=None, device_map=\"auto\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                installer.status(f\"...loading pipeline\")\n",
        "                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)\n",
        "            pipe_pixart_alpha.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing PixArt-Œ±...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_pixart'] = pixart_model\n",
        "        status['loaded_pixart_8bit'] = use_8bit\n",
        "    else:\n",
        "        clear_pipes('pixart_alpha')\n",
        "        if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "            pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)\n",
        "    clear_last()\n",
        "    s = \"\" if len(pixart_alpha_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your PixArt-Œ± Image{s}...\")\n",
        "    for pr in pixart_alpha_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        guidance_scale = pr['guidance_scale']\n",
        "        num_inference_steps = pr['num_inference_steps']\n",
        "        if 'LCM' in pixart_model:\n",
        "            guidance_scale = 0.\n",
        "            if num_inference_steps > 10:\n",
        "                num_inference_steps = 8\n",
        "        try:\n",
        "            if not use_8bit:\n",
        "                images = pipe_pixart_alpha(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    clean_caption=pixart_alpha_prefs['clean_caption'],\n",
        "                    use_resolution_binning=pixart_alpha_prefs['resolution_binning'],\n",
        "                    #mask_feature=pixart_alpha_prefs['mask_feature'],resolution_binning\n",
        "                    generator=generator,\n",
        "                    callback=callback_fnc,\n",
        "                ).images\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe_pixart_alpha_encoder.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'], num_images_per_prompt=pr['num_images'], clean_caption=pixart_alpha_prefs['clean_caption'])\n",
        "                #del text_encoder\n",
        "                #del pipe_pixart_alpha_encoder\n",
        "                #flush()\n",
        "                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                latents = pipe_pixart_alpha(\n",
        "                    negative_prompt=None, \n",
        "                    prompt_embeds=prompt_embeds,\n",
        "                    negative_prompt_embeds=negative_embeds,\n",
        "                    prompt_attention_mask=prompt_attention_mask,\n",
        "                    negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    output_type=\"latent\",\n",
        "                    clean_caption=pixart_alpha_prefs['clean_caption'],\n",
        "                    use_resolution_binning=pixart_alpha_prefs['resolution_binning'],\n",
        "                    generator=generator,\n",
        "                    callback=callback_fnc,\n",
        "                ).images\n",
        "                del pipe_pixart_alpha.transformer\n",
        "                flush()\n",
        "                with torch.no_grad():\n",
        "                    images = pipe_pixart_alpha.vae.decode(latents / pipe_pixart_alpha.vae.config.scaling_factor, return_dict=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(pixart_alpha_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, pixart_alpha_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{pixart_alpha_prefs[\"file_prefix\"]}{fname}'\n",
        "            if use_8bit:\n",
        "                image = pipe_pixart_alpha.image_processor.postprocess(image, output_type=\"pil\")[0]\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not pixart_alpha_prefs['display_upscaled_image'] or not pixart_alpha_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': pixart_alpha_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if pixart_alpha_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=pixart_alpha_prefs[\"enlarge_scale\"], face_enhance=pixart_alpha_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                if pixart_alpha_prefs['display_upscaled_image']:\n",
        "                    #prt(Row([Img(src=upscaled_path, width=pr['width'] * float(pixart_alpha_prefs[\"enlarge_scale\"]), height=pr['height'] * float(pixart_alpha_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(pixart_alpha_prefs[\"enlarge_scale\"]), height=pr['height'] * float(pixart_alpha_prefs[\"enlarge_scale\"]), data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {pixart_alpha_prefs['enlarge_scale']}x with ESRGAN\" if pixart_alpha_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"PixArt-Œ±\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = pixart_alpha_prefs.copy()\n",
        "                    config_json['model_path'] = pixart_model\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    del text_encoder\n",
        "    #del pipe_pixart_alpha_encoder\n",
        "    flush()\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_lmd_plus(page, from_list=False, with_params=False):\n",
        "    global lmd_plus_prefs, pipe_lmd_plus, prefs\n",
        "    from aeionic_utils import llm_template\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    lmd_plus_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            lmd_plus_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':lmd_plus_prefs['guidance_scale'], 'num_inference_steps':lmd_plus_prefs['num_inference_steps'], 'width':lmd_plus_prefs['width'], 'height':lmd_plus_prefs['height'], 'num_images':lmd_plus_prefs['num_images'], 'seed':lmd_plus_prefs['seed']})\n",
        "        else:\n",
        "            lmd_plus_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(lmd_plus_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      lmd_plus_prompts.append({'prompt': lmd_plus_prefs['prompt'], 'negative_prompt':lmd_plus_prefs['negative_prompt'], 'guidance_scale':lmd_plus_prefs['guidance_scale'], 'num_inference_steps':lmd_plus_prefs['num_inference_steps'], 'width':lmd_plus_prefs['width'], 'height':lmd_plus_prefs['height'], 'num_images':lmd_plus_prefs['num_images'], 'seed':lmd_plus_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.LMD_Plus.controls.append(line)\n",
        "        if update:\n",
        "          page.LMD_Plus.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.LMD_Plus, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.LMD_Plus.auto_scroll = scroll\n",
        "        page.LMD_Plus.update()\n",
        "      else:\n",
        "        page.LMD_Plus.auto_scroll = scroll\n",
        "        page.LMD_Plus.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.LMD_Plus.controls = page.LMD_Plus.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = lmd_plus_prefs['num_inference_steps']\n",
        "    def callback_fn(pipe, step, timestep, callback_kwargs):\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing LMD+ Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"lmd_plus\")\n",
        "    if 'GPT' in lmd_plus_prefs['AI_engine']:\n",
        "        try:\n",
        "            import openai\n",
        "        except:\n",
        "            installer.status(\"...installing OpenAI Library\")\n",
        "            run_sp(\"pip install --upgrade openai\", realtime=False)\n",
        "            import openai\n",
        "            pass\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "        except:\n",
        "            alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "            return\n",
        "        status['installed_OpenAI'] = True\n",
        "    elif lmd_plus_prefs['AI_engine'] == \"Google Gemini\":\n",
        "        if not bool(prefs['PaLM_api_key']):\n",
        "            alert_msg(page, \"You must provide your Google Gemini MakerSuite API key in Settings first\")\n",
        "            return\n",
        "        try:\n",
        "            import google.generativeai as genai\n",
        "            if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "        except:\n",
        "            installer.status(\"...installing Google MakerSuite Library\")\n",
        "            run_sp(\"pip install --upgrade google-generativeai\", realtime=False)\n",
        "            import google.generativeai as genai\n",
        "            pass\n",
        "        try:\n",
        "            genai.configure(api_key=prefs['PaLM_api_key'])\n",
        "        except:\n",
        "            alert_msg(page, \"Invalid Google Gemini API Key. Change in Settings...\")\n",
        "            return\n",
        "        gemini_model = genai.GenerativeModel(model_name='models/gemini-pro')\n",
        "    def get_response(prompt_full, AI_engine=\"ChatGPT-3.5 Turbo\", temperature=0.7):\n",
        "        if AI_engine == \"OpenAI GPT-3\":\n",
        "            response = openai_client.completions.create(engine=\"text-davinci-003\", prompt=prompt_full, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)\n",
        "            result = response.choices[0].text.strip()\n",
        "        elif AI_engine == \"ChatGPT-3.5 Turbo\":\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-16k\",\n",
        "                temperature=temperature,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_full}]\n",
        "            )\n",
        "            result = response.choices[0].message.content.strip()\n",
        "        elif \"GPT-4\" in AI_engine:\n",
        "            gpt_model = \"gpt-4-1106-preview\" if \"Turbo\" in AI_engine else \"gpt-4\"\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=gpt_model,\n",
        "                temperature=temperature,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_full}]\n",
        "            )\n",
        "            result = response.choices[0].message.content.strip()\n",
        "        elif AI_engine == \"Google Gemini\":\n",
        "            completion = gemini_model.generate_content(prompt_full, generation_config={\n",
        "                'temperature': temperature,\n",
        "                'max_output_tokens': 1024\n",
        "            })\n",
        "            #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt_full, temperature=temperature, max_output_tokens=1024)\n",
        "            result = completion.text.strip()\n",
        "        return result\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = lmd_plus_prefs['cpu_offload']\n",
        "    lmd_plus_model = \"longlian/lmd_plus\" if lmd_plus_prefs['lmd_plus_model'] == \"longlian/lmd_plus\" else lmd_plus_prefs['custom_model']\n",
        "    if 'loaded_lmd_plus' not in status: status['loaded_lmd_plus'] = \"\"\n",
        "    if lmd_plus_model != status['loaded_lmd_plus']:\n",
        "        clear_pipes()\n",
        "    if pipe_lmd_plus == None:\n",
        "        installer.status(f\"...initialize LMD+ Pipeline\")\n",
        "        try:\n",
        "            torch.backends.cuda.enable_flash_sdp(False)\n",
        "            torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "            from diffusers import DiffusionPipeline\n",
        "            pipe_lmd_plus = DiffusionPipeline.from_pretrained(lmd_plus_model, custom_pipeline=\"AlanB/llm_grounded_diffusion_fix\", variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, requires_safety_checker=False\n",
        "            #custom_pipeline=\"llm_grounded_diffusion\"\n",
        "            #pipe_lmd_plus = pipeline_scheduler(pipe_lmd_plus)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling transformer\")\n",
        "                pipe_lmd_plus.transformer = torch.compile(pipe_lmd_plus.transformer, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_lmd_plus = pipe_lmd_plus.to(torch_device)\n",
        "            elif cpu_offload:\n",
        "                pipe_lmd_plus.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_lmd_plus = pipe_lmd_plus.to(torch_device)\n",
        "            pipe_lmd_plus.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing LMD+...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_lmd_plus'] = lmd_plus_model\n",
        "    else:\n",
        "        clear_pipes('lmd_plus')\n",
        "        #if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        #    pipe_lmd_plus = pipeline_scheduler(pipe_lmd_plus)\n",
        "    clear_last()\n",
        "    s = \"\" if len(lmd_plus_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your LMD+ Image{s}...\")\n",
        "    for pr in lmd_plus_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator().manual_seed(random_seed)\n",
        "        prompt_full = llm_template.format(prompt=pr['prompt'].strip().rstrip(\".\"), width=pr['width'], height=pr['height'])\n",
        "        response = get_response(prompt_full, AI_engine=lmd_plus_prefs['AI_engine'])\n",
        "        clear_last()\n",
        "        prt(Markdown(response))\n",
        "        prt(progress)\n",
        "        phrases, boxes, bg_prompt, neg_prompt = pipe_lmd_plus.parse_llm_response(response.strip())\n",
        "        if bool(pr['negative_prompt']):\n",
        "            neg_prompt += f\", {pr['negative_prompt']}\"\n",
        "        try:\n",
        "            images = pipe_lmd_plus(\n",
        "                prompt=pr['prompt'],\n",
        "                negative_prompt=neg_prompt,\n",
        "                phrases=phrases,\n",
        "                boxes=boxes,\n",
        "                num_images_per_prompt=pr['num_images'],\n",
        "                height=pr['height'],\n",
        "                width=pr['width'],\n",
        "                num_inference_steps=pr['num_inference_steps'],\n",
        "                guidance_scale=pr['guidance_scale'],\n",
        "                gligen_scheduled_sampling_beta=lmd_plus_prefs['gligen_scheduled_sampling_beta'],\n",
        "                #generator=generator,\n",
        "                callback=callback_fnc,\n",
        "                lmd_guidance_kwargs={}\n",
        "            ).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(lmd_plus_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, lmd_plus_prefs['batch_folder_name'])\n",
        "        make_dir(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{lmd_plus_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not lmd_plus_prefs['display_upscaled_image'] or not lmd_plus_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name'])\n",
        "            make_dir(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': lmd_plus_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if lmd_plus_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=lmd_plus_prefs[\"enlarge_scale\"], face_enhance=lmd_plus_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if lmd_plus_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(lmd_plus_prefs[\"enlarge_scale\"]), height=pr['height'] * float(lmd_plus_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, width=pr['width'] * float(lmd_plus_prefs[\"enlarge_scale\"]), height=pr['height'] * float(lmd_plus_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {lmd_plus_prefs['enlarge_scale']}x with ESRGAN\" if lmd_plus_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"LMD+\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = lmd_plus_prefs.copy()\n",
        "                    config_json['model_path'] = lmd_plus_model\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_lcm(page, from_list=False, with_params=False):\n",
        "    global lcm_prefs, pipe_lcm, prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    lcm_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            lcm_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':lcm_prefs['guidance_scale'], 'num_inference_steps':lcm_prefs['num_inference_steps'], 'width':lcm_prefs['width'], 'height':lcm_prefs['height'], 'init_image':lcm_prefs['init_image'], 'init_image_strength':lcm_prefs['init_image_strength'], 'num_images':lcm_prefs['num_images'], 'seed':lcm_prefs['seed']})\n",
        "        else:\n",
        "            lcm_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(lcm_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      lcm_prompts.append({'prompt': lcm_prefs['prompt'], 'negative_prompt':lcm_prefs['negative_prompt'], 'guidance_scale':lcm_prefs['guidance_scale'], 'num_inference_steps':lcm_prefs['num_inference_steps'], 'width':lcm_prefs['width'], 'height':lcm_prefs['height'], 'init_image':lcm_prefs['init_image'], 'init_image_strength':lcm_prefs['init_image_strength'], 'num_images':lcm_prefs['num_images'], 'seed':lcm_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.LCM.controls.append(line)\n",
        "        if update:\n",
        "          page.LCM.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.LCM, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.LCM.auto_scroll = scroll\n",
        "        page.LCM.update()\n",
        "      else:\n",
        "        page.LCM.auto_scroll = scroll\n",
        "        page.LCM.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.LCM.controls = page.LCM.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = lcm_prefs['num_inference_steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing LCM Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"lcm\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = lcm_prefs['cpu_offload']\n",
        "    lcm_model = \"SimianLuo/LCM_Dreamshaper_v7\" if lcm_prefs['lcm_model'] == \"LCM_Dreamshaper_v7\" else \"Lykon/dreamshaper-8-lcm\" if lcm_prefs['lcm_model'] == \"LCM_Dreamshaper_v8\" else lcm_prefs['lcm_custom_model']\n",
        "    if 'loaded_lcm' not in status: status['loaded_lcm'] = \"\"\n",
        "    if 'loaded_lcm_mode' not in status: status['loaded_lcm_mode'] = \"\"\n",
        "    if lcm_model != status['loaded_lcm']:\n",
        "        clear_pipes()\n",
        "    #from optimum.intel import OVLatentConsistencyModelPipeline\n",
        "    #pipe = OVLatentConsistencyModelPipeline.from_pretrained(\"rupeshs/LCM-dreamshaper-v7-openvino-int8\", ov_config={\"CACHE_DIR\": \"\"})\n",
        "    \n",
        "    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, LCMScheduler\n",
        "    if pipe_lcm == None:\n",
        "        installer.status(f\"...initialize LCM Pipeline\")\n",
        "        try:\n",
        "            if bool(lcm_prefs['init_image']):\n",
        "                pipe_lcm = AutoPipelineForImage2Image.from_pretrained(lcm_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                status['loaded_lcm_mode'] = \"Image2Image\"\n",
        "            else:\n",
        "                pipe_lcm = AutoPipelineForText2Image.from_pretrained(lcm_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                status['loaded_lcm_mode'] = \"Text2Image\"\n",
        "            #pipe_lcm = pipeline_scheduler(pipe_lcm)\n",
        "            pipe_lcm.scheduler = LCMScheduler.from_config(pipe_lcm.scheduler.config)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling transformer\")\n",
        "                pipe_lcm.transformer = torch.compile(pipe_lcm.transformer, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_lcm = pipe_lcm.to(torch_device)\n",
        "            elif cpu_offload:\n",
        "                pipe_lcm.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_lcm.to(torch_device)\n",
        "            pipe_lcm.set_progress_bar_config(disable=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing LCM...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_lcm'] = lcm_model\n",
        "    else:\n",
        "        clear_pipes('lcm')\n",
        "    ip_adapter_arg = {}\n",
        "    if lcm_prefs['use_ip_adapter']:\n",
        "        installer.status(f\"...initialize IP-Adapter\")\n",
        "        ip_adapter_img = None\n",
        "        if lcm_prefs['ip_adapter_image'].startswith('http'):\n",
        "          i_response = requests.get(lcm_prefs['ip_adapter_image'])\n",
        "          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "        else:\n",
        "          if os.path.isfile(lcm_prefs['ip_adapter_image']):\n",
        "            ip_adapter_img = PILImage.open(lcm_prefs['ip_adapter_image'])\n",
        "          else:\n",
        "            clear_last()\n",
        "            prt(f\"ERROR: Couldn't find your ip_adapter_image {lcm_prefs['ip_adapter_image']}\")\n",
        "        if bool(ip_adapter_img):\n",
        "          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img\n",
        "        if bool(ip_adapter_arg):\n",
        "            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == lcm_prefs['ip_adapter_model'])\n",
        "            pipe_lcm.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "            pipe_lcm.set_ip_adapter_scale(lcm_prefs['ip_adapter_strength'])\n",
        "\n",
        "    clear_last()\n",
        "    s = \"\" if len(lcm_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your LCM Image{s}...\")\n",
        "    for pr in lcm_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "        init_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        try:\n",
        "            if init_img is not None:\n",
        "                if status['loaded_lcm_mode'] != \"Image2Image\":\n",
        "                    pipe_lcm = AutoPipelineForImage2Image.from_pipe(pipe_lcm)\n",
        "                    status['loaded_lcm_mode'] = \"Image2Image\"\n",
        "                images = pipe_lcm(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['num_inference_steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    init_image=init_img,\n",
        "                    init_image_strength=pr['init_image_strength'],\n",
        "                    generator=generator,\n",
        "                    #callback_on_step_end=callback_fnc,\n",
        "                    **ip_adapter_arg,\n",
        "                ).images\n",
        "            else:\n",
        "                if status['loaded_lcm_mode'] != \"Text2Image\":\n",
        "                    pipe_lcm = AutoPipelineForText2Image.from_pipe(pipe_lcm)\n",
        "                    status['loaded_lcm_mode'] = \"Text2Image\"\n",
        "                images = pipe_lcm(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['num_inference_steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    #callback_on_step_end=callback_fnc,\n",
        "                    **ip_adapter_arg,\n",
        "                ).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        #clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(lcm_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, lcm_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{lcm_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not lcm_prefs['display_upscaled_image'] or not lcm_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': lcm_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if lcm_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=lcm_prefs[\"enlarge_scale\"], face_enhance=lcm_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if lcm_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(lcm_prefs[\"enlarge_scale\"]), height=pr['height'] * float(lcm_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {lcm_prefs['enlarge_scale']}x with ESRGAN\" if lcm_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"LCM\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = lcm_prefs.copy()\n",
        "                    config_json['model_path'] = lcm_model\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_lcm_interpolation(page):\n",
        "    global lcm_interpolation_prefs, pipe_lcm_interpolation, prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if len(lcm_interpolation_prefs['mixes']) < 2:\n",
        "      alert_msg(page, \"You must provide layers to interpolate to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.LCMInterpolation.controls.append(line)\n",
        "      page.LCMInterpolation.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.LCMInterpolation, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.LCMInterpolation.auto_scroll = scroll\n",
        "      page.LCMInterpolation.update()\n",
        "    def clear_list():\n",
        "      page.LCMInterpolation.controls = page.LCMInterpolation.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = lcm_interpolation_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      #callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing LCM Interpolation Engine & Models... See console log for progress.\"))\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from diffusers import DiffusionPipeline\n",
        "    images_texts = []\n",
        "    mix_names = []\n",
        "    for mix in lcm_interpolation_prefs['mixes']:\n",
        "        if 'prompt' in mix:\n",
        "            images_texts.append(mix['prompt'])\n",
        "            mix_names.append(mix['prompt'])\n",
        "    mix_name = \" - \".join(mix_names)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    model = \"SimianLuo/LCM_Dreamshaper_v7\" #get_model(prefs['model_ckpt'])['path']\n",
        "    if pipe_lcm_interpolation == None or status['loaded_model'] != model:\n",
        "        clear_pipes()\n",
        "        try:\n",
        "            pipe_lcm_interpolation = DiffusionPipeline.from_pretrained(model, custom_pipeline=\"latent_consistency_interpolate\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_lcm_interpolation = optimize_pipe(pipe_lcm_interpolation)\n",
        "            #pipe_lcm_interpolation.enable_vae_slicing()\n",
        "            if prefs['enable_torch_compile']:\n",
        "                pipe_lcm_interpolation.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_lcm_interpolation.unet = torch.compile(pipe_lcm_interpolation.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "            else:\n",
        "                pipe_lcm_interpolation.to(\"cuda\")\n",
        "            status['loaded_model'] = model\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing LCM, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes(\"lcm_interpolation\")\n",
        "    clear_last()\n",
        "    prt(\"Generating your LCM Interpolation Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(lcm_interpolation_prefs['seed']) if int(lcm_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "    try:\n",
        "        images = pipe_lcm_interpolation(\n",
        "            prompt=images_texts,\n",
        "            #num_images_per_prompt=lcm_interpolation_prefs['num_images'],\n",
        "            height=lcm_interpolation_prefs['height'],\n",
        "            width=lcm_interpolation_prefs['width'],\n",
        "            num_interpolation_steps=lcm_interpolation_prefs['num_interpolation_steps'],\n",
        "            num_inference_steps=lcm_interpolation_prefs['steps'],\n",
        "            guidance_scale=lcm_interpolation_prefs['guidance_scale'],\n",
        "            embedding_interpolation_type=lcm_interpolation_prefs['embedding_interpolation_type'],\n",
        "            latent_interpolation_type=lcm_interpolation_prefs['latent_interpolation_type'],\n",
        "            process_batch_size=lcm_interpolation_prefs['process_batch_size'],\n",
        "            generator=generator,\n",
        "            #callback_on_step_end=callback_fnc,\n",
        "        ).images\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    clear_last(2)\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(mix_name)\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{lcm_interpolation_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(lcm_interpolation_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, lcm_interpolation_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, idx, zfill=4)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not lcm_interpolation_prefs['display_upscaled_image'] or not lcm_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        batch_output = os.path.join(prefs['image_output'], lcm_interpolation_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(batch_output):\n",
        "            os.makedirs(batch_output)\n",
        "        if storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': lcm_interpolation_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "\n",
        "        if lcm_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, image_path, scale=lcm_interpolation_prefs[\"enlarge_scale\"], face_enhance=lcm_interpolation_prefs[\"face_enhance\"])\n",
        "            if lcm_interpolation_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=image_path, width=lcm_interpolation_prefs['width'] * float(lcm_interpolation_prefs[\"enlarge_scale\"]), height=lcm_interpolation_prefs['height'] * float(lcm_interpolation_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            time.sleep(0.2)\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        idx += 1\n",
        "    if lcm_interpolation_prefs['save_video']:\n",
        "        try:\n",
        "            installer = Installing(\"Running Google FILM: Frame Interpolation for Large Motion...\")\n",
        "            prt(installer)\n",
        "            out_file = available_file(out_path, fname, no_num=True, ext=\"mp4\")\n",
        "            if lcm_interpolation_prefs['interpolate_video']:\n",
        "                installer.set_message(\"Saving Frames to Video using FFMPEG with Deflicker...\")\n",
        "                interpolate_video(images, input_fps=lcm_interpolation_prefs['source_fps'], output_fps=lcm_interpolation_prefs['target_fps'], output_video=out_file, installer=installer)\n",
        "            else:\n",
        "                frames_to_video(out_path, pattern=fname+\"-%04d.png\", input_fps=lcm_interpolation_prefs['source_fps'], output_fps=lcm_interpolation_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't interpolate video, but frames still saved...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            pass\n",
        "        clear_last()\n",
        "        prt(f\"Video saved to {out_file}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    if lcm_interpolation_prefs['save_video']:\n",
        "        try:\n",
        "            prt(Row([VideoContainer(out_file, fps=lcm_interpolation_prefs['target_fps'], width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'])], alignment=MainAxisAlignment.CENTER))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def run_ldm3d(page, from_list=False, with_params=False):\n",
        "    global ldm3d_prefs, pipe_ldm3d, pipe_ldm3d_upscale, prefs\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    ldm3d_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            ldm3d_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':ldm3d_prefs['guidance_scale'], 'num_inference_steps':ldm3d_prefs['num_inference_steps'], 'width':ldm3d_prefs['width'], 'height':ldm3d_prefs['height'], 'num_images':ldm3d_prefs['num_images'], 'seed':ldm3d_prefs['seed']})\n",
        "        else:\n",
        "            ldm3d_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(ldm3d_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      ldm3d_prompts.append({'prompt': ldm3d_prefs['prompt'], 'negative_prompt':ldm3d_prefs['negative_prompt'], 'guidance_scale':ldm3d_prefs['guidance_scale'], 'num_inference_steps':ldm3d_prefs['num_inference_steps'], 'width':ldm3d_prefs['width'], 'height':ldm3d_prefs['height'], 'num_images':ldm3d_prefs['num_images'], 'seed':ldm3d_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.LDM3D.controls.append(line)\n",
        "        if update:\n",
        "          page.LDM3D.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.LDM3D, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.LDM3D.auto_scroll = scroll\n",
        "        page.LDM3D.update()\n",
        "      else:\n",
        "        page.LDM3D.auto_scroll = scroll\n",
        "        page.LDM3D.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.LDM3D.controls = page.LDM3D.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = ldm3d_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing LDM3D Engine & Models... See console for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"ldm3d\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = ldm3d_prefs['cpu_offload']\n",
        "    ldm3d_model = ldm3d_prefs['ldm3d_model'] if ldm3d_prefs['ldm3d_model'] != \"Custom\" else ldm3d_prefs['ldm3d_custom_model']\n",
        "    if 'loaded_ldm3d' not in status: status['loaded_ldm3d'] = \"\"\n",
        "    if ldm3d_model != status['loaded_ldm3d']:\n",
        "        clear_pipes()\n",
        "    from diffusers import StableDiffusionLDM3DPipeline, LCMScheduler\n",
        "    if pipe_ldm3d == None:\n",
        "        installer.status(f\"...initialize LDM3D Pipeline\")\n",
        "        try:\n",
        "            pipe_ldm3d = StableDiffusionLDM3DPipeline.from_pretrained(ldm3d_model, torch_dtype=torch.float16, safety=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_ldm3d = pipeline_scheduler(pipe_ldm3d)\n",
        "            #pipe_ldm3d.scheduler = LCMScheduler.from_config(pipe_ldm3d.scheduler.config)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...Torch compiling transformer\")\n",
        "                pipe_ldm3d.transformer = torch.compile(pipe_ldm3d.transformer, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_ldm3d = pipe_ldm3d.to(torch_device)\n",
        "            elif cpu_offload:\n",
        "                pipe_ldm3d.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_ldm3d.to(torch_device)\n",
        "            pipe_ldm3d.set_progress_bar_config(disable=True)\n",
        "            if ldm3d_prefs['use_upscale']:\n",
        "                installer.status(f\"...initialize LDM3D Upscale Pipeline\")\n",
        "                from diffusers import DiffusionPipeline#StableDiffusionUpscaleLDM3DPipeline\n",
        "                pipe_ldm3d_upscale = DiffusionPipeline.from_pretrained(\"Intel/ldm3d-sr\", custom_pipeline=\"pipeline_stable_diffusion_upscale_ldm3d\")#StableDiffusionUpscaleLDM3DPipeline.from_pretrained(\"Intel/ldm3d-sr\")\n",
        "                pipe_ldm3d_upscale.to(\"cuda\")\n",
        "                '''low_res_img = Image.open(f\"lemons_ldm3d_rgb.jpg\").convert(\"RGB\")\n",
        "                low_res_depth = Image.open(f\"lemons_ldm3d_depth.png\").convert(\"L\")\n",
        "                outputs = pipe_ldm3d_upscale(prompt=\"high quality high resolution uhd 4k image\", rgb=low_res_img, depth=low_res_depth, num_inference_steps=50, target_res=[1024, 1024])\n",
        "                upscaled_rgb, upscaled_depth =outputs.rgb[0], outputs.depth[0]'''\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing LDM3D...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        status['loaded_ldm3d'] = ldm3d_model\n",
        "    else:\n",
        "        clear_pipes('ldm3d')\n",
        "        if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "            pipe_ldm3d = pipeline_scheduler(pipe_ldm3d)\n",
        "    ip_adapter_arg = {}\n",
        "    if ldm3d_prefs['use_ip_adapter']:\n",
        "      installer.status(f\"...initialize IP-Adapter\")\n",
        "      ip_adapter_img = None\n",
        "      if ldm3d_prefs['ip_adapter_image'].startswith('http'):\n",
        "        i_response = requests.get(ldm3d_prefs['ip_adapter_image'])\n",
        "        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "      else:\n",
        "        if os.path.isfile(ldm3d_prefs['ip_adapter_image']):\n",
        "          ip_adapter_img = PILImage.open(ldm3d_prefs['ip_adapter_image'])\n",
        "        else:\n",
        "          clear_last()\n",
        "          prt(f\"ERROR: Couldn't find your ip_adapter_image {ldm3d_prefs['ip_adapter_image']}\")\n",
        "      if bool(ip_adapter_img):\n",
        "        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img\n",
        "      if bool(ip_adapter_arg):\n",
        "          ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == ldm3d_prefs['ip_adapter_model'])\n",
        "          pipe_ldm3d.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])\n",
        "          pipe_ldm3d.set_ip_adapter_scale(ldm3d_prefs['ip_adapter_strength'])\n",
        "\n",
        "    clear_last()\n",
        "    s = \"\" if len(ldm3d_prompts) == 0 else \"s\"\n",
        "    prt(f\"Generating your LDM3D Image{s}...\")\n",
        "    for pr in ldm3d_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['num_inference_steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_ldm3d(\n",
        "                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                num_images_per_prompt=pr['num_images'],\n",
        "                height=pr['height'],\n",
        "                width=pr['width'],\n",
        "                num_inference_steps=pr['num_inference_steps'],\n",
        "                guidance_scale=pr['guidance_scale'],\n",
        "                generator=generator,\n",
        "                callback=callback_fnc,\n",
        "                **ip_adapter_arg,\n",
        "            )\n",
        "            if ldm3d_prefs['use_upscale']:\n",
        "                target_width=pr['width'] * 2\n",
        "                target_height=pr['height'] * 2\n",
        "                prt(f\"Upscaling Image to {target_width}x{target_height}...\")\n",
        "                images = pipe_ldm3d_upscale(prompt=\"high quality high resolution uhd 4k image\", rgb=images.rgb, depth=images.depth, num_inference_steps=pr['num_inference_steps'], target_res=[target_width, target_height]).images\n",
        "                clear_last()\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(ldm3d_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, ldm3d_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        #for image in images:\n",
        "        for i in range(len(images.rgb)):\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            fname = f'{ldm3d_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname+\"-rgb\", i, no_num=True)\n",
        "            depth_image_path = available_file(txt2img_output, fname+\"-depth\", i, no_num=True)\n",
        "            rgb_image, depth_image = images.rgb[i], images.depth[i]\n",
        "            rgb_image.save(image_path)\n",
        "            depth_image.save(depth_image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            depth_output_file = image_path.rpartition(slash)[2]\n",
        "            if not ldm3d_prefs['display_upscaled_image'] or not ldm3d_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([ImageButton(src=depth_image_path, width=pr['width'], height=pr['height'], data=depth_image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': ldm3d_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            depth_upscaled_path = os.path.join(out_path, depth_output_file)\n",
        "            if ldm3d_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=ldm3d_prefs[\"enlarge_scale\"], face_enhance=ldm3d_prefs[\"face_enhance\"])\n",
        "                upscale_image(depth_image_path, depth_upscaled_path, scale=ldm3d_prefs[\"enlarge_scale\"], face_enhance=ldm3d_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                depth_image_path = depth_upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if ldm3d_prefs['display_upscaled_image']:\n",
        "                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(ldm3d_prefs[\"enlarge_scale\"]), height=pr['height'] * float(ldm3d_prefs[\"enlarge_scale\"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    prt(Row([ImageButton(src=depth_upscaled_path, width=pr['width'] * float(ldm3d_prefs[\"enlarge_scale\"]), height=pr['height'] * float(ldm3d_prefs[\"enlarge_scale\"]), data=depth_image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                imgs = [image_path, depth_image_path]\n",
        "                for image_file in imgs:\n",
        "                    img = PILImage.open(image_file)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {ldm3d_prefs['enlarge_scale']}x with ESRGAN\" if ldm3d_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"LDM3D\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                        config_json = ldm3d_prefs.copy()\n",
        "                        config_json['model_path'] = ldm3d_model\n",
        "                        config_json['seed'] = random_seed\n",
        "                        del config_json['num_images']\n",
        "                        del config_json['display_upscaled_image']\n",
        "                        del config_json['batch_folder_name']\n",
        "                        if not config_json['apply_ESRGAN_upscale']:\n",
        "                            del config_json['enlarge_scale']\n",
        "                            del config_json['apply_ESRGAN_upscale']\n",
        "                        metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_file, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(image_path))\n",
        "                depth_new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(depth_image_path))\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "                shutil.copy(depth_image_path, depth_new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(image_path))\n",
        "                depth_new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(depth_image_path))\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "                shutil.copy(depth_image_path, depth_new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_text_to_video(page):\n",
        "    global text_to_video_prefs, prefs, status, pipe_text_to_video, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TextToVideo.controls.append(line)\n",
        "      page.TextToVideo.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.TextToVideo, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.TextToVideo.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = text_to_video_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Text-To-Video Pipeline...\"))\n",
        "    if text_to_video_prefs['model'] == \"damo-vilab/text-to-video-ms-1.7b\":\n",
        "        model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    elif text_to_video_prefs['model'] == \"modelscope-damo-text2video-synthesis\":\n",
        "        model_id = \"damo-vilab/modelscope-damo-text2video-synthesis\"\n",
        "    elif text_to_video_prefs['model'] == \"modelscope-damo-text2video-pruned-weights\":\n",
        "        model_id = \"kabachuha/modelscope-damo-text2video-pruned-weights\"\n",
        "    elif text_to_video_prefs['model'] == \"cerspense/zeroscope_v2_576w\":\n",
        "        model_id = \"cerspense/zeroscope_v2_576w\"\n",
        "    elif text_to_video_prefs['model'] == \"cerspense/zeroscope_v2_XL\":\n",
        "        model_id = \"cerspense/zeroscope_v2_XL\"\n",
        "    clear_pipes()\n",
        "    #clear_pipes('text_to_video')\n",
        "    if pipe_text_to_video is None:\n",
        "        from diffusers import TextToVideoSDPipeline, DPMSolverMultistepScheduler\n",
        "        pipe_text_to_video = TextToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)\n",
        "        pipe_text_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video.scheduler.config)\n",
        "        if prefs['enable_freeu']:\n",
        "            pipe_text_to_video.enable_freeu(s1=0.9, s2=0.2, b1=1.1, b2=1.2)\n",
        "        if text_to_video_prefs['lower_memory']:\n",
        "            pipe_text_to_video.enable_sequential_cpu_offload()\n",
        "            #pipe_text_to_video.enable_model_cpu_offload()\n",
        "            pipe_text_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)\n",
        "            pipe_text_to_video.enable_vae_tiling()\n",
        "            pipe_text_to_video.enable_vae_slicing()\n",
        "        else:\n",
        "            pipe_text_to_video = pipe_text_to_video.to(torch_device)\n",
        "        #pipe_text_to_video = optimize_pipe(pipe_text_to_video, vae_tiling=True, vae=True, to_gpu=False)\n",
        "        pipe_text_to_video.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)\n",
        "    clear_last()\n",
        "    prt(\"Generating Text-To-Video of your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, text_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(text_to_video_prefs['seed']) if int(text_to_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = text_to_video_prefs['width']\n",
        "    height = text_to_video_prefs['height']\n",
        "    try:\n",
        "      #print(f\"prompt={text_to_video_prefs['prompt']}, negative_prompt={text_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_prefs['num_inference_steps']}, eta={text_to_video_prefs['eta']}, guidance_scale={text_to_video_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      frames = pipe_text_to_video(prompt=text_to_video_prefs['prompt'], negative_prompt=text_to_video_prefs['negative_prompt'], num_frames=text_to_video_prefs['num_frames'], num_inference_steps=text_to_video_prefs['num_inference_steps'], eta=text_to_video_prefs['eta'], guidance_scale=text_to_video_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Text-To-Video your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    save_path = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(text_to_video_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    if text_to_video_prefs['export_to_video']:\n",
        "        from diffusers.utils import export_to_video\n",
        "        video_path = export_to_video(frames)\n",
        "        shutil.copy(video_path, available_file(local_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        shutil.copy(video_path, available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        #print(f\"video_path: {video_path}\")\n",
        "    #video = frames.cpu().numpy()\n",
        "    #print(f\"video: {video}\")\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image: {type(image)} to {image_path}\")\n",
        "        cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(np_image).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not text_to_video_prefs['display_upscaled_image'] or not text_to_video_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if text_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=text_to_video_prefs[\"enlarge_scale\"], face_enhance=text_to_video_prefs[\"face_enhance\"])\n",
        "            image_path = upscaled_path\n",
        "            if text_to_video_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_prefs[\"enlarge_scale\"]), height=height * float(text_to_video_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {text_to_video_prefs['enlarge_scale']}x with ESRGAN\" if text_to_video_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Text-To-Video\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = text_to_video_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['lower_memory']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    if bool(video_path):\n",
        "        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_text_to_video_zero(page):\n",
        "    global text_to_video_zero_prefs, prefs, status, pipe_text_to_video_zero, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TextToVideo.controls.append(line)\n",
        "      page.TextToVideo.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.TextToVideo, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.TextToVideo.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = text_to_video_zero_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Text-To-Video Zero Pipeline...\"))\n",
        "    model_id = get_model(prefs['model_ckpt'])['path'] if not text_to_video_zero_prefs['use_SDXL'] else get_SDXL_model(prefs['SDXL_model'])['path']\n",
        "    if 'loaded_text_to_video_zero' not in status: status['loaded_text_to_video_zero'] = \"\"\n",
        "    if model_id != status['loaded_text_to_video_zero']:\n",
        "        clear_pipes()\n",
        "    else:\n",
        "        clear_pipes('text_to_video_zero')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_text_to_video_zero is None:\n",
        "        if not text_to_video_zero_prefs['use_SDXL']:\n",
        "            from diffusers import TextToVideoZeroPipeline, DPMSolverMultistepScheduler\n",
        "            pipe_text_to_video_zero = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        else:\n",
        "            from diffusers import TextToVideoZeroSDXLPipeline, DPMSolverMultistepScheduler\n",
        "            pipe_text_to_video_zero = TextToVideoZeroSDXLPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)\n",
        "        if prefs['enable_freeu']:\n",
        "            pipe_text_to_video_zero.enable_freeu(s1=0.9, s2=0.2, b1=1.1, b2=1.2)\n",
        "        pipe_text_to_video_zero.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video_zero.scheduler.config)\n",
        "        pipe_text_to_video_zero = pipe_text_to_video_zero.to(torch_device)\n",
        "        #pipe_text_to_video_zero = optimize_pipe(pipe_text_to_video_zero, vae_tiling=True, vae=True, to_gpu=False)\n",
        "        pipe_text_to_video_zero.set_progress_bar_config(disable=True)\n",
        "    #else:\n",
        "    #    pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)\n",
        "    try:\n",
        "        import imageio\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"pip install imageio\", realtime=False)\n",
        "        import imageio\n",
        "        pass\n",
        "    clear_last()\n",
        "    prt(\"Generating Text-To-Video Zero from your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, text_to_video_zero_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(text_to_video_zero_prefs['seed']) if int(text_to_video_zero_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = text_to_video_zero_prefs['width']\n",
        "    height = text_to_video_zero_prefs['height']\n",
        "    try:\n",
        "      #print(f\"prompt={text_to_video_zero_prefs['prompt']}, negative_prompt={text_to_video_zero_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_zero_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_zero_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_zero_prefs['num_inference_steps']}, eta={text_to_video_zero_prefs['eta']}, guidance_scale={text_to_video_zero_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      frames = pipe_text_to_video_zero(prompt=text_to_video_zero_prefs['prompt'], negative_prompt=text_to_video_zero_prefs['negative_prompt'], video_length=text_to_video_zero_prefs['num_frames'], num_inference_steps=text_to_video_zero_prefs['num_inference_steps'], eta=text_to_video_zero_prefs['eta'], guidance_scale=text_to_video_zero_prefs['guidance_scale'], motion_field_strength_x=text_to_video_zero_prefs['motion_field_strength_x'], motion_field_strength_y=text_to_video_zero_prefs['motion_field_strength_y'], t0=text_to_video_zero_prefs['t0'], t1=text_to_video_zero_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Text-To-Video Zero your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    save_path = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(text_to_video_zero_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    if text_to_video_zero_prefs['export_to_video']:\n",
        "        #from diffusers.utils import export_to_video\n",
        "        #video_path = export_to_video(frames)\n",
        "        local_file = available_file(local_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        save_file = available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        imageio.mimsave(local_file, frames, fps=4)\n",
        "        shutil.copy(local_file, save_file)\n",
        "        #print(f\"video_path: {video_path}\")\n",
        "    #video = frames.cpu().numpy()\n",
        "    #print(f\"video: {video}\")\n",
        "    #print(f\"frames type: {type(frames)} len: {len(frames)}\")\n",
        "    #result = [(r * 255).astype(\"uint8\") for r in result]\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        img = PILImage.fromarray((image * 255).astype(\"uint8\"))\n",
        "        #print(f\"img type: {type(img)}\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image type: {type(image)} to {image_path}\")\n",
        "        #cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(image).save(image_path)\n",
        "        img.save(image_path)\n",
        "        #image.save(image_path)\n",
        "        #imageio.imwrite(local_file, image, extension=\".png\")\n",
        "        #img = pipe_text_to_video_zero.numpy_to_pil(image)\n",
        "        #img.save(image_path)\n",
        "        #imageio.imsave(local_file, img, extension=\".png\")\n",
        "        #PILImage.fromarray(img).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not text_to_video_zero_prefs['display_upscaled_image'] or not text_to_video_zero_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if text_to_video_zero_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=text_to_video_zero_prefs[\"enlarge_scale\"], face_enhance=text_to_video_zero_prefs[\"face_enhance\"])\n",
        "            image_path = upscaled_path\n",
        "            if text_to_video_zero_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_zero_prefs[\"enlarge_scale\"]), height=height * float(text_to_video_zero_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {text_to_video_zero_prefs['enlarge_scale']}x with ESRGAN\" if text_to_video_zero_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Text-To-Video Zero\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = text_to_video_zero_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_video_to_video(page):\n",
        "    global video_to_video_prefs, prefs, status, pipe_video_to_video, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.VideoToVideo.controls.append(line)\n",
        "      page.VideoToVideo.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.VideoToVideo, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.VideoToVideo.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.VideoToVideo.auto_scroll = scroll\n",
        "      page.VideoToVideo.update()\n",
        "    if not bool(video_to_video_prefs['init_video']):\n",
        "      alert_msg(page, \"You must provide the Input Initial Video Clip to process...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = video_to_video_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Video-To-Video Pipeline...\")\n",
        "    prt(installer)\n",
        "    #), dropdown.Option(), dropdown.Option(), dropdown.Option(), dropdown.Option(\n",
        "    #model_id = \"cerspense/zeroscope_v2_XL\"\n",
        "    if video_to_video_prefs['model'] == \"damo-vilab/text-to-video-ms-1.7b\":\n",
        "        model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    elif video_to_video_prefs['model'] == \"modelscope-damo-text2video-synthesis\":\n",
        "        model_id = \"damo-vilab/modelscope-damo-text2video-synthesis\"\n",
        "    elif video_to_video_prefs['model'] == \"modelscope-damo-text2video-pruned-weights\":\n",
        "        model_id = \"kabachuha/modelscope-damo-text2video-pruned-weights\"\n",
        "    elif video_to_video_prefs['model'] == \"cerspense/zeroscope_v2_576w\":\n",
        "        model_id = \"cerspense/zeroscope_v2_576w\"\n",
        "    elif video_to_video_prefs['model'] == \"cerspense/zeroscope_v2_XL\":\n",
        "        model_id = \"cerspense/zeroscope_v2_XL\"\n",
        "    clear_pipes('video_to_video')\n",
        "    if pipe_video_to_video is None:\n",
        "        installer.status(\"...initializing VideoToVideo SDPipeline\")\n",
        "        from diffusers import VideoToVideoSDPipeline, DPMSolverMultistepScheduler\n",
        "        pipe_video_to_video = VideoToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)\n",
        "        pipe_video_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_video_to_video.scheduler.config)\n",
        "        if video_to_video_prefs['lower_memory']:\n",
        "            #pipe_video_to_video.enable_sequential_cpu_offload()\n",
        "            pipe_video_to_video.enable_model_cpu_offload()\n",
        "            pipe_video_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)\n",
        "            #pipe_video_to_video.enable_vae_tiling()\n",
        "            pipe_video_to_video.enable_vae_slicing()\n",
        "        else:\n",
        "            pipe_video_to_video = pipe_video_to_video.to(torch_device)\n",
        "        #pipe_video_to_video = optimize_pipe(pipe_video_to_video, vae_tiling=True, vae=True, to_gpu=False)\n",
        "        pipe_video_to_video.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)\n",
        "    #clear_last()\n",
        "    def prep_video(vid):\n",
        "        nonlocal width, height\n",
        "        if vid.startswith('http'):\n",
        "            init_vid = download_file(vid, stable_dir)\n",
        "            installer.status(\"...extracting frames from video\")\n",
        "        else:\n",
        "            if os.path.isfile(vid):\n",
        "                init_vid = vid\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_video {vid}\")\n",
        "                return\n",
        "        try:\n",
        "            start_time = float(controlnet_prefs['start_time'])\n",
        "            end_time = float(controlnet_prefs['end_time'])\n",
        "            fps = int(controlnet_prefs['fps'])\n",
        "            max_size = controlnet_prefs['max_size']\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        installer.status(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                #image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))\n",
        "                video.append(image)\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        #clear_last()\n",
        "        return video\n",
        "    init_frames = prep_video(video_to_video_prefs['init_video'])\n",
        "    clear_last()\n",
        "    prt(f\"Generating Video-To-Video on {len(frames)} Frames with your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, video_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(video_to_video_prefs['seed']) if int(video_to_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = video_to_video_prefs['width']\n",
        "    height = video_to_video_prefs['height']\n",
        "    try:\n",
        "      #print(f\"prompt={video_to_video_prefs['prompt']}, negative_prompt={video_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={video_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={video_to_video_prefs['edit_mom_beta']}, num_inference_steps={video_to_video_prefs['num_inference_steps']}, eta={video_to_video_prefs['eta']}, guidance_scale={video_to_video_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      frames = pipe_video_to_video(prompt=video_to_video_prefs['prompt'], negative_prompt=video_to_video_prefs['negative_prompt'], video=init_frames, num_inference_steps=video_to_video_prefs['num_inference_steps'], eta=video_to_video_prefs['eta'], guidance_scale=video_to_video_prefs['guidance_scale'], strength=video_to_video_prefs['strength'], generator=generator, callback=callback_fnc, callback_steps=1).frames\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Video-To-Video your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    save_path = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(video_to_video_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    if video_to_video_prefs['export_to_video']:\n",
        "        from diffusers.utils import export_to_video\n",
        "        video_path = export_to_video(frames)\n",
        "        shutil.copy(video_path, available_file(local_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        shutil.copy(video_path, available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        #print(f\"video_path: {video_path}\")\n",
        "    #video = frames.cpu().numpy()\n",
        "    #print(f\"video: {video}\")\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image: {type(image)} to {image_path}\")\n",
        "        cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(np_image).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not video_to_video_prefs['display_upscaled_image'] or not video_to_video_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if video_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=video_to_video_prefs[\"enlarge_scale\"], face_enhance=video_to_video_prefs[\"face_enhance\"])\n",
        "            image_path = upscaled_path\n",
        "            if video_to_video_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(video_to_video_prefs[\"enlarge_scale\"]), height=height * float(video_to_video_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {video_to_video_prefs['enlarge_scale']}x with ESRGAN\" if video_to_video_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Video-To-Video\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = video_to_video_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['lower_memory']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    if bool(video_path):\n",
        "        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet_temporalnet(page):\n",
        "    global controlnet_temporalnet_prefs, prefs, status, pipe_controlnet, controlnet\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TemporalNet_XL.controls.append(line)\n",
        "      page.TemporalNet_XL.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.TemporalNet_XL, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.TemporalNet_XL.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TemporalNet_XL.auto_scroll = scroll\n",
        "      page.TemporalNet_XL.update()\n",
        "    if not bool(controlnet_temporalnet_prefs['init_video']):\n",
        "      alert_msg(page, \"You must provide the Input Initial Video Clip to process...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_temporalnet_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Controlnet TemporalNet XL Pipeline...\")\n",
        "    prt(installer)\n",
        "    from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "    from diffusers.utils import load_image\n",
        "    import numpy as np\n",
        "    from PIL import Image as PILImage\n",
        "    fps = 25\n",
        "    def split_video_into_frames(video_path, frames_dir):\n",
        "        nonlocal fps\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        success, image = vidcap.read()\n",
        "        count = 0\n",
        "        while success:\n",
        "            if max(image.shape[:2]) != controlnet_temporalnet_prefs['max_size']:\n",
        "                scale_factor = controlnet_temporalnet_prefs['max_size'] / max(image.shape[:2])\n",
        "                image = cv2.resize(image, (0, 0), fx=scale_factor, fy=scale_factor)\n",
        "            frame_path = os.path.join(frames_dir, f\"frame{count:04d}.png\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "    def frame_number(frame_filename):\n",
        "        return int(frame_filename[5:-4])\n",
        "    def count_frame_images(frames_dir):\n",
        "        frame_files = [f for f in os.listdir(frames_dir) if f.startswith('frame') and f.endswith('.png')]\n",
        "        return len(frame_files)\n",
        "    def write_video(video_out, frames_dir, fps):\n",
        "        frames = [os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if os.path.isfile(os.path.join(frames_dir, f))]\n",
        "        frame = cv2.imread(frames[0])\n",
        "        height, width, _ = frame.shape\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(video_out, fourcc, fps, (width, height))\n",
        "        for frame in frames:\n",
        "            img = cv2.imread(frame)\n",
        "            out.write(img)\n",
        "        out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "    \n",
        "    save_dir = os.path.join(stable_dir, controlnet_temporalnet_prefs['batch_folder_name'])\n",
        "    frames_dir = os.path.join(save_dir, 'frames')\n",
        "    make_dir(frames_dir)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_temporalnet_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    output_frames_dir = os.path.join(save_dir, 'output_frames')\n",
        "    make_dir(output_frames_dir)\n",
        "    if controlnet_temporalnet_prefs['save_canny']:\n",
        "        canny_frames_dir = os.path.join(batch_output, 'canny_frames')\n",
        "        make_dir(canny_frames_dir)\n",
        "    if controlnet_temporalnet_prefs['save_frames']:\n",
        "        save_frames_dir = os.path.join(batch_output, 'frames')\n",
        "        make_dir(save_frames_dir)\n",
        "    init_image_path = controlnet_temporalnet_prefs['init_image']\n",
        "    video_path = controlnet_temporalnet_prefs['init_video']\n",
        "    if video_path.startswith('http'):\n",
        "        video_path = download_file(video_path, uploads_dir, ext=\"mp4\")\n",
        "    else:\n",
        "        if not os.path.isfile(video_path):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {video_path}\")\n",
        "            return\n",
        "    if count_frame_images(frames_dir) == 0:\n",
        "        installer.status(\"...split_video_into_frames\")\n",
        "        split_video_into_frames(video_path, frames_dir)\n",
        "    if bool(init_image_path):\n",
        "        last_generated_image = load_image(init_image_path)\n",
        "    else:\n",
        "        initial_frame_path = os.path.join(frames_dir, \"frame0000.png\")\n",
        "        last_generated_image = load_image(initial_frame_path)\n",
        "\n",
        "    base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    controlnet1_path = \"CiaraRowles/controlnet-temporalnet-sdxl-1.0\"\n",
        "    controlnet2_path = \"diffusers/controlnet-canny-sdxl-1.0\"\n",
        "    if status['loaded_controlnet_type'] == \"TemporalNet\":\n",
        "        clear_pipes('controlnet')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    if pipe_controlnet is None:\n",
        "        controlnet = [\n",
        "            ControlNetModel.from_pretrained(controlnet1_path, torch_dtype=torch.float16,use_safetensors=True),\n",
        "            ControlNetModel.from_pretrained(controlnet2_path, torch_dtype=torch.float16)\n",
        "        ]\n",
        "        pipe_controlnet = StableDiffusionXLControlNetPipeline.from_pretrained(base_model_path, controlnet=controlnet, torch_dtype=torch.float16)\n",
        "        pipe_controlnet = pipeline_scheduler(pipe_controlnet)\n",
        "        #pipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)\n",
        "        #pipe_controlnet.enable_xformers_memory_efficient_attention()\n",
        "        if controlnet_temporalnet_prefs['lower_memory']:\n",
        "            pipe_controlnet.enable_model_cpu_offload()\n",
        "        else:\n",
        "            pipe_controlnet.to(torch_device)\n",
        "        status['loaded_controlnet_type'] = \"TemporalNet\"\n",
        "    clear_last()\n",
        "    prt(f\"Generating Controlnet TemporalNet XL on Frames with your Prompt...\")\n",
        "    fname = format_filename(controlnet_temporalnet_prefs['prompt'])\n",
        "    random_seed = int(controlnet_temporalnet_prefs['seed']) if int(controlnet_temporalnet_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.manual_seed(random_seed)\n",
        "    frame_files = sorted(os.listdir(frames_dir), key=frame_number)\n",
        "\n",
        "    for i, frame_file in enumerate(frame_files):\n",
        "        # Use the original video frame to create Canny edge-detected image as the conditioning image for the first ControlNetModel\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        control_image_path = os.path.join(frames_dir, frame_file)\n",
        "        control_image = load_image(control_image_path)\n",
        "        canny_image = np.array(control_image)\n",
        "        canny_image = cv2.Canny(canny_image, controlnet_temporalnet_prefs['low_threshold'], controlnet_temporalnet_prefs['high_threshold'])\n",
        "        canny_image = canny_image[:, :, None]\n",
        "        canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
        "        canny_image = PILImage.fromarray(canny_image)\n",
        "        image = pipe_controlnet(\n",
        "            controlnet_temporalnet_prefs['prompt'], negative_prompt=controlnet_temporalnet_prefs['negative_prompt'], num_inference_steps=controlnet_temporalnet_prefs['num_inference_steps'], guidance=controlnet_temporalnet_prefs['guidance'], generator=generator, image=[last_generated_image, canny_image], controlnet_conditioning_scale=[controlnet_temporalnet_prefs['temporalnet_strength'], controlnet_temporalnet_prefs['canny_strength']], callback=callback_fnc\n",
        "        ).images[0]\n",
        "        w, h = image.size\n",
        "        output_path = os.path.join(output_frames_dir, f\"frame{str(i).zfill(4)}.png\")\n",
        "        image.save(output_path)\n",
        "        if controlnet_temporalnet_prefs['save_canny']:\n",
        "            canny_image_path = os.path.join(canny_frames_dir, f\"outputcanny{str(i).zfill(4)}.png\")\n",
        "            canny_image.save(canny_image_path)\n",
        "        last_generated_image = image\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        prt(Row([Img(src=output_path, fit=ImageFit.CONTAIN, width=w, height=h, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([Text(output_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(Installing(f\"Saving Video File... Frames at {output_frames_dir if not controlnet_temporalnet_prefs['save_frames'] else save_frames_dir}\"))\n",
        "    if controlnet_temporalnet_prefs['save_frames']:\n",
        "        shutil.copytree(output_frames_dir, save_frames_dir, dirs_exist_ok=True)\n",
        "    video_out = available_file(batch_output, fname, 0, no_num=True, ext=\"mp4\")\n",
        "    write_video(video_out, output_frames_dir, fps)\n",
        "    clear_last\n",
        "    prt(f\"Saved to {video_out}\")\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_infinite_zoom(page):\n",
        "    global prefs, infinite_zoom_prefs, pipe_infinite_zoom, status\n",
        "    def prt(line):\n",
        "        if type(line) == str:\n",
        "            line = Text(line, size=17)\n",
        "        page.InfiniteZoom.controls.append(line)\n",
        "        page.InfiniteZoom.update()\n",
        "    def clear_last(lines=1):\n",
        "        clear_line(page.InfiniteZoom, lines=lines)\n",
        "    def clear_list():\n",
        "        page.InfiniteZoom.controls = page.InfiniteZoom.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "        page.InfiniteZoom.auto_scroll = scroll\n",
        "        page.InfiniteZoom.update()\n",
        "    if not status['installed_diffusers']:\n",
        "        alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "        return\n",
        "    if len(infinite_zoom_prefs['animation_prompts']) == 0:\n",
        "        if bool(infinite_zoom_prefs['prompt']):\n",
        "            infinite_zoom_prefs['animation_prompts']['0'] = infinite_zoom_prefs['prompt']\n",
        "        else:\n",
        "            alert_msg(page, \"You need to provide at least one keyframe prompt to animate...\")\n",
        "            return\n",
        "    if not bool(infinite_zoom_prefs['batch_folder_name']):\n",
        "        alert_msg(page, \"You must give Batch Folder Name to save your project output...\")\n",
        "        return\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = infinite_zoom_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "        callback_fnc.has_been_called = True\n",
        "        nonlocal progress, total_steps\n",
        "        #total_steps = len(latents)\n",
        "        percent = (step +1)/ total_steps\n",
        "        progress.value = percent\n",
        "        progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "        progress.update()\n",
        "    installer = Installing(\"Installing Infinite Zoom Stable Diffusion Pipeline...\")\n",
        "    prt(installer)\n",
        "    pip_install(\"transformers scipy ftfy accelerate\", installer=installer)\n",
        "\n",
        "    from PIL import Image as PILImage\n",
        "    from io import BytesIO\n",
        "    import numpy as np\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "    #from IPython.display import clear_output\n",
        "    from datetime import datetime\n",
        "\n",
        "    fname = format_filename(infinite_zoom_prefs['batch_folder_name'])\n",
        "    max_size = infinite_zoom_prefs['max_size']\n",
        "    batch_output = os.path.join(stable_dir, infinite_zoom_prefs['batch_folder_name'])\n",
        "    output_dir = batch_output\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], infinite_zoom_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    #print(\"3/3: Define helper functions\")\n",
        "    def write_video(file_path, frames, fps, reversed = True, start_frame_dupe_amount = 15, last_frame_dupe_amount = 30, as_gif=False):\n",
        "        if reversed == True:\n",
        "            frames.reverse()\n",
        "        w, h = frames[0].size\n",
        "        if as_gif:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        else:\n",
        "            fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "        writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
        "        for x in range(start_frame_dupe_amount):\n",
        "            np_frame = np.array(frames[0].convert('RGB'))\n",
        "            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)\n",
        "            writer.write(cv_frame)\n",
        "        for frame in frames:\n",
        "            np_frame = np.array(frame.convert('RGB'))\n",
        "            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)\n",
        "            writer.write(cv_frame)\n",
        "        for x in range(last_frame_dupe_amount):\n",
        "            np_frame = np.array(frames[len(frames) - 1].convert('RGB'))\n",
        "            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)\n",
        "            writer.write(cv_frame)\n",
        "        writer.release()\n",
        "    import imageio\n",
        "\n",
        "    def convert_mp4_to_gif(input_file, output_file, fps=30):\n",
        "        try:\n",
        "            video_reader = imageio.get_reader(input_file)\n",
        "            frames = [frame for frame in video_reader]\n",
        "            duration = int(1000 * 1/fps)\n",
        "            imageio.mimsave(output_file, frames, 'GIF', duration=duration)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with convert_mp4_to_gif: {e}\")\n",
        "    \n",
        "    def image_grid(imgs, rows, cols):\n",
        "        assert len(imgs) == rows*cols\n",
        "        w, h = imgs[0].size\n",
        "        grid = PILImage.new('RGB', size=(cols*w, rows*h))\n",
        "        grid_w, grid_h = grid.size\n",
        "        for i, img in enumerate(imgs):\n",
        "            grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "        return grid\n",
        "\n",
        "    def shrink_and_paste_on_blank(current_image, mask_width):\n",
        "        width, height = current_image.size\n",
        "        #shrink down by mask_width\n",
        "        prev_image = current_image.resize((width-2*mask_width,height-2*mask_width))\n",
        "        prev_image = prev_image.convert(\"RGBA\")\n",
        "        prev_image = np.array(prev_image)\n",
        "        #create blank non-transparent image\n",
        "        blank_image = np.array(current_image.convert(\"RGBA\"))*0\n",
        "        blank_image[:,:,3] = 1\n",
        "        #paste shrinked onto blank\n",
        "        blank_image[mask_width:height-mask_width,mask_width:width-mask_width,:] = prev_image\n",
        "        prev_image = PILImage.fromarray(blank_image)\n",
        "        return prev_image\n",
        "        \n",
        "        new_width = width - 2 * mask_width\n",
        "        new_height = int(new_width / (width / height))\n",
        "        prev_image = current_image.resize((new_width, new_height))\n",
        "        prev_image = prev_image.convert(\"RGBA\")\n",
        "        blank_image = PILImage.new(\"RGBA\", (width, height))\n",
        "        paste_x = (width - new_width) // 2\n",
        "        paste_y = (height - new_height) // 2\n",
        "        blank_image.paste(prev_image, (paste_x, paste_y))\n",
        "        return blank_image\n",
        "\n",
        "    def load_img(address, res=(512, 512)):\n",
        "        if address.startswith('http'):\n",
        "            image = PILImage.open(requests.get(address, stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(address):\n",
        "                image = PILImage.open(address)\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {address}\")\n",
        "                return\n",
        "            image = PILImage.open(address)\n",
        "        image = image.convert('RGB')\n",
        "        image = image.resize(res, resample=PILImage.LANCZOS)\n",
        "        return image\n",
        "    use_SDXL = infinite_zoom_prefs['use_SDXL']\n",
        "    model_id = infinite_zoom_prefs['inpainting_model'] #param [\"stabilityai/stable-diffusion-2-inpainting\", \"runwayml/stable-diffusion-inpainting\", \"ImNoOne/f222-inpainting-diffusers\",\"parlance/dreamlike-diffusion-1.0-inpainting\",\"ghunkins/stable-diffusion-liberty-inpainting\"] {allow-input: true}\n",
        "    SDXL_model = get_SDXL_model(prefs['SDXL_model'])\n",
        "    model_id_SDXL = SDXL_model['path']\n",
        "    if 'loaded_infinite_zoom' not in status:\n",
        "        status['loaded_infinite_zoom'] = ''\n",
        "    if status['loaded_infinite_zoom'] != (model_id_SDXL if use_SDXL else model_id):\n",
        "        clear_pipes()\n",
        "    else:\n",
        "        clear_pipes('infinite_zoom')\n",
        "        if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "            pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)\n",
        "    if pipe_infinite_zoom == None:\n",
        "        installer.status(f\"...initializing{' SDXL' if use_SDXL else ''} Inpaint Pipeline\")\n",
        "        try:\n",
        "            if use_SDXL:\n",
        "                from diffusers import AutoPipelineForInpainting, AutoencoderKL\n",
        "                #StableDiffusionXLInpaintPipeline\n",
        "                vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, force_upcast=False)\n",
        "                pipe_infinite_zoom = AutoPipelineForInpainting.from_pretrained(\n",
        "                    #model_id, variant=SDXL_model['revision']\n",
        "                    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", variant=\"fp16\",\n",
        "                    torch_dtype=torch.float16, use_safetensors=True,\n",
        "                    vae=vae,\n",
        "                    add_watermarker=False,\n",
        "                    cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "                    safety_checker=None, requires_safety_checker=False,\n",
        "                )\n",
        "                pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)\n",
        "                pipe_infinite_zoom = optimize_SDXL(pipe_infinite_zoom)\n",
        "                pipe_infinite_zoom.set_progress_bar_config(disable=True)\n",
        "                status['loaded_infinite_zoom'] = model_id_SDXL\n",
        "            else:\n",
        "                from diffusers import StableDiffusionInpaintPipeline\n",
        "                pipe_infinite_zoom = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "                    infinite_zoom_prefs['inpainting_model'],\n",
        "                    torch_dtype=torch.float16,\n",
        "                    cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "                    safety_checker=None, requires_safety_checker=False,\n",
        "                )\n",
        "                pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)\n",
        "                #pipe_infinite_zoom.scheduler = DPMSolverMultistepScheduler.from_config(pipe_infinite_zoom.scheduler.config)\n",
        "                #pipe_infinite_zoom = pipe_infinite_zoom.to(\"cuda\")\n",
        "                def dummy(images, **kwargs):\n",
        "                    return images, False\n",
        "                #pipe_infinite_zoom.safety_checker = dummy\n",
        "                #pipe_infinite_zoom.enable_attention_slicing() #This is useful to save some memory in exchange for a small speed decrease.\n",
        "                pipe_infinite_zoom = optimize_pipe(pipe_infinite_zoom, vae_slicing=False)\n",
        "                pipe_infinite_zoom.set_progress_bar_config(disable=True)\n",
        "                status['loaded_infinite_zoom'] = model_id\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Problem Initializing Pipeline...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    installer.status(\"...preparing run\")\n",
        "    g_cuda = torch.Generator(device=torch_device)\n",
        "    animation_prompts = {int(k): v for k, v in infinite_zoom_prefs['animation_prompts'].items()}\n",
        "    prompt = animation_prompts[0]\n",
        "    negative_prompt = infinite_zoom_prefs['negative_prompt']\n",
        "    num_init_images = 1 #TODO: Make multiple to pick from before processing\n",
        "    random_seed = int(infinite_zoom_prefs['seed']) if int(infinite_zoom_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    num_inference_steps = infinite_zoom_prefs['num_inference_steps']\n",
        "    guidance_scale = infinite_zoom_prefs['guidance_scale']\n",
        "    width = infinite_zoom_prefs['width']\n",
        "    height = infinite_zoom_prefs['height']#infinite_zoom_prefs['max_size']\n",
        "    init_image = infinite_zoom_prefs['init_image']#\"/content/gdrive/MyDrive/init/image.jpeg\"#param {type:\"string\"}\n",
        "    num_outpainting_steps = infinite_zoom_prefs['num_outpainting_steps']\n",
        "    mask_width = infinite_zoom_prefs['mask_width']\n",
        "    num_interpol_frames = infinite_zoom_prefs['num_interpol_frames']\n",
        "    init_image_selected = 0 #param\n",
        "    fps = infinite_zoom_prefs['fps']\n",
        "    start_frame_dupe_amount = infinite_zoom_prefs['frame_dupe_amount']\n",
        "    last_frame_dupe_amount = infinite_zoom_prefs['frame_dupe_amount']\n",
        "    # Since the model was trained on 512 images increasing the resolution to e.g. 1024 will drastically reduce its imagination, so the video will vary a lot less compared to 512\n",
        "\n",
        "    current_image = PILImage.new(mode=\"RGBA\", size=(height, width))\n",
        "    mask_image = np.array(current_image)[:,:,3]\n",
        "    mask_image = PILImage.fromarray(255-mask_image).convert(\"RGB\")\n",
        "    current_image = current_image.convert(\"RGB\")\n",
        "    clear_last()\n",
        "    if not bool(init_image):\n",
        "        prt(progress)\n",
        "        try:\n",
        "            if use_SDXL:\n",
        "                SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}\n",
        "                init_images = pipe_infinite_zoom(prompt=[prompt]*num_init_images,\n",
        "                                negative_prompt=[negative_prompt]*num_init_images,\n",
        "                                image=current_image,\n",
        "                                mask_image=mask_image,\n",
        "                                #target_size=(width, height),\n",
        "                                guidance_scale = guidance_scale,\n",
        "                                generator = g_cuda.manual_seed(random_seed),\n",
        "                                num_inference_steps=num_inference_steps,\n",
        "                                callback=callback_fnc, callback_steps=1).images\n",
        "                                #, **SDXL_negative_conditions\n",
        "            else:\n",
        "                init_images =  pipe_infinite_zoom(prompt=[prompt]*num_init_images,\n",
        "                                    negative_prompt=[negative_prompt]*num_init_images,\n",
        "                                    image=current_image,\n",
        "                                    mask_image=mask_image,\n",
        "                                    guidance_scale = guidance_scale,\n",
        "                                    height = height,\n",
        "                                    width = width,\n",
        "                                    generator = g_cuda.manual_seed(random_seed),\n",
        "                                    callback=callback_fnc,\n",
        "                                    num_inference_steps=num_inference_steps).images\n",
        "        except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Something went wrong generating image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "        #for image in images:\n",
        "            #prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        #image_grid(init_images, rows=1, cols=num_init_images)\n",
        "        clear_last()\n",
        "        if num_init_images == 1:\n",
        "            init_image_selected = 0\n",
        "        else:\n",
        "            init_image_selected = init_image_selected - 1\n",
        "            #TODO: Display numbered init_images and give selection prompt to pick which\n",
        "        current_image = init_images[init_image_selected]\n",
        "    else:\n",
        "        current_image = load_img(init_image,(width,height))\n",
        "    interpol_path = available_file(output_dir, fname, 0)\n",
        "    current_image.save(interpol_path)\n",
        "    prt(Row([ImageButton(src=interpol_path, width=width, height=height, data=interpol_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    all_frames = []\n",
        "    all_frames.append(current_image)\n",
        "\n",
        "    for i in range(num_outpainting_steps):\n",
        "        prt(f'Generating Image {i+1} / {num_outpainting_steps} - \"{animation_prompts[max(k for k in animation_prompts.keys() if k <= i)]}\"')\n",
        "        autoscroll(False)\n",
        "        prt(progress)\n",
        "        prev_image_fix = current_image\n",
        "        prev_image = shrink_and_paste_on_blank(current_image, mask_width)\n",
        "        current_image = prev_image\n",
        "        mask_image = np.array(current_image)[:,:,3]\n",
        "        mask_image = PILImage.fromarray(255-mask_image).convert(\"RGB\")\n",
        "        current_image = current_image.convert(\"RGB\")\n",
        "        try:\n",
        "            if use_SDXL:\n",
        "                SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}\n",
        "                images = pipe_infinite_zoom(prompt=animation_prompts[max(k for k in animation_prompts.keys() if k <= i)],\n",
        "                                negative_prompt=negative_prompt,\n",
        "                                image=current_image,\n",
        "                                mask_image=mask_image,\n",
        "                                #target_size=(width, height),\n",
        "                                guidance_scale = guidance_scale,\n",
        "                                #generator = g_cuda.manual_seed(random_seed),\n",
        "                                num_inference_steps=num_inference_steps,\n",
        "                                callback=callback_fnc, callback_steps=1).images\n",
        "                                #, **SDXL_negative_conditions\n",
        "            else:\n",
        "                images = pipe_infinite_zoom(prompt=animation_prompts[max(k for k in animation_prompts.keys() if k <= i)],\n",
        "                                negative_prompt=negative_prompt,\n",
        "                                image=current_image,\n",
        "                                mask_image=mask_image,\n",
        "                                guidance_scale = guidance_scale,\n",
        "                                height = height,\n",
        "                                width = width,\n",
        "                                #this can make the whole thing deterministic but the output less exciting\n",
        "                                #generator = g_cuda.manual_seed(random_seed),\n",
        "                                num_inference_steps=num_inference_steps,\n",
        "                                callback=callback_fnc, callback_steps=1).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        current_image = images[0]\n",
        "        autoscroll(True)\n",
        "        clear_last()\n",
        "        current_image.paste(prev_image, mask=prev_image)\n",
        "        #interpolation steps bewteen 2 inpainted images (=sequential zoom and crop)\n",
        "        for j in range(num_interpol_frames - 1):\n",
        "            interpol_image = current_image\n",
        "            interpol_width = round((1- ( 1-2*mask_width/width )**( 1-(j+1)/num_interpol_frames ) )*width/2 )\n",
        "            interpol_height = round((1- ( 1-2*mask_width/height )**( 1-(j+1)/num_interpol_frames ) )*height/2 )\n",
        "            interpol_image = interpol_image.crop((interpol_width, interpol_height, width - interpol_width, height - interpol_height))\n",
        "            interpol_image = interpol_image.resize((width, height))\n",
        "            #paste the higher resolution previous image in the middle to avoid drop in quality caused by zooming\n",
        "            interpol_width2 = round(( 1 - (width-2*mask_width) / (height-2*interpol_width) ) / 2*height)\n",
        "            prev_image_fix_crop = shrink_and_paste_on_blank(prev_image_fix, interpol_width2)\n",
        "            interpol_image.paste(prev_image_fix_crop, mask = prev_image_fix_crop)\n",
        "            all_frames.append(interpol_image)\n",
        "            #TODO: Give option to save all Interpolation frames\n",
        "        all_frames.append(current_image)\n",
        "        interpol_path = available_file(output_dir, fname, 1)\n",
        "        interpol_image.save(interpol_path)\n",
        "        prt(Row([ImageButton(src=interpol_path, width=width, height=height, data=interpol_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if infinite_zoom_prefs['save_frames']:\n",
        "            save_path = available_file(batch_output, fname, 1)\n",
        "            shutil.copy(interpol_path, save_path)\n",
        "        #clear_output(wait=True)\n",
        "        #interpol_image.show()\n",
        "    now = datetime.now()\n",
        "    date_time = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "    out_vid = os.path.join(batch_output, fname + \"_out_\"+date_time+\".mp4\")\n",
        "    in_vid = os.path.join(batch_output, fname + \"_in_\"+date_time+\".mp4\")\n",
        "    if infinite_zoom_prefs['save_video']:\n",
        "        prt(\"Generating Video MP4 File...\")\n",
        "        write_video(in_vid, all_frames, fps, False, start_frame_dupe_amount, last_frame_dupe_amount)\n",
        "        write_video(out_vid, all_frames, fps, True, start_frame_dupe_amount, last_frame_dupe_amount)\n",
        "        clear_last()\n",
        "        prt(f\"Saved Videos as {in_vid} and {out_vid}\")\n",
        "        #prt(Row([VideoContainer(out_vid)], alignment=MainAxisAlignment.CENTER))\n",
        "    if infinite_zoom_prefs['save_gif']:\n",
        "        prt(\"Generating Animated GIF File...\")\n",
        "        out_gif = os.path.join(batch_output, fname + \"_out_\"+date_time+\".gif\")\n",
        "        in_gif = os.path.join(batch_output, fname + \"_in_\"+date_time+\".gif\")\n",
        "        convert_mp4_to_gif(out_vid, out_gif, fps)\n",
        "        convert_mp4_to_gif(in_vid, in_gif, fps)\n",
        "        #write_video(in_gif, all_frames, fps, False, start_frame_dupe_amount, last_frame_dupe_amount, as_gif=True)\n",
        "        #write_video(out_gif, all_frames, fps, True, start_frame_dupe_amount, last_frame_dupe_amount, as_gif=True)\n",
        "        clear_last()\n",
        "        prt(Row([ImageButton(src=in_gif, width=width, height=height, data=in_gif, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([ImageButton(src=out_gif, width=width, height=height, data=out_gif, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_potat1(page):\n",
        "    global potat1_prefs, prefs, status, pipe_potat1, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Potat1.controls.append(line)\n",
        "      page.Potat1.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Potat1, lines=lines)\n",
        "    def clear_list():\n",
        "      page.Potat1.controls = page.Potat1.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Potat1.auto_scroll = scroll\n",
        "      page.Potat1.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = potat1_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Potat1 Text-To-Video Pipeline...\")\n",
        "    prt(installer)\n",
        "    #model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    clear_pipes()\n",
        "    potat1_dir = os.path.join(root_dir, \"potat1\")\n",
        "    finetuning_dir = os.path.join(root_dir, \"Text-To-Video-Finetuning\")\n",
        "    torch_installed = False\n",
        "    try:\n",
        "        import torch\n",
        "        torch_installed = True\n",
        "    except:\n",
        "        pass\n",
        "    if torch_installed:\n",
        "        if version.parse(torch.__version__).base_version >= version.parse(\"2.0.1\"):\n",
        "            torch_installed = False\n",
        "    if not torch_installed:\n",
        "        installer.status(\"...installing Torch 1.13.1\")\n",
        "        run_sp(\"pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\", realtime=False)\n",
        "    run_sp(\"pip install imageio[ffmpeg] -U einops omegaconf decord\", realtime=False)\n",
        "    if not os.path.exists(finetuning_dir):\n",
        "        installer.status(\"...camenduru/Text-To-Video-Finetuning\")\n",
        "        run_sp(\"git clone -b dev https://github.com/camenduru/Text-To-Video-Finetuning\", realtime=False, cwd=root_dir)\n",
        "    if not os.path.exists(potat1_dir):\n",
        "        installer.status(\"...camenduru/potat1\")\n",
        "        run_sp(\"git clone https://huggingface.co/camenduru/potat1\", realtime=False, cwd=root_dir)\n",
        "    #clear_pipes('potat1')\n",
        "    clear_last()\n",
        "    prt(\"Generating Potat1 of your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, potat1_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(potat1_prefs['seed']) if int(potat1_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    #generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = potat1_prefs['width']\n",
        "    height = potat1_prefs['height']\n",
        "    x = \" -x\" if status['installed_xformers'] else \"\"\n",
        "    rw = \" -rw\" if potat1_prefs['remove_watermark'] else \"\"\n",
        "    try:\n",
        "        run_sp(f'python inference.py -m \"{potat1_dir}\" -p \"{potat1_prefs[\"prompt\"]}\" -n \"{potat1_prefs[\"negative_prompt\"]}\" -W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {potat1_prefs[\"num_inference_steps\"]} -g {potat1_prefs[\"guidance_scale\"]} -f {potat1_prefs[\"fps\"]} -T {potat1_prefs[\"num_frames\"]}', cwd=finetuning_dir)\n",
        "      #print(f\"prompt={potat1_prefs['prompt']}, negative_prompt={potat1_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={potat1_prefs['edit_momentum_scale']}, edit_mom_beta={potat1_prefs['edit_mom_beta']}, num_inference_steps={potat1_prefs['num_inference_steps']}, eta={potat1_prefs['eta']}, guidance_scale={potat1_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      #frames = pipe_potat1(prompt=potat1_prefs['prompt'], negative_prompt=potat1_prefs['negative_prompt'], num_frames=potat1_prefs['num_frames'], num_inference_steps=potat1_prefs['num_inference_steps'], eta=potat1_prefs['eta'], guidance_scale=potat1_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Potat1 Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    filename = f\"{format_filename(potat1_prefs['prompt'])}\"\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    '''if potat1_prefs['export_to_video']:\n",
        "        from diffusers.utils import export_to_video\n",
        "        video_path = export_to_video(frames)\n",
        "        shutil.copy(video_path, available_file(local_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        shutil.copy(video_path, available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image: {type(image)} to {image_path}\")\n",
        "        cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(np_image).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not potat1_prefs['display_upscaled_image'] or not potat1_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if potat1_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'\n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if potat1_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {potat1_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if potat1_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(potat1_prefs[\"enlarge_scale\"]), height=height * float(potat1_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {potat1_prefs['enlarge_scale']}x with ESRGAN\" if potat1_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Text-To-Video\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = potat1_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['lower_memory']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1'''\n",
        "    #prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(f\"Done creating video... Check {batch_output}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_stable_animation(page):\n",
        "    global stable_animation_prefs, prefs, status\n",
        "    if not bool(prefs['Stability_api_key']):\n",
        "        alert_msg(e.page, \"You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will cost you tokens.\")\n",
        "        return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.StableAnimation.controls.append(line)\n",
        "      page.StableAnimation.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.StableAnimation, lines=lines)\n",
        "    def clear_list():\n",
        "      page.StableAnimation.controls = page.StableAnimation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.StableAnimation.auto_scroll = scroll\n",
        "      page.StableAnimation.update()\n",
        "    abort_run = False\n",
        "    def abort_diffusion(e):\n",
        "      nonlocal abort_run\n",
        "      abort_run = True\n",
        "      page.snd_error.play()\n",
        "      page.snd_delete.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = stable_animation_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Stable Animation Stability.ai Pipeline...\"))\n",
        "    #run_sp('pip install \"stability_sdk[anim_ui]\"', realtime=True)\n",
        "    try:\n",
        "        from stability_sdk import api\n",
        "    except Exception:\n",
        "        run_sp('pip install \"stability_sdk[anim_ui]\"', realtime=True) #\n",
        "        from stability_sdk import api\n",
        "        pass\n",
        "    from tqdm import tqdm\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "    #batch_output = os.path.join(stable_dir, stable_animation_prefs['batch_folder_name'])\n",
        "    #if not os.path.isdir(batch_output):\n",
        "    #  os.makedirs(batch_output)\n",
        "    #local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(stable_animation_prefs['seed']) if int(stable_animation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    width = stable_animation_prefs['width']\n",
        "    height = stable_animation_prefs['height']\n",
        "    animation_prompts = stable_animation_prefs['animation_prompts']\n",
        "    '''try:\n",
        "        prompts = json.loads(animation_prompts)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            prompts = eval(animation_prompts)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"Invalid JSON or Python code for animation_prompts.\")\n",
        "            return'''\n",
        "    prompts = {int(k): v for k, v in animation_prompts.items()}\n",
        "    from stability_sdk.api import (ClassifierException, Context, OutOfCreditsException)\n",
        "    context = Context(\"grpc.stability.ai:443\", prefs['Stability_api_key'])\n",
        "    try:\n",
        "        balance, profile_picture = context.get_user_info()\n",
        "    except:\n",
        "        alert_msg(page, \"Error getting Stability.ai User Info\")\n",
        "        return\n",
        "    from stability_sdk.animation import (\n",
        "        AnimationArgs,\n",
        "        Animator,\n",
        "        interpolate_frames\n",
        "    )\n",
        "    from stability_sdk.utils import (create_video_from_frames, extract_frames_from_video, interpolate_mode_from_string)\n",
        "    args = AnimationArgs()\n",
        "    args.width = width\n",
        "    args.height = height\n",
        "    args.sampler = stable_animation_prefs['sampler']\n",
        "    #args_generation.custom_model = stable_animation_prefs['custom_model']\n",
        "    args.model = stable_animation_prefs['model'].lower()\n",
        "    args.seed = random_seed\n",
        "    args.cfg_scale = stable_animation_prefs['guidance_scale']\n",
        "    args.clip_guidance = stable_animation_prefs['clip_guidance']\n",
        "    args.init_image = stable_animation_prefs['init_image']\n",
        "    args.init_sizing = stable_animation_prefs['init_sizing'].lower()\n",
        "    args.mask_path = stable_animation_prefs['mask_image']\n",
        "    args.mask_invert = stable_animation_prefs['mask_invert']\n",
        "    args.preset = stable_animation_prefs['style_preset']\n",
        "    args.animation_mode = stable_animation_prefs['animation_mode']\n",
        "    args.max_frames = stable_animation_prefs['max_frames']\n",
        "    args.border = stable_animation_prefs['border'].lower()\n",
        "    args.noise_add_curve = stable_animation_prefs['noise_add_curve']\n",
        "    args.noise_scale_curve = stable_animation_prefs['noise_scale_curve']\n",
        "    args.strength_curve = stable_animation_prefs['strength_curve']\n",
        "    args.steps_curve = stable_animation_prefs['steps_curve']\n",
        "    args.steps_strength_adj = stable_animation_prefs['steps_strength_adj']\n",
        "    args.interpolate_prompts = stable_animation_prefs['interpolate_prompts']\n",
        "    args.locked_seed = stable_animation_prefs['locked_seed']\n",
        "    args.angle = stable_animation_prefs['angle']\n",
        "    args.zoom = stable_animation_prefs['zoom']\n",
        "    args.translation_x = stable_animation_prefs['translation_x']\n",
        "    args.translation_y = stable_animation_prefs['translation_y']\n",
        "    args.translation_z = stable_animation_prefs['translation_z']\n",
        "    args.rotation_x = stable_animation_prefs['rotation_x']\n",
        "    args.rotation_y = stable_animation_prefs['rotation_y']\n",
        "    args.rotation_z = stable_animation_prefs['rotation_z']\n",
        "    args.diffusion_cadence_curve = stable_animation_prefs['diffusion_cadence_curve']\n",
        "    args.cadence_interp = stable_animation_prefs['cadence_interp'].lower()\n",
        "    args.cadence_spans = stable_animation_prefs['cadence_spans']\n",
        "    args.color_coherence = stable_animation_prefs['color_coherence']\n",
        "    args.brightness_curve = stable_animation_prefs['brightness_curve']\n",
        "    args.contrast_curve = stable_animation_prefs['contrast_curve']\n",
        "    args.hue_curve = stable_animation_prefs['hue_curve']\n",
        "    args.saturation_curve = stable_animation_prefs['saturation_curve']\n",
        "    args.lightness_curve = stable_animation_prefs['lightness_curve']\n",
        "    args.color_match_animate = stable_animation_prefs['color_match_animate']\n",
        "    args.depth_model_weight = stable_animation_prefs['depth_model_weight']\n",
        "    args.near_plane = stable_animation_prefs['near_plane']\n",
        "    args.far_plane = stable_animation_prefs['far_plane']\n",
        "    args.fov_curve = stable_animation_prefs['fov_curve']\n",
        "    args.depth_blur_curve = stable_animation_prefs['depth_blur_curve']\n",
        "    args.depth_warp_curve = stable_animation_prefs['depth_warp_curve']\n",
        "    #args_depth.save_depth_maps = stable_animation_prefs['save_depth_maps']\n",
        "    args.camera_type = stable_animation_prefs['camera_type'].lower()\n",
        "    args.render_mode = stable_animation_prefs['render_mode'].lower()\n",
        "    args.mask_power = stable_animation_prefs['mask_power']\n",
        "    args.use_inpainting_model = stable_animation_prefs['use_inpainting_model']\n",
        "    args.inpaint_border = stable_animation_prefs['inpaint_border']\n",
        "    args.mask_min_value = stable_animation_prefs['mask_min_value']\n",
        "    args.mask_binarization_thr = stable_animation_prefs['mask_binarization_thr']\n",
        "    #args_inpaint.save_inpaint_masks = False\n",
        "    args.video_init_path = stable_animation_prefs['video_init_path']\n",
        "    args.extract_nth_frame = int(stable_animation_prefs['extract_nth_frame'])\n",
        "    args.video_mix_in_curve = stable_animation_prefs['video_mix_in_curve']\n",
        "    args.video_flow_warp = stable_animation_prefs['video_flow_warp']\n",
        "    args.fps = stable_animation_prefs['output_fps']\n",
        "    args.resume = stable_animation_prefs['resume']\n",
        "    args.resume_from = int(stable_animation_prefs['resume_from'])\n",
        "    args.reverse = False\n",
        "    #arg_objs = AnimationArgs(args_generation, args_animation, args_camera, args_coherence, args_color, args_depth, args_render_3d, args_inpaint, args_vid_in, args_vid_out)\n",
        "    try:\n",
        "        animator = Animator(\n",
        "            api_context=context,\n",
        "            animation_prompts=prompts,\n",
        "            negative_prompt=stable_animation_prefs['negative_prompt'],\n",
        "            negative_prompt_weight=stable_animation_prefs['negative_prompt_weight'],\n",
        "            args=args,\n",
        "            #out_dir=batch_output\n",
        "            #negative_prompt_weight=negative_prompt_weight,\n",
        "            #resume=resume,\n",
        "        )\n",
        "    except ClassifierException as e:\n",
        "        alert_msg(page, \"Animation terminated early due to NSFW classifier. Sorry for the Censorship...\")\n",
        "        return\n",
        "    except OutOfCreditsException as e:\n",
        "        alert_msg(page, f\"Animation terminated early, out of credits. Refill them Tokens...\", content=Text(f\"{e.details}\"))\n",
        "        return\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Animation terminated early due to exception:\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    total_steps = args.max_frames\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(prompts[0])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "    clear_last()\n",
        "    prt(Row([Text(f\"Generating Stable Animation from your Prompts... Available Credits: {round(balance, 1)}\"), Container(content=None, expand=True), IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Run\", on_click=abort_diffusion)]))\n",
        "    prt(progress)\n",
        "    autoscroll(True)\n",
        "    try:\n",
        "      #frames = pipe_stable_animation(prompt=stable_animation_prefs['prompt'], negative_prompt=stable_animation_prefs['negative_prompt'], video_length=stable_animation_prefs['max_frames'], num_inference_steps=stable_animation_prefs['num_inference_steps'], eta=stable_animation_prefs['eta'], guidance_scale=stable_animation_prefs['guidance_scale'], motion_field_strength_x=stable_animation_prefs['motion_field_strength_x'], motion_field_strength_y=stable_animation_prefs['motion_field_strength_y'], t0=stable_animation_prefs['t0'], t1=stable_animation_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "      for num, image in enumerate(tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames), start=animator.start_frame_idx):\n",
        "        if abort_run:\n",
        "            #clear_last()\n",
        "            #clear_last()\n",
        "            prt(\"üõë  Aborted Current Animation Run\")\n",
        "            return\n",
        "        callback_fnc(num)\n",
        "        fname = f\"frame_{num:05d}\"\n",
        "        #image_path = available_file(batch_output, fname, num, no_num=True)\n",
        "        image_path = os.path.join(batch_output, f\"{fname}.png\")\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "        if stable_animation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscale_image(image_path, upscaled_path, scale=stable_animation_prefs[\"enlarge_scale\"])\n",
        "            image_path = upscaled_path\n",
        "            if stable_animation_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(stable_animation_prefs[\"enlarge_scale\"]), height=height * float(stable_animation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                time.sleep(0.2)\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {stable_animation_prefs['enlarge_scale']}x with ESRGAN\" if stable_animation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Stable Animation\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "                config_json = stable_animation_prefs.copy()\n",
        "                config_json['model_path'] = model_path\n",
        "                config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "                config_json['seed'] = random_seed\n",
        "                del config_json['max_frames']\n",
        "                del config_json['width']\n",
        "                del config_json['height']\n",
        "                del config_json['display_upscaled_image']\n",
        "                del config_json['batch_folder_name']\n",
        "                if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        if not stable_animation_prefs['display_upscaled_image'] or not stable_animation_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([Text(image_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    except Exception as e:\n",
        "      #clear_last()\n",
        "      #clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Stable Animation your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    #clear_last()\n",
        "    #clear_last()\n",
        "    save_path = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])\n",
        "    #filename = f\"{prefs['file_prefix']}{format_filename(prompts[0])}\"\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    #autoscroll(True)\n",
        "    progress = Container(content=None)\n",
        "    try:\n",
        "        new_balance, profile_picture = context.get_user_info()\n",
        "    except:\n",
        "        alert_msg(page, \"Error getting User Info\")\n",
        "        return\n",
        "    used_balance = balance - new_balance\n",
        "    cost = f\"${round(used_balance * 0.01, 2)}\"\n",
        "    prt(f\"ü´∞  Done Generating Animation... Used {round(used_balance, 2)} Credits Total ~ {cost}\")\n",
        "    if stable_animation_prefs['export_to_video']:\n",
        "        prt(\"Exporting Frames to Video\")\n",
        "        #from diffusers.utils import export_to_video\n",
        "        #video_path = export_to_video(frames)\n",
        "        #local_file = available_file(local_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        save_file = available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        create_video_from_frames(batch_output, save_file, fps=stable_animation_prefs['output_fps'])\n",
        "        #imageio.mimsave(local_file, frames, fps=4)\n",
        "        #shutil.copy(local_file, save_file)\n",
        "        clear_last()\n",
        "        try:\n",
        "            prt(Row([VideoContainer(save_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(VideoPlayer(save_file, width, height))\n",
        "        except Exception as e:\n",
        "            print(f\"Error showing VideoPlayer: {e}\")\n",
        "            pass\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_svd(page):\n",
        "    global svd_prefs, prefs, status, pipe_svd\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(svd_prefs['init_image']):\n",
        "      alert_msg(page, \"You need to provide an Initial Image to animate before using...\")\n",
        "      return\n",
        "    if not bool(svd_prefs['batch_folder_name']):\n",
        "      alert_msg(page, \"You need to provide a unique Video Folder Name for your project...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.SVD.controls.append(line)\n",
        "      page.SVD.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.SVD, lines=lines)\n",
        "    def clear_list():\n",
        "      page.SVD.controls = page.SVD.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.SVD.auto_scroll = scroll\n",
        "      page.SVD.update()\n",
        "    \n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing SVD Image-To-Video Pipeline...\")\n",
        "    prt(installer)\n",
        "    model_id = \"stabilityai/stable-video-diffusion-img2vid-xt\" if 'XT' in svd_prefs['svd_model'] else \"stabilityai/stable-video-diffusion-img2vid\"\n",
        "    svd_prefs['num_frames'] = 25 if 'XT' in svd_prefs['svd_model'] else 14\n",
        "    from diffusers import StableVideoDiffusionPipeline\n",
        "    if 'loaded_svd' not in status: status['loaded_svd'] = \"\"\n",
        "    if model_id != status['loaded_svd']:\n",
        "        clear_pipes()\n",
        "    else:\n",
        "        clear_pipes(\"svd\")\n",
        "    if pipe_svd == None:\n",
        "        pipe_svd = StableVideoDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        if svd_prefs['cpu_offload']:\n",
        "            pipe_svd.enable_model_cpu_offload()\n",
        "            #pipe_svd.unet.enable_forward_chunking()\n",
        "        else:\n",
        "            pipe_svd.to(torch_device)\n",
        "        if prefs['enable_torch_compile']:\n",
        "            #pipe_svd.unet.to(memory_format=torch.channels_last)\n",
        "            pipe_svd.unet = torch.compile(pipe_svd.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "        if prefs['enable_deepcache']:\n",
        "            try:\n",
        "                from DeepCache import DeepCacheSDHelper\n",
        "            except Exception:\n",
        "                run_sp(\"pip install DeepCache\", realtime=False)\n",
        "                from DeepCache import DeepCacheSDHelper\n",
        "                pass\n",
        "            helper = DeepCacheSDHelper(pipe=pipe_svd)\n",
        "            helper.set_params(cache_interval=3, cache_branch_id=0)\n",
        "            helper.enable()\n",
        "        status['loaded_svd'] = model_id\n",
        "    #clear_pipes('svd')\n",
        "    vid_length = svd_prefs['num_frames'] / svd_prefs['fps']\n",
        "    if svd_prefs['resume_frame']:\n",
        "        vid_length = vid_length * (svd_prefs['continue_times'] + 1)\n",
        "    clear_last()\n",
        "    progress = Progress(f\"Generating Stable Video of your Image... Length: {round(vid_length, 1)} seconds\")\n",
        "    #progress = ProgressBar(bar_height=8)\n",
        "    #total_steps = svd_prefs['num_inference_steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):\n",
        "      nonlocal progress\n",
        "      total_steps = pipe.num_timesteps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.progress.value = percent\n",
        "      progress.progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.progress.update()\n",
        "    #prt(f\"Generating Stable Video of your Image... Length: {round(vid_length, 1)} seconds\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if svd_prefs['init_image'].startswith('http'):\n",
        "      init_img = PILImage.open(requests.get(svd_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(svd_prefs['init_image']):\n",
        "        init_img = PILImage.open(svd_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {svd_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = init_img.size\n",
        "    width, height = scale_dimensions(width, height, svd_prefs['max_size'], multiple=64)\n",
        "    init_img = init_img.resize((width, height))#, resample=PILImage.Resampling.BICUBIC)\n",
        "    init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    batch_output = os.path.join(prefs['image_output'], svd_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    #for v in range(svd_prefs['num_videos']):\n",
        "    \n",
        "    random_seed = int(svd_prefs['seed']) if int(svd_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.manual_seed(random_seed)\n",
        "    try: #, callback_on_step_end=callback_fnc\n",
        "        frames_batch = pipe_svd(init_img, width=width, height=height, num_frames=svd_prefs[\"num_frames\"], decode_chunk_size=svd_prefs[\"decode_chunk_size\"], motion_bucket_id=svd_prefs['motion_bucket_id'], noise_aug_strength=svd_prefs['noise_aug_strength'], num_inference_steps=svd_prefs['num_inference_steps'], min_guidance_scale=svd_prefs['min_guidance_scale'], max_guidance_scale=svd_prefs['max_guidance_scale'], fps=svd_prefs['fps'], generator=generator).frames\n",
        "        if svd_prefs['resume_frame']:\n",
        "            new_frames = []\n",
        "            for n, f in enumerate(frames_batch):\n",
        "                new_frames[n] = []\n",
        "                last_frame = f[-1]\n",
        "                for t in range(svd_prefs['continue_times']):\n",
        "                    progress.status(f\"...Video {n}, Continue {t + 1}/{svd_prefs['continue_times']}\")\n",
        "                    frames_continued = pipe_svd(last_frame, width=width, height=height, num_frames=svd_prefs[\"num_frames\"], decode_chunk_size=svd_prefs[\"decode_chunk_size\"], motion_bucket_id=svd_prefs['motion_bucket_id'], noise_aug_strength=0.01, num_inference_steps=svd_prefs['num_inference_steps'], min_guidance_scale=svd_prefs['min_guidance_scale'], max_guidance_scale=svd_prefs['max_guidance_scale'], fps=svd_prefs['fps'], generator=generator).frames[0]\n",
        "                    new_frames[n].append(frames_continued)\n",
        "                    last_frame = frames_continued[-1]\n",
        "            for n, c in enumerate(new_frames):\n",
        "                frames_batch[n].append(c)\n",
        "    except Exception as e:\n",
        "      print(f\"SVD {frames_batch} type: {type(frames_batch)}\")\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: SVD Image-To-Video failed for some reason. Possibly out of memory or something wrong with the code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    b = 0\n",
        "    for frames in frames_batch:\n",
        "        frames_dir = os.path.join(batch_output, f\"frames-{b}\")\n",
        "        exists = True\n",
        "        while exists:\n",
        "            if os.path.isdir(frames_dir):\n",
        "                b += 1\n",
        "                frames_dir = os.path.join(batch_output, f\"frames-{b}\")\n",
        "            else:\n",
        "                exists = False\n",
        "        make_dir(frames_dir)\n",
        "        idx = 0\n",
        "        for image in frames:\n",
        "            fname = f\"{svd_prefs['file_prefix']}{format_filename(svd_prefs['batch_folder_name'])}-{b}\"\n",
        "            image_path = available_file(frames_dir, fname, idx, zfill=4)\n",
        "            image.save(image_path)\n",
        "            new_file = os.path.basename(image_path)\n",
        "            if not svd_prefs['display_upscaled_image'] or not svd_prefs['apply_ESRGAN_upscale']:\n",
        "                #prt(Row([Img(src=image_path, width=svd_prefs['width'], height=svd_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([ImageButton(src=image_path, width=svd_prefs['width'], height=svd_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if svd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, image_path, scale=svd_prefs[\"enlarge_scale\"], face_enhance=svd_prefs[\"face_enhance\"])\n",
        "                if svd_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([Img(src=image_path, width=svd_prefs['width'] * float(svd_prefs[\"enlarge_scale\"]), height=svd_prefs['height'] * float(svd_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            #else:\n",
        "            #    time.sleep(0.2)\n",
        "            #    shutil.copy(image_path, os.path.join(frames_dir, new_file))\n",
        "            # TODO: Add Metadata\n",
        "            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            idx += 1\n",
        "        if svd_prefs['export_to_video']:\n",
        "            try:\n",
        "                installer = Installing(\"Running Google FILM: Frame Interpolation for Large Motion...\")\n",
        "                prt(installer)\n",
        "                out_file = available_file(batch_output, fname, no_num=True, ext=\"mp4\")\n",
        "                if svd_prefs['interpolate_video']:\n",
        "                    interpolate_video(frames, input_fps=svd_prefs['fps'], output_fps=svd_prefs['target_fps'], output_video=out_file, installer=installer)\n",
        "                else:\n",
        "                    installer.set_message(\"Saving Frames to Video using FFMPEG with Deflicker...\")\n",
        "                    frames_to_video(batch_output, pattern=fname+\"-%04d.png\", input_fps=svd_prefs['fps'], output_fps=svd_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Couldn't interpolate video, but frames still saved...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                pass\n",
        "            clear_last()\n",
        "            prt(Markdown(f\"Video saved to [{out_file}]({out_file})\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "            #prt(Row([VideoContainer(out_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        b += 1\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    prt(f\"Done creating animation... Check {batch_output}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_roop(page):\n",
        "    global roop_prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.Roop.controls.append(line)\n",
        "      page.Roop.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Roop, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "        page.Roop.auto_scroll = scroll\n",
        "        page.Roop.update()\n",
        "    if not bool(roop_prefs['source_image']) or not bool(roop_prefs['target_image']):\n",
        "        alert_msg(page, \"You must provide a source image and target video or image...\")\n",
        "        return\n",
        "    page.Roop.controls = page.Roop.controls[:1]\n",
        "    autoscroll()\n",
        "    installer = Installing(\"Installing ROOP Libraries...\")\n",
        "    prt(installer)\n",
        "    roop_dir = os.path.join(root_dir, \"roop\")\n",
        "    if not os.path.exists(roop_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning s0md3v/roop.git\")\n",
        "            run_process(\"git clone https://github.com/s0md3v/roop.git\", cwd=root_dir)\n",
        "            installer.status(\"...installing requirements\")\n",
        "            #run_process(\"pip install -r requirements.txt\", cwd=roop_dir)\n",
        "            installer.status(\"...downloading roop inswapper\")\n",
        "            download_file(\"https://huggingface.co/camenduru/roop/resolve/main/inswapper_128.onnx\", to=roop_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Point-E Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    pip_install(\"ffmpeg opencv-python|cv2 onnx==1.14.0 insightface==0.7.3 tk==0.1.0 customtkinter==5.1.3 onnxruntime-gpu==1.15.0|onnxruntime opennsfw2==0.10.2 protobuf==4.24.3 gfpgan\", installer=installer)\n",
        "    \n",
        "    clear_pipes()\n",
        "\n",
        "    from PIL import ImageOps\n",
        "    if bool(roop_prefs['output_name']):\n",
        "        fname = format_filename(roop_prefs['output_name'], force_underscore=True)\n",
        "    elif bool(roop_prefs['batch_folder_name']):\n",
        "        fname = format_filename(roop_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        fname = \"output\"\n",
        "    #TODO: Add prefix\n",
        "    if bool(roop_prefs['batch_folder_name']):\n",
        "        batch_output = os.path.join(stable_dir, roop_prefs['batch_folder_name'])\n",
        "    else:\n",
        "        batch_output = stable_dir\n",
        "    if not os.path.exists(batch_output):\n",
        "        os.makedirs(batch_output)\n",
        "    #.rpartition(slash)[0], 'roop'\n",
        "    output_path = os.path.join(prefs['image_output'], roop_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    init_img = None\n",
        "    if roop_prefs['source_image'].startswith('http'):\n",
        "        installer.status(\"...downloading source image\")\n",
        "        init_img = PILImage.open(requests.get(roop_prefs['source_image'], stream=True).raw)\n",
        "    else:\n",
        "        if os.path.isfile(roop_prefs['source_image']):\n",
        "            init_img = PILImage.open(roop_prefs['source_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your source_image {roop_prefs['source_image']}\")\n",
        "            return\n",
        "    source_name = roop_prefs['source_image'].rpartition(\"/\")[2] if \"/\" in roop_prefs['source_image'] else roop_prefs['source_image'].rpartition(slash)[2]\n",
        "    source_path = \"\"\n",
        "    if init_img != None:\n",
        "        installer.status(\"...resizing source image\")\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, roop_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        source_path = os.path.join(batch_output, f'{source_name.rpartition(\".\")[0]}.png')\n",
        "        init_img.save(source_path)\n",
        "    is_video = roop_prefs['target_image'].endswith('mp4') or roop_prefs['target_image'].endswith('avi')\n",
        "\n",
        "    target_img = None\n",
        "    target_name = roop_prefs['target_image'].rpartition(\"/\")[2] if \"/\" in roop_prefs['target_image'] else roop_prefs['target_image'].rpartition(slash)[2]\n",
        "    target_path = \"\"\n",
        "    if roop_prefs['target_image'].startswith('http'):\n",
        "        if is_video:\n",
        "            installer.status(\"...downloading target video\")\n",
        "            download_file(roop_prefs['target_image'], batch_output)\n",
        "            target_path = os.path.join(batch_output, target_name)\n",
        "        else:\n",
        "            installer.status(\"...downloading target image\")\n",
        "            target_img = PILImage.open(requests.get(roop_prefs['target_image'], stream=True).raw)\n",
        "    else:\n",
        "        if os.path.isfile(roop_prefs['target_image']):\n",
        "            if is_video:\n",
        "                target_path = roop_prefs['target_image']\n",
        "            else:\n",
        "                target_img = PILImage.open(roop_prefs['target_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your target_image {roop_prefs['target_image']}\")\n",
        "            return\n",
        "    if target_img != None:\n",
        "        installer.status(\"...resizing target image\")\n",
        "        width, height = target_img.size\n",
        "        width, height = scale_dimensions(width, height, roop_prefs['max_size'])\n",
        "        target_img = target_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        target_img = ImageOps.exif_transpose(target_img).convert(\"RGB\")\n",
        "        target_path = os.path.join(batch_output, f'{target_name.rpartition(\".\")[0]}.png')\n",
        "        target_img.save(target_path)\n",
        "    clear_last()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(f\"Generating your ROOP Face-Swap {'Video' if is_video else 'Image'}...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    total_steps = 100 #?\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    output_file = available_file(output_path, fname, 0, ext='mp4' if is_video else 'png')\n",
        "    #output_file = os.path.join(output_path, f\"{fname}{'.mp4' if is_video else '.png'}\")\n",
        "    cmd = f'python run.py -s \"{source_path}\" -t \"{target_path}\" -o \"{output_file}\"'\n",
        "    cmd += f\" --frame-processor {roop_prefs['frame_processor']}\"\n",
        "    cmd += f\" --execution-provider {torch_device}\"\n",
        "    if roop_prefs['keep_fps'] and is_video: cmd += \" --keep-fps\"\n",
        "    if roop_prefs['keep_audio'] and is_video: cmd += \" --keep-audio\"\n",
        "    if roop_prefs['keep_frames'] and is_video: cmd += \" --keep-frames\"\n",
        "    if roop_prefs['many_faces']: cmd += \" --many-faces\"\n",
        "    if is_video:\n",
        "        cmd += f\" --video-encoder {roop_prefs['video_encoder']}\"\n",
        "        cmd += f\" --video-quality {roop_prefs['video_quality']}\"\n",
        "    #--max-menory\n",
        "    prt(f\"Running {cmd}\")\n",
        "    try:\n",
        "        run_process(cmd, cwd=roop_dir, page=page, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Python.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    #TODO: Upscale Image\n",
        "    if os.path.isfile(output_file):\n",
        "        if is_video:\n",
        "            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            prt(Row([ImageButton(src=output_file, data=output_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    else:\n",
        "        prt(\"Error Generating Output File! A NSFW Image may have been detected.\")\n",
        "    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_video_retalking(page):\n",
        "    global video_retalking_prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.Video_ReTalking.controls.append(line)\n",
        "      page.Video_ReTalking.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Video_ReTalking, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "        page.Video_ReTalking.auto_scroll = scroll\n",
        "        page.Video_ReTalking.update()\n",
        "    if not bool(video_retalking_prefs['input_audio']) or not bool(video_retalking_prefs['target_video']):\n",
        "        alert_msg(page, \"You must provide a source audio and target video...\")\n",
        "        return\n",
        "    page.Video_ReTalking.controls = page.Video_ReTalking.controls[:1]\n",
        "    autoscroll()\n",
        "    installer = Installing(\"Installing Video-ReTalking Packages...\")\n",
        "    prt(installer)\n",
        "    video_retalking_dir = os.path.join(root_dir, \"video-retalking\")\n",
        "    video_retalking_checkpoints = os.path.join(video_retalking_dir, \"checkpoints\")\n",
        "    if not os.path.exists(video_retalking_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning vinthony/video_retalking.git\")\n",
        "            run_process(\"git clone https://github.com/OpenTalker/video-retalking.git\", cwd=root_dir)\n",
        "            installer.status(\"...installing requirements\")\n",
        "            #run_process(\"pip install -r requirements.txt\", cwd=video_retalking_dir)\n",
        "            pip_install(\"basicsr==1.4.2 kornia==0.5.1 face-alignment==1.3.4 ninja==1.10.2.3 einops facexlib==0.2.5 librosa==0.9.2 dlib==19.24.0 gradio numpy\", installer=installer)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Video-ReTalking Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    make_dir(video_retalking_checkpoints)\n",
        "    if not os.path.isfile(video_retalking_checkpoints, '30_net_gen.pth'):\n",
        "        installer.status(\"...downloading checkpoints\")\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth\", to=video_retalking_checkpoints)\n",
        "        download_file(\"https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat\", to=video_retalking_checkpoints)\n",
        "        installer.status(\"...unziping BFM\")\n",
        "        run_sp(f\"unzip {os.path.join(video_retalking_checkpoints, 'BFM.zip')} -d BFM\", realtime=False, cwd=video_retalking_checkpoints)\n",
        "        os.remove(os.path.join(video_retalking_checkpoints, 'BFM.zip'))\n",
        "    pip_install(\"ffmpeg opencv-python|cv2\", installer=installer)\n",
        "    clear_pipes()\n",
        "    import glob\n",
        "    from base64 import b64encode\n",
        "    from PIL import ImageOps\n",
        "    if bool(video_retalking_prefs['output_name']):\n",
        "        fname = format_filename(video_retalking_prefs['output_name'], force_underscore=True)\n",
        "    elif bool(video_retalking_prefs['batch_folder_name']):\n",
        "        fname = format_filename(video_retalking_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        fname = \"output\"\n",
        "    #TODO: Add prefix\n",
        "    if bool(video_retalking_prefs['batch_folder_name']):\n",
        "        batch_output = os.path.join(stable_dir, video_retalking_prefs['batch_folder_name'])\n",
        "    else:\n",
        "        batch_output = stable_dir\n",
        "    make_dir(batch_output)\n",
        "    results_dir = os.path.join(video_retalking_dir, \"results\")\n",
        "    make_dir(results_dir)\n",
        "    inputs_dir = os.path.join(video_retalking_dir, \"inputs\")\n",
        "    make_dir(inputs_dir)\n",
        "    output_path = os.path.join(prefs['image_output'], video_retalking_prefs['batch_folder_name'])\n",
        "    make_dir(output_path)\n",
        "\n",
        "    expression_img = None\n",
        "    target_name = video_retalking_prefs['target_video'].rpartition(\"/\")[2] if \"/\" in video_retalking_prefs['target_video'] else video_retalking_prefs['target_video'].rpartition(slash)[2]\n",
        "    target_path = \"\"\n",
        "    if video_retalking_prefs['target_video'].startswith('http'):\n",
        "        installer.status(\"...downloading target video\")\n",
        "        download_file(video_retalking_prefs['target_video'], inputs_dir)\n",
        "        target_path = os.path.join(inputs_dir, target_name)\n",
        "    else:\n",
        "        if os.path.isfile(video_retalking_prefs['target_video']):\n",
        "            target_path = video_retalking_prefs['target_video']\n",
        "            shutil.copy(target_path, os.path.join(inputs_dir, os.path.basename(target_path)))\n",
        "            target_path = os.path.join(inputs_dir, os.path.basename(target_path))\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your target_video {video_retalking_prefs['target_video']}\")\n",
        "            return\n",
        "    input_audio = \"\"\n",
        "    if video_retalking_prefs['input_audio'].startswith('http'):\n",
        "        installer.status(\"...downloading input audio\")\n",
        "        download_file(video_retalking_prefs['input_audio'], inputs_dir)\n",
        "        input_audio = os.path.join(inputs_dir, target_name)\n",
        "    else:\n",
        "        if os.path.isfile(video_retalking_prefs['input_audio']):\n",
        "            input_audio = video_retalking_prefs['input_audio']\n",
        "            shutil.copy(input_audio, os.path.join(inputs_dir, os.path.basename(input_audio)))\n",
        "            input_audio = os.path.join(inputs_dir, os.path.basename(input_audio))\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your input_audio {video_retalking_prefs['input_audio']}\")\n",
        "            return\n",
        "    if video_retalking_prefs['exp_img'] == \"Image\":\n",
        "        if video_retalking_prefs['exp_image'].startswith('http'):\n",
        "            installer.status(\"...downloading expression image\")\n",
        "            download_file(video_retalking_prefs['exp_image'], inputs_dir)\n",
        "            expression_img = os.path.join(inputs_dir, target_name)\n",
        "        else:\n",
        "            if os.path.isfile(video_retalking_prefs['exp_image']):\n",
        "                expression_img = video_retalking_prefs['exp_image']\n",
        "                shutil.copy(expression_img, os.path.join(inputs_dir, os.path.basename(expression_img)))\n",
        "                expression_img = os.path.join(inputs_dir, os.path.basename(expression_img))\n",
        "    clear_last()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(f\"Generating your Video-ReTalking Video... See Connsole for Progress.\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    total_steps = 100 #?\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)\n",
        "    video_file = os.path.join(results_dir, os.path.basename(output_file))\n",
        "    extras = \"\"\n",
        "    if video_retalking_prefs['exp_img'] != 'neutral':\n",
        "        if video_retalking_prefs['exp_img'] != 'Image':\n",
        "            if bool(expression_img):\n",
        "                extras += f' --exp_img \"inputs/{os.path.basename(expression_img)}\"'\n",
        "        else:\n",
        "            extras += f\" --exp_img {video_retalking_prefs['exp_img']}\"\n",
        "    if video_retalking_prefs['up_face'] != 'original':\n",
        "        extras += f\" --up_face {video_retalking_prefs['up_face']}\"\n",
        "    if video_retalking_prefs['fps'] != 25:\n",
        "        extras += f\" --fps {video_retalking_prefs['fps']}\"\n",
        "    if video_retalking_prefs['img_size'] != 384:\n",
        "        extras += f\" --img_size {video_retalking_prefs['img_size']}\"\n",
        "    cmd = f'python inference.py --face \"inputs/{os.path.basename(target_path)}\"  --audio \"inputs/{os.path.basename(input_audio)}\"{extras} --outfile \"results/{os.path.basename(output_file)}\"'\n",
        "    prt(f\"Running {cmd}\")\n",
        "    try:\n",
        "        run_process(cmd, cwd=video_retalking_dir, page=page, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, \"Error running Python inference.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    shutil.copy(video_file, output_file)\n",
        "    clear_last(3)\n",
        "    autoscroll(True)\n",
        "    #TODO: Upscale Image\n",
        "    if os.path.isfile(output_file):\n",
        "        prt(Markdown(f\"Video saved to [{output_file}]({output_file})\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "        #prt(Row([Text(\"Saved to {output_file}\")], alignment=MainAxisAlignment.CENTER))\n",
        "        try:\n",
        "            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        prt(\"üí¢  Error Generating Output File!\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_style_crafter(page):\n",
        "    global style_crafter_prefs, prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.StyleCrafter.controls.append(line)\n",
        "      page.StyleCrafter.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.StyleCrafter, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.StyleCrafter.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.StyleCrafter.auto_scroll = scroll\n",
        "      page.StyleCrafter.update()\n",
        "    if not bool(style_crafter_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide the Stylized Prompt to process...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing StyleCrafter Pipeline...\")\n",
        "    prt(installer)\n",
        "    #pytorch_lightning==1.9.3\n",
        "    pip_install(\"decord einops imageio omegaconf pandas pytorch_lightning PyYAML|yaml setuptools moviepy av xformers gradio timm scikit-learn|sklearn open_clip_torch==2.22.0|open_clip kornia\", installer=installer, upgrade=True)\n",
        "    status['installed_xformers'] = True\n",
        "    style_crafter_dir = os.path.join(root_dir, \"StyleCrafter\")\n",
        "    checkpoints_dir = os.path.join(style_crafter_dir, \"checkpoints\")\n",
        "    \n",
        "    if not os.path.exists(style_crafter_dir) or force_updates:\n",
        "        try:\n",
        "            installer.status(\"...cloning GongyeLiu/StyleCrafter.git\")\n",
        "            run_sp(\"git clone https://github.com/GongyeLiu/StyleCrafter.git\", cwd=root_dir, realtime=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Installing github.com/GongyeLiu/StyleCrafter...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    sys.path.append(os.path.join(style_crafter_dir, \"scripts\", \"evaluation\"))\n",
        "    try:\n",
        "        installer.status(\"...install open_clip\")\n",
        "        run_sp(\"git lfs install\", cwd=os.path.join(checkpoints_dir, \"open_clip\"), realtime=False)\n",
        "        run_sp(\"git clone https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", cwd=os.path.join(checkpoints_dir, \"open_clip\"), realtime=False)\n",
        "        installer.status(\"...get Text2Video-512 ckpt\")\n",
        "        download_file(\"https://huggingface.co/VideoCrafter/Text2Video-512/blob/main/model.ckpt\", to=os.path.join(checkpoints_dir, \"videocrafter_t2v_320_512\"))\n",
        "        installer.status(\"...get adapter_v1.pth\")\n",
        "        download_file(\"https://huggingface.co/liuhuohuo/StyleCrafter/blob/main/adapter_v1.pth\", to=os.path.join(checkpoints_dir, \"stylecrafter\"))\n",
        "        installer.status(\"...get temporal_v1.pth\")\n",
        "        download_file(\"https://huggingface.co/liuhuohuo/StyleCrafter/blob/main/temporal_v1.pth\", to=os.path.join(checkpoints_dir, \"stylecrafter\"))\n",
        "        installer.status(\"\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"Error Setting up Dependancies...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    import numpy as np\n",
        "    from PIL import Image as PILImage\n",
        "    from PIL import ImageOps\n",
        "    outputs_dir = os.path.join(style_crafter_dir, \"outputs\")\n",
        "    if os.path.exists(outputs_dir):\n",
        "        shutil.rmtree(outputs_dir, ignore_errors=True)\n",
        "    make_dir(outputs_dir)\n",
        "    save_dir = os.path.join(style_crafter_dir, \"my_eval_data\")\n",
        "    if os.path.exists(save_dir):\n",
        "        shutil.rmtree(save_dir, ignore_errors=True)\n",
        "    make_dir(save_dir)\n",
        "    prompt_json = os.path.join(save_dir, 'eval_prompt.json')\n",
        "    #frames_dir = os.path.join(save_dir, 'frames')\n",
        "    #make_dir(frames_dir)\n",
        "    batch_output = os.path.join(prefs['image_output'], style_crafter_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    output_frames_dir = os.path.join(save_dir, 'output_frames')\n",
        "    make_dir(output_frames_dir)\n",
        "    data_dir = os.path.join(save_dir, 'data')\n",
        "    make_dir(data_dir)\n",
        "    style_images = []\n",
        "    init_images = []\n",
        "    for fl in page.style_file_list.controls:\n",
        "        f = fl.title.value\n",
        "        style_images.append(f)\n",
        "    if len(style_images) == 0:\n",
        "        if bool(style_crafter_prefs['init_image']):\n",
        "            style_images.append(style_crafter_prefs['init_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: You need to provide Input Style Image(s)\")\n",
        "            return\n",
        "    #init_image = style_crafter_prefs['init_image']\n",
        "    #if bool(init_image):\n",
        "    for init_image in style_images:\n",
        "        fname = init_image.rpartition(slash)[2]\n",
        "        init_file = os.path.join(data_dir, fname)\n",
        "        if init_image.startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(init_image, stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(init_image):\n",
        "                init_img = PILImage.open(init_image)\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {init_image}\")\n",
        "                return\n",
        "        init_img = init_img.resize((style_crafter_prefs['max_size'], style_crafter_prefs['max_size']), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        init_img.save(init_file)\n",
        "        init_images.append(f'data/{os.path.basename(init_file)}')\n",
        "    if len(init_images) == 1:\n",
        "        init_images = init_images[0]\n",
        "    clear_last()\n",
        "    prt(f\"Generating StyleCrafter on Frames with your Prompt... See console for progress.\")\n",
        "    fname = format_filename(style_crafter_prefs['prompt'])\n",
        "    class CustomEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            if isinstance(obj, dict) and len(obj) == 1:\n",
        "                return [obj]\n",
        "            return super().default(obj)\n",
        "    prompt_list = [{'prompt': style_crafter_prefs['prompt'], 'style_path': init_images}]\n",
        "    with open(prompt_json, \"w\") as f:\n",
        "        json.dump(prompt_list, f, ensure_ascii=False, indent=4, cls=CustomEncoder)\n",
        "    random_seed = int(style_crafter_prefs['seed']) if int(style_crafter_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    config=\"configs/inference_video_320_512.yaml\"\n",
        "    ckpt=\"checkpoints/videocrafter_t2v_320_512/model.ckpt\"\n",
        "    adapter_ckpt=\"checkpoints/stylecrafter/adapter_v1.pth\"\n",
        "    temporal_ckpt=\"checkpoints/stylecrafter/temporal_v1.pth\"\n",
        "    prompt_dir=\"my_eval_data\"\n",
        "    filename=\"eval_prompt.json\"\n",
        "    res_dir=\"output\"\n",
        "    n_samples=1\n",
        "    total_steps = style_crafter_prefs['num_inference_steps']#{\"video\" if style_crafter_prefs[\"output_video\"] else \"image\"}\n",
        "    mode = style_crafter_prefs[\"selected_mode\"].split('\"')[1]\n",
        "    cmd = f'python scripts/evaluation/style_inference.py --out_type \"{mode}\" --adapter_ckpt {adapter_ckpt} --temporal_ckpt {temporal_ckpt} --seed {random_seed} --ckpt_path {ckpt} --base {config} --savedir {res_dir}'\n",
        "    cmd += f' --n_samples {n_samples} --bs {style_crafter_prefs[\"batch_size\"]} --height {style_crafter_prefs[\"height\"]} --width {style_crafter_prefs[\"width\"]} --unconditional_guidance_scale 15.0 --unconditional_guidance_scale_style {style_crafter_prefs[\"guidance_scale\"]} --ddim_steps {total_steps} --ddim_eta {style_crafter_prefs[\"eta\"]} --prompt_dir {prompt_dir} --filename {filename}'\n",
        "    prt(f\"Running {cmd}\")\n",
        "    try:\n",
        "        run_sp(cmd, cwd=style_crafter_dir, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, \"Error running Python.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    clear_last(2)\n",
        "    frame_files = [f for f in os.listdir(outputs_dir) if f.endswith('.png')]\n",
        "    #frame_files = sorted(os.listdir(frames_dir), key=int(frame_filename[5:-4]))\n",
        "    for i, frame_file in enumerate(frame_files):\n",
        "        image_path = os.path.join(outputs_dir, frame_file)\n",
        "        output_path = os.path.join(batch_output, frame_file)#os.path.join(outputs_dir, f\"frame{str(i).zfill(4)}.png\")\n",
        "        shutil.copy(image_path, output_path)\n",
        "        prt(Row([Img(src=output_path, fit=ImageFit.CONTAIN, width=style_crafter_prefs[\"width\"], height=style_crafter_prefs[\"height\"], gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([Text(output_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    if mode == \"video\":\n",
        "        installer = Installing(f\"Saving Video File... Frames at {outputs_dir if not style_crafter_prefs['save_frames'] else batch_output}\")\n",
        "        prt(installer)\n",
        "        #if style_crafter_prefs['save_frames']:\n",
        "        #    shutil.copytree(output_frames_dir, save_frames_dir, dirs_exist_ok=True)\n",
        "        video_out = available_file(batch_output, fname, 0, no_num=True, ext=\"mp4\")\n",
        "        interpolate_video(outputs_dir, output_video=video_out, input_fps=8, output_fps=25, installer=installer)\n",
        "        clear_last\n",
        "        prt(Markdown(f\"Video saved to [{video_out}]({video_out})\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "        #prt(f\"Saved to {video_out}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_rave(page):\n",
        "    global rave_prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.RAVE.controls.append(line)\n",
        "      page.RAVE.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.RAVE, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "        page.RAVE.auto_scroll = scroll\n",
        "        page.RAVE.update()\n",
        "    if not bool(rave_prefs['init_video']):\n",
        "        alert_msg(page, \"You must provide a target init video...\")\n",
        "        return\n",
        "    if not bool(rave_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide an interesting prompt to guide the video...\")\n",
        "        return\n",
        "    if not bool(rave_prefs['batch_folder_name']):\n",
        "        alert_msg(page, \"You must give a unique Batch Folder Name to save to...\")\n",
        "        return\n",
        "    page.RAVE.controls = page.RAVE.controls[:1]\n",
        "    autoscroll()\n",
        "    installer = Installing(\"Installing RAVE Libraries...\")\n",
        "    prt(installer)\n",
        "    rave_dir = os.path.join(root_dir, \"RAVE\")\n",
        "    if not os.path.exists(rave_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning rehg-lab/RAVE\")\n",
        "            run_sp(\"git clone https://github.com/rehg-lab/RAVE.git\", cwd=root_dir, realtime=False)\n",
        "            installer.status(\"...installing RAVE requirements\")\n",
        "            #run_sp(\"pip install -r requirements.txt\", realtime=True) #pytorch-lightning==1.5.0\n",
        "            pip_install(\"addict==2.4.0 basicsr==1.4.2 beautifulsoup4|bs4 caffe2==0.8.1 cityscapesscripts==2.2.2 dominate==2.9.0 einops external==0.0.1 fairscale==0.4.13 ftfy fvcore==0.1.5.post20221221 hydra-core==1.3.2 imageio imutils iopath==0.1.10 kornia==0.7.0 lmdb==1.4.1 matplotlib mc mediapipe mmdet==3.2.0 mmpose==1.2.0 omegaconf onnx==1.15.0 onnxruntime==1.16.3 opencv_python openvino==2023.2.0 packaging pandas parrots==0.1.7 prettytable==3.9.0 Pygments==2.17.2 pytorch_lightning==2.1.2 PyYAML|yaml regex scipy setuptools Shapely==2.0.2 skimage==0.0 std_msgs==0.0.1 tabulate==0.9.0 tensorboardX==2.6.2.2 tensorflow==2.15.0.post1 termcolor==2.4.0 tifffile==2023.7.10 timm tqdm turbojpeg==0.0.2 yapf==0.40.2 watchdog\", installer=installer, upgrade=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing RAVE Requirements:\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_pipes()\n",
        "    import yaml\n",
        "    from PIL import ImageOps\n",
        "    if bool(rave_prefs['output_name']):\n",
        "        fname = format_filename(rave_prefs['output_name'], force_underscore=True)\n",
        "    elif bool(rave_prefs['prompt']):\n",
        "        fname = format_filename(rave_prefs['prompt'], force_underscore=True)\n",
        "    elif bool(rave_prefs['batch_folder_name']):\n",
        "        fname = format_filename(rave_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else: fname = \"output\"\n",
        "    if bool(rave_prefs['file_prefix']):\n",
        "        fname = f\"{rave_prefs['file_prefix']}{fname}\"\n",
        "    data_dir = os.path.join(rave_dir, \"data\")\n",
        "    results_dir = os.path.join(rave_dir, \"results\")\n",
        "    if os.path.exists(results_dir):\n",
        "        shutil.rmtree(results_dir, ignore_errors=True)\n",
        "    make_dir(results_dir)\n",
        "    #if bool(rave_prefs['batch_folder_name']):\n",
        "    #    batch_output = os.path.join(stable_dir, rave_prefs['batch_folder_name'])\n",
        "    #else: batch_output = stable_dir\n",
        "    #if not os.path.exists(batch_output):\n",
        "    #    os.makedirs(batch_output)\n",
        "    output_path = os.path.join(prefs['image_output'], rave_prefs['batch_folder_name'])\n",
        "    make_dir(output_path)\n",
        "    init_vid = rave_prefs['init_video']\n",
        "    if init_vid.startswith('http'):\n",
        "        init_vid = download_file(init_vid, uploads_dir, ext=\"mp4\")\n",
        "    else:\n",
        "        if not os.path.isfile(init_vid):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "            return\n",
        "    installer.status(\"...scaleing video\")\n",
        "    w, h = scale_video(init_vid, os.path.join(data_dir, f\"{fname}.mp4\"), rave_prefs[\"max_size\"])\n",
        "    #shutil.copy(init_vid, os.path.join(data_dir, f\"{fname}.mp4\"))\n",
        "    #video_out_path = os.path.join(data_dir, rave_prefs['batch_folder_name'])\n",
        "    #make_dir(video_out_path)\n",
        "    installer.status(\"...preparing yaml\")\n",
        "    random_seed = int(rave_prefs['seed']) if int(rave_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)\n",
        "    config_yaml_file = os.path.join(rave_dir, \"configs\", \"aeionic_rave.yaml\")\n",
        "    config_yaml = {\n",
        "        \"video_name\": fname,\n",
        "        \"preprocess_name\": rave_prefs['control_task'].lower().replace(' ', '_'),\n",
        "        \"batch_size\": rave_prefs['batch_size'],\n",
        "        \"batch_size_vae\": rave_prefs['batch_size_vae'],\n",
        "        \"cond_step_start\": 1 - rave_prefs['controlnet_strength'],\n",
        "        \"controlnet_conditioning_scale\": rave_prefs['controlnet_conditioning_scale'],\n",
        "        \"controlnet_guidance_end\": rave_prefs['controlnet_guidance_end'],\n",
        "        \"controlnet_guidance_start\": rave_prefs['controlnet_guidance_start'],\n",
        "        \"give_control_inversion\": rave_prefs['give_control_inversion'],\n",
        "        \"grid_size\": 3,\n",
        "        \"sample_size\": -1,\n",
        "        \"pad\": 1,\n",
        "        \"guidance_scale\": rave_prefs['guidance_scale'],\n",
        "        \"inversion_prompt\": \"\",\n",
        "        \"is_ddim_inversion\": rave_prefs['is_ddim_inversion'],\n",
        "        \"is_shuffle\": rave_prefs['is_shuffle'],\n",
        "        \"negative_prompts\": rave_prefs['negative_prompt'],\n",
        "        \"positive_prompts\": rave_prefs['prompt'],\n",
        "        \"is_shuffle\": rave_prefs['is_shuffle'],\n",
        "        \"num_inference_steps\": rave_prefs['num_inference_steps'],\n",
        "        \"num_inversion_steps\": rave_prefs['num_inversion_steps'],\n",
        "        \"save_folder\": fname,\n",
        "        \"seed\": random_seed,\n",
        "        \"model_id\": 'None', #['CIVIT_AI/diffusers_models/realisticVisionV60B1_v51VAE']\n",
        "    }\n",
        "    with open(config_yaml_file, \"w\") as outfile:\n",
        "        yaml.dump(config_yaml, outfile, sort_keys=False)\n",
        "    #output_file = os.path.join(output_path, f\"{fname}{'.mp4' if is_video else '.png'}\")\n",
        "    if not os.path.exists(config_yaml_file):\n",
        "        print(f\"Error creating json file {config_yaml_file}\")\n",
        "    clear_last()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(f\"Generating your RAVE...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    cmd = f'python scripts/run_experiment.py configs/aeionic_rave.yaml'\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self, event):\n",
        "        nonlocal img_idx, w, h\n",
        "        if event.is_directory:\n",
        "          return None\n",
        "        elif event.event_type == 'created' and (event.src_path.endswith(\"png\") or event.src_path.endswith(\"jpg\") or event.src_path.endswith(\"gif\")):\n",
        "          autoscroll(True)\n",
        "          if w == 0:\n",
        "            time.sleep(0.2)\n",
        "            try:\n",
        "              frame = PILImage.open(event.src_path)\n",
        "              w, h = frame.size\n",
        "              clear_last()\n",
        "            except Exception:\n",
        "              pass\n",
        "          clear_last()\n",
        "          if rave_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          else:\n",
        "            fpath = event.src_path\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          prt(progress)\n",
        "          if rave_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "            shutil.copy(event.src_path, fpath)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "          img_idx += 1\n",
        "        elif event.event_type == 'created' and (event.src_path.endswith(\"mp4\") or event.src_path.endswith(\"avi\")):\n",
        "          autoscroll(True)\n",
        "          #clear_last()\n",
        "          prt(Divider(height=6, thickness=2))\n",
        "          fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          time.sleep(0.2)\n",
        "          shutil.copy(event.src_path, fpath)\n",
        "          prt(f\"Video saved to {fpath} from {event.src_path}\")\n",
        "          #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          #page.update()Image\n",
        "          #prt(progress)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, results_dir, recursive=True)\n",
        "    observer.start()\n",
        "    #console = RunConsole(f\"Running {cmd}\")\n",
        "    #console.run_process(cmd, cwd=rave_dir)\n",
        "    #prt(f\"Running {cmd}\")\n",
        "    #prt(progress)\n",
        "    try:\n",
        "        #TODO: Parse output to get percent current for progress callback_fnc\n",
        "        run_sp(cmd, cwd=rave_dir, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        observer.stop()\n",
        "        alert_msg(page, \"Error running Rave Python\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    #clear_last()\n",
        "    observer.stop()\n",
        "    #clear_last()\n",
        "    autoscroll(True)\n",
        "    #TODO: Upscale Image\n",
        "    if os.path.isfile(output_file):\n",
        "        prt(f\"Saved video to {output_file}\")\n",
        "        #prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))\n",
        "    else:\n",
        "        prt(\"Error Generating Output File! Maybe NSFW Image detected or Out of Memory?\")\n",
        "    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_tokenflow(page):\n",
        "    global tokenflow_prefs, prefs, status, pipe_tokenflow, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TokenFlow.controls.append(line)\n",
        "      page.TokenFlow.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.TokenFlow, lines=lines)\n",
        "    def clear_list():\n",
        "      page.TokenFlow.controls = page.TokenFlow.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TokenFlow.auto_scroll = scroll\n",
        "      page.TokenFlow.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = tokenflow_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing TokenFlow Text-To-Video Pipeline...\")\n",
        "    prt(installer)\n",
        "    #model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    clear_pipes()\n",
        "    tokenflow_dir = os.path.join(root_dir, \"TokenFlow\")\n",
        "    if not os.path.exists(tokenflow_dir):\n",
        "        installer.status(\"...Skquark/TokenFlow\") #XmYx/TokenFlow\n",
        "        run_sp(\"git clone https://github.com/Skquark/TokenFlow.git\", realtime=False, cwd=root_dir)\n",
        "    data_dir = os.path.join(tokenflow_dir, \"data\")\n",
        "    pip_install(\"ftfy opencv-python|cv2 tqdm numpy pyyaml|yaml xformers tensorboard av kornia\", installer=installer)\n",
        "    #clear_pipes('tokenflow')\n",
        "    clear_last()\n",
        "    #prt(\"Generating TokenFlow of your Video...\")\n",
        "    progressbar = Progress(\"Generating TokenFlow of your Video... See console for progress.\")\n",
        "    prt(progressbar)\n",
        "    autoscroll(False)\n",
        "    #batch_output = os.path.join(stable_dir, tokenflow_prefs['batch_folder_name'])\n",
        "    #if not os.path.isdir(batch_output):\n",
        "    #  os.makedirs(batch_output)\n",
        "    #local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], tokenflow_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    data_folder = format_filename(tokenflow_prefs['batch_folder_name'], use_dash=True)\n",
        "    make_dir(os.path.join(data_dir, data_folder))\n",
        "    init_vid = tokenflow_prefs['init_video']\n",
        "    if init_vid.startswith('http'):\n",
        "        progressbar.status(\"...Downloading Video\")\n",
        "        init_vid = download_file(init_vid, uploads_dir, ext=\"mp4\")\n",
        "    else:\n",
        "        if not os.path.isfile(init_vid):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "            return\n",
        "    video_file = os.path.basename(init_vid)\n",
        "    if not video_file.endswith(\"mp4\"):\n",
        "        video_file += \".mp4\"\n",
        "    progressbar.status(\"...Preparing Run\")\n",
        "    shutil.copy(init_vid, os.path.join(data_dir, data_folder, video_file))\n",
        "    random_seed = int(tokenflow_prefs['seed']) if int(tokenflow_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    #width = tokenflow_prefs['width']\n",
        "    #height = tokenflow_prefs['height']\n",
        "    selected_mode = tokenflow_prefs['selected_mode']\n",
        "    cache = prefs[\"cache_dir\"]\n",
        "    cache_dir = f' --cache_dir \"{cache}\"' if bool(cache) else '' \n",
        "    #x = \" -x\" if status['installed_xformers'] else \"\"\n",
        "    import yaml\n",
        "    def literal_presenter(dumper, data):\n",
        "        #return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
        "        if isinstance(data, str):\n",
        "            return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
        "        return dumper.represent_str(data)\n",
        "    def represent_str(dumper, data):\n",
        "        if '\\n' in data:\n",
        "            return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
        "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
        "    def represent_none(dumper, _):\n",
        "        return dumper.represent_scalar('tag:yaml.org,2002:null', '')\n",
        "    def save_yaml(config, yaml_out):\n",
        "        with open(yaml_out, 'w') as file:\n",
        "            for key, value in config.items():\n",
        "                if isinstance(value, str):\n",
        "                    file.write(f'{key}: \"{value}\"\\n')\n",
        "                else:\n",
        "                    file.write(f'{key}: {value}\\n')\n",
        "\n",
        "    config_yaml = os.path.join(tokenflow_dir, 'configs', 'aeionic_config.yaml')\n",
        "    config = {'seed': random_seed, 'device': torch_device, 'output_path': 'tokenflow-results', 'data_path': f'data/{data_folder}/{video_file}', 'latents_path': 'latents', 'n_inversion_steps': tokenflow_prefs[\"num_inversion_steps\"], 'n_frames': tokenflow_prefs['num_frames']}\n",
        "    config['sd_version'] = tokenflow_prefs['sd_version']\n",
        "    config['guidance_scale'] = tokenflow_prefs['guidance_scale']\n",
        "    config['n_timesteps'] = tokenflow_prefs[\"num_inference_steps\"]\n",
        "    config['prompt'] = tokenflow_prefs['prompt']\n",
        "    config['negative_prompt'] = tokenflow_prefs['negative_prompt']\n",
        "    config['batch_size'] = tokenflow_prefs['batch_size']\n",
        "    config['fps'] = tokenflow_prefs['fps']\n",
        "    if selected_mode == \"pnp\":\n",
        "        config['pnp_attn_t'] = tokenflow_prefs['pnp_attn_t']\n",
        "        config['pnp_f_t'] = tokenflow_prefs['pnp_f_t']\n",
        "        run_py = \"run_tokenflow_pnp.py\"\n",
        "    else:\n",
        "        config['start'] = tokenflow_prefs['start']\n",
        "        config['use_ddim_noise'] = tokenflow_prefs['use_ddim_noise']\n",
        "        run_py = \"run_tokenflow_sdedit.py\"\n",
        "    #with open(config_yaml, 'w') as file:\n",
        "    #    #yaml.add_representer(str, literal_presenter)\n",
        "    #    yaml.add_representer(str, represent_str)\n",
        "    #    yaml.add_representer(type(None), represent_none)\n",
        "    #    yaml.dump(config, file, indent=4, default_flow_style=False, sort_keys=False)\n",
        "    save_yaml(config, config_yaml)\n",
        "    try:\n",
        "        progressbar.status(\"...Preprocessing Inverted Video\")\n",
        "        run_sp(f'python preprocess.py --data_path \"data/{data_folder}/{video_file}\" --inversion_prompt \"{tokenflow_prefs[\"inversion_prompt\"]}\" --steps {tokenflow_prefs[\"num_inversion_steps\"]} --sd_version \"{tokenflow_prefs[\"sd_version\"]}\"{cache_dir}', cwd=tokenflow_dir)\n",
        "               #-W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {tokenflow_prefs[\"num_inference_steps\"]} -g {tokenflow_prefs[\"guidance_scale\"]} -f {tokenflow_prefs[\"fps\"]} -T {tokenflow_prefs[\"num_frames\"]}', cwd=data_dir)\n",
        "        progressbar.status(f\"...Processing TokenFlow {selected_mode}\")\n",
        "        run_sp(f'python {run_py} --config_path \"configs/aeionic_config.yaml\"{cache_dir}', cwd=tokenflow_dir, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: TokenFlow Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    filename = f\"{format_filename(tokenflow_prefs['prompt'])}\"\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    output_path = os.path.join(tokenflow_dir, \"tokenflow-results\", \"tokenflow_PnP.mp4\")\n",
        "    video_path = available_file(batch_output, filename, ext=\"mp4\", no_num=True)\n",
        "    if os.path.exists(output_path):\n",
        "        shutil.copy(output_path, video_path)\n",
        "        prt(f\"Done creating video... Check {video_path}\")\n",
        "        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    else:\n",
        "        prt(\"Something went wrong generating video...\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_animate_diff(page):\n",
        "    global animate_diff_prefs, prefs, status, pipe_animate_diff, model_path\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "    #  return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.AnimateDiff.controls.append(line)\n",
        "      page.AnimateDiff.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.AnimateDiff, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.AnimateDiff.auto_scroll = scroll\n",
        "      page.AnimateDiff.update()\n",
        "    def clear_list():\n",
        "      page.AnimateDiff.controls = page.AnimateDiff.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = animate_diff_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing AnimateDiff Requirements...\")\n",
        "    prt(installer)\n",
        "    try:\n",
        "        import diffusers\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing diffusers\")\n",
        "        run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import transformers\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing transformers\")\n",
        "        run_sp(\"pip install --upgrade transformers==4.30.2\", realtime=False) #4.28\n",
        "        pass\n",
        "    pip_install(\"omegaconf einops cmake colorama rich ninja copier==8.1.0 pydantic shellingham typer gdown black ruff setuptools-scm controlnet_aux mediapipe matplotlib watchdog imageio==2.27.0\", installer=installer)\n",
        "\n",
        "    animatediff_dir = os.path.join(root_dir, 'animatediff-cli-prompt-travel')\n",
        "    if 'installed_animate_diff' not in status:#not os.path.exists(animatediff_dir) or force_updates:\n",
        "        installer.status(\"...clone s9roll7/animatediff\")\n",
        "        run_sp(\"git clone https://github.com/s9roll7/animatediff-cli-prompt-travel\", realtime=False, cwd=root_dir)\n",
        "        #run_sp(\"git clone https://github.com/Skquark/animatediff-cli\", realtime=False, cwd=root_dir) #/neggles\n",
        "        installer.status(\"...install animatediff-prompt-travel\")\n",
        "        run_sp(\"git lfs install\", cwd=animatediff_dir, realtime=False)\n",
        "        status['installed_animate_diff'] = True\n",
        "    os.chdir(animatediff_dir)\n",
        "    #if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':\n",
        "    try:\n",
        "        import xformers\n",
        "    except ModuleNotFoundError:\n",
        "        installer.status(\"...installing FaceBook's Xformers\")\n",
        "        #run_sp(\"pip install --pre -U triton\", realtime=False)\n",
        "        run_sp(\"pip install -U xformers\", realtime=False)\n",
        "        status['installed_xformers'] = True\n",
        "        pass\n",
        "    pip_install(\"ffmpeg-python|ffmpeg opencv-python|cv2 onnxruntime-gpu|onnxruntime sentencepiece>=0.1.99 safetensors\", installer=installer)\n",
        "    from safetensors import safe_open\n",
        "    try:\n",
        "        from animatediff.cli import generate\n",
        "    except ModuleNotFoundError:\n",
        "        try:\n",
        "            installer.status(\"...installing AnimateDiff Requirements\")\n",
        "            run_sp(\"pip install -e .[dev]\", cwd=animatediff_dir, realtime=False) #'.[dev]'\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Install AnimateDiff Requirements for some reason...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    if animate_diff_prefs['save_video']:\n",
        "        import platform\n",
        "        rife_dir = os.path.join(animatediff_dir, 'data', 'rife')\n",
        "        if len(os.listdir(rife_dir)) <= 1:\n",
        "            installer.status(\"...downloading RiFE\")\n",
        "            if platform.system() == 'Linux':\n",
        "                rife_zip = download_file(\"https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-ubuntu.zip\")\n",
        "            elif platform.system() == 'Windows':\n",
        "                rife_zip = download_file(\"https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-windows.zip\")\n",
        "            elif platform.system() == 'Darwin':\n",
        "                rife_zip = download_file(\"https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-macos.zip\")\n",
        "            installer.status(\"...extracting RiFE\")\n",
        "            shutil.unpack_archive(rife_zip, rife_dir)\n",
        "            os.remove(rife_zip)\n",
        "            for folder_name in os.listdir(rife_dir):\n",
        "                if os.path.isdir(os.path.join(rife_dir, folder_name)):\n",
        "                    rife_folder = os.path.join(rife_dir, folder_name)\n",
        "                    break\n",
        "            for file_name in os.listdir(rife_folder):\n",
        "                shutil.move(os.path.join(rife_folder, file_name), rife_dir)\n",
        "            run_sp(f\"chmod 755 {rife_dir}\", realtime=False)\n",
        "            run_sp(f\"chmod 755 {os.path.join(rife_dir, 'rife-ncnn-vulkan')}\", realtime=False)\n",
        "            #TODO: Fix Colab https://github.com/nihui/rife-ncnn-vulkan/issues/46\n",
        "            os.chmod(os.path.join(rife_dir, 'rife-ncnn-vulkan'), 0o777)\n",
        "            run_sp(\"apt-get install libvulkan-dev\", realtime=False)\n",
        "    from pathlib import Path\n",
        "    output_path = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    #run_sp(\"apt -y install -qq aria2\", realtime=False)\n",
        "    sd_models = os.path.join(animatediff_dir, 'data', 'models', 'StableDiffusion')\n",
        "    motion_module = os.path.join(animatediff_dir, 'data', 'models', 'motion-module')\n",
        "    if not os.path.isdir(motion_module):\n",
        "        os.makedirs(motion_module)\n",
        "    #run_sp(f\"rm -rf {sd_models}\", realtime=False)\n",
        "    if os.path.isdir(sd_models):\n",
        "        shutil.rmtree(sd_models)\n",
        "    installer.status(\"...downloading stable-diffusion-v1-5\")\n",
        "    run_sp(f\"git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 {sd_models}\", realtime=False, cwd=root_dir)\n",
        "    mm_model = [mm for mm in animate_diff_motion_modules if mm['name'] == animate_diff_prefs['motion_module']][0]\n",
        "    mm_path = os.path.join(motion_module, mm_model['file'])\n",
        "    mm_ckpt = mm_model['file']\n",
        "    if not os.path.isfile(mm_path):\n",
        "        installer.status(f\"...downloading {mm_model['name']} Motion Module\")\n",
        "        download_file(mm_model['path'], to=motion_module, filename=mm_model['file'])\n",
        "\n",
        "    #sd_models = \"runwayml/stable-diffusion-v1-5\"\n",
        "    lora_model = {'name': 'None', 'file': '', 'path': '', 'weights': None}\n",
        "    lora_dir = os.path.join(animatediff_dir, 'data', 'models', 'sd')\n",
        "    #lora_dir = os.path.join(animatediff_dir, 'models', 'DreamBooth_LoRA')\n",
        "    if not os.path.isdir(lora_dir):\n",
        "        os.makedirs(lora_dir)\n",
        "    lora_path = \"\"\n",
        "    if animate_diff_prefs['dreambooth_lora'] == \"Custom\":\n",
        "        lora = animate_diff_prefs['custom_lora']\n",
        "        if lora.startswith(\"http\"):\n",
        "            installer.status(f\"...downloading Custom LoRA\")\n",
        "            lora_file = download_file(lora_model['path'], to=lora_dir, ext=\"safetensors\")\n",
        "            if os.path.isfile(lora_file):\n",
        "                lora_path = lora_file\n",
        "        else:\n",
        "            if os.path.isfile(lora):\n",
        "                fname = os.path.basename(lora)\n",
        "                lora_path = os.path.join(lora_dir, fname)\n",
        "                shutil.copy(lora, lora_path)\n",
        "    elif animate_diff_prefs['dreambooth_lora'] != \"None\":\n",
        "        for lora in animate_diff_loras:\n",
        "            if lora['name'] == animate_diff_prefs['dreambooth_lora']:\n",
        "                lora_model = lora\n",
        "                break\n",
        "        lora_path = os.path.join(lora_dir, lora_model['file'])\n",
        "        if not os.path.isfile(lora_path):\n",
        "            installer.status(f\"...downloading {lora_model['name']}\")\n",
        "            download_file(lora_model['path'], to=lora_dir, ext=\"safetensors\")\n",
        "            #run_sp(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {lora_model['path']} -d {lora_dir} -o {lora_model['file']}\", realtime=False)\n",
        "    if bool(lora_path):\n",
        "        fname = lora_path.rpartition(slash)[2]\n",
        "        lora_path = f\"models{slash}sd{slash}{fname}\"\n",
        "    lora_map = {}\n",
        "    if len(animate_diff_prefs['lora_map']) > 0:\n",
        "        lora_map_dir = os.path.join(animatediff_dir, 'data', 'share', 'Lora')\n",
        "        if not os.path.isdir(lora_map_dir):\n",
        "            os.makedirs(lora_map_dir)\n",
        "        for l in animate_diff_prefs['lora_map']:\n",
        "            file = l['file']\n",
        "            if l['name'] == \"Custom\":\n",
        "                if '/models' in l['path']:\n",
        "                    file = l['path'].split('/models/')[1].split('?')[0]\n",
        "                else:\n",
        "                    file = l['path'].rpartition('/')[1].split('?')[0]\n",
        "                if \".safetensors\" not in file:\n",
        "                    file += \".safetensors\"\n",
        "            if l['path'].startswith(\"http\"):\n",
        "                lora_layer_path = os.path.join(lora_map_dir, file)\n",
        "                if not os.path.isfile(lora_layer_path):\n",
        "                    installer.status(f\"...downloading {file} LoRA\")\n",
        "                    lora_file = download_file(l['path'], to=lora_map_dir, filename=file, ext=\"safetensors\")\n",
        "                else:\n",
        "                    lora_file = lora_layer_path\n",
        "                if os.path.isfile(lora_file):\n",
        "                    lora_path = lora_file\n",
        "            elif os.path.isfile(l['path']):\n",
        "                lora_path = os.path.join(lora_map_dir, file)\n",
        "                shutil.copy(l['path'], lora_path)\n",
        "            elif os.path.isfile(os.path.join(lora_map_dir, file)):\n",
        "                lora_path = os.path.join(lora_map_dir, file)\n",
        "            lora_map[f\"share{slash}Lora{slash}{file}\"] = float(l['scale'])\n",
        "    clear_pipes()\n",
        "\n",
        "    samples_dir = os.path.join(animatediff_dir, 'samples')\n",
        "    #batch_output = os.path.join(stable_dir, animate_diff_prefs['batch_folder_name'])\n",
        "    #if not os.path.isdir(batch_output):\n",
        "    #  os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    editing_prompts = []\n",
        "    prompt_map = {}\n",
        "    negative_prompts = []\n",
        "    seeds = []\n",
        "    '''if len(animate_diff_prefs['editing_prompts']) == 0:\n",
        "        if not bool(animate_diff_prefs['prompt']):\n",
        "            alert_msg(page, \"Error: You must provide at least one Prompt to render...\")\n",
        "            return\n",
        "        else:\n",
        "            #editing_prompts.append(animate_diff_prefs['prompt'])\n",
        "            prompt_map[\"0\"] = animate_diff_prefs['prompt']\n",
        "            negative_prompts.append(animate_diff_prefs['negative_prompt'])\n",
        "            random_seed = int(animate_diff_prefs['seed']) if int(animate_diff_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            seeds.append(random_seed)\n",
        "    else:\n",
        "        num = 0\n",
        "        for ep in animate_diff_prefs['editing_prompts']:\n",
        "            #editing_prompts.append(ep['prompt'])\n",
        "            prompt_map[str(int(num * (animate_diff_prefs['video_length'] / len(animate_diff_prefs['editing_prompts']))))] = animate_diff_prefs['prompt']\n",
        "            negative_prompts.append(ep['negative_prompt'])\n",
        "            random_seed = int(ep['seed']) if int(ep['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            seeds.append(random_seed)\n",
        "            num += 1'''\n",
        "    if len(animate_diff_prefs['animation_prompts']) == 0:\n",
        "        if not bool(animate_diff_prefs['prompt']):\n",
        "            alert_msg(page, \"Error: You must provide at least one Prompt to render...\")\n",
        "            return\n",
        "        else:\n",
        "            #editing_prompts.append(animate_diff_prefs['prompt'])\n",
        "            prompt_map[\"0\"] = animate_diff_prefs['prompt']\n",
        "    else:\n",
        "        prompt_map = animate_diff_prefs['animation_prompts']\n",
        "    if not bool(animate_diff_prefs['batch_folder_name']):\n",
        "        alert_msg(page, \"It's highly recommended to give a unique Batch Folder Name to save to...\")\n",
        "        return\n",
        "    installer.status(\"...preparing json\")\n",
        "    negative_prompts.append(animate_diff_prefs['negative_prompt'])\n",
        "    random_seed = int(animate_diff_prefs['seed']) if int(animate_diff_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    seeds.append(random_seed)\n",
        "    context = min(animate_diff_prefs['video_length'], animate_diff_prefs['context'])\n",
        "\n",
        "    def extract_frames(video_file, fps, save_dir, start_frame=0):\n",
        "        vidcap = cv2.VideoCapture(video_file)\n",
        "        source_fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        count = 0\n",
        "        while vidcap.isOpened():\n",
        "            success, image = vidcap.read()\n",
        "            if success:\n",
        "                if (count % (int(source_fps/fps)) == 0):\n",
        "                    cv2.imwrite(os.path.join(save_dir, f\"{str(count + start_frame).zfill(4)}.png\"), cv2.resize(image, (animate_diff_prefs['width'], animate_diff_prefs['height']), cv2.INTER_AREA))\n",
        "                if count >= (context - start_frame):\n",
        "                    break\n",
        "                count += 1\n",
        "            else:\n",
        "                break\n",
        "        cv2.destroyAllWindows()\n",
        "        vidcap.release()\n",
        "    '''else:\n",
        "        num = 0\n",
        "        for ep in animate_diff_prefs['animation_prompts']:\n",
        "            #editing_prompts.append(ep['prompt'])\n",
        "            prompt_map[ep[frame]] = animate_diff_prefs['prompt']\n",
        "            negative_prompts.append(ep['negative_prompt'])\n",
        "            random_seed = int(ep['seed']) if int(ep['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            seeds.append(random_seed)\n",
        "            num += 1'''\n",
        "    prompts_json = {\n",
        "        'name': 'AEIONic',\n",
        "        #'base': \"\",\n",
        "        'path': lora_path,\n",
        "        'motion_module': f\"models{slash}motion-module{slash}{mm_ckpt}\",\n",
        "        #'motion_module': os.path.join(motion_module, f\"{animate_diff_prefs['motion_module']}.ckpt\"),\n",
        "        'apply_lcm_lora': animate_diff_prefs['apply_lcm_lora'],\n",
        "        'lcm_lora_scale': 1.0,\n",
        "        'compile': prefs['enable_torch_compile'],\n",
        "        'seed': seeds,\n",
        "        'scheduler': animate_diff_prefs['scheduler'],\n",
        "        'context_schedule': animate_diff_prefs['context_schedule'].lower(),\n",
        "        'steps': int(animate_diff_prefs['steps']),\n",
        "        'guidance_scale': float(animate_diff_prefs['guidance_scale']),\n",
        "        'clip_skip': int(animate_diff_prefs['clip_skip']),\n",
        "        #'lora_alpha': float(animate_diff_prefs['lora_alpha']),\n",
        "        #'prompt': editing_prompts,\n",
        "        'head_prompt': animate_diff_prefs['head_prompt'],\n",
        "        'tail_prompt': animate_diff_prefs['tail_prompt'],\n",
        "        'prompt_map': prompt_map,\n",
        "        'n_prompt': negative_prompts,\n",
        "    }\n",
        "    ref_image = \"\"\n",
        "    if bool(animate_diff_prefs['ref_image']):\n",
        "        ref_file = os.path.join(animate_diff_prefs['ref_image'])\n",
        "        if os.path.exists(ref_file):\n",
        "            ref_image = f\"ref_image{slash}{os.path.basename(animate_diff_prefs['ref_image'])}\"\n",
        "            shutil.copy(ref_file, os.path.join(animatediff_dir, 'data', 'ref_image', os.path.basename(ref_file)))\n",
        "    controlnet_map = {\n",
        "      \"input_image_dir\" : f\"controlnet_image{slash}test\",\n",
        "      \"max_samples_on_vram\": 200,\n",
        "      \"max_models_on_vram\" : 3,\n",
        "      \"save_detectmap\": True,\n",
        "      \"preprocess_on_gpu\": True,\n",
        "      \"is_loop\": animate_diff_prefs['is_loop'],\n",
        "      \"controlnet_tile\":{\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_ip2p\":{\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_lineart_anime\":{\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_openpose\":{\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_softedge\":{\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_shuffle\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_depth\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_canny\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_inpaint\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_lineart\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_mlsd\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_normalbae\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_scribble\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_seg\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"qr_code_monster_v1\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"qr_code_monster_v2\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_mediapipe_face\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"animatediff_controlnet\": {\n",
        "        \"enable\": False,\n",
        "        \"use_preprocessor\":True,\n",
        "        \"guess_mode\":False,\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0,\n",
        "        \"control_scale_list\":[0.5,0.4,0.3,0.2,0.1]\n",
        "      },\n",
        "      \"controlnet_ref\": {\n",
        "        \"enable\": bool(ref_image),\n",
        "        \"ref_image\": ref_image if bool(ref_image) else \"ref_image/ref_sample.png\",\n",
        "        \"attention_auto_machine_weight\": 1.0,\n",
        "        \"gn_auto_machine_weight\": 1.0,\n",
        "        \"style_fidelity\": 0.5,\n",
        "        \"reference_attn\": True,\n",
        "        \"reference_adain\": False,\n",
        "        \"scale_pattern\":[0.5]\n",
        "      }\n",
        "    }\n",
        "    for l in animate_diff_prefs['controlnet_layers']:\n",
        "        if l['control_task'].startswith(\"QR\"):\n",
        "            controlnet_task = l['control_task'].lower()\n",
        "        elif l['control_task'].startswith(\"AnimateDiff\"):\n",
        "            controlnet_task = \"animatediff_controlnet\"\n",
        "        else:\n",
        "            controlnet_task = f\"controlnet_{l['control_task'].lower()}\"\n",
        "        try:\n",
        "            scale_list = [float(x.strip()) for x in l['control_scale_list'].split(',')]\n",
        "        except Exception:\n",
        "            print(f\"Error converting Scale List {l['control_scale_list']} to list of floats. Using default..\")\n",
        "            scale_list = [0.5,0.4,0.3,0.2,0.1]\n",
        "            pass\n",
        "        input_image_dir = os.path.join(animatediff_dir, 'data', 'controlnet_image', 'test', controlnet_task)\n",
        "        for old in os.listdir(input_image_dir):\n",
        "            installer.status(f\"...deleting {old}\")\n",
        "            os.remove(os.path.join(input_image_dir, old))\n",
        "        control_images = l['control_images']\n",
        "        for f, ip_image in l['control_images'].items():\n",
        "            if bool(ip_image):\n",
        "                if ip_image.endswith('mp4') or ip_image.endswith('avi'):\n",
        "                    if os.path.isfile(ip_image):\n",
        "                        installer.status(f\"...extracting frames from {os.path.basename(ip_image)}\")\n",
        "                        extract_frames(ip_image, 8, input_image_dir, start_frame=f)\n",
        "                else:\n",
        "                    img_path = os.path.join(input_image_dir, f'{str(f).zfill(4)}.png')\n",
        "                    installer.status(f\"...saving {os.path.basename(ip_image)}\")\n",
        "                    #TODO: Resize image\n",
        "                    if os.path.isfile(ip_image):\n",
        "                        shutil.copy(ip_image, img_path)\n",
        "                    elif ip_image.startswith('https://drive'):\n",
        "                        pip_install(\"gdown\", installer=installer)\n",
        "                        import gdown\n",
        "                        gdown.download(ip_image, img_path, quiet=True)\n",
        "                    elif ip_image.startswith('http'):\n",
        "                        download_file(ip_image, img_path)\n",
        "        controlnet_map[controlnet_task] = {\n",
        "          \"enable\": True,\n",
        "          \"use_preprocessor\":True,\n",
        "          \"guess_mode\":False,\n",
        "          \"controlnet_conditioning_scale\": float(l['conditioning_scale']),\n",
        "          \"control_guidance_start\": float(l['control_guidance_start']),\n",
        "          \"control_guidance_end\": float(l['control_guidance_end']),\n",
        "          \"control_scale_list\":scale_list,\n",
        "        }\n",
        "    if bool(ip_image):\n",
        "        ip_adapter_image_dir = os.path.join(animatediff_dir, 'data', 'ip_adapter_image', 'test')\n",
        "        if os.path.isdir(ip_adapter_image_dir):\n",
        "            os.rmdir(ip_adapter_image_dir)\n",
        "        make_dir(ip_adapter_image_dir)\n",
        "        for f, ip_image in animate_diff_prefs['ip_adapter_layers'].items():\n",
        "            img_path = os.path.join(ip_adapter_image_dir, f'{str(f).zfill(4)}.png')\n",
        "            installer.status(f\"...saving {os.path.basename(ip_image)}\")\n",
        "            #TODO: Resize image\n",
        "            if os.path.isfile(ip_image):\n",
        "                shutil.copy(ip_image, img_path)\n",
        "            elif ip_image.startswith('https://drive'):\n",
        "                pip_install(\"gdown\", installer=installer)\n",
        "                import gdown\n",
        "                gdown.download(ip_image, img_path, quiet=True)\n",
        "            elif ip_image.startswith('http'):\n",
        "                download_file(ip_image, img_path)\n",
        "    ip_adapter_map = {\n",
        "      \"enable\": animate_diff_prefs['use_ip_adapter'],\n",
        "      \"input_image_dir\": f\"ip_adapter_image{slash}test\",\n",
        "      \"save_input_image\": False,\n",
        "      \"scale\": animate_diff_prefs['ip_adapter_scale'],\n",
        "      \"is_full_face\": animate_diff_prefs['ip_adapter_is_full_face'],\n",
        "      \"is_plus_face\": animate_diff_prefs['ip_adapter_is_plus_face'],\n",
        "      \"is_plus\": animate_diff_prefs['ip_adapter_is_plus'],\n",
        "      \"is_light\": animate_diff_prefs['ip_adapter_light']\n",
        "    }\n",
        "    if animate_diff_prefs['use_img2img']:\n",
        "        img2img_image_dir = os.path.join(animatediff_dir, 'data', 'img2img_image', 'test')\n",
        "        if os.path.isdir(img2img_image_dir):\n",
        "            os.rmdir(img2img_image_dir)\n",
        "        make_dir(img2img_image_dir)\n",
        "        for f, img2img_image in animate_diff_prefs['img2img_layers'].items():\n",
        "            if bool(img2img_image):\n",
        "                img_path = os.path.join(img2img_image_dir, f'{str(f).zfill(4)}.png')\n",
        "                installer.status(f\"...saving {os.path.basename(img2img_image)}\")\n",
        "                #TODO: Resize image\n",
        "                if os.path.isfile(img2img_image):\n",
        "                    shutil.copy(img2img_image, img_path)\n",
        "                elif img2img_image.startswith('https://drive'):\n",
        "                    pip_install(\"gdown\", installer=installer)\n",
        "                    import gdown\n",
        "                    gdown.download(img2img_image, img_path, quiet=True)\n",
        "                elif img2img_image.startswith('http'):\n",
        "                    download_file(img2img_image, img_path)\n",
        "        img2img_map = {\n",
        "            \"enable\": animate_diff_prefs['use_img2img'],\n",
        "            \"init_image_dir\": f\"img2img_image{slash}test\",\n",
        "            \"save_init_image\": True,\n",
        "            \"denoising_strength\": animate_diff_prefs['img2img_strength'],\n",
        "        }\n",
        "    motion_lora_map = {}\n",
        "    for m in animate_diff_prefs['motion_loras']:\n",
        "      for mm in animate_diff_motion_loras:\n",
        "        if mm['name'] == m:\n",
        "          mm_path = os.path.join(animatediff_dir, 'data', 'models', 'motion_lora')\n",
        "          if not os.path.isfile(os.path.join(mm_path, mm['file'])):\n",
        "              installer.status(f\"...downloading {mm['name']}\")\n",
        "              download_file(mm['path'], to=mm_path, filename=mm['file'], ext=\"ckpt\")\n",
        "          motion_lora_map[f\"models{slash}motion_lora{slash}{mm['file']}\"] = animate_diff_prefs['motion_loras_strength']\n",
        "    \n",
        "    installer.status(\"...preparing json\")\n",
        "    upscale_config = {\n",
        "      \"scheduler\": animate_diff_prefs['scheduler'],\n",
        "      \"steps\": animate_diff_prefs['upscale_steps'],\n",
        "      \"strength\": animate_diff_prefs['upscale_strength'],\n",
        "      \"guidance_scale\": animate_diff_prefs['upscale_guidance_scale'],\n",
        "      \"controlnet_tile\": {\n",
        "        \"enable\": animate_diff_prefs['upscale_tile'],\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"guess_mode\": False,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0\n",
        "      },\n",
        "      \"controlnet_line_anime\": {\n",
        "        \"enable\": animate_diff_prefs['upscale_lineart_anime'],\n",
        "        \"controlnet_conditioning_scale\": 1.0,\n",
        "        \"guess_mode\": False,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0\n",
        "      },\n",
        "      \"controlnet_ip2p\": {\n",
        "        \"enable\": animate_diff_prefs['upscale_ip2p'],\n",
        "        \"controlnet_conditioning_scale\": 0.5,\n",
        "        \"guess_mode\": False,\n",
        "        \"control_guidance_start\": 0.0,\n",
        "        \"control_guidance_end\": 1.0\n",
        "      },\n",
        "      \"controlnet_ref\": {\n",
        "        \"enable\": bool(ref_image),\n",
        "        \"use_frame_as_ref_image\": False,\n",
        "        \"use_1st_frame_as_ref_image\": True,\n",
        "        \"ref_image\": ref_image,\n",
        "        \"attention_auto_machine_weight\": 1.0,\n",
        "        \"gn_auto_machine_weight\": 1.0,\n",
        "        \"style_fidelity\": 0.25,\n",
        "        \"reference_attn\": True,\n",
        "        \"reference_adain\": False\n",
        "      }\n",
        "    }\n",
        "    prompts_json['lora_map'] = lora_map\n",
        "    prompts_json['motion_lora_map'] = motion_lora_map\n",
        "    prompts_json['controlnet_map'] = controlnet_map\n",
        "    prompts_json['ip_adapter_map'] = ip_adapter_map\n",
        "    if animate_diff_prefs['use_img2img']:\n",
        "        prompts_json['img2img_map'] = img2img_map\n",
        "    prompts_json['upscale_config'] = upscale_config\n",
        "    #if bool(lora_path):\n",
        "    #    prompts_json['lora_scale'] = animate_diff_prefs['lora_alpha']\n",
        "    json_file = os.path.join(animatediff_dir, \"config\", \"prompts\", \"aeionic_prompt.json\")\n",
        "    cli_file = os.path.join(animatediff_dir, \"src\", \"animatediff\")\n",
        "    out_dir = os.path.join(animatediff_dir, \"output\")\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(prompts_json, outfile, indent=4)\n",
        "    #cmd = f\"python -m scripts.animate --config {yaml_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}\"\n",
        "    cmd2 = f\"python -m cli --config-path {json_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}\"\n",
        "    cmd2 += f\" --L {animate_diff_prefs['video_length']} --W {animate_diff_prefs['width']} --H {animate_diff_prefs['height']}\"\n",
        "    cmd = f\"animatediff generate --config-path {json_file} --model-path {sd_models}\"\n",
        "    cmd += f\" -L {animate_diff_prefs['video_length']} -W {animate_diff_prefs['width']} -H {animate_diff_prefs['height']} -C {context} -S {animate_diff_prefs['stride']}\"\n",
        "    if animate_diff_prefs['save_gif']:\n",
        "        cmd += \" --save-merged\"\n",
        "    if animate_diff_prefs['is_simple_composite']:\n",
        "        cmd += \" --simple_composite\"\n",
        "    #cmd += f\" -O {animate_diff_prefs['overlap']}\"\n",
        "    w = 0\n",
        "    h = 0\n",
        "    frame_dir = \"\"\n",
        "    output_dir = \"\"\n",
        "    output_dirs = []\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import LoggingEventHandler, FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self, event):\n",
        "        nonlocal img_idx, w, h, output_dir\n",
        "        if event.is_directory:\n",
        "          output_dir = event.src_path\n",
        "          output_dirs.append(event.src_path)\n",
        "          return None\n",
        "        elif event.event_type == 'created' and (event.src_path.endswith(\"png\") or event.src_path.endswith(\"gif\") or event.src_path.endswith(\"jpg\")):\n",
        "          autoscroll(True)\n",
        "          if w == 0:\n",
        "            time.sleep(0.8)\n",
        "            try:\n",
        "              frame = PILImage.open(event.src_path)\n",
        "              w, h = frame.size\n",
        "              frame_dir = os.path.dirname(event.src_path)\n",
        "              #clear_last()\n",
        "            except Exception:\n",
        "              frame_dir = os.path.dirname(event.src_path)\n",
        "              pass\n",
        "          clear_last()\n",
        "          if animate_diff_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          else:\n",
        "            fpath = event.src_path\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          prt(progress)\n",
        "          if animate_diff_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "            shutil.copy(event.src_path, fpath)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "          img_idx += 1\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, out_dir, recursive=True)\n",
        "    observer.start()\n",
        "    #prt(f\"Running {cmd}\")\n",
        "    #console = RunConsole(show_progress=False)\n",
        "    #prt(console)\n",
        "    try:\n",
        "      print(f\"Running {cmd}\")\n",
        "      clear_last()\n",
        "      prt(\"Generating AnimateDiff of your Prompts... See console for progress.\")\n",
        "      prt(progress)\n",
        "      time.sleep(0.5)\n",
        "      autoscroll(False)\n",
        "      run_sp(cmd, cwd=animatediff_dir, realtime=True)\n",
        "      #    pass\n",
        "      #console.run_process(cmd, cwd=animatediff_dir)\n",
        "      #run_sp(cmd, cwd=animatediff_dir, realtime=True)\n",
        "      #print(f\"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}\")\n",
        "      #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      observer.stop()\n",
        "      alert_msg(page, f\"ERROR: Couldn't run AnimateDiff for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    if animate_diff_prefs['upscale_tile'] or animate_diff_prefs['upscale_ip2p'] or animate_diff_prefs['upscale_lineart_anime']:\n",
        "        u_w = int(w * float(animate_diff_prefs['width']))\n",
        "        u_h = int(h * float(animate_diff_prefs['height']))\n",
        "        upscale_cmd = f\"animatediff tile-upscale {Path(frame_dir)} -c {json_file} -W {u_w} -H {u_h}\"\n",
        "        print(f\"Running {upscale_cmd}\")\n",
        "        prt(\"Upscaling AnimateDiff of your Frames... See console for progress.\")\n",
        "        try:\n",
        "            run_sp(upscale_cmd, cwd=animatediff_dir, realtime=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            observer.stop()\n",
        "            alert_msg(page, f\"ERROR: Couldn't run Tile-Upscale on AnimateDiff for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    time.sleep(3)\n",
        "    observer.stop()\n",
        "    #filename = f\"{format_filename(editing_prompts[0]['prompt'])}\"\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    if animate_diff_prefs['save_video']:\n",
        "        class VidHandler(FileSystemEventHandler):\n",
        "          def __init__(self):\n",
        "            super().__init__()\n",
        "          def on_created(self, event):\n",
        "            nonlocal w, h, output_dir\n",
        "            if event.event_type == 'created' and (event.src_path.endswith(\"mp4\") or event.src_path.endswith(\"avi\") or event.src_path.endswith(\"webm\")):\n",
        "              autoscroll(True)\n",
        "              #clear_last()\n",
        "              prt(Divider(height=6, thickness=2))\n",
        "              fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "              time.sleep(1)\n",
        "              shutil.copy(event.src_path, fpath)\n",
        "              prt(f\"Video saved to {fpath} from {event.src_path}\")\n",
        "              #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))\n",
        "              #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))\n",
        "              #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "              #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "              page.update()\n",
        "              time.sleep(0.2)\n",
        "              autoscroll(False)\n",
        "        video_handler = VidHandler()\n",
        "        vidobserver = Observer()\n",
        "        vidobserver.schedule(video_handler, out_dir, recursive=True)\n",
        "        vidobserver.start()\n",
        "        for dir in output_dirs:\n",
        "            if len(os.listdir(dir)) < 4:\n",
        "                continue\n",
        "            interpolate_cmd = f\"animatediff rife interpolate --temporal-tta --uhd {Path(dir)}\" #--out_file --codec VideoCodec.h264\n",
        "            try:\n",
        "              installer = Installing(\"Running Google FILM: Frame Interpolation for Large Motion...\")\n",
        "              prt(installer)\n",
        "              out_file = available_file(out_dir, \"interpolated\", no_num=True, ext=\"mp4\")\n",
        "              interpolate_video(dir, input_fps=8, output_fps=30, output_video=out_file, installer=installer)\n",
        "              installer.set_message(\"Running RiFE Temporal Video Interpolation...\")\n",
        "              run_sp(interpolate_cmd, cwd=animatediff_dir, realtime=True)\n",
        "              installer.show_progress(False)\n",
        "              #print(f\"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}\")\n",
        "              #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            except Exception as e:\n",
        "              #clear_last()\n",
        "              alert_msg(page, f\"ERROR: Couldn't interpolate video with RiFE, but frames & gif still saved...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "              pass\n",
        "        time.sleep(2)\n",
        "        vidobserver.stop()\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_hotshot_xl(page):\n",
        "    global hotshot_xl_prefs, prefs, status, pipe_hotshot_xl, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.HotshotXL.controls.append(line)\n",
        "      page.HotshotXL.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.HotshotXL, lines=lines)\n",
        "    def clear_list():\n",
        "      page.HotshotXL.controls = page.HotshotXL.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.HotshotXL.auto_scroll = scroll\n",
        "      page.HotshotXL.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = hotshot_xl_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Hotshot-XL Text-To-Video Pipeline...\")\n",
        "    prt(installer)\n",
        "    #model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    clear_pipes()\n",
        "    hotshot_xl_dir = os.path.join(root_dir, \"Hotshot-XL\")\n",
        "    if not os.path.exists(hotshot_xl_dir):\n",
        "        installer.status(\"...hotshotco/Hotshot-XL\")\n",
        "        run_sp(\"git clone https://github.com/hotshotco/Hotshot-XL\", realtime=False, cwd=root_dir)\n",
        "        #run_sp(\"git clone https://github.com/Skquark/Hotshot-XL\", realtime=False, cwd=root_dir)\n",
        "    try:\n",
        "        pip_install(\"appdirs==1.4.4 certifi==2023.7.22 charset-normalizer==3.3.0 click==8.1.7 cmake decorator==4.4.2 docker-pycreds==0.4.0 einops filelock==3.12.4 fsspec==2023.9.2 gitdb==4.0.10 GitPython==3.1.37 idna==3.4 imageio imageio-ffmpeg importlib-metadata==6.8.0 Jinja2==3.1.2 lit==17.0.2 MarkupSafe==2.1.3 moviepy mpmath==1.3.0 networkx==3.1 numpy pathtools proglog==0.1.10 protobuf==4.24.3 psutil PyYAML regex safetensors sentry-sdk==1.31.0 setproctitle==1.3.3 six==1.16.0 smmap==5.0.1 sympy==1.12 tokenizers==0.14.0 tqdm transformers triton typing_extensions urllib3 wandb zipp==3.17.0\", installer=installer)\n",
        "        #pip_install(\"nvidia-cublas-cu11==11.10.3.66 nvidia-cuda-cupti-cu11==11.7.101 nvidia-cuda-nvrtc-cu11==11.7.99 nvidia-cuda-runtime-cu11==11.7.99 nvidia-cudnn-cu11==8.5.0.96 nvidia-cufft-cu11==10.9.0.58 nvidia-curand-cu11==10.2.10.91 nvidia-cusolver-cu11==11.4.0.1 nvidia-cusparse-cu11==11.7.4.91 nvidia-nccl-cu11==2.14.3 nvidia-nvtx-cu11==11.7.91\")\n",
        "        run_sp(\"apt-get install git-lfs\", realtime=False)\n",
        "        run_sp(\"git lfs install\", realtime=False)\n",
        "        #installer.status(\"...huggingface hotshotco/Hotshot-XL\")\n",
        "        #run_sp(\"git clone https://huggingface.co/hotshotco/Hotshot-XL\", realtime=False, cwd=root_dir)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Hotshot-XL Text-To-Video requirements failed for some reason...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    try:\n",
        "        import RealESRGAN\n",
        "    except:\n",
        "        installer.status(\"...istalling Real-ESRGAN\")\n",
        "        run_sp(\"pip install git+https://github.com/sberbank-ai/Real-ESRGAN.git\", realtime=False)\n",
        "        pass\n",
        "    hotshot_lora = os.path.join(hotshot_xl_dir, 'lora')\n",
        "    hotshot_input = os.path.join(hotshot_xl_dir, 'input')\n",
        "    hotshot_output = os.path.join(hotshot_xl_dir, 'output')\n",
        "    os.makedirs(hotshot_lora, exist_ok=True)\n",
        "    os.makedirs(hotshot_input, exist_ok=True)\n",
        "    os.makedirs(hotshot_output, exist_ok=True)\n",
        "    #hsxl = \"hsxl_temporal_layers.f16.safetensors\"\n",
        "    #hotshot_model = os.path.join(hotshot_xl_dir, hsxl)\n",
        "    #if not os.path.isfile(hotshot_model):\n",
        "    #    installer.status(\"...downloading hsxl_temporal_layers\")\n",
        "    #    hotshot_model = download_file(\"https://huggingface.co/hotshotco/Hotshot-XL/resolve/main/hsxl_temporal_layers.f16.safetensors?download=true\", to=hotshot_xl_dir, filename=hsxl)\n",
        "    lora = hotshot_xl_prefs['lora_layer']\n",
        "    lora_path = \"\"\n",
        "    if lora != \"None\":\n",
        "        lora_model = get_SDXL_LoRA_model(lora_model)\n",
        "        installer.status(f\"...getting LoRA {lora_model['name']}\")\n",
        "        if lora == \"Custom\":\n",
        "            lora_model['name'] = \"Custom SDXL LoRA\"\n",
        "            lora_model['path'] = hotshot_xl_prefs['custom_lora_layer']\n",
        "            if lora_model['path'].count('/') == 1:\n",
        "                from huggingface_hub import HfFileSystem\n",
        "                fs = HfFileSystem()\n",
        "                files = fs.ls(lora_model['path'], detail=False)\n",
        "                lora_model['weights'] = [file.rpartition('/')[2] for file in files if file.endswith(\".safetensors\")]\n",
        "        if lora_model['path'].count('/') == 1:\n",
        "            lora_path = download_file(f\"https://huggingface.co/{lora_model['path']}/blob/main/{lora_model['weights']}\", to=hotshot_lora)\n",
        "        elif os.path.isfile(lora_model['path']):\n",
        "            lora_path = lora_model['path']\n",
        "        elif lora_model['path'].startswith(\"http\"):\n",
        "            lora_path = download_file(lora_model['path'], to=hotshot_lora, ext=\"safetensors\")\n",
        "        else:\n",
        "            print(f\"Couldn't download LoRA {lora_model['path']}\")\n",
        "    \n",
        "    local_output = os.path.join(stable_dir, hotshot_xl_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(local_output):\n",
        "        os.makedirs(local_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], hotshot_xl_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "        os.makedirs(batch_output)\n",
        "    width = hotshot_xl_prefs['width']\n",
        "    height = hotshot_xl_prefs['height']\n",
        "    filename = format_filename(hotshot_xl_prefs[\"prompt\"])\n",
        "    out_file = f\"{filename}.{'mp4' if hotshot_xl_prefs['export_to_video'] else 'gif'}\"\n",
        "    x = \" --xformers\" if status['installed_xformers'] else \"\"\n",
        "    lora_arg = f' --lora \"lora/{os.path.basename(lora_path)}\"' if bool(lora_path) else \"\"\n",
        "    upscale = f' --upscale {int(hotshot_xl_prefs[\"enlarge_scale\"])}' if hotshot_xl_prefs[\"apply_ESRGAN_upscale\"] else ''\n",
        "    scale = int(hotshot_xl_prefs[\"enlarge_scale\"]) if hotshot_xl_prefs[\"apply_ESRGAN_upscale\"] else 1\n",
        "    if bool(hotshot_xl_prefs[\"gif\"]):\n",
        "        if os.path.isfile(hotshot_xl_prefs[\"gif\"]):\n",
        "            shutil.copy(hotshot_xl_prefs[\"gif\"], os.path.join(hotshot_input, os.path.basename(hotshot_xl_prefs[\"gif\"])))\n",
        "    gif = f' --control_type {hotshot_xl_prefs[\"controlnet_type\"].lower()} --controlnet_conditioning_scale {hotshot_xl_prefs[\"conditioning_scale\"]} --control_guidance_start {hotshot_xl_prefs[\"control_guidance_start\"]} --control_guidance_end {hotshot_xl_prefs[\"control_guidance_end\"]} --gif \"input/{os.path.basename(hotshot_xl_prefs[\"gif\"])}\"' if bool(hotshot_xl_prefs[\"gif\"]) else ''\n",
        "    clear_last()\n",
        "    for num in range(hotshot_xl_prefs[\"num_images\"]):\n",
        "        random_seed = (int(hotshot_xl_prefs['seed']) + num) if int(hotshot_xl_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        prt(\"Generating Hotshot-XL of your Prompt...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        try: # --pretrained_path \"{os.path.basename(hotshot_model)}\"\n",
        "            run_sp(f'python inference.py --prompt \"{hotshot_xl_prefs[\"prompt\"]}\" --negative_prompt \"{hotshot_xl_prefs[\"negative_prompt\"]}\" --width {width} --height {height} --seed {random_seed} --steps {hotshot_xl_prefs[\"num_inference_steps\"]} --video_length {hotshot_xl_prefs[\"video_length\"]} --video_duration {hotshot_xl_prefs[\"video_duration\"]} --scheduler {hotshot_xl_prefs[\"scheduler\"]}{gif}{\" --low_vram_mode\" if not prefs[\"higher_vram_mode\"] else \"\"}{x}{lora_arg}{upscale} --output \"output/{out_file}\"', realtime=True, cwd=hotshot_xl_dir)\n",
        "            #print(f\"prompt={hotshot_xl_prefs['prompt']}, negative_prompt={hotshot_xl_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={hotshot_xl_prefs['edit_momentum_scale']}, edit_mom_beta={hotshot_xl_prefs['edit_mom_beta']}, num_inference_steps={hotshot_xl_prefs['num_inference_steps']}, eta={hotshot_xl_prefs['eta']}, guidance_scale={hotshot_xl_prefs['guidance_scale']}\")\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Hotshot-XL Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        video_path = os.path.join(hotshot_output, out_file)\n",
        "        if not os.path.isfile(video_path):\n",
        "            prt(f\"Problem creating file {video_path}\")\n",
        "            return\n",
        "        output_path = os.path.join(prefs['image_output'], rerender_a_video_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(output_path):\n",
        "            os.makedirs(output_path)\n",
        "        output_file = available_file(output_path, out_file, 0, ext=\"mp4\" if hotshot_xl_prefs['export_to_video'] else 'gif', no_num=True)\n",
        "        shutil.copy(video_path, output_file)\n",
        "        if hotshot_xl_prefs['export_to_video']:\n",
        "            prt(f\"Saved Video file to {output_file}\")\n",
        "        else:\n",
        "            prt(Row([ImageButton(src=output_file, data=output_file, width=width * scale, height=height * scale, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    #prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(f\"Done creating video... Check {batch_output}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_rerender_a_video(page):\n",
        "    global rerender_a_video_prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.Rerender_a_video.controls.append(line)\n",
        "      page.Rerender_a_video.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Rerender_a_video, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "        page.Rerender_a_video.auto_scroll = scroll\n",
        "        page.Rerender_a_video.update()\n",
        "    if not bool(rerender_a_video_prefs['init_video']):\n",
        "        alert_msg(page, \"You must provide a target init video...\")\n",
        "        return\n",
        "    if not bool(rerender_a_video_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide an interesting prompt to guide the video...\")\n",
        "        return\n",
        "    if not bool(rerender_a_video_prefs['batch_folder_name']):\n",
        "        alert_msg(page, \"You must give a unique Batch Folder Name to save to...\")\n",
        "        return\n",
        "    page.Rerender_a_video.controls = page.Rerender_a_video.controls[:1]\n",
        "    autoscroll()\n",
        "    installer = Installing(\"Installing Rerender-a-Video Libraries...\")\n",
        "    prt(installer)\n",
        "    rerender_a_video_dir = os.path.join(root_dir, \"Rerender_A_Video\")\n",
        "    if not os.path.exists(rerender_a_video_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning williamyang1991/Rerender_A_Video\")\n",
        "            run_sp(\"git clone https://github.com/williamyang1991/Rerender_A_Video.git --recursive\", cwd=root_dir, realtime=False)\n",
        "            installer.status(\"...installing Rerender_A_Video requirements\")\n",
        "            #run_sp(\"pip install -r requirements.txt\", realtime=True) #pytorch-lightning==1.5.0\n",
        "            pip_install(\"addict==2.4.0 albumentations==1.3.0 basicsr==1.4.2 blendmodes einops gradio imageio imageio-ffmpeg invisible-watermark kornia==0.6 numba omegaconf open_clip_torch prettytable==3.6.0 pytorch-lightning safetensors streamlit==1.12.1 streamlit-drawable-canvas==0.8.0 test-tube==0.7.5 timm torchmetrics transformers webdataset yapf==0.32.0 watchdog\", installer=installer, upgrade=True)\n",
        "            installer.status(\"...downloading SD models\")\n",
        "            run_sp(\"python install.py\", cwd=rerender_a_video_dir, realtime=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Rerender_A_Video Requirements:\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    #import logging\n",
        "    #logging.set_verbosity_error()\n",
        "    clear_pipes()\n",
        "\n",
        "    from PIL import ImageOps\n",
        "    if bool(rerender_a_video_prefs['output_name']):\n",
        "        fname = format_filename(rerender_a_video_prefs['output_name'], force_underscore=True)\n",
        "    elif bool(rerender_a_video_prefs['prompt']):\n",
        "        fname = format_filename(rerender_a_video_prefs['prompt'], force_underscore=True)\n",
        "    elif bool(rerender_a_video_prefs['batch_folder_name']):\n",
        "        fname = format_filename(rerender_a_video_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else: fname = \"output\"\n",
        "    if bool(rerender_a_video_prefs['file_prefix']):\n",
        "        fname = f\"{rerender_a_video_prefs['file_prefix']}{fname}\"\n",
        "    video_dir = os.path.join(rerender_a_video_dir, \"videos\")\n",
        "    #if bool(rerender_a_video_prefs['batch_folder_name']):\n",
        "    #    batch_output = os.path.join(stable_dir, rerender_a_video_prefs['batch_folder_name'])\n",
        "    #else: batch_output = stable_dir\n",
        "    #if not os.path.exists(batch_output):\n",
        "    #    os.makedirs(batch_output)\n",
        "    output_path = os.path.join(prefs['image_output'], rerender_a_video_prefs['batch_folder_name'])\n",
        "    make_dir(output_path)\n",
        "    init_vid = rerender_a_video_prefs['init_video']\n",
        "    if init_vid.startswith('http'):\n",
        "        init_vid = download_file(init_vid, uploads_dir, ext=\"mp4\")\n",
        "    else:\n",
        "        if not os.path.isfile(init_vid):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "            return\n",
        "    video_file = os.path.basename(init_vid)\n",
        "    if not video_file.endswith(\"mp4\"):\n",
        "        video_file += \".mp4\"\n",
        "    shutil.copy(init_vid, os.path.join(video_dir, video_file))\n",
        "    video_out_path = os.path.join(video_dir, rerender_a_video_prefs['batch_folder_name'])\n",
        "    make_dir(video_out_path)\n",
        "    lora_dir = os.path.join(rerender_a_video_dir, 'models')\n",
        "    make_dir(lora_dir)\n",
        "    lora_path = \"\"\n",
        "    if rerender_a_video_prefs['dreambooth_lora'] == \"Custom\":\n",
        "        lora = rerender_a_video_prefs['custom_lora']\n",
        "        if lora.startswith(\"http\"):\n",
        "            installer.status(f\"...downloading Custom LoRA\")\n",
        "            lora_file = download_file(lora_model['path'], to=lora_dir, ext=\"safetensors\")\n",
        "            if os.path.isfile(lora_file):\n",
        "                lora_path = lora_file\n",
        "        else:\n",
        "            if os.path.isfile(lora):\n",
        "                fname = os.path.basename(lora)\n",
        "                lora_path = os.path.join(lora_dir, fname)\n",
        "                shutil.copy(lora, lora_path)\n",
        "    else:\n",
        "        for lora in animate_diff_loras:\n",
        "            if lora['name'] == rerender_a_video_prefs['dreambooth_lora']:\n",
        "                lora_model = lora\n",
        "                break\n",
        "        lora_path = os.path.join(lora_dir, lora_model['file'])\n",
        "        if not os.path.isfile(lora_path):\n",
        "            installer.status(f\"...downloading {lora_model['name']}\")\n",
        "            download_file(lora_model['path'], to=lora_dir, ext=\"safetensors\")\n",
        "    sd_model = os.path.basename(lora_path)\n",
        "    installer.status(\"...preparing json\")\n",
        "    random_seed = int(rerender_a_video_prefs['seed']) if int(rerender_a_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    '''total_steps = rerender_a_video_prefs['steps']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()'''\n",
        "    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)\n",
        "    config_json_file = os.path.join(rerender_a_video_dir, \"config\", \"aeionic_rerender.json\")\n",
        "    config_json = {\n",
        "        \"input\": f\"videos/{video_file}\",\n",
        "        \"output\": f\"videos/{rerender_a_video_prefs['batch_folder_name']}/{os.path.basename(output_file)}\",\n",
        "        \"work_dir\": f\"videos/{rerender_a_video_prefs['batch_folder_name']}\",\n",
        "        \"key_subdir\": \"keys\",\n",
        "        \"sd_model\": f\"models/{sd_model}\", #realisticVisionV20_v20.safetensors\",\n",
        "        \"frame_count\": rerender_a_video_prefs['frame_count'],\n",
        "        \"interval\": rerender_a_video_prefs['interval'],\n",
        "        \"crop\": [\n",
        "            rerender_a_video_prefs['crop']['left'],\n",
        "            rerender_a_video_prefs['crop']['right'],\n",
        "            rerender_a_video_prefs['crop']['top'],\n",
        "            rerender_a_video_prefs['crop']['bottom']\n",
        "        ],\n",
        "        \"prompt\": rerender_a_video_prefs['prompt'],\n",
        "        \"a_prompt\": rerender_a_video_prefs['a_prompt'],#\"RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\",\n",
        "        \"n_prompt\": rerender_a_video_prefs['negative_prompt'],\n",
        "        \"x0_strength\": rerender_a_video_prefs['x0_strength'],\n",
        "        \"control_type\": rerender_a_video_prefs['control_task'] if rerender_a_video_prefs['control_task'] == \"HED\" else rerender_a_video_prefs['control_task'].lower(),\n",
        "        \"canny_low\": rerender_a_video_prefs['low_threshold'],\n",
        "        \"canny_high\": rerender_a_video_prefs['high_threshold'],\n",
        "        \"control_strength\": rerender_a_video_prefs['controlnet_strength'],\n",
        "        \"style_update_freq\": rerender_a_video_prefs['style_update_freq'],\n",
        "        \"loose_cfattn\": rerender_a_video_prefs['loose_cfattn'],\n",
        "        \"inner_strength\": rerender_a_video_prefs['inner_strength'],\n",
        "        \"smooth_boundary\": rerender_a_video_prefs['smooth_boundary'],\n",
        "        \"color_preserve\": rerender_a_video_prefs['color_preserve'],\n",
        "        \"loose_cfattn\": rerender_a_video_prefs['loose_cfattn'],\n",
        "        \"seed\": random_seed,\n",
        "        \"image_resolution\": rerender_a_video_prefs['max_size'],\n",
        "        \"warp_period\": [0, 0.1],\n",
        "        \"ada_period\": [0.8, 1],\n",
        "    }\n",
        "    if rerender_a_video_prefs['enable_freeu']:\n",
        "        config_json['freeu_args'] = [rerender_a_video_prefs['freeu_args']['b1'], rerender_a_video_prefs['freeu_args']['b2'], rerender_a_video_prefs['freeu_args']['s1'], rerender_a_video_prefs['freeu_args']['s2']]\n",
        "    with open(config_json_file, \"w\") as outfile:\n",
        "        json.dump(config_json, outfile, indent=4)\n",
        "    #output_file = os.path.join(output_path, f\"{fname}{'.mp4' if is_video else '.png'}\")\n",
        "    if not os.path.exists(config_json_file):\n",
        "        print(f\"Error creating json file {config_json_file}\")\n",
        "    clear_last()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(f\"Generating your Rerender-a-Video...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    cmd = f'python rerender.py --cfg config/aeionic_rerender.json -{\"nb\" if rerender_a_video_prefs[\"first_frame\"] else \"nr\"}'\n",
        "    w = 0\n",
        "    h = 0\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self, event):\n",
        "        nonlocal img_idx, w, h\n",
        "        if event.is_directory:\n",
        "          return None\n",
        "        elif event.event_type == 'created' and event.src_path.endswith(\"png\"):\n",
        "          autoscroll(True)\n",
        "          if w == 0:\n",
        "            time.sleep(0.8)\n",
        "            try:\n",
        "              frame = PILImage.open(event.src_path)\n",
        "              w, h = frame.size\n",
        "              clear_last()\n",
        "            except Exception:\n",
        "              pass\n",
        "          clear_last()\n",
        "          if rerender_a_video_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          else:\n",
        "            fpath = event.src_path\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          prt(progress)\n",
        "          if rerender_a_video_prefs['save_frames']:\n",
        "            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "            shutil.copy(event.src_path, fpath)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "          img_idx += 1\n",
        "        elif event.event_type == 'created' and (event.src_path.endswith(\"mp4\") or event.src_path.endswith(\"avi\")):\n",
        "          autoscroll(True)\n",
        "          #clear_last()\n",
        "          prt(Divider(height=6, thickness=2))\n",
        "          fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])\n",
        "          time.sleep(1)\n",
        "          shutil.copy(event.src_path, fpath)\n",
        "          prt(f\"Video saved to {fpath} from {event.src_path}\")\n",
        "          #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          #page.update()Image\n",
        "          #prt(progress)\n",
        "          time.sleep(0.2)\n",
        "          autoscroll(False)\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, video_out_path, recursive=True)\n",
        "    observer.start()\n",
        "    #prt(f\"Running {cmd}\")\n",
        "    #prt(progress)\n",
        "    try:\n",
        "        #os.system(f'cd {rerender_a_video_dir};{cmd}')\n",
        "        #if is_Colab:\n",
        "        #  os.chdir(rerender_a_video_dir)\n",
        "        #  $cmd\n",
        "        #  os.chdir(root_dir)\n",
        "        #else:\n",
        "        #TODO: Parse output to get percent current for progress callback_fnc\n",
        "        run_sp(cmd, cwd=rerender_a_video_dir, realtime=True)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        observer.stop()\n",
        "        alert_msg(page, \"Error running rerender.py!\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    #clear_last()\n",
        "    observer.stop()\n",
        "    #clear_last()\n",
        "    autoscroll(True)\n",
        "    #TODO: Upscale Image\n",
        "    if os.path.isfile(output_file):\n",
        "        prt(f\"Saved video to {output_file}\")\n",
        "        #prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))\n",
        "    else:\n",
        "        prt(\"Error Generating Output File! Maybe NSFW Image detected or Out of Memory?\")\n",
        "    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_materialdiffusion(page):\n",
        "    global materialdiffusion_prefs, prefs\n",
        "    if not bool(materialdiffusion_prefs['material_prompt']):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your material...\")\n",
        "      return\n",
        "    if not bool(prefs['Replicate_api_key']):\n",
        "      alert_msg(page, \"You must provide your Replicate API Token in Settings to process your material...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.MaterialDiffusion.controls.append(line)\n",
        "      page.MaterialDiffusion.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.MaterialDiffusion, lines=lines)\n",
        "    def clear_list():\n",
        "      page.MaterialDiffusion.controls = page.MaterialDiffusion.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.MaterialDiffusion.auto_scroll = scroll\n",
        "      page.MaterialDiffusion.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    '''try:\n",
        "      run_sp(\"pip install git+https://github.com/TomMoore515/material_stable_diffusion.git@main#egg=predict\", realtime=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        #alert_msg(page, f\"Error installing Material Diffusion from TomMoore515...\", content=Text(str(e)))\n",
        "        pass'''\n",
        "    prt(Installing(\"Installing Replicate Material Diffusion Pipeline...\"))\n",
        "    try:\n",
        "        import replicate\n",
        "    except ModuleNotFoundError as e:\n",
        "        run_process(\"pip install replicate -qq\", realtime=False)\n",
        "        import replicate\n",
        "        pass\n",
        "    os.environ[\"REPLICATE_API_TOKEN\"] = prefs['Replicate_api_key']\n",
        "    #export REPLICATE_API_TOKEN=\n",
        "    try:\n",
        "        rep_model = replicate.models.get(\"tommoore515/material_stable_diffusion\")\n",
        "        rep_version = rep_model.versions.get(\"3b5c0242f8925a4ab6c79b4c51e9b4ce6374e9b07b5e8461d89e692fd0faa449\")\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Seems like your Replicate API Token is Invalid. Check it again...\", content=Text(str(e)))\n",
        "        return\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    init_img = None\n",
        "    if bool(materialdiffusion_prefs['init_image']):\n",
        "        if materialdiffusion_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(materialdiffusion_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(materialdiffusion_prefs['init_image']):\n",
        "                init_img = PILImage.open(materialdiffusion_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {materialdiffusion_prefs['init_image']}\")\n",
        "                return\n",
        "        #width, height = init_img.size\n",
        "        #width, height = scale_dimensions(materialdiffusion_prefs['width'], materialdiffusion_prefs['height'])\n",
        "        init_img = init_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    mask_img = None\n",
        "    if bool(materialdiffusion_prefs['mask_image']):\n",
        "        if materialdiffusion_prefs['mask_image'].startswith('http'):\n",
        "            mask_img = PILImage.open(requests.get(materialdiffusion_prefs['mask_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(materialdiffusion_prefs['mask_image']):\n",
        "                mask_img = PILImage.open(materialdiffusion_prefs['mask_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your mask_image {materialdiffusion_prefs['mask_image']}\")\n",
        "                return\n",
        "            if materialdiffusion_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "                mask_img = mask_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.NEAREST)\n",
        "                mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    prt(\"Generating your Material Diffusion Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(materialdiffusion_prefs['seed']) if int(materialdiffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    #input = {'prompt':materialdiffusion_prefs['material_prompt'], 'width':materialdiffusion_prefs['width'], 'height':materialdiffusion_prefs['height'], 'init_image':init_img, 'mask':mask_img, 'prompt_strength':materialdiffusion_prefs['prompt_strength'], 'num_outputs':materialdiffusion_prefs['num_outputs'], 'num_inference_steps':materialdiffusion_prefs['steps'], 'guidance_scale':materialdiffusion_prefs['guidance_scale'], 'seed':random_seed}\n",
        "    try:\n",
        "        if bool(init_img) and bool(mask_img):\n",
        "            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, mask=mask_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)\n",
        "        elif bool(init_img):\n",
        "            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)\n",
        "            #images = version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img if bool(init_img) else \"\", mask=mask_img if bool(mask_img) else \"\", prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)\n",
        "        else:\n",
        "            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, f\"ERROR: Couldn't create your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "        return\n",
        "    autoscroll(True)\n",
        "    clear_last(2)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        random_seed += idx\n",
        "        fname = format_filename(materialdiffusion_prefs['material_prompt'])\n",
        "        seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{materialdiffusion_prefs[\"file_prefix\"]}{fname}{seed_suffix}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(materialdiffusion_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, materialdiffusion_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        #image.save(image_path)\n",
        "        response = requests.get(image, stream=True)\n",
        "        with open(image_path, \"wb\") as f:\n",
        "          f.write(response.content)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not materialdiffusion_prefs['display_upscaled_image'] or not materialdiffusion_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        #if save_to_GDrive:\n",
        "        batch_output = os.path.join(prefs['image_output'], materialdiffusion_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(batch_output):\n",
        "            os.makedirs(batch_output)\n",
        "        if storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': materialdiffusion_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "\n",
        "        if materialdiffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            upscale_image(image_path, upscaled_path, scale=materialdiffusion_prefs[\"enlarge_scale\"])\n",
        "            image_path = upscaled_path\n",
        "            if materialdiffusion_prefs['display_upscaled_image']:\n",
        "                prt(Row([Img(src=upscaled_path, width=materialdiffusion_prefs['width'] * float(materialdiffusion_prefs[\"enlarge_scale\"]), height=materialdiffusion_prefs['height'] * float(materialdiffusion_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            try:\n",
        "              shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "            except shutil.SameFileError: pass\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    del rep_model, rep_version\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_DiT(page, from_list=False):\n",
        "    global DiT_prefs, pipe_DiT\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.DiT.controls.append(line)\n",
        "      if update:\n",
        "        page.DiT.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.DiT, lines=lines)\n",
        "    def clear_list():\n",
        "      page.DiT.controls = page.DiT.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DiT.auto_scroll = scroll\n",
        "      page.DiT.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = DiT_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    DiT_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        DiT_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(DiT_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      DiT_prompts.append(DiT_prefs['prompt'])\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes('DiT')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_DiT == None:\n",
        "        from diffusers import DiTPipeline\n",
        "        prt(Installing(\"Downloading DiT Pipeline...\"))\n",
        "        try:\n",
        "            pipe_DiT = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_DiT.to(torch_device)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading DiT Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_DiT.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if DiT_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating DiT{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, DiT_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    for pr in DiT_prompts:\n",
        "        for num in range(DiT_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(DiT_prefs['seed']) + num) if int(DiT_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            words = [w.strip() for w in pr.split(',')]\n",
        "            try:\n",
        "                ids = pipe_DiT.get_label_ids(words)\n",
        "            except ValueError as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"ImageNet Token not found...\", content=Text(str(e)))\n",
        "                pass\n",
        "            if len(ids) == 0:\n",
        "                clear_last()\n",
        "                alert_msg(\"No ImageNet class phases found in your prompt list. See full 1000 list for classifications.\", page)\n",
        "                return\n",
        "            try:\n",
        "                images = pipe_DiT(ids, num_inference_steps=DiT_prefs['num_inference_steps'], guidance_scale=DiT_prefs['guidance_scale'], generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running DiT Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, DiT_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not DiT_prefs['display_upscaled_image'] or not DiT_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if DiT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=DiT_prefs[\"enlarge_scale\"], face_enhance=DiT_prefs[\"face_enhance\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if DiT_prefs['display_upscaled_image']:\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(DiT_prefs[\"enlarge_scale\"]), height=512 * float(DiT_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {DiT_prefs['enlarge_scale']}x with ESRGAN\" if DiT_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"DiT\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr)\n",
        "                      config_json = DiT_prefs.copy()\n",
        "                      config_json['model_path'] = \"facebook/DiT-XL-2-512\"\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def get_dreamfusion(page):\n",
        "    os.chdir(root_dir)\n",
        "    run_process(\"git clone https://github.com/ashawkey/stable-dreamfusion.git -q\", page=page)\n",
        "    os.chdir(os.path.join(root_dir, \"stable-dreamfusion\"))\n",
        "    run_process(\"pip install -r requirements.txt -q\", page=page)\n",
        "    run_process(\"pip install git+https://github.com/NVlabs/nvdiffrast/ -q\", page=page)\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_dreamfusion(page):\n",
        "    global dreamfusion_prefs, status\n",
        "    def add_to_dreamfusion_output(o):\n",
        "      page.dreamfusion_output.controls.append(o)\n",
        "      page.dreamfusion_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.dreamfusion_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install HuggingFace Diffusers Pipeline before running...\")\n",
        "      return\n",
        "    if not bool(dreamfusion_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.dreamfusion_output.controls = []\n",
        "    page.dreamfusion_output.update()\n",
        "    if not status['installed_dreamfusion']:\n",
        "      add_to_dreamfusion_output(Installing(\"Installing Stable DreamFusion 3D Pipeline...\"))\n",
        "      get_dreamfusion(page)\n",
        "      status['installed_dreamfusion'] = True\n",
        "      clear_last()\n",
        "    def convert(seconds):\n",
        "      seconds = seconds % (24 * 3600)\n",
        "      hour = seconds // 3600\n",
        "      seconds %= 3600\n",
        "      minutes = seconds // 60\n",
        "      seconds %= 60\n",
        "      return \"%d:%02d\" % (hour, minutes)\n",
        "    estimate = convert(int(dreamfusion_prefs[\"training_iters\"] * 0.7))\n",
        "    add_to_dreamfusion_output(Text(\"Generating your 3D model, this'll take a while...  Estimating \" + estimate))\n",
        "    add_to_dreamfusion_output(ProgressBar())\n",
        "    df_path = os.path.join(root_dir, \"stable-dreamfusion\")\n",
        "    os.chdir(df_path)\n",
        "    run_str = f'python main.py -O --text \"{dreamfusion_prefs[\"prompt_text\"]}\" --workspace {dreamfusion_prefs[\"workspace\"]} --iters {dreamfusion_prefs[\"training_iters\"]} --lr {dreamfusion_prefs[\"learning_rate\"]} --w {dreamfusion_prefs[\"training_nerf_resolution\"]} --h {dreamfusion_prefs[\"training_nerf_resolution\"]} --seed {dreamfusion_prefs[\"seed\"]} --lambda_entropy {dreamfusion_prefs[\"lambda_entropy\"]} --ckpt {dreamfusion_prefs[\"checkpoint\"]} --save_mesh --max_steps {dreamfusion_prefs[\"max_steps\"]}'\n",
        "    print(run_str)\n",
        "    torch.cuda.empty_cache()\n",
        "    try:\n",
        "      run_process(run_str, page=page)\n",
        "    except:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running DreamFusion, probably Out of Memory. Adjust settings & try again.\")\n",
        "      return\n",
        "    clear_last()\n",
        "    add_to_dreamfusion_output(Text(\"Finished generating obj model, texture and video... Hope it's good.\"))\n",
        "    df_out = os.path.join(df_path, dreamfusion_prefs[\"workspace\"])\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      dreamfusion_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out', dreamfusion_prefs[\"workspace\"])\n",
        "      #os.makedirs(dreamfusion_out, exist_ok=True)\n",
        "      if os.path.exists(dreamfusion_out):\n",
        "        dreamfusion_out = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out'), dreamfusion_prefs[\"workspace\"], 1)\n",
        "      shutil.copytree(df_out, dreamfusion_out)\n",
        "      add_to_dreamfusion_output(Text(f\"Saved to {dreamfusion_out}\"))\n",
        "    else:\n",
        "      add_to_dreamfusion_output(Text(f\"Saved to {df_out}\"))\n",
        "    # TODO: PyDrive2\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "base_diffusion = upsampler_diffusion = point_sampler = None\n",
        "\n",
        "def run_point_e(page):\n",
        "    global point_e_prefs, status, base_diffusion, upsampler_diffusion, point_sampler\n",
        "    def add_to_point_e_output(o):\n",
        "      page.point_e_output.controls.append(o)\n",
        "      page.point_e_output.update()\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.point_e_output.controls.append(line)\n",
        "      page.point_e_output.update()\n",
        "    def prt_status(text):\n",
        "        nonlocal status_txt\n",
        "        status_txt.value = text\n",
        "        status_txt.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.point_e_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install HuggingFace Diffusers Pipeline before running...\")\n",
        "      return\n",
        "    if not bool(point_e_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.point_e_output.controls = []\n",
        "    page.point_e_output.update()\n",
        "    point_e_dir = os.path.join(root_dir, \"point-e\")\n",
        "    if not os.path.exists(point_e_dir):\n",
        "        add_to_point_e_output(Installing(\"Installing OpenAI Point-E 3D Library...\"))\n",
        "        try:\n",
        "            run_process(\"pip install -U scikit-image\")\n",
        "            run_process(\"git clone https://github.com/openai/point-e.git\", cwd=root_dir)\n",
        "            run_process(\"pip install .\", cwd=point_e_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Point-E Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "    clear_pipes()\n",
        "    from tqdm.auto import tqdm\n",
        "    from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
        "    from point_e.util.pc_to_mesh import marching_cubes_mesh\n",
        "    from point_e.diffusion.sampler import PointCloudSampler\n",
        "    from point_e.models.download import load_checkpoint\n",
        "    from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
        "    from point_e.util.plotting import plot_point_cloud\n",
        "    from point_e.util.point_cloud import PointCloud\n",
        "    import skimage.measure\n",
        "    from PIL import ImageOps\n",
        "\n",
        "    if bool(point_e_prefs['batch_folder_name']):\n",
        "        fname = format_filename(point_e_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        if bool(point_e_prefs['prompt_text']):\n",
        "            fname = format_filename(point_e_prefs['prompt_text'])\n",
        "        else:\n",
        "            alert_msg(page, \"If you're not using Prompt Text, provide a name for your 3D Model.\")\n",
        "            return\n",
        "    filename = format_filename(point_e_prefs['prompt_text'])\n",
        "    #fname = f\"{point_e_prefs['file_prefix']}{fname}\"\n",
        "    if bool(point_e_prefs['batch_folder_name']):\n",
        "        point_e_out = os.path.join(point_e_dir, point_e_prefs['batch_folder_name'])\n",
        "    else:\n",
        "        point_e_out = point_e_dir\n",
        "    if not os.path.exists(point_e_out):\n",
        "        os.makedirs(point_e_out)\n",
        "    #point_e_out = os.path.join(point_e_out, fname)\n",
        "    #estimate = convert(int(point_e_prefs[\"training_iters\"] * 0.7))\n",
        "    init_img = None\n",
        "    if bool(point_e_prefs['init_image']):\n",
        "        if point_e_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(point_e_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(point_e_prefs['init_image']):\n",
        "                init_img = PILImage.open(point_e_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {point_e_prefs['init_image']}\")\n",
        "                if not bool(point_e_prefs['prompt_text']):\n",
        "                    return\n",
        "        if init_img != None:\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, 960)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "\n",
        "    status_txt = Text(\"Generating your 3D model, may take a while...\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = 130\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    add_to_point_e_output(status_txt)\n",
        "    add_to_point_e_output(progress)\n",
        "    base_name = point_e_prefs['base_model']\n",
        "    base_model = model_from_config(MODEL_CONFIGS[base_name], torch_device)\n",
        "    base_model.eval()\n",
        "    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
        "\n",
        "    prt_status('Creating Upsample model...')\n",
        "    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], torch_device)\n",
        "    upsampler_model.eval()\n",
        "    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
        "\n",
        "    prt_status('Downloading Base Checkpoint...')\n",
        "    base_model.load_state_dict(load_checkpoint(base_name, torch_device))\n",
        "\n",
        "    prt_status('Downloading Upsampler Checkpoint...')\n",
        "    upsampler_model.load_state_dict(load_checkpoint('upsample', torch_device))\n",
        "    point_sampler = PointCloudSampler(\n",
        "        device=torch_device,\n",
        "        models=[base_model, upsampler_model],\n",
        "        diffusions=[base_diffusion, upsampler_diffusion],\n",
        "        num_points=[1024, 4096 - 1024],\n",
        "        aux_channels=['R', 'G', 'B'],\n",
        "        guidance_scale=[point_e_prefs['guidance_scale'], 0.0],\n",
        "        model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all\n",
        "    )\n",
        "    samples = None\n",
        "    prt_status(\"Generating Point-E Samples...\") #images=[img]\n",
        "    step = 0\n",
        "    if init_img != None:\n",
        "        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(images=[init_img]))):\n",
        "            samples = x\n",
        "            step += 1\n",
        "            callback_fnc(step)\n",
        "    else:\n",
        "        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(texts=[point_e_prefs['prompt_text']]))):\n",
        "            samples = x\n",
        "            step += 1\n",
        "            callback_fnc(step)\n",
        "    #print(f\"Total steps: {step}\")\n",
        "    pc = point_sampler.output_to_point_clouds(samples)[0]\n",
        "    fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
        "\n",
        "    prt_status('Saving PointCloud NPZ file...')\n",
        "    pc_file = os.path.join(point_e_out, f'{filename}.npz')\n",
        "    PointCloud.save(pc, pc_file)\n",
        "    sdf = 'sdf'\n",
        "    model = model_from_config(MODEL_CONFIGS[sdf], torch_device)\n",
        "    model.eval()\n",
        "\n",
        "    prt_status('Loading SDF model...')\n",
        "    model.load_state_dict(load_checkpoint(sdf, torch_device))\n",
        "    torch.cuda.empty_cache()\n",
        "    #pc = PointCloud.load('example_data/pc_corgi.npz')\n",
        "    # Plot the point cloud as a sanity check.\n",
        "    #fig = plot_point_cloud(pc, grid_size=2)\n",
        "    #fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
        "    # Produce a mesh (with vertex colors)\n",
        "    try:\n",
        "        mesh = marching_cubes_mesh(\n",
        "            pc=pc,\n",
        "            model=model,\n",
        "            batch_size=4096,\n",
        "            grid_size=32, # increase to 128 for resolution used in evals\n",
        "            progress=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running Point-E marching_cubes_mesh.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    # Write the mesh to a PLY file to import into some other program.\n",
        "    ply_file = os.path.join(point_e_out, f'{filename}.ply')\n",
        "    with open(ply_file, 'wb') as f:\n",
        "        mesh.write_ply(f)\n",
        "\n",
        "    #with open(\"mesh.ply\", 'r') as file:\n",
        "    #    print(file.name)\n",
        "    #https://colab.research.google.com/drive/1Ok3ye2xWsuYOcmbAU3INN7AHy5gvvq5m\n",
        "    del point_sampler\n",
        "    flush()\n",
        "    point_sampler = None\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    prt(\"Finished generating Point Cloud and Mesh... Hope it's good.\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      point_e_save = os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e', point_e_prefs['batch_folder_name'])\n",
        "      #os.makedirs(point_e_out, exist_ok=True)\n",
        "      #if os.path.exists(point_e_save):\n",
        "      #  point_e_save = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e'), fname, 1)\n",
        "      shutil.copytree(point_e_out, point_e_save)\n",
        "      prt(Text(f\"Saved npz & ply files to {point_e_save}\"))\n",
        "    else:\n",
        "      prt(Text(f\"Saved npz & ply files to {point_e_out}\"))\n",
        "    # TODO: PyDrive2\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_shap_e(page):\n",
        "    global shap_e_prefs, status\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.shap_e_output.controls.append(line)\n",
        "      page.shap_e_output.update()\n",
        "    def prt_status(text):\n",
        "        nonlocal status_txt\n",
        "        status_txt.value = text\n",
        "        status_txt.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.shap_e_output, lines=lines)\n",
        "    if not bool(shap_e_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    page.shap_e_output.controls = []\n",
        "    page.shap_e_output.update()\n",
        "    installer = Installing(\"Installing OpenAI Shap-E 3D Libraries...\")\n",
        "    prt(installer)\n",
        "    \n",
        "    shap_e_dir = os.path.join(root_dir, \"shap-e\")\n",
        "    if not os.path.exists(shap_e_dir):\n",
        "        try:\n",
        "            #run_process(\"pip install -U scikit-image\")\n",
        "            run_process(\"git clone https://github.com/openai/shap-e.git\", cwd=root_dir)\n",
        "            run_process(\"pip install .\", cwd=shap_e_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Shap-E Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_pipes()\n",
        "    from shap_e.diffusion.sample import sample_latents\n",
        "    from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
        "    from shap_e.models.download import load_model, load_config\n",
        "    from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
        "    from shap_e.util.image_util import load_image\n",
        "    from PIL import ImageOps\n",
        "    try:\n",
        "      import imageio\n",
        "    except Exception:\n",
        "      run_sp(\"pip install imageio\", realtime=False)\n",
        "      import imageio\n",
        "      pass\n",
        "    try:\n",
        "        xm = load_model('transmitter', device=torch_device)\n",
        "        shap_e_model = load_model('image300M' if bool(shap_e_prefs['init_image']) else 'text300M' , device=torch_device)\n",
        "        diffusion = diffusion_from_config(load_config('diffusion'))\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Shap-E models.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    if bool(shap_e_prefs['prompt_text']):\n",
        "        fname = format_filename(shap_e_prefs['prompt_text'])\n",
        "    elif bool(shap_e_prefs['init_image']):\n",
        "        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])\n",
        "    elif bool(shap_e_prefs['batch_folder_name']):\n",
        "        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        alert_msg(page, \"If you're not using Prompt Text, provide a name for your 3D Model.\")\n",
        "        return\n",
        "\n",
        "    #filename = format_filename(shap_e_prefs['prompt_text'])\n",
        "    #fname = f\"{shap_e_prefs['file_prefix']}{fname}\"\n",
        "    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])\n",
        "    #if bool(shap_e_prefs['batch_folder_name']):\n",
        "    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])\n",
        "    #else:\n",
        "    #    shap_e_out = shap_e_dir\n",
        "    if not os.path.exists(shap_e_out):\n",
        "        os.makedirs(shap_e_out)\n",
        "    #shap_e_out = os.path.join(shap_e_out, fname)\n",
        "    #estimate = convert(int(shap_e_prefs[\"training_iters\"] * 0.7))\n",
        "    init_img = None\n",
        "    if bool(shap_e_prefs['init_image']):\n",
        "        if shap_e_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(shap_e_prefs['init_image']):\n",
        "                init_img = PILImage.open(shap_e_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}\")\n",
        "                if not bool(shap_e_prefs['prompt_text']):\n",
        "                    return\n",
        "        if init_img != None:\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, 512)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "\n",
        "    status_txt = Text(\"Generating your 3D model... See console for progress.\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = shap_e_prefs['karras_steps']\n",
        "    \n",
        "    clear_last(update=False)\n",
        "    prt(status_txt)\n",
        "    prt(progress)\n",
        "    if init_img == None:\n",
        "        model_kwargs = dict(texts=[shap_e_prefs['prompt_text']] * shap_e_prefs['batch_size'])\n",
        "    else:\n",
        "        model_kwargs = dict(images=[init_img] * shap_e_prefs['batch_size'])\n",
        "    try:\n",
        "        latents = sample_latents(\n",
        "            batch_size=shap_e_prefs['batch_size'],\n",
        "            model=shap_e_model,\n",
        "            diffusion=diffusion,\n",
        "            guidance_scale=shap_e_prefs['guidance_scale'],\n",
        "            model_kwargs=model_kwargs,\n",
        "            progress=True,\n",
        "            clip_denoised=True,\n",
        "            use_fp16=True,\n",
        "            use_karras=shap_e_prefs['use_karras'],\n",
        "            karras_steps=shap_e_prefs['karras_steps'],\n",
        "            sigma_min=1e-3,\n",
        "            sigma_max=160,\n",
        "            s_churn=0,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Shap-E sample_latents.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    prt_status(\"Generating Shap-E 3D Models...\") #images=[img]\n",
        "    step = 0\n",
        "    try:\n",
        "        cameras = create_pan_cameras(shap_e_prefs['size'], torch_device)\n",
        "        for i, latent in enumerate(latents):\n",
        "            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')\n",
        "            images = decode_latent_images(xm, latent, cameras, rendering_mode=shap_e_prefs['render_mode'].lower())\n",
        "            #images.save(img_file)\n",
        "            if is_Colab:\n",
        "                display(gif_widget(images))\n",
        "            #callback_fnc(i)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Shap-E decode_latent_images.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    if shap_e_prefs['save_frames']:\n",
        "        imgs = []\n",
        "        for i, img in enumerate(images):\n",
        "            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')\n",
        "            img.save(img_file)\n",
        "            #imgs.append(imageio.imread(np.asarray(img)))\n",
        "    gif_file = os.path.join(shap_e_out, f'{fname}.gif')\n",
        "    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)\n",
        "    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)\n",
        "\n",
        "    prt_status('Saving PLY mesh file...')\n",
        "    from shap_e.util.notebooks import decode_latent_mesh\n",
        "    try:\n",
        "        for i, latent in enumerate(latents):\n",
        "            pc_file = os.path.join(shap_e_out, f'{fname}_{i}.ply')\n",
        "            with open(pc_file, 'wb') as f:\n",
        "                decode_latent_mesh(xm, latent).tri_mesh().write_ply(f)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running Shap-E decode_latent_mesh.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    del xm\n",
        "    del shap_e_model\n",
        "    del diffusion\n",
        "    del latents\n",
        "    flush()\n",
        "    clear_last(update=False)\n",
        "    clear_last(update=False)\n",
        "    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=pc_file, page=page))\n",
        "    prt(\"Finished generating Shap-E Mesh... Hope it's good.\")\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_shap_e2(page):\n",
        "    global shap_e_prefs, pipe_shap_e, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.shap_e_output.controls.append(line)\n",
        "      page.shap_e_output.update()\n",
        "    def prt_status(text):\n",
        "        nonlocal status_txt\n",
        "        status_txt.value = text\n",
        "        status_txt.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.shap_e_output, lines=lines)\n",
        "    if not bool(shap_e_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.shap_e_output.controls = []\n",
        "    page.shap_e_output.update()\n",
        "    shap_e_dir = os.path.join(root_dir, \"shap-e\")\n",
        "    installer = Installing(\"Installing OpenAI Shap-E 3D Libraries...\")\n",
        "    prt(installer)\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers.utils import export_to_ply\n",
        "    if 'loaded_shap_e' not in status: status['loaded_shap_e'] = \"\"\n",
        "    shap_e_type = \"text\"\n",
        "    if bool(shap_e_prefs['prompt_text']):\n",
        "        fname = format_filename(shap_e_prefs['prompt_text'])\n",
        "    elif bool(shap_e_prefs['init_image']):\n",
        "        shap_e_type = \"img2img\"\n",
        "        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])\n",
        "    elif bool(shap_e_prefs['batch_folder_name']):\n",
        "        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        alert_msg(page, \"If you're not using Prompt Text, provide a name for your 3D Model.\")\n",
        "        return\n",
        "    repo = \"openai/shap-e\"\n",
        "    if status['loaded_shap_e'] == shap_e_type:\n",
        "        clear_pipes(\"shap_e\")\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    if pipe_shap_e == None:\n",
        "        try:\n",
        "            installer.status(f\"...loading Shap-E {shap_e_type} pipeline\")\n",
        "            if shap_e_type == \"img2img\":\n",
        "                from diffusers import ShapEImg2ImgPipeline\n",
        "                pipe_shap_e = ShapEImg2ImgPipeline.from_pretrained(\"openai/shap-e-img2img\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
        "            elif shap_e_type == \"text\":\n",
        "                from diffusers import ShapEPipeline\n",
        "                pipe_shap_e = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
        "            pipe_shap_e = pipe_shap_e.to(torch_device)\n",
        "            status['loaded_shap_e'] = shap_e_type\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Shap-E Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    from PIL import ImageOps\n",
        "    try:\n",
        "      import trimesh\n",
        "    except Exception:\n",
        "      installer.status(\"...installing trimesh\")\n",
        "      run_sp(\"pip install trimesh\", realtime=False)\n",
        "      import trimesh\n",
        "      pass\n",
        "    \n",
        "    #filename = format_filename(shap_e_prefs['prompt_text'])\n",
        "    #fname = f\"{shap_e_prefs['file_prefix']}{fname}\"\n",
        "    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])\n",
        "    #if bool(shap_e_prefs['batch_folder_name']):\n",
        "    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])\n",
        "    #else:\n",
        "    #    shap_e_out = shap_e_dir\n",
        "    if not os.path.exists(shap_e_out):\n",
        "        os.makedirs(shap_e_out)\n",
        "    #shap_e_out = os.path.join(shap_e_out, fname)\n",
        "    #estimate = convert(int(shap_e_prefs[\"training_iters\"] * 0.7))\n",
        "    init_img = None\n",
        "    if bool(shap_e_prefs['init_image']):\n",
        "      # TODO: Make Multi-Image List\n",
        "        if shap_e_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(shap_e_prefs['init_image']):\n",
        "                init_img = PILImage.open(shap_e_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}\")\n",
        "                if not bool(shap_e_prefs['prompt_text']):\n",
        "                    return\n",
        "        if init_img != None:\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, 512)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "\n",
        "    status_txt = Text(\"Generating your 3D model... See console for progress.\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = shap_e_prefs['karras_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"[{int(percent * 100)}%] {step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    clear_last(update=False)\n",
        "    prt(status_txt)\n",
        "    prt(progress)\n",
        "    if shap_e_type == \"text\":\n",
        "        model_kwargs = dict(texts=shap_e_prefs['prompt_text'])\n",
        "    else:\n",
        "        model_kwargs = dict(images=init_img)\n",
        "    try:\n",
        "        images = pipe_shap_e(**model_kwargs, num_images_per_prompt=shap_e_prefs['batch_size'], guidance_scale=shap_e_prefs['guidance_scale'], num_inference_steps=shap_e_prefs['karras_steps'], frame_size=shap_e_prefs['size'], callback=callback_fnc)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Shap-E sample_latents.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    #images = pipe_shap_e(shap_e_prefs['prompt'], guidance_scale=shap_e_prefs['guidance_scale'], num_inference_steps=shap_e_prefs['karras_steps'], frame_size=shap_e_prefs['size'], output_type=\"mesh\").images\n",
        "    \n",
        "    prt_status(\"Generating Shap-E 3D Models...\") #images=[img]\n",
        "    step = 0\n",
        "    if shap_e_prefs['save_frames']:\n",
        "        imgs = []\n",
        "        for i, img in enumerate(images):\n",
        "            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')\n",
        "            img.save(img_file)\n",
        "            #imgs.append(imageio.imread(np.asarray(img)))\n",
        "    gif_file = os.path.join(shap_e_out, f'{fname}.gif')\n",
        "    ply_file = os.path.join(shap_e_out, f'{fname}.ply')\n",
        "    glb_file = os.path.join(shap_e_out, f'{fname}.glb')\n",
        "    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)\n",
        "    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)\n",
        "\n",
        "    prt_status('Saving PLY mesh file...')\n",
        "    ply_path = export_to_ply(images[0], ply_file)\n",
        "    print(f\"saved to folder: {ply_path}\")\n",
        "    mesh = trimesh.load(ply_path)\n",
        "    rot = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\n",
        "    mesh = mesh.apply_transform(rot)\n",
        "    mesh.export(glb_file, file_type=\"glb\")\n",
        "\n",
        "    flush()\n",
        "    clear_last(update=False)\n",
        "    clear_last(update=False)\n",
        "    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=ply_path, page=page))\n",
        "    prt(\"Finished generating Shap-E Mesh... Hope it's good.\")\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_zoe_depth(page):\n",
        "    global zoe_depth_prefs, pipe_zoe_depth\n",
        "    def prt(line):\n",
        "        if type(line) == str:\n",
        "            line = Text(line, size=17)\n",
        "        page.ZoeDepth.controls.append(line)\n",
        "        page.ZoeDepth.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.ZoeDepth, lines=lines)\n",
        "    def clear_list():\n",
        "      page.ZoeDepth.controls = page.ZoeDepth.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.ZoeDepth.auto_scroll = scroll\n",
        "      page.ZoeDepth.update()\n",
        "    if not bool(zoe_depth_prefs['init_image']):\n",
        "        alert_msg(page, f\"ERROR: You must provide an init image to prrocess.\")\n",
        "        return\n",
        "    file_name = \"zoedepth\"\n",
        "    if zoe_depth_prefs['init_image'].startswith('http'):\n",
        "        image = PILImage.open(requests.get(zoe_depth_prefs['init_image'], stream=True).raw)\n",
        "        file_name = zoe_depth_prefs['init_image'].rpartition('/')[2]\n",
        "    else:\n",
        "        if os.path.isfile(zoe_depth_prefs['init_image']):\n",
        "            image = PILImage.open(zoe_depth_prefs['init_image'])\n",
        "            file_name = os.path.basename(zoe_depth_prefs['init_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {zoe_depth_prefs['init_image']}\")\n",
        "            return\n",
        "    if '.' in file_name:\n",
        "        file_name = file_name.rpartition('.')[0]\n",
        "    file_name = format_filename(file_name)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Installing ZoeDepth Image-to-3D Pipeline...\")\n",
        "    clear_list()\n",
        "    prt(installer)\n",
        "    pip_install(\"timm trimesh h5py hdf5 matplotlib matplotlib-base opencv scipy\", installer=installer, upgrade=True)\n",
        "    zoe_depth_dir = os.path.join(root_dir, \"ZoeDepth\")\n",
        "    if not os.path.exists(zoe_depth_dir):\n",
        "        try:\n",
        "            installer.status(\"...cloning isl-org/ZoeDepth.git\")\n",
        "            run_sp(\"git clone https://github.com/isl-org/ZoeDepth.git\", cwd=root_dir, realtime=False)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Installing github.com/isl-org/ZoeDepth...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    sys.path.append(zoe_depth_dir)\n",
        "    try:\n",
        "        installer.status(\"...running sanity check\")\n",
        "        run_sp(\"python sanity.py\", cwd=zoe_depth_dir, realtime=False)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"Error Running ZoeDepth sanity.py...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    from zoedepth.utils.misc import get_image_from_url, colorize, save_raw_16bit\n",
        "    import matplotlib.pyplot as plt\n",
        "    import trimesh\n",
        "    from zoedepth.utils.geometry import depth_to_points, create_triangles\n",
        "    from functools import partial\n",
        "\n",
        "    def depth_edges_mask(depth):\n",
        "        depth_dx, depth_dy = np.gradient(depth)\n",
        "        depth_grad = np.sqrt(depth_dx ** 2 + depth_dy ** 2)\n",
        "        mask = depth_grad > 0.05\n",
        "        return mask\n",
        "    def pano_depth_to_world_points(depth):\n",
        "        radius = depth.flatten()\n",
        "        lon = np.linspace(-np.pi, np.pi, depth.shape[1])\n",
        "        lat = np.linspace(-np.pi/2, np.pi/2, depth.shape[0])\n",
        "        lon, lat = np.meshgrid(lon, lat)\n",
        "        lon = lon.flatten()\n",
        "        lat = lat.flatten()\n",
        "        # Convert to cartesian coordinates\n",
        "        x = radius * np.cos(lat) * np.cos(lon)\n",
        "        y = radius * np.cos(lat) * np.sin(lon)\n",
        "        z = radius * np.sin(lat)\n",
        "        pts3d = np.stack([x, y, z], axis=1)\n",
        "        return pts3d\n",
        "    \n",
        "    if zoe_depth_prefs['zoe_model'] == zoe_depth_prefs['loaded_model']:\n",
        "        clear_pipes('zoe_depth')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    if pipe_zoe_depth == None:\n",
        "        installer.status(\"...loading isl-org/ZoeDepth\")#'isl-org/ZoeDepth'\n",
        "        pipe_zoe_depth = torch.hub.load(zoe_depth_dir, zoe_depth_prefs[\"zoe_model\"], pretrained=True).to(torch_device).eval()\n",
        "        zoe_depth_prefs['loaded_model'] = zoe_depth_prefs['zoe_model']\n",
        "    batch_output = os.path.join(prefs['image_output'], zoe_depth_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    installer.set_message(\"Running ZoeDepth on your Image...\")\n",
        "    installer.show_progress(False)\n",
        "    installer.status(\"...infering depth\")\n",
        "    prt(progress)\n",
        "    glb_path = available_file(batch_output, file_name, ext=\"glb\", no_num=True)\n",
        "    depth_path = available_file(batch_output, file_name, no_num=True)\n",
        "    colored_path = available_file(batch_output, f\"{file_name}-colored\", no_num=True)\n",
        "    image.thumbnail((zoe_depth_prefs['max_size'],zoe_depth_prefs['max_size']))  # limit the size of the input image\n",
        "    depth = pipe_zoe_depth.infer_pil(image)\n",
        "    installer.status(\"...save depth\")\n",
        "    save_raw_16bit(depth, depth_path)\n",
        "    if zoe_depth_prefs['pano_360']:\n",
        "        installer.status(\"...pano depth to world points\")\n",
        "        pts3d = pano_depth_to_world_points(depth)\n",
        "    else:\n",
        "        installer.status(\"...depth to points\")\n",
        "        pts3d = depth_to_points(depth[None])\n",
        "    verts = pts3d.reshape(-1, 3)\n",
        "    if zoe_depth_prefs['colorize']:\n",
        "        installer.status(\"...colorize depth\")\n",
        "        colored = colorize(depth)\n",
        "        PILImage.fromarray(colored).save(colored_path)\n",
        "    image = np.array(image)\n",
        "    installer.status(\"...create triangles\")\n",
        "    if zoe_depth_prefs['keep_edges']:\n",
        "        triangles = create_triangles(image.shape[0], image.shape[1])\n",
        "    else:\n",
        "        triangles = create_triangles(image.shape[0], image.shape[1], mask=~depth_edges_mask(depth))\n",
        "    colors = image.reshape(-1, 3)\n",
        "    installer.status(\"...create trimesh\")\n",
        "    mesh = trimesh.Trimesh(vertices=verts, faces=triangles, vertex_colors=colors)\n",
        "    mesh.export(glb_path)\n",
        "    autoscroll(True)\n",
        "    clear_last(2)\n",
        "    prt(ImageButton(src=depth_path, width=depth.shape[1], height=depth.shape[0], data=depth_path, subtitle=depth_path, page=page))\n",
        "    if zoe_depth_prefs['colorize']:\n",
        "        prt(ImageButton(src=colored_path, width=colored.shape[1], height=colored.shape[0], data=colored_path, subtitle=colored_path, page=page))\n",
        "    \n",
        "    prt(f\"Saved to {glb_path}\")\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_marigold_depth(page):\n",
        "    global marigold_depth_prefs, pipe_marigold_depth\n",
        "    def prt(line):\n",
        "        if type(line) == str:\n",
        "            line = Text(line, size=17)\n",
        "        page.MarigoldDepth.controls.append(line)\n",
        "        page.MarigoldDepth.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.MarigoldDepth, lines=lines)\n",
        "    def clear_list():\n",
        "      page.MarigoldDepth.controls = page.MarigoldDepth.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.MarigoldDepth.auto_scroll = scroll\n",
        "      page.MarigoldDepth.update()\n",
        "    file_name = \"Marigold\"\n",
        "    if marigold_depth_prefs['init_image'].startswith('http'):\n",
        "        image = PILImage.open(requests.get(marigold_depth_prefs['init_image'], stream=True).raw)\n",
        "        file_name = marigold_depth_prefs['init_image'].rpartition('/')[2]\n",
        "    else:\n",
        "        if os.path.isfile(marigold_depth_prefs['init_image']):\n",
        "            image = PILImage.open(marigold_depth_prefs['init_image'])\n",
        "            file_name = os.path.basename(marigold_depth_prefs['init_image'])\n",
        "        else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {marigold_depth_prefs['init_image']}\")\n",
        "            return\n",
        "    if '.' in file_name:\n",
        "        file_name = file_name.rpartition('.')[0]\n",
        "    file_name = format_filename(file_name)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    installer = Installing(\"Installing Marigold Depth Pipeline...\")\n",
        "    clear_list()\n",
        "    prt(installer)\n",
        "    from diffusers import DiffusionPipeline\n",
        "    clear_pipes('marigold_depth')\n",
        "    if pipe_marigold_depth == None:\n",
        "        installer.status(\"...loading Bingxin/Marigold\")\n",
        "        mem_kwargs = {} if prefs['higher_vram_mode'] else {'torch_dtype': torch.float16}\n",
        "        pipe_marigold_depth = DiffusionPipeline.from_pretrained(\"Bingxin/Marigold\", custom_pipeline=\"marigold_depth_estimation\", **mem_kwargs)\n",
        "        pipe_marigold_depth.to(torch_device)\n",
        "    batch_output = os.path.join(prefs['image_output'], marigold_depth_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    clear_last()\n",
        "    prt(\"Generating Marigold Depth Estimation Images...\")\n",
        "    prt(progress)\n",
        "    marigold_output = pipe_marigold_depth(\n",
        "        image,                  # Input image.\n",
        "        denoising_steps=marigold_depth_prefs['denoising_steps'],     # (optional) Number of denoising steps of each inference pass. Default: 10.\n",
        "        ensemble_size=marigold_depth_prefs['ensemble_size'],       # (optional) Number of inference passes in the ensemble. Default: 10.\n",
        "        processing_res=marigold_depth_prefs['processing_res'],     # (optional) Maximum resolution of processing. If set to 0: will not resize at all. Defaults to 768.\n",
        "        match_input_res=marigold_depth_prefs['match_input_res'],   # (optional) Resize depth prediction to match input resolution.\n",
        "        # batch_size=0,           # (optional) Inference batch size, no bigger than `num_ensemble`. If set to 0, the script will automatically decide the proper batch size. Defaults to 0.\n",
        "        color_map=marigold_depth_prefs['color_map'],   # (optional) Colormap used to colorize the depth map. Defaults to \"Spectral\".\n",
        "        show_progress_bar=True, # (optional) If true, will show progress bars of the inference progress.\n",
        "    )\n",
        "    depth_path = available_file(batch_output, file_name, no_num=True)\n",
        "    colored_path = available_file(batch_output, f\"{file_name}-colored\", no_num=True)\n",
        "    depth: np.ndarray = marigold_output.depth_np                    # Predicted depth map\n",
        "    depth_colored: PILImage.Image = marigold_output.depth_colored      # Colorized prediction\n",
        "    # Save as uint16 PNG\n",
        "    depth_uint16 = (depth * 65535.0).astype(np.uint16)\n",
        "    PILImage.fromarray(depth_uint16).save(depth_path, mode=\"I;16\")\n",
        "    depth_colored.save(colored_path)\n",
        "    autoscroll(True)\n",
        "    clear_last(2)\n",
        "    prt(Row([ImageButton(src=depth_path, width=depth.shape[1], height=depth.shape[0], data=depth_path, subtitle=depth_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(Row([ImageButton(src=colored_path, width=depth.shape[1], height=depth.shape[0], data=colored_path, subtitle=colored_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(ImageButton(src=depth_path, width=depth.shape[1], height=depth.shape[0], data=depth_path, subtitle=depth_path, page=page))\n",
        "    prt(ImageButton(src=colored_path, width=depth.shape[1], height=depth.shape[0], data=colored_path, subtitle=colored_path, page=page))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_instant_ngp(page):\n",
        "    global instant_ngp_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.instant_ngp_output.controls.append(line)\n",
        "      page.instant_ngp_output.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.instant_ngp_output, lines=lines)\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_ngp\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.instant_ngp_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.instant_ngp_output.controls.clear()\n",
        "    page.instant_ngp_output.update()\n",
        "    prt(Installing(\"Downloading Instant-NGP Packages...\"))\n",
        "    instant_ngp_dir = os.path.join(root_dir, 'instant-ngp')\n",
        "    '''if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    '''\n",
        "    try:\n",
        "        run_sp(\"apt-get install \\\n",
        "            cmake \\\n",
        "            libgoogle-glog-dev \\\n",
        "            libgflags-dev \\\n",
        "            libatlas-base-dev \\\n",
        "            libeigen3-dev \\\n",
        "            libsuitesparse-dev \\\n",
        "            libboost-program-options-dev \\\n",
        "            libboost-filesystem-dev \\\n",
        "            libboost-graph-dev \\\n",
        "            libboost-system-dev \\\n",
        "            libboost-test-dev \\\n",
        "            libfreeimage-dev \\\n",
        "            libmetis-dev \\\n",
        "            libglew-dev \\\n",
        "            qtbase5-dev \\\n",
        "            libqt5opengl5-dev \\\n",
        "            libcgal-dev\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Instant-NGP Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "    if not os.path.exists(root_dir, 'ceres-solver'):\n",
        "      run_sp(\"wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/ceres-solver.zip\", realtime=False)\n",
        "      run_sp(f\"unzip {os.path.join(root_dir, 'ceres-solver.zip')} -d ceres-solver\", realtime=False)\n",
        "      os.remove(os.path.join(root_dir, 'ceres-solver.zip'))\n",
        "    run_sp(f\"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'lib', '.')} {os.path.join('/usr', 'local', 'lib')}\", realtime=False)\n",
        "    run_sp(f\"chmod 755 {os.path.join(root_dir, 'ceres-solver', 'bin', 'colmap')}\", realtime=False)\n",
        "    run_sp(f\"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'bin', '.')} {os.path.join('/usr', 'local', 'bin')}\", realtime=False)\n",
        "    #run_sp(f\"cp -r /content/ceres-solver/bin/. /usr/local/bin\")\n",
        "    if not os.path.exists(root_dir, 'instant-ngp'):\n",
        "      run_sp(\"wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/instant-ngp.zip\", realtime=False)\n",
        "      #run_sp(\"unzip /content/instant-ngp.zip -d instant-ngp\")\n",
        "      run_sp(f\"unzip {os.path.join(root_dir, 'instant-ngp.zip')} -d instant-ngp\", realtime=False)\n",
        "      os.remove(os.path.join(root_dir, 'instant-ngp.zip'))\n",
        "    #run_sp(\"pip install commentjson huggingface-hub\", realtime=False)\n",
        "    os.chdir(instant_ngp_dir)\n",
        "    #from huggingface_hub import create_repo, upload_folder\n",
        "    #scene_path = \"/content/drive/MyDrive/fox\"\n",
        "    train_path = os.path.join(root_dir, 'my_ngp')\n",
        "    #train_path = \"/content/train\"\n",
        "    #if not os.path.isdir(scene_path):\n",
        "    #    raise NotADirectoryError(scene_path)\n",
        "    '''\n",
        "    rm -rf {train_path}\n",
        "    mkdir {train_path}\n",
        "    cp -r {scene_path}/. /content/train\n",
        "    '''\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    prt(Text(\"Running training on Images... This'll take a while, see console...\", weight=FontWeight.BOLD))\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    name_of_your_model = instant_ngp_prefs['name_of_your_model']\n",
        "    transforms_path = os.path.join(train_path, f\"transforms.json\")\n",
        "    train_steps = instant_ngp_prefs['train_steps']\n",
        "    snapshot_path = os.path.join(train_path, f\"{train_steps}.msgpack\")\n",
        "    mesh_path = os.path.join(train_path, f\"{name_of_your_model or train_steps}.ply\")\n",
        "    os.chdir(instant_ngp_dir)\n",
        "    run_args = f\"--scene {train_path} --mode nerf --n_steps {train_steps} --save_snapshot {snapshot_path} --save_mesh {mesh_path} --screenshot_dir {train_path}\"\n",
        "    if instant_ngp_prefs[\"vr_mode\"]: run_args += \" --vr\"\n",
        "    if instant_ngp_prefs[\"sharpen\"] != 0.0: run_args += f\" --sharpen {instant_ngp_prefs['sharpen']}\"\n",
        "    if instant_ngp_prefs[\"exposure\"] != 0.0: run_args += f\" --exposure {instant_ngp_prefs['exposure']}\"\n",
        "    try:\n",
        "        run_sp(f\"python ./scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 4 --images {train_path} --out {transforms_path}\", cwd=instant_ngp_dir)\n",
        "        run_sp(f\"python ./scripts/run.py {run_args}\", cwd=instant_ngp_dir)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Running InstantNGP Training...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "    #instant_ngp_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    #instant_ngp_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    #save_path = \"./my_model\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "    output_dir = prefs['image_output'].rpartition(slash)[0] + slash + '3D_out'\n",
        "    if bool(name_of_your_model):\n",
        "      output_dir = os.path.join(output_dir, name_of_your_model)\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    shutil.copytree(train_path, output_dir, dirs_exist_ok=True)\n",
        "    '''video_camera_path = os.path.join(scene_path, \"base_cam.json\")\n",
        "    if not os.path.isfile(video_camera_path):\n",
        "      raise FileNotFoundError(video_camera_path)\n",
        "    video_n_seconds = 5\n",
        "    video_fps = 25\n",
        "    width = 720\n",
        "    height = 720\n",
        "    output_video_path = os.path.join(scene_path, \"output_video.mp4\")\n",
        "    python scripts/run.py {snapshot_path} --video_camera_path {video_camera_path} --video_n_seconds 2 --video_fps 25 --width 720 --height 720 --video_output {output_video_path}\n",
        "    print(f\"Generated video saved to:\\n{output_video_path}\")'''\n",
        "    clear_last(2)\n",
        "    prt(Markdown(f\"## Your model was saved successfully to _{output_dir}_.\\nNow take those files and load then locally on Windows following [instant-ngp gui instructions](https://github.com/NVlabs/instant-ngp#testbed-controls) to export videos and meshes or try MeshLab... (wish we can do that for ya here)\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_luma_vid_to_3d(page):\n",
        "    global luma_vid_to_3d_prefs\n",
        "    def prt(line):\n",
        "        if type(line) == str:\n",
        "            line = Text(line, size=17)\n",
        "        page.Luma.controls.append(line)\n",
        "        page.Luma.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Luma, lines=lines)\n",
        "    def clear_list():\n",
        "      page.Luma.controls = page.Luma.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Luma.auto_scroll = scroll\n",
        "      page.Luma.update()\n",
        "    if not bool(luma_vid_to_3d_prefs['init_video']):\n",
        "        alert_msg(page, f\"ERROR: You must provide your mp4 or avi file with a smooth walk-thru of scene or walk-around object.\")\n",
        "        return\n",
        "    if not bool(prefs[\"luma_api_key\"]):\n",
        "        alert_msg(page, f\"ERROR: You must provide your own LumaLabs.ai API Key to use...\")\n",
        "        return\n",
        "    installer = Installing(\"Installing Luma Video-to-3D API Client...\")\n",
        "    clear_list()\n",
        "    prt(installer)\n",
        "    pip_install(\"lumaapi\", installer=installer, upgrade=True)\n",
        "    from lumaapi import LumaClient, CameraType\n",
        "    \n",
        "    video_path = luma_vid_to_3d_prefs[\"init_video\"]\n",
        "    if video_path.startswith('http'):\n",
        "        installer.status(\"...downloading url\")\n",
        "        video_path = download_file(video_path, uploads_dir, ext=\"mp4\")\n",
        "    else:\n",
        "        if not os.path.isfile(video_path):\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_video {video_path}\")\n",
        "            return\n",
        "    title = luma_vid_to_3d_prefs[\"title\"]\n",
        "    file_name = format_filename(title)\n",
        "    camera_type = CameraType.EQUIRECTANGULAR if luma_vid_to_3d_prefs[\"camera_type\"] == \"Equirectangular 360\" else CameraType.FISHEYE if luma_vid_to_3d_prefs[\"camera_type\"] == \"Fisheye Lens\" else CameraType.NORMAL\n",
        "    batch_output = os.path.join(prefs['image_output'], luma_vid_to_3d_prefs['batch_folder_name'])\n",
        "    make_dir(batch_output)\n",
        "    clear_last()\n",
        "    pb = Progress(\"Running Luma Video-to-3D on your file...\")\n",
        "    prt(pb)\n",
        "    try:\n",
        "        luma_client = LumaClient(prefs[\"luma_api_key\"])\n",
        "        credits = luma_client.credits()\n",
        "        pb.set_message(f\"Running Luma Video-to-3D on your file... Credits {credits.remaining}/{credits.total}\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Problem Authenticating API Client...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    slug = luma_client.submit(video_path, title, cam_model=camera_type)\n",
        "    pb.status(f\"...submitted with slug: {slug}\")\n",
        "    while True:\n",
        "        status_info = luma_client.status(slug)\n",
        "        capture_status = str(status_info.status)\n",
        "        if '.' in capture_status:\n",
        "            capture_status = capture_status.rpartition('.')[2]\n",
        "        pb.status(f\"...status: {capture_status}\")\n",
        "        if str(capture_status) == \"COMPLETE\":\n",
        "            break\n",
        "        time.sleep(5)\n",
        "    pb.status(f\"...getting {slug}\")\n",
        "    captures = luma_client.get(title)\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    for capture_info in captures:\n",
        "        for artifact in capture_info.latest_run.artifacts:\n",
        "            artifact_type = artifact[\"type\"]\n",
        "            artifact_url = artifact[\"url\"]\n",
        "            fname = artifact_url.rpartition('/')[2]\n",
        "            ext = fname.rpartition('.')[2]\n",
        "            fname = fname.rpartition('.')[0]\n",
        "            filename = available_file(batch_output, fname, ext=ext, no_num=True)\n",
        "            fname = os.path.basename(filename).rpartition('.')[0]\n",
        "            #print(f\"type: {artifact_type} url:{artifact_url}\")\n",
        "            filename = download_file(artifact_url, batch_output, fname, ext=ext)\n",
        "            #if ext == \"jpg\":\n",
        "            f = filepath_to_url(filename)\n",
        "            prt(Markdown(f\"Saved {artifact_type} [{fname}]({f})\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_dall_e(page, from_list=False):\n",
        "    global dall_e_prefs, prefs, prompts\n",
        "    if (not bool(dall_e_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "      return\n",
        "    if not bool(prefs['OpenAI_api_key']):\n",
        "      alert_msg(page, \"You must provide your OpenAI API Key in Settings to process your Dall-e 2 Creation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.DallE2.controls.append(line)\n",
        "      page.DallE2.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.DallE2, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DallE2.auto_scroll = scroll\n",
        "      page.DallE2.update()\n",
        "    def clear_list():\n",
        "      page.DallE2.controls = page.DallE2.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    try:\n",
        "        import openai\n",
        "    except:\n",
        "        prt(Installing(\"Installing OpenAi DALL‚Ä¢E 2 API...\"))\n",
        "        run_process(\"pip install -q openai\", realtime=False)\n",
        "        clear_last()\n",
        "        import openai\n",
        "        pass\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Seems like your OpenAI API Key is Invalid. Check it again...\", content=Text(str(e)))\n",
        "        return\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "\n",
        "    save_dir = os.path.join(root_dir, 'dalle_inputs')\n",
        "    init_img = None\n",
        "    dall_e_list = []\n",
        "    if from_list:\n",
        "        if len(prompts) > 0:\n",
        "            for p in prompts:\n",
        "                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})\n",
        "        else:\n",
        "            alert_msg(page, f\"Your Prompts List is empty. Add to your batch list to use feature.\")\n",
        "            return\n",
        "    else:\n",
        "        dall_e_list.append({'prompt': dall_e_prefs['prompt'], 'init_image': dall_e_prefs['init_image'], 'mask_image': dall_e_prefs['mask_image']})\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    for p in dall_e_list:\n",
        "        init_image = p['init_image']\n",
        "        mask_image = p['mask_image']\n",
        "        if bool(init_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            init_file = os.path.join(save_dir, fname)\n",
        "            if init_image.startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(init_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(init_image):\n",
        "                    init_img = PILImage.open(init_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {init_image}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            init_img.save(init_file)\n",
        "        mask_img = None\n",
        "        if bool(mask_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            mask_file = os.path.join(save_dir, fname)\n",
        "            if mask_image.startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mask_image):\n",
        "                    mask_img = PILImage.open(mask_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {mask_image}\")\n",
        "                    return\n",
        "                if dall_e_prefs['invert_mask']:\n",
        "                    mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "            mask_img = mask_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.NEAREST)\n",
        "            mask_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            mask_img.save(mask_file)\n",
        "        #print(f'Resize to {width}x{height}')\n",
        "        #clear_pipes()\n",
        "        prt(\"Generating your DALL‚Ä¢E 2 Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        try:\n",
        "            if bool(init_image) and bool(dall_e_prefs['variation']):\n",
        "                response = openai_client.images.create_variation(image=open(init_file, 'rb'), size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])\n",
        "            elif bool(init_image) and not bool(mask_image):\n",
        "                response = openai_client.images.edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'))\n",
        "            elif bool(init_image) and bool(mask_image):\n",
        "                response = openai_client.images.edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))\n",
        "            else:\n",
        "                response = openai_client.images.generate(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating image form API...\", content=Text(str(e)))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = os.path.join(prefs['image_output'], dall_e_prefs['batch_folder_name'])\n",
        "        make_dir(batch_output)\n",
        "        #print(str(images))\n",
        "        if response is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        #print(str(response))\n",
        "        idx = 0\n",
        "        for i in response.data:\n",
        "            image = i.url\n",
        "            #random_seed += idx\n",
        "            fname = format_filename(p['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{dall_e_prefs[\"file_prefix\"]}{fname}'\n",
        "            txt2img_output = stable_dir\n",
        "            if bool(dall_e_prefs['batch_folder_name']):\n",
        "                txt2img_output = os.path.join(stable_dir, dall_e_prefs['batch_folder_name'])\n",
        "            make_dir(txt2img_output)\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            #image.save(image_path)\n",
        "            response = requests.get(image, stream=True)\n",
        "            with open(image_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            #img = i['url']\n",
        "            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]\n",
        "            size = int(dall_e_prefs['size'].rpartition('x')[0])\n",
        "            out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "            new_path = available_file(out_path, new_file, idx)\n",
        "            if not dall_e_prefs['display_upscaled_image'] or not dall_e_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, data=new_file, width=size, height=size, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=image_path, width=size, height=size, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            #if save_to_GDrive:\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': dall_e_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "\n",
        "            if dall_e_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscaled_path = new_path\n",
        "                upscale_image(image_path, upscaled_path, scale=dall_e_prefs[\"enlarge_scale\"], face_enhance=dall_e_prefs[\"face_enhance\"])\n",
        "                if dall_e_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=size * float(dall_e_prefs[\"enlarge_scale\"]), height=size * float(dall_e_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            else:\n",
        "                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))\n",
        "            # TODO: Add Metadata\n",
        "            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_dall_e_3(page, from_list=False):\n",
        "    global dall_e_3_prefs, prefs, prompts\n",
        "    if (not bool(dall_e_3_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "      return\n",
        "    if not bool(prefs['OpenAI_api_key']):\n",
        "      alert_msg(page, \"You must provide your OpenAI API Key in Settings to process your DALL‚Ä¢E 3 Creation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.DallE3.controls.append(line)\n",
        "      page.DallE3.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.DallE3, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DallE3.auto_scroll = scroll\n",
        "      page.DallE3.update()\n",
        "    def clear_list():\n",
        "      page.DallE3.controls = page.DallE3.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    try:\n",
        "        import openai\n",
        "        print(f\"OpenAI {version.parse(openai.__version__).base_version} v{openai.__version__}\")\n",
        "        if version.parse(openai.__version__).base_version < version.parse(\"1.2.2\"):\n",
        "            run_process(\"pip uninstall -y openai\", realtime=False)\n",
        "            raise ModuleNotFoundError(\"Forcing update\")\n",
        "        if force_updates or True: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except:\n",
        "        prt(Installing(\"Installing OpenAi DALL‚Ä¢E 3 API...\"))\n",
        "        run_process(\"pip install -q --upgrade openai\", realtime=False)\n",
        "        clear_last()\n",
        "        import openai\n",
        "        pass\n",
        "    try:\n",
        "        #openai.api_key = prefs['OpenAI_api_key']\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=prefs['OpenAI_api_key'])\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Seems like your OpenAI API Key is Invalid. Check it again...\", content=Text(str(e)))\n",
        "        return\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    save_dir = os.path.join(root_dir, 'dalle_inputs')\n",
        "    init_img = None\n",
        "    dall_e_list = []\n",
        "    if from_list:\n",
        "        if len(prompts) > 0:\n",
        "            for p in prompts:\n",
        "                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})\n",
        "        else:\n",
        "            alert_msg(page, f\"Your Prompts List is empty. Add to your batch list to use feature.\")\n",
        "            return\n",
        "    else:\n",
        "        dall_e_list.append({'prompt': dall_e_3_prefs['prompt'], 'init_image': dall_e_3_prefs['init_image'], 'mask_image': dall_e_3_prefs['mask_image']})\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    for p in dall_e_list:\n",
        "        init_image = p['init_image']\n",
        "        mask_image = p['mask_image']\n",
        "        size = dall_e_3_prefs['size'].partition('x')\n",
        "        w, h = int(size[0]), int(size[2])\n",
        "        if bool(init_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            init_file = os.path.join(save_dir, fname)\n",
        "            if init_image.startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(init_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(init_image):\n",
        "                    init_img = PILImage.open(init_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {init_image}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((w, h), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            init_img.save(init_file)\n",
        "        mask_img = None\n",
        "        if bool(mask_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            mask_file = os.path.join(save_dir, fname)\n",
        "            if mask_image.startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mask_image):\n",
        "                    mask_img = PILImage.open(mask_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {mask_image}\")\n",
        "                    return\n",
        "                if dall_e_3_prefs['invert_mask']:\n",
        "                    mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "            mask_img = mask_img.resize((w, h), resample=PILImage.NEAREST)\n",
        "            mask_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            mask_img.save(mask_file)\n",
        "        prt(\"Generating your DALL‚Ä¢E 3 Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        try:\n",
        "            if bool(init_image) and bool(dall_e_3_prefs['variation']):\n",
        "                response = client.images.create_variation(image=open(init_file, 'rb'), size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'])\n",
        "            elif bool(init_image) and not bool(mask_image):\n",
        "                response = client.images.edit(prompt=p['prompt'], size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'], image=open(init_file, 'rb'))\n",
        "            elif bool(init_image) and bool(mask_image):\n",
        "                response = client.images.edit(prompt=p['prompt'], size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))\n",
        "            else:\n",
        "                response = client.images.generate(model=\"dall-e-3\", prompt=p['prompt'], size=dall_e_3_prefs['size'], n=1, quality=\"hd\" if dall_e_3_prefs['hd_quality'] else \"standard\", style=\"natural\" if dall_e_3_prefs['natural_style'] else \"vivid\")\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating image form API...\", content=Text(str(e)))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        #print(str(images))\n",
        "        if response is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        #print(str(response))\n",
        "        idx = 0\n",
        "        for i in response.data:\n",
        "            image = i.url #i['url']\n",
        "            fname = format_filename(p['prompt'])\n",
        "            fname = f'{dall_e_3_prefs[\"file_prefix\"]}{fname}'\n",
        "            txt2img_output = stable_dir\n",
        "            if bool(dall_e_3_prefs['batch_folder_name']):\n",
        "                txt2img_output = os.path.join(stable_dir, dall_e_3_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(txt2img_output):\n",
        "                os.makedirs(txt2img_output)\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            response = requests.get(image, stream=True)\n",
        "            with open(image_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]\n",
        "            #size = int(dall_e_3_prefs['size'].rpartition('x')[0])\n",
        "            batch_output = os.path.join(prefs['image_output'], dall_e_3_prefs['batch_folder_name'])\n",
        "            make_dir(batch_output)\n",
        "            out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "            new_path = available_file(out_path, new_file, idx)\n",
        "            if not dall_e_3_prefs['display_upscaled_image'] or not dall_e_3_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, data=new_file, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': dall_e_3_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "\n",
        "            if dall_e_3_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscaled_path = new_path\n",
        "                upscale_image(image_path, upscaled_path, scale=dall_e_3_prefs[\"enlarge_scale\"], face_enhance=dall_e_3_prefs[\"face_enhance\"])\n",
        "                if dall_e_3_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=w * float(dall_e_3_prefs[\"enlarge_scale\"]), height=h * float(dall_e_3_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            else:\n",
        "                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))\n",
        "            # TODO: Add Metadata\n",
        "            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "loaded_kandinsky_task = \"\"\n",
        "def run_kandinsky3(page, from_list=False, with_params=False):\n",
        "    global kandinsky_3_prefs, pipe_kandinsky, prefs, loaded_kandinsky_task\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if int(status['cpu_memory']) <= 10:\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 3 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.\")\n",
        "      return\n",
        "    kandinsky_3_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            kandinsky_3_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_3_prefs['init_image'], 'mask_image':kandinsky_3_prefs['mask_image'], 'guidance_scale':kandinsky_3_prefs['guidance_scale'], 'steps':kandinsky_3_prefs['steps'], 'width':kandinsky_3_prefs['width'], 'height':kandinsky_3_prefs['height'], 'strength':kandinsky_3_prefs['strength'], 'num_images':kandinsky_3_prefs['num_images'], 'seed':kandinsky_3_prefs['seed']})\n",
        "        else:\n",
        "            kandinsky_3_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(kandinsky_3_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      kandinsky_3_prompts.append({'prompt': kandinsky_3_prefs['prompt'], 'negative_prompt':kandinsky_3_prefs['negative_prompt'], 'init_image':kandinsky_3_prefs['init_image'], 'mask_image':kandinsky_3_prefs['mask_image'], 'guidance_scale':kandinsky_3_prefs['guidance_scale'], 'steps':kandinsky_3_prefs['steps'], 'width':kandinsky_3_prefs['width'], 'height':kandinsky_3_prefs['height'], 'strength':kandinsky_3_prefs['strength'], 'num_images':kandinsky_3_prefs['num_images'], 'seed':kandinsky_3_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.Kandinsky.controls.append(line)\n",
        "        if update:\n",
        "          page.Kandinsky.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.Kandinsky, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.Kandinsky.auto_scroll = scroll\n",
        "        page.Kandinsky.update()\n",
        "      else:\n",
        "        page.Kandinsky.auto_scroll = scroll\n",
        "        page.Kandinsky.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.Kandinsky.controls = page.Kandinsky.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = kandinsky_3_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None: #(pipe, step, timestep, callback_kwargs):#\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    model_id = \"kandinsky-community/kandinsky-3\" if kandinsky_3_prefs['kandinsky_model'] == \"Kandinsky 3\" else \"kandinsky-community/kandinsky-2-2-decoder\"\n",
        "    installer = Installing(f\"Installing {kandinsky_3_prefs['kandinsky_model']} Engine & Models... See console log for progress.\")\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    cpu_offload = kandinsky_3_prefs['cpu_offload']\n",
        "    for pr in kandinsky_3_prompts:\n",
        "        prt(installer)\n",
        "        init_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        mask_img = None\n",
        "        if bool(pr['mask_image']):\n",
        "            fname = pr['mask_image'].rpartition(slash)[2]\n",
        "            if pr['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['mask_image']):\n",
        "                    mask_img = PILImage.open(pr['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                    return\n",
        "            if kandinsky_3_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.NEAREST)\n",
        "            mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "        task_type = \"inpainting\" if bool(pr['init_image']) and bool(pr['mask_image']) else \"img2img\" if bool(pr['init_image']) and not bool(pr['mask_image']) else \"text2img\"\n",
        "        installer.status(f\"...{kandinsky_3_prefs['kandinsky_model']} {task_type} Pipeline\")\n",
        "        if pipe_kandinsky == None:\n",
        "            clear_pipes('kandinsky')\n",
        "            try:\n",
        "                if task_type == \"text2img\":\n",
        "                    from diffusers import AutoPipelineForText2Image\n",
        "                    pipe_kandinsky = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"img2img\":\n",
        "                    from diffusers import AutoPipelineForImage2Image\n",
        "                    pipe_kandinsky = AutoPipelineForImage2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"inpainting\":\n",
        "                    from diffusers import AutoPipelineForInpainting#\"kandinsky-community/kandinsky-2-2-decoder-inpaint\"\n",
        "                    pipe_kandinsky = AutoPipelineForInpainting.from_pretrained(model_id+\"-inpaint\", variant=\"fp16\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                if prefs['enable_torch_compile']:\n",
        "                    installer.status(f\"...Torch compiling unet\")\n",
        "                    pipe_kandinsky.unet.to(memory_format=torch.channels_last)\n",
        "                    pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                    pipe_kandinsky = pipe_kandinsky.to(\"cuda\")\n",
        "                elif cpu_offload:\n",
        "                    pipe_kandinsky.enable_model_cpu_offload()\n",
        "                else:\n",
        "                    pipe_kandinsky.to(\"cuda\")\n",
        "                loaded_kandinsky_task = task_type\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "        elif loaded_kandinsky_task != task_type:\n",
        "            clear_pipes('kandinsky')\n",
        "            try:\n",
        "                if task_type == \"text2img\":\n",
        "                    from diffusers import AutoPipelineForText2Image\n",
        "                    pipe_kandinsky = AutoPipelineForText2Image.from_pipe(pipe_kandinsky)\n",
        "                elif task_type == \"img2img\":\n",
        "                    from diffusers import AutoPipelineForImage2Image\n",
        "                    pipe_kandinsky = AutoPipelineForImage2Image.from_pipe(pipe_kandinsky)\n",
        "                elif task_type == \"inpainting\":\n",
        "                    from diffusers import AutoPipelineForInpainting#\"kandinsky-community/kandinsky-2-2-decoder-inpaint\"\n",
        "                    pipe_kandinsky = AutoPipelineForInpainting.from_pipe(pipe_kandinsky)\n",
        "                loaded_kandinsky_task = task_type\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR Converting Kandinsky from pipe...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "        else:\n",
        "            clear_pipes('kandinsky')\n",
        "        clear_last()\n",
        "        prt(f\"Generating your {kandinsky_3_prefs['kandinsky_model']} Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        try:\n",
        "            if task_type == \"text2img\":\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            elif task_type == \"img2img\":\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    image=init_img,\n",
        "                    strength=pr['strength'],\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            elif task_type == \"inpainting\":\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    image=init_img,\n",
        "                    mask_image=mask_img,\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "        except Exception as e:\n",
        "            clear_last(2)\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating {task_type} images...\", content=Text(str(e)))\n",
        "            return\n",
        "        clear_last(2)\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky_3_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky_3_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        #print(str(images))\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            fname = f'{kandinsky_3_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not kandinsky_3_prefs['display_upscaled_image'] or not kandinsky_3_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            batch_output = os.path.join(prefs['image_output'], kandinsky_3_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if kandinsky_3_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=kandinsky_3_prefs[\"enlarge_scale\"], face_enhance=kandinsky_3_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                if kandinsky_3_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([Img(src=upscaled_path, width=pr['width'] * float(kandinsky_3_prefs[\"enlarge_scale\"]), height=pr['height'] * float(kandinsky_3_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {kandinsky_3_prefs['enlarge_scale']}x with ESRGAN\" if kandinsky_3_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"Kandinsky 3 {task_type}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    #metadata.add_text(\"title\", kandinsky_3_prefs['file_name'])\n",
        "                    # TODO: Merge Metadata with pr[]\n",
        "                    config_json = kandinsky_3_prefs.copy()\n",
        "                    config_json['model_path'] = \"kandinsky-community/kandinsky-2-2-decoder\"\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], kandinsky_3_prefs['batch_folder_name']), fname, 0)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_kandinsky(page, from_list=False, with_params=False):\n",
        "    global kandinsky_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if int(status['cpu_memory']) <= 10:\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.\")\n",
        "      return\n",
        "    kandinsky_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})\n",
        "        else:\n",
        "            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(kandinsky_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      kandinsky_prompts.append({'prompt': kandinsky_prefs['prompt'], 'negative_prompt':kandinsky_prefs['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.Kandinsky.controls.append(line)\n",
        "        if update:\n",
        "          page.Kandinsky.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.Kandinsky, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.Kandinsky.auto_scroll = scroll\n",
        "        page.Kandinsky.update()\n",
        "      else:\n",
        "        page.Kandinsky.auto_scroll = scroll\n",
        "        page.Kandinsky.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.Kandinsky.controls = page.Kandinsky.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = kandinsky_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Kandinsky 2.2 Engine & Models... See console log for progress.\")\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    #from diffusers import KandinskyV22PriorPipeline\n",
        "    #installer.status(\"...kandinsky-2-2-prior Pipeline\")\n",
        "    #if pipe_kandinsky_prior == None:\n",
        "    #    pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16)\n",
        "    #    pipe_kandinsky_prior.to(\"cuda\")\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky_inputs')\n",
        "    cpu_offload = False\n",
        "    for pr in kandinsky_prompts:\n",
        "        prt(installer)\n",
        "        init_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            #init_img.save(init_file)\n",
        "        mask_img = None\n",
        "        if bool(pr['mask_image']):\n",
        "            fname = pr['mask_image'].rpartition(slash)[2]\n",
        "            #mask_file = os.path.join(save_dir, fname)\n",
        "            if pr['mask_image'].startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['mask_image']):\n",
        "                    mask_img = PILImage.open(pr['mask_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                    return\n",
        "            if kandinsky_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.NEAREST)\n",
        "            mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "            #mask_img = np.asarray(mask_img)\n",
        "            #mask_img.save(mask_file)\n",
        "        #print(f'Resize to {width}x{height}')\n",
        "        task_type = \"inpainting\" if bool(pr['init_image']) and bool(pr['mask_image']) else \"img2img\" if bool(pr['init_image']) and not bool(pr['mask_image']) else \"text2img\"\n",
        "        installer.status(f\"...kandinsky-2-2 {task_type} Pipeline\")\n",
        "        if pipe_kandinsky == None or loaded_kandinsky_task != task_type:\n",
        "            clear_pipes('kandinsky_prior')\n",
        "            try:\n",
        "                #if bool(kandinsky_prefs['init_image']) and not bool(kandinsky_prefs['mask_image']):\n",
        "                #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                #pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                '''if task_type == \"text2img\":\n",
        "                    from diffusers import KandinskyV22Pipeline\n",
        "                    pipe_kandinsky = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"img2img\":\n",
        "                    from diffusers import KandinskyV22Img2ImgPipeline\n",
        "                    pipe_kandinsky = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"inpainting\":\n",
        "                    from diffusers import KandinskyV22InpaintPipeline\n",
        "                    pipe_kandinsky = KandinskyV22InpaintPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                '''\n",
        "                if task_type == \"text2img\":\n",
        "                    from diffusers import AutoPipelineForText2Image\n",
        "                    pipe_kandinsky = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"img2img\":\n",
        "                    from diffusers import AutoPipelineForImage2Image\n",
        "                    pipe_kandinsky = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                elif task_type == \"inpainting\":\n",
        "                    from diffusers import AutoPipelineForInpainting\n",
        "                    pipe_kandinsky = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                if prefs['enable_torch_compile']:\n",
        "                    installer.status(f\"...Torch compiling unet\")\n",
        "                    pipe_kandinsky.unet.to(memory_format=torch.channels_last)\n",
        "                    pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                    pipe_kandinsky = pipe_kandinsky.to(\"cuda\")\n",
        "                elif cpu_offload:\n",
        "                    pipe_kandinsky.enable_model_cpu_offload()\n",
        "                else:\n",
        "                    pipe_kandinsky.to(\"cuda\")\n",
        "                loaded_kandinsky_task = task_type\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "        else:\n",
        "            clear_pipes('kandinsky')\n",
        "        clear_last()\n",
        "        prt(\"Generating your Kandinsky 2.2 Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        total_steps = pr['steps']\n",
        "        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "        try:\n",
        "            #image_embeds, negative_image_embeds = pipe_kandinsky_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()\n",
        "            #negative_image_embeds = pipe_kandinsky_prior(pr['negative_prompt'], guidance_scale=2.0, num_inference_steps=25, generator=generator, negative_prompt=pr['negative_prompt']).images\n",
        "            #guidance_scale=pr['guidance_scale'], num_inference_steps=pr['steps']\n",
        "            if task_type == \"text2img\":\n",
        "                #images_texts = [kandinsky_prefs['prompt'], init_img]\n",
        "                #weights = [0.5, kandinsky_prefs['strength']]\n",
        "                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],\n",
        "                    #pr['prompt'],\n",
        "                    #image_embeds=image_embeds,\n",
        "                    #negative_image_embeds=negative_image_embeds,\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            elif task_type == \"img2img\":\n",
        "                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],\n",
        "                    #pr['prompt'],\n",
        "                    image=init_img,\n",
        "                    strength=pr['strength'],\n",
        "                    #image_embeds=image_embeds,\n",
        "                    #negative_image_embeds=negative_image_embeds,\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "            elif task_type == \"inpainting\":\n",
        "                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "                images = pipe_kandinsky(\n",
        "                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],\n",
        "                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],\n",
        "                    #pr['prompt'],\n",
        "                    image=init_img,\n",
        "                    mask_image=mask_img,\n",
        "                    #strength=pr['strength'],\n",
        "                    #image_embeds=image_embeds,\n",
        "                    #negative_image_embeds=negative_image_embeds,\n",
        "                    num_images_per_prompt=pr['num_images'],\n",
        "                    width=pr['width'],\n",
        "                    height=pr['height'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "        except Exception as e:\n",
        "            clear_last(2)\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating {task_type} images...\", content=Text(str(e)))\n",
        "            return\n",
        "        clear_last(2)\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        #print(str(images))\n",
        "        if images is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        idx = 0\n",
        "        for image in images:\n",
        "            fname = format_filename(pr['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{kandinsky_prefs[\"file_prefix\"]}{fname}'\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            image.save(image_path)\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            if not kandinsky_prefs['display_upscaled_image'] or not kandinsky_prefs['apply_ESRGAN_upscale']:\n",
        "                #prt(Row([Img(src=image_path, width=kandinsky_prefs['width'], height=kandinsky_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #if save_to_GDrive:\n",
        "            batch_output = os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "            if storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': kandinsky_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            #out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "            if kandinsky_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                upscale_image(image_path, upscaled_path, scale=kandinsky_prefs[\"enlarge_scale\"], face_enhance=kandinsky_prefs[\"face_enhance\"])\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if kandinsky_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([Img(src=upscaled_path, width=pr['width'] * float(kandinsky_prefs[\"enlarge_scale\"]), height=pr['height'] * float(kandinsky_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            #else:\n",
        "            #    time.sleep(1.2)\n",
        "            #    shutil.copy(image_path, os.path.join(out_path, output_file))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {kandinsky_prefs['enlarge_scale']}x with ESRGAN\" if kandinsky_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", f\"Kandinsky 2.1 {task_type}\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    #metadata.add_text(\"title\", kandinsky_prefs['file_name'])\n",
        "                    # TODO: Merge Metadata with pr[]\n",
        "                    config_json = kandinsky_prefs.copy()\n",
        "                    config_json['model_path'] = \"kandinsky-community/kandinsky-2-2-decoder\"\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "loaded_kandinsky21_task = \"\"\n",
        "def run_kandinsky21(page):\n",
        "    global kandinsky21_prefs, pipe_kandinsky, prefs, loaded_kandinsky21_task\n",
        "    #if status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"Sorry, currently incompatible with Diffusers installed...\", content=Text(\"To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix.\"))\n",
        "    #  return\n",
        "    if not bool(kandinsky21_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Kandinsky21.controls.append(line)\n",
        "      page.Kandinsky21.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Kandinsky21, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Kandinsky21.auto_scroll = scroll\n",
        "      page.Kandinsky21.update()\n",
        "    def clear_list():\n",
        "      page.Kandinsky21.controls = page.Kandinsky21.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Kandinsky 2.1 Engine & Models... See console log for progress.\"))\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    '''try:\n",
        "        if transformers.__version__ != \"4.23.1\": # Kandinsky conflict\n",
        "          run_sp(\"pip uninstall -y transformers\", realtime=True)\n",
        "          run_process(\"pip uninstall -y git+https://github.com/huggingface/transformers\", realtime=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    finally:\n",
        "        run_sp(\"pip install --target lib --upgrade transformers==4.23.1 -q\", realtime=True)\n",
        "        #print(f\"Installed transformers v{transformers.__version__}\")\n",
        "    run_process(\"pip install -q sentencepiece\", realtime=False)'''\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"pip install -q --upgrade git+https://github.com/huggingface/accelerate.git\", realtime=True)\n",
        "        pass\n",
        "    try:\n",
        "        import clip\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=True)\n",
        "        pass\n",
        "    try:\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "    except ModuleNotFoundError:\n",
        "        #run_process(\"pip install transformers==4.23.1 --upgrade --force-reinstall -q\", realtime=False)\n",
        "        #run_process(\"pip install -q git+https://github.com/ai-forever/Kandinsky-2.0.git\", realtime=False)\n",
        "        #run_sp('pip install -q \"git+https://github.com/ai-forever/Kandinsky-2.0.git\"', realtime=True)\n",
        "        run_sp('pip install \"git+https://github.com/Skquark/Kandinsky-2.git\"', realtime=True)\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "        pass\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky21_inputs')\n",
        "    init_img = None\n",
        "    if bool(kandinsky21_prefs['init_image']):\n",
        "        fname = kandinsky21_prefs['init_image'].rpartition(slash)[2]\n",
        "        #init_file = os.path.join(save_dir, fname)\n",
        "        if kandinsky21_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(kandinsky21_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(kandinsky21_prefs['init_image']):\n",
        "                init_img = PILImage.open(kandinsky21_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {kandinsky21_prefs['init_image']}\")\n",
        "                return\n",
        "        init_img = init_img.resize((kandinsky21_prefs['width'], kandinsky21_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        #init_img.save(init_file)\n",
        "    mask_img = None\n",
        "    if bool(kandinsky21_prefs['mask_image']):\n",
        "        fname = kandinsky21_prefs['init_image'].rpartition(slash)[2]\n",
        "        #mask_file = os.path.join(save_dir, fname)\n",
        "        if kandinsky21_prefs['mask_image'].startswith('http'):\n",
        "            mask_img = PILImage.open(requests.get(kandinsky21_prefs['mask_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(kandinsky21_prefs['mask_image']):\n",
        "                mask_img = PILImage.open(kandinsky21_prefs['mask_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your mask_image {kandinsky21_prefs['mask_image']}\")\n",
        "                return\n",
        "            if kandinsky21_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "        mask_img = mask_img.resize((kandinsky21_prefs['width'], kandinsky21_prefs['height']), resample=PILImage.NEAREST)\n",
        "        mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "        mask_img = np.asarray(mask_img)\n",
        "        #mask_img.save(mask_file)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    task_type = \"inpainting\" if bool(kandinsky21_prefs['init_image']) and bool(kandinsky21_prefs['mask_image']) else \"text2img\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky21_task != task_type:\n",
        "        clear_pipes()\n",
        "        try:\n",
        "            #if bool(kandinsky21_prefs['init_image']) and not bool(kandinsky21_prefs['mask_image']):\n",
        "            #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            loaded_kandinsky21_task = task_type\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('kandinsky')\n",
        "    clear_last()\n",
        "    prt(\"Generating your Kandinsky 2.1 Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    try:\n",
        "        if bool(kandinsky21_prefs['init_image']) and not bool(kandinsky21_prefs['mask_image']):\n",
        "            images_texts = [kandinsky21_prefs['prompt'], init_img]\n",
        "            weights = [0.5, kandinsky21_prefs['strength']]\n",
        "            #images = pipe_kandinsky.generate_img2img(kandinsky21_prefs['prompt'], init_img, strength=kandinsky21_prefs['strength'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "            #images = pipe_kandinsky.generate_img2img(kandinsky21_prefs['prompt'], init_img, strength=kandinsky21_prefs['strength'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "        elif bool(kandinsky21_prefs['init_image']) and bool(kandinsky21_prefs['mask_image']):\n",
        "            #images = pipe_kandinsky.generate_inpainting(kandinsky21_prefs['prompt'], init_img, mask_img, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.generate_inpainting(kandinsky21_prefs['prompt'], init_img, mask_img, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "        else:\n",
        "            #images = pipe_kandinsky.generate_text2img(kandinsky21_prefs['prompt'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.generate_text2img(kandinsky21_prefs['prompt'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last(2)\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(kandinsky21_prefs['prompt'])\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{kandinsky21_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky21_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky21_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not kandinsky21_prefs['display_upscaled_image'] or not kandinsky21_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=kandinsky21_prefs['width'], height=kandinsky21_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=kandinsky21_prefs['width'], height=kandinsky21_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #if save_to_GDrive:\n",
        "        batch_output = os.path.join(prefs['image_output'], kandinsky21_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(batch_output):\n",
        "            os.makedirs(batch_output)\n",
        "        if storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': kandinsky21_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "\n",
        "        if kandinsky21_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            upscale_image(image_path, upscaled_path, scale=kandinsky21_prefs[\"enlarge_scale\"], face_enhance=kandinsky21_prefs[\"face_enhance\"])\n",
        "            if kandinsky21_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=kandinsky21_prefs['width'] * float(kandinsky21_prefs[\"enlarge_scale\"]), height=kandinsky21_prefs['height'] * float(kandinsky21_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_kandinsky_fuse(page):\n",
        "    global kandinsky_fuse_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if int(status['cpu_memory']) <= 12:\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.\")\n",
        "      return\n",
        "    if len(kandinsky_fuse_prefs['mixes']) < 1:\n",
        "      alert_msg(page, \"You must provide layers to fuse to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.KandinskyFuse.controls.append(line)\n",
        "      page.KandinskyFuse.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.KandinskyFuse, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.KandinskyFuse.auto_scroll = scroll\n",
        "      page.KandinskyFuse.update()\n",
        "    def clear_list():\n",
        "      page.KandinskyFuse.controls = page.KandinskyFuse.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = kandinsky_fuse_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Kandinsky 2.2 Engine & Models... See console log for progress.\"))\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
        "    if pipe_kandinsky_prior == None:\n",
        "        pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained(\n",
        "            \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n",
        "        )\n",
        "        pipe_kandinsky_prior.to(\"cuda\")\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky_fuse_inputs')\n",
        "    images_texts = []\n",
        "    weights = []\n",
        "    mix_names = []\n",
        "    for mix in kandinsky_fuse_prefs['mixes']:\n",
        "        if 'prompt' in mix:\n",
        "            images_texts.append(mix['prompt'])\n",
        "            mix_names.append(mix['prompt'])\n",
        "        else:\n",
        "            init_img = None\n",
        "            fname = mix['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if mix['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mix['init_image']):\n",
        "                    init_img = PILImage.open(mix['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {mix['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((kandinsky_fuse_prefs['width'], kandinsky_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            images_texts.append(init_img)\n",
        "        weights.append(mix['weight'])\n",
        "    mix_name = \" - \".join(mix_names)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    task_type = \"text2img\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:\n",
        "        clear_pipes('kandinsky_prior')\n",
        "        try:\n",
        "            #pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_kandinsky = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                pipe_kandinsky.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "            else:\n",
        "                pipe_kandinsky.to(\"cuda\")\n",
        "            loaded_kandinsky_task = \"text2img\"\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes(\"kandinsky\")\n",
        "    clear_last()\n",
        "    prt(\"Generating your Kandinsky 2.2 Fused Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(kandinsky_fuse_prefs['seed']) if int(kandinsky_fuse_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "\n",
        "    try:\n",
        "        prior_out = pipe_kandinsky_prior.interpolate(images_texts, weights)\n",
        "        #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_fuse_prefs['num_images'], w=kandinsky_fuse_prefs['width'], h=kandinsky_fuse_prefs['height'], num_steps=kandinsky_fuse_prefs['steps'], prior_cf_scale=kandinsky_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky_fuse_prefs['prior_steps']), sampler=kandinsky_fuse_prefs['sampler'], guidance_scale=kandinsky_fuse_prefs['guidance_scale'])\n",
        "        images = pipe_kandinsky(\n",
        "            \"\",\n",
        "            **prior_out,\n",
        "            num_images_per_prompt=kandinsky_fuse_prefs['num_images'],\n",
        "            height=kandinsky_fuse_prefs['height'],\n",
        "            width=kandinsky_fuse_prefs['width'],\n",
        "            num_inference_steps=kandinsky_fuse_prefs['steps'],\n",
        "            guidance_scale=kandinsky_fuse_prefs['guidance_scale'],\n",
        "            generator=generator,\n",
        "            callback_on_step_end=callback_fnc,\n",
        "        ).images\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last(2)\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(mix_name)\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{kandinsky_fuse_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky_fuse_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky_fuse_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not kandinsky_fuse_prefs['display_upscaled_image'] or not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        #if save_to_GDrive:\n",
        "        batch_output = os.path.join(prefs['image_output'], kandinsky_fuse_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(batch_output):\n",
        "            os.makedirs(batch_output)\n",
        "        if storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': kandinsky_fuse_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "\n",
        "        if kandinsky_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            upscale_image(image_path, upscaled_path, scale=kandinsky_fuse_prefs[\"enlarge_scale\"], face_enhance=kandinsky_fuse_prefs[\"face_enhance\"])\n",
        "            if kandinsky_fuse_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=kandinsky_fuse_prefs['width'] * float(kandinsky_fuse_prefs[\"enlarge_scale\"]), height=kandinsky_fuse_prefs['height'] * float(kandinsky_fuse_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            time.sleep(1.2)\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_kandinsky21_fuse(page):\n",
        "    global kandinsky21_fuse_prefs, pipe_kandinsky, prefs, loaded_kandinsky21_task\n",
        "    #if status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"Sorry, currently incompatible with Diffusers installed...\", content=Text(\"To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix.\"))\n",
        "    #  return\n",
        "    if len(kandinsky21_fuse_prefs['mixes']) < 1:\n",
        "      alert_msg(page, \"You must provide layers to fuse to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Kandinsky21Fuse.controls.append(line)\n",
        "      page.Kandinsky21Fuse.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.Kandinsky21Fuse, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Kandinsky21Fuse.auto_scroll = scroll\n",
        "      page.Kandinsky21Fuse.update()\n",
        "    def clear_list():\n",
        "      page.Kandinsky21Fuse.controls = page.Kandinsky21Fuse.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Kandinsky 2.1 Engine & Models... See console log for progress.\"))\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    try:\n",
        "        import clip\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=False)\n",
        "    try:\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install -q \"git+https://github.com/Skquark/Kandinsky-2.git\"', realtime=False)\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "        pass\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky21_fuse_inputs')\n",
        "    images_texts = []\n",
        "    weights = []\n",
        "    mix_names = []\n",
        "    for mix in kandinsky21_fuse_prefs['mixes']:\n",
        "        if 'prompt' in mix:\n",
        "            images_texts.append(mix['prompt'])\n",
        "            mix_names.append(mix['prompt'])\n",
        "        else:\n",
        "            init_img = None\n",
        "            fname = mix['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if mix['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mix['init_image']):\n",
        "                    init_img = PILImage.open(mix['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {mix['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((kandinsky21_fuse_prefs['width'], kandinsky21_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            images_texts.append(init_img)\n",
        "        weights.append(mix['weight'])\n",
        "    mix_name = \" - \".join(mix_names)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    task_type = \"text2img\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky21_task != task_type:\n",
        "        clear_pipes()\n",
        "        try:\n",
        "            pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            loaded_kandinsky21_task = \"text2img\"\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes(\"kandinsky\")\n",
        "    clear_last()\n",
        "    prt(\"Generating your Kandinsky 2.1 Fused Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    try:\n",
        "        images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky21_fuse_prefs['num_images'], w=kandinsky21_fuse_prefs['width'], h=kandinsky21_fuse_prefs['height'], num_steps=kandinsky21_fuse_prefs['steps'], prior_cf_scale=kandinsky21_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_fuse_prefs['prior_steps']), sampler=kandinsky21_fuse_prefs['sampler'], guidance_scale=kandinsky21_fuse_prefs['guidance_scale'])\n",
        "    except Exception as e:\n",
        "        clear_last(2)\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last(2)\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(mix_name)\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{kandinsky21_fuse_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky21_fuse_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky21_fuse_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not kandinsky21_fuse_prefs['display_upscaled_image'] or not kandinsky21_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=kandinsky21_fuse_prefs['width'], height=kandinsky21_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=kandinsky21_fuse_prefs['width'], height=kandinsky21_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        #if save_to_GDrive:\n",
        "        batch_output = os.path.join(prefs['image_output'], kandinsky21_fuse_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(batch_output):\n",
        "            os.makedirs(batch_output)\n",
        "        if storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': kandinsky21_fuse_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "\n",
        "        if kandinsky21_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            upscale_image(image_path, upscaled_path, scale=kandinsky21_fuse_prefs[\"enlarge_scale\"], face_enhance=kandinsky21_fuse_prefs[\"face_enhance\"])\n",
        "            if kandinsky21_fuse_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=kandinsky21_fuse_prefs['width'] * float(kandinsky21_fuse_prefs[\"enlarge_scale\"]), height=kandinsky21_fuse_prefs['height'] * float(kandinsky21_fuse_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_kandinsky_controlnet(page, from_list=False, with_params=False):\n",
        "    global kandinsky_controlnet_prefs, pipe_kandinsky, pipe_kandinsky_controlnet_prior, prefs, loaded_kandinsky_task, depth_estimator\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if int(status['cpu_memory']) <= 12:\n",
        "      alert_msg(page, f\"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.\")\n",
        "      return\n",
        "    kandinsky_controlnet_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if with_params:\n",
        "            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})\n",
        "        else:\n",
        "            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['n_iterations'], 'batch_size':p['batch_size'], 'seed':p['seed']})\n",
        "    else:\n",
        "      if not bool(kandinsky_controlnet_prefs['prompt']):\n",
        "        alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "        return\n",
        "      kandinsky_controlnet_prompts.append({'prompt': kandinsky_controlnet_prefs['prompt'], 'negative_prompt':kandinsky_controlnet_prefs['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.KandinskyControlNet.controls.append(line)\n",
        "        if update:\n",
        "          page.KandinskyControlNet.update()\n",
        "    def clear_last(lines=1):\n",
        "      if from_list:\n",
        "        clear_line(page.imageColumn, lines=lines)\n",
        "      else:\n",
        "        clear_line(page.KandinskyControlNet, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.KandinskyControlNet.auto_scroll = scroll\n",
        "        page.KandinskyControlNet.update()\n",
        "      else:\n",
        "        page.KandinskyControlNet.auto_scroll = scroll\n",
        "        page.KandinskyControlNet.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.KandinskyControlNet.controls = page.KandinskyControlNet.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = kandinsky_controlnet_prefs['steps']\n",
        "    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    installer = Installing(\"Installing Kandinsky 2.2 Engine & Models... See console log for progress.\")\n",
        "    prt(installer)\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n",
        "    from transformers import pipeline\n",
        "    if pipe_kandinsky_controlnet_prior == None:\n",
        "        installer.status(\"...kandinsky-2-2-prior Emb2Emb Pipeline\")\n",
        "        pipe_kandinsky_controlnet_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\",torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        pipe_kandinsky_controlnet_prior.to(\"cuda\")\n",
        "    task_type = \"controlnet\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:\n",
        "        installer.status(f\"...kandinsky-2-2 {task_type} Pipeline\")\n",
        "        clear_pipes('kandinsky_controlnet_prior')\n",
        "        try:\n",
        "            if task_type == \"controlnet\":\n",
        "                pipe_kandinsky = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            if prefs['enable_torch_compile']:\n",
        "                installer.status(f\"...run torch compile\")\n",
        "                pipe_kandinsky.unet.to(memory_format=torch.channels_last)\n",
        "                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "                pipe_kandinsky = pipe_kandinsky.to(\"cuda\")\n",
        "            elif int(status['cpu_memory']) <= 12:\n",
        "                installer.status(f\"...enable_model_cpu_offload\")\n",
        "                pipe_kandinsky.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe_kandinsky.to(\"cuda\")\n",
        "            loaded_kandinsky_task = task_type\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing KandinskyV22ControlnetImg2ImgPipeline...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('kandinsky')\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky_controlnet_inputs')\n",
        "    def make_hint(image, depth_estimator):\n",
        "        image = depth_estimator(image)['depth']\n",
        "        image = np.array(image)\n",
        "        image = image[:, :, None]\n",
        "        image = np.concatenate([image, image, image], axis=2)\n",
        "        detected_map = torch.from_numpy(image).float() / 255.0\n",
        "        hint = detected_map.permute(2, 0, 1)\n",
        "        return hint\n",
        "    # step1: create models and pipelines\n",
        "    if depth_estimator == None:\n",
        "        installer.status(\"...depth-estimation Pipeline\")\n",
        "        depth_estimator = pipeline('depth-estimation')\n",
        "    clear_last()\n",
        "    for pr in kandinsky_controlnet_prompts:\n",
        "        init_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            fname = pr['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            #init_img.save(init_file)\n",
        "        #clear_last()\n",
        "        prt(\"Generating your Kandinsky 2.2 ControlNet Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        hint = make_hint(init_img, depth_estimator).unsqueeze(0).half().to('cuda')\n",
        "        for num in range(pr['num_images']):\n",
        "            random_seed = int(pr['seed'] + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "            try:\n",
        "                image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['prompt'], image=init_img, strength=kandinsky_controlnet_prefs['prior_strength'], num_inference_steps = kandinsky_controlnet_prefs['prior_steps'], generator=generator)\n",
        "                negative_image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['negative_prompt'], image=init_img, strength=1, num_inference_steps=kandinsky_controlnet_prefs['prior_steps'], generator=generator)\n",
        "                #image_embeds, negative_image_embeds = pipe_kandinsky_controlnet_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()\n",
        "                #if task_type == \"controlnet\":\n",
        "                images = pipe_kandinsky(\n",
        "                    #pr['prompt'],\n",
        "                    image=init_img,\n",
        "                    image_embeds=image_embeds.image_embeds,\n",
        "                    negative_image_embeds=negative_image_embeds.image_embeds,\n",
        "                    hint=hint,\n",
        "                    strength=pr['strength'],\n",
        "                    height=pr['height'],\n",
        "                    width=pr['width'],\n",
        "                    num_inference_steps=pr['steps'],\n",
        "                    guidance_scale=pr['guidance_scale'],\n",
        "                    num_images_per_prompt=pr['batch_size'],\n",
        "                    generator=generator,\n",
        "                    callback_on_step_end=callback_fnc,\n",
        "                ).images\n",
        "\n",
        "            except Exception as e:\n",
        "                clear_last(2)\n",
        "                alert_msg(page, f\"ERROR: Something went wrong generating {task_type} images...\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last(2)\n",
        "            autoscroll(True)\n",
        "            txt2img_output = stable_dir\n",
        "            batch_output = prefs['image_output']\n",
        "            txt2img_output = stable_dir\n",
        "            if bool(kandinsky_controlnet_prefs['batch_folder_name']):\n",
        "                txt2img_output = os.path.join(stable_dir, kandinsky_controlnet_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(txt2img_output):\n",
        "                os.makedirs(txt2img_output)\n",
        "            #print(str(images))\n",
        "            if images is None:\n",
        "                prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "                return\n",
        "            idx = 0\n",
        "            for image in images:\n",
        "                fname = format_filename(pr['prompt'])\n",
        "                #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "                fname = f'{kandinsky_controlnet_prefs[\"file_prefix\"]}{fname}'\n",
        "                image_path = available_file(txt2img_output, fname, 1)\n",
        "                image.save(image_path)\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                if not kandinsky_controlnet_prefs['display_upscaled_image'] or not kandinsky_controlnet_prefs['apply_ESRGAN_upscale']:\n",
        "                    #prt(Row([Img(src=image_path, width=kandinsky_controlnet_prefs['width'], height=kandinsky_controlnet_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #if save_to_GDrive:\n",
        "                batch_output = os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name'])\n",
        "                if not os.path.exists(batch_output):\n",
        "                    os.makedirs(batch_output)\n",
        "                if storage_type == \"PyDrive Google Drive\":\n",
        "                    newFolder = gdrive.CreateFile({'title': kandinsky_controlnet_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                    newFolder.Upload()\n",
        "                    batch_output = newFolder\n",
        "                #out_path = batch_output# if save_to_GDrive else txt2img_output\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "\n",
        "                if kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    upscale_image(image_path, upscaled_path, scale=kandinsky_controlnet_prefs[\"enlarge_scale\"], face_enhance=kandinsky_controlnet_prefs[\"face_enhance\"])\n",
        "                    image_path = upscaled_path\n",
        "                    if kandinsky_controlnet_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([Img(src=upscaled_path, width=pr['width'] * float(kandinsky_controlnet_prefs[\"enlarge_scale\"]), height=pr['height'] * float(kandinsky_controlnet_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                #else:\n",
        "                #    time.sleep(1.2)\n",
        "                #    shutil.copy(image_path, os.path.join(out_path, output_file))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {kandinsky_controlnet_prefs['enlarge_scale']}x with ESRGAN\" if kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"Kandinsky 2.1 {task_type}\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                        #metadata.add_text(\"title\", kandinsky_controlnet_prefs['file_name'])\n",
        "                        # TODO: Merge Metadata with pr[]\n",
        "                        config_json = kandinsky_controlnet_prefs.copy()\n",
        "                        config_json['model_path'] = \"kandinsky-community/kandinsky-2-2-controlnet-depth\"\n",
        "                        config_json['seed'] = random_seed\n",
        "                        del config_json['num_images']\n",
        "                        del config_json['display_upscaled_image']\n",
        "                        del config_json['batch_folder_name']\n",
        "                        if not config_json['apply_ESRGAN_upscale']:\n",
        "                            del config_json['enlarge_scale']\n",
        "                            del config_json['apply_ESRGAN_upscale']\n",
        "                        metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_deep_daze(page):\n",
        "    global deep_daze_prefs, status\n",
        "    #https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing#scrollTo=6IPQ_rdA2Sa7\n",
        "    def add_to_deep_daze_output(o):\n",
        "      page.DeepDaze.controls.append(o)\n",
        "      page.DeepDaze.update()\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.DeepDaze.controls.append(line)\n",
        "      page.DeepDaze.update()\n",
        "    def clear_last(lines=1):\n",
        "      clear_line(page.DeepDaze, lines=lines)\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DeepDaze.auto_scroll = scroll\n",
        "      page.DeepDaze.update()\n",
        "    def clear_list():\n",
        "      page.DeepDaze.controls = page.DeepDaze.controls[:1]\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    progress = ProgressBar(bar_height=8, expand=True)\n",
        "    total_steps = deep_daze_prefs['iterations']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    prt(Installing(\"Initializing Deep Daze Pipeline...\"))\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from tqdm.notebook import trange\n",
        "    try:\n",
        "        from deep_daze import Imagine\n",
        "    except Exception:\n",
        "        run_process(\"pip install deep-daze --upgrade\", page=page)\n",
        "        from deep_daze import Imagine\n",
        "    output_dir = os.path.join(root_dir, \"deep_daze\")\n",
        "    if bool(deep_daze_prefs['batch_folder_name']):\n",
        "        output_dir = os.path.join(output_dir, deep_daze_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    os.chdir(output_dir)\n",
        "    abort_run = False\n",
        "    def abort_daze(e):\n",
        "        nonlocal abort_run\n",
        "        abort_run = True\n",
        "        autoscroll(False)\n",
        "        page.snd_error.play()\n",
        "        page.snd_delete.play()\n",
        "    pipe_deep_daze = Imagine(\n",
        "        text = deep_daze_prefs['prompt'],\n",
        "        num_layers = deep_daze_prefs['num_layers'],\n",
        "        save_every = deep_daze_prefs['save_every'],\n",
        "        image_width = deep_daze_prefs['max_size'],\n",
        "        lr = deep_daze_prefs['learning_rate'],\n",
        "        iterations = deep_daze_prefs['iterations'],\n",
        "        save_progress = True,#deep_daze_prefs['save_progress']\n",
        "    )\n",
        "    clear_last()\n",
        "    prt(\"    Dreaming up your DeepDaze Image...\")\n",
        "    autoscroll(False)\n",
        "    prt(Row([progress, IconButton(icon=icons.CANCEL, tooltip=\"Abort Daze & Save Progress\", on_click=abort_daze)], alignment=MainAxisAlignment.SPACE_BETWEEN))\n",
        "    #filename = format_filename(deep_daze_prefs['prompt'])\n",
        "    filename = deep_daze_prefs['prompt'].replace(' ', '_')\n",
        "    img = None\n",
        "    image_files = []\n",
        "    s = 0\n",
        "    for epoch in trange(20, desc = 'epochs'):\n",
        "        for i in trange(deep_daze_prefs['iterations'], desc = 'iteration'):\n",
        "            pipe_deep_daze.train_step(epoch, i)\n",
        "            callback_fnc(i)\n",
        "            if abort_run: break\n",
        "            if i % pipe_deep_daze.save_every != 0:\n",
        "                continue\n",
        "            num_str = '' if s==0 else f'.{str(s).zfill(6)}'\n",
        "            fname = f\"{filename}{num_str}.jpg\"\n",
        "            output_file = os.path.join(output_dir, fname)\n",
        "            image_files.append(output_file)\n",
        "            if s == 0:\n",
        "                img = Img(src=output_file, fit=ImageFit.FIT_WIDTH, gapless_playback=True)\n",
        "                prt(Row([img], alignment=MainAxisAlignment.CENTER))\n",
        "            else:\n",
        "                img.src=output_file\n",
        "                img.update()\n",
        "            s += 1\n",
        "        if abort_run: break\n",
        "            #image = Image(f'./{filename}.jpg')\n",
        "            #display(image)\n",
        "    output_filename = deep_daze_prefs['file_prefix'] + format_filename(deep_daze_prefs['prompt'])\n",
        "    unscaled_path = fname\n",
        "    image_path = available_file(output_dir, output_filename, 0)\n",
        "    unscaled_path = image_path\n",
        "    output_file = image_path.rpartition(slash)[2]\n",
        "    out_path = image_path.rpartition(slash)[0]\n",
        "    upscaled_path = os.path.join(out_path, output_file)\n",
        "    input = PILImage.open(fname)\n",
        "    input.save(image_path)\n",
        "    #shutil.copy(fname, image_path)\n",
        "    clear_last(3)\n",
        "    autoscroll(True)\n",
        "    if not deep_daze_prefs['display_upscaled_image'] or not deep_daze_prefs['apply_ESRGAN_upscale']:\n",
        "        prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    if deep_daze_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        upscale_image(image_path, upscaled_path, scale=deep_daze_prefs[\"enlarge_scale\"])\n",
        "        image_path = upscaled_path\n",
        "    if prefs['save_image_metadata']:\n",
        "        img = PILImage.open(image_path)\n",
        "        metadata = PngInfo()\n",
        "        metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "        metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "        metadata.add_text(\"software\", \"AEIONic Diffusion Deluxe\" + f\", upscaled {deep_daze_prefs['enlarge_scale']}x with ESRGAN\" if deep_daze_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "        metadata.add_text(\"pipeline\", f\"Deep Daze\")\n",
        "        if prefs['save_config_in_metadata']:\n",
        "            metadata.add_text(\"title\", deep_daze_prefs['prompt'])\n",
        "            config_json = deep_daze_prefs.copy()\n",
        "            #config_json['model_path'] = model_id\n",
        "            del config_json['num_images']\n",
        "            del config_json['display_upscaled_image']\n",
        "            del config_json['batch_folder_name']\n",
        "            if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "            del config_json['apply_ESRGAN_upscale']\n",
        "            metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "        img.save(image_path, pnginfo=metadata)\n",
        "    #TODO: PyDrive\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "    elif bool(prefs['image_output']):\n",
        "        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "    if deep_daze_prefs['save_progress']:\n",
        "        for f in image_files:\n",
        "            if os.path.exists(f):\n",
        "                shutil.copy(f, os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name'], f.rpartition(slash)[2]))\n",
        "    else:\n",
        "        for f in image_files[:-1]:\n",
        "            if os.path.exists(f):\n",
        "                os.remove(f)\n",
        "    if deep_daze_prefs['display_upscaled_image']:\n",
        "        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(deep_daze_prefs[\"enlarge_scale\"]), height=512 * float(deep_daze_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "    del pipe_deep_daze\n",
        "    flush()\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def main(page: Page):\n",
        "    global status\n",
        "    page.title = \"AEIONic Diffusion Deluxe - FletUI\"\n",
        "    def open_url(e):\n",
        "        page.launch_url(e.data)\n",
        "    def exit_disconnect(e):\n",
        "        save_settings_file(page, change_icon=is_Colab)\n",
        "        if is_Colab:\n",
        "          #run_sp(\"install pyautogui\", realtime=False)\n",
        "          #import pyautogui\n",
        "          #pyautogui.hotkey('ctrl', 'w')\n",
        "          #import keyboard\n",
        "          #keyboard.press_and_release('ctrl+w')\n",
        "          #time.sleep(1.5)\n",
        "          #close_tab()\n",
        "          page.window_close()\n",
        "          from google.colab import runtime\n",
        "          runtime.unassign()\n",
        "        else:\n",
        "          page.window_close()\n",
        "    def minimize_window(e):\n",
        "        page.window_minimized = True\n",
        "        page.update()\n",
        "    def maximize_window(e):\n",
        "        if page.window_maximized:\n",
        "            page.window_maximized = False\n",
        "            appbar.actions[3].icon=icons.CHECK_BOX_OUTLINE_BLANK\n",
        "            appbar.actions[3].tooltip = \"Maximize Window\"\n",
        "        else:\n",
        "            page.window_maximized = True\n",
        "            appbar.actions[3].icon=icons.FILTER_NONE\n",
        "            appbar.actions[3].tooltip = \"Restore Window\"\n",
        "        page.update()\n",
        "    def get_memory():\n",
        "        from subprocess import getoutput\n",
        "        s = getoutput('nvidia-smi -L')\n",
        "        if 'T4' in s: gpu = 'T4'\n",
        "        elif 'P100' in s: gpu = 'P100'\n",
        "        elif 'V100' in s: gpu = 'V100'\n",
        "        elif 'A100' in s: gpu = 'A100'\n",
        "        else:\n",
        "            gpu = re.sub(r\"\\([^()]*\\)\", \"\", s).strip()\n",
        "        status['cpu_used'] = psutil.virtual_memory().used / (1024 * 1024 * 1024)\n",
        "        #memory_stats = torch.cuda.memory_stats(device=torch.device(\"cuda\"))\n",
        "        #available_memory = memory_stats[\"reserved_bytes.all.allocated\"] - memory_stats[\"allocated_bytes.all.current\"]\n",
        "        try:\n",
        "            status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device(\"cuda\")) / (1024 * 1024 * 1024)\n",
        "            gpu_mem = f\"{status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB\"\n",
        "        except Exception:\n",
        "            status['gpu_used'] = 0\n",
        "            gpu_mem = \"N/A\"\n",
        "            pass\n",
        "        memory = f\"GPU VRAM: {gpu_mem} - CPU RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB | Runnning {gpu}\"\n",
        "        return memory\n",
        "    help_dlg = None\n",
        "    def open_help_dlg(e):\n",
        "        nonlocal help_dlg\n",
        "        def clear_the_pipes(e):\n",
        "            nonlocal memory_text\n",
        "            clear_pipes()\n",
        "            time.sleep(0.8)\n",
        "            memory_text.value = get_memory()\n",
        "            memory_text.update()\n",
        "            time.sleep(3.2)\n",
        "            memory_text.value = get_memory()\n",
        "            memory_text.update()\n",
        "            time.sleep(4.6)\n",
        "            memory_text.value = get_memory()\n",
        "            memory_text.update()\n",
        "        memory_text = Text(get_memory())\n",
        "        clear_pipes_btn = ElevatedButton(content=Text(\"Clear Memory\", size=16), on_click=clear_the_pipes)\n",
        "        memory_row = Row([memory_text, Container(content=None, expand=True), clear_pipes_btn])\n",
        "        help_dlg = AlertDialog(\n",
        "            title=Text(\"üíÅ   Help/Information - AEIONic Diffusion Deluxe \" + AEIONic_version), content=Column([memory_row, Text(\"If you don't know what Stable Diffusion is, you're in for a pleasant surprise.. If you're already familiar, you're gonna love how easy it is to be an artist with the help of our AI Friends using our pretty interface.\"),\n",
        "                  Text(\"Simply go through the self-explanitory tabs step-by-step and set your preferences to get started. The default values are good for most, but you can have some fun experimenting. Most values are automatically saved as you make changes and change tabs.\"),\n",
        "                  Text(\"Each time you open the app, you should start in the Installers section, turn on all the components you plan on using in you session, then Run the Installers and let them download. You can multitask and work in other tabs while it's installing.\"),\n",
        "                  Text(\"In the Prompts List, add as many text prompts as you can think of, and edit any prompt to override any default Image Parameter.  Once you're ready, Run Diffusion on your Prompts List and watch it fill your Drive with beauty..\"),\n",
        "                  Text(\"Try out any and all of our Prompt Helpers to use practical text AIs to make unique descriptive prompts fast, with our Prompt Generator, Remixer, Brainstormer and Advanced Writer.  You'll never run out of inspiration again...\"),\n",
        "                  Text(\"Use the other Stable Diffusers to get unique results with advanced AI tools to refine your art to the next level with powerful surpises.. There's a lot of specialized Diffusers that are pure magic when you challenge it. Have fun, and get creative...\"),\n",
        "            ], scroll=ScrollMode.AUTO),\n",
        "            actions=[TextButton(\"üëç  Thanks! \", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "        )\n",
        "        page.dialog = help_dlg\n",
        "        help_dlg.open = True\n",
        "        page.update()\n",
        "    def close_help_dlg(e):\n",
        "        nonlocal help_dlg\n",
        "        help_dlg.open = False\n",
        "        page.update()\n",
        "    def open_credits_dlg(e):\n",
        "        page.dialog = credits_dlg\n",
        "        credits_dlg.open = True\n",
        "        page.update()\n",
        "    def close_credits_dlg(e):\n",
        "        credits_dlg.open = False\n",
        "        page.update()\n",
        "    credits_markdown = '''This toolkit is an Open-Source side project by [Skquark, Inc.](https://Skquark.com), primarily created by Alan Bedian for fun and full-feature functionality.  Official website is [AEIONic.com](https://AEIONic.com) for more info.\n",
        "\n",
        "The real credit goes to the team at [Stability.ai](https://Stability.ai) for making Stable Diffusion so great, and [HuggingFace](https://HuggingFace.co) for their work on the [Diffusers Pipelines](https://github.com/huggingface/diffusers). The HuggingFace Diffusers team includes Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj & Thomas Wolf.\n",
        "\n",
        "For the great app UI framework, we thank [Flet](https://Flet.dev) with the amazing Flutter based Python library with a very functional dev platform that made this possible.\n",
        "\n",
        "For the brains behind our Prompt Helpers, we thank our friends [OpenAI](https://beta.OpenAI.com), [Bloom-AI](https://huggingface.co/bigscience/bloom), [TextSynth](https://TextSynth.com) and others for making an AIs so fun to talk to and use.\n",
        "\n",
        "Shoutouts to the Discord Community of [Disco Diffusion](https://discord.gg/d5ZVbAfm), [Stable Diffusion](https://discord.gg/stablediffusion), [HuggingFace](https://discord.gg/hugging-face-879548962464493619) and [Flet](https://discord.gg/nFqy742h) for their support and user contributions.'''\n",
        "    credits_dlg = AlertDialog(\n",
        "        title=Text(\"üôå   Credits/Acknowledgments\"), content=Column([Markdown(credits_markdown, extension_set=\"gitHubWeb\", on_tap_link=open_url)\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üëä   Good Stuff... \", on_click=close_credits_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    def open_contact_dlg(e):\n",
        "        page.dialog = contact_dlg\n",
        "        contact_dlg.open = True\n",
        "        page.update()\n",
        "    def close_contact_dlg(e):\n",
        "        contact_dlg.open = False\n",
        "        page.update()\n",
        "    contact_dlg = AlertDialog(\n",
        "        title=Text(\"üì®  Contact Us\"), content=Column([\n",
        "          Text(\"If you want to reach Alan Bedian/Skquark, Inc. for any reason, feel free to send me a message (as long as it's not spam) by Email or on Discord @Skquark#0394 where I'm usually available.\"),\n",
        "          Text(\"If you're a developer and want to help with this project (or one of my other almost done apps) then you can Join my Discord Channel and get involved. It's fairly quiet in there though...\"),\n",
        "          Row([ft.FilledButton(\"Email Skquark\", on_click=lambda _:page.launch_url(\"mailto:Alan@Skquark.com\")), ft.FilledButton(\"Skquark Discord\", on_click=lambda _:page.launch_url(\"https://discord.gg/fTraJ96Z\")), ft.FilledButton(\"Skquark.com\", on_click=lambda _:page.launch_url(\"https://Skquark.com\"))], alignment=MainAxisAlignment.CENTER),\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üëÅÔ∏è‚Äçüó®Ô∏è  Maybe...\", on_click=close_contact_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    def open_donate_dlg(e):\n",
        "        page.dialog = donate_dlg\n",
        "        donate_dlg.open = True\n",
        "        page.update()\n",
        "    def close_donate_dlg(e):\n",
        "        donate_dlg.open = False\n",
        "        page.update()\n",
        "    donate_dlg = AlertDialog(\n",
        "        title=Text(\"üéÅ  Donate to the Cause\"), content=Column([\n",
        "          Text(\"This app has been a one-man labour of love with a lot of effort poured into it over several months. It's powerful enough to be a paid commercial application, however it's all open-source code behind it created by many contributers to make the tools as cool as they are.\"),\n",
        "          Text(\"While we offer this software free of charge, your support would be greatly appreciated to continue the efforts to keep making it better. If you find value in this software and are able to show some love, any amount you can offer is welcomed... I don't even own a GPU good enough to run my own app locally!\"),\n",
        "          Text(\"If you're technical enough, you can also donate by making contributions to the code, offering suggestions for improvements, adding to the Community Fine-Tuned Models list, or whatever enhancements to the project you can provide...\"),\n",
        "          Row([ft.FilledButton(\"Donate with PayPal\", on_click=lambda _:page.launch_url(\"https://paypal.me/StarmaTech\")), ft.FilledButton(\"Donate with Venmo\", on_click=lambda _:page.launch_url(\"https://venmo.com/u/Alan-Bedian\"))], alignment=MainAxisAlignment.CENTER),\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üí∏  Much appreciated\", on_click=close_donate_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    #page.window_full_screen = True\n",
        "    page.etas = []\n",
        "    page.theme_mode = prefs['theme_mode'].lower()\n",
        "    if prefs['theme_mode'] == 'Dark':\n",
        "        page.dark_theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())#, use_material3=True)\n",
        "    else:\n",
        "        page.theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())\n",
        "    app_icon_color = colors.AMBER_800\n",
        "    space = \" \"  if (page.width if page.web else page.window_width) >= 1024 else \"\"\n",
        "    def clear_memory(e):\n",
        "        clear_pipes()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    page.stats = Column([Text(\"\", size=10), Text(\"\", size=10)], tight=True, spacing=4)\n",
        "    appbar=AppBar(title=ft.WindowDragArea(Row([Container(Text(f\"üë®‚Äçüé®Ô∏è{space}  Stable Diffusion - Deluxe Edition  {space}üß∞\" if ((page.width or page.window_width) if page.web else page.window_width) >= 768 else \"AEIONic Diffusion Deluxe  üñåÔ∏è\", weight=FontWeight.BOLD, color=colors.ON_SURFACE, overflow=ft.TextOverflow.ELLIPSIS, expand=True))], alignment=MainAxisAlignment.CENTER, expand=True), expand=False), elevation=20,\n",
        "      center_title=True,\n",
        "      bgcolor=colors.SURFACE,\n",
        "      toolbar_height=46,\n",
        "      leading_width=46,\n",
        "      leading=IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Save Settings File\", on_click=lambda _: app_icon_save()),\n",
        "      #leading_width=40,\n",
        "      actions=[ft.GestureDetector(page.stats, on_tap=lambda _:update_stats(page), on_double_tap=lambda _:toggle_stats(page), on_long_press_end=clear_memory), \n",
        "        PopupMenuButton(items=[\n",
        "          PopupMenuItem(text=\"ü§î  Help/Info\", on_click=open_help_dlg),\n",
        "          PopupMenuItem(text=\"üëè  Credits\", on_click=open_credits_dlg),\n",
        "          PopupMenuItem(text=\"ü§ß  Issues/Suggestions\", on_click=lambda _:page.launch_url(\"https://github.com/Skquark/AEIONic/issues\")),\n",
        "          PopupMenuItem(text=\"üòã  Contact Skquark\", on_click=open_contact_dlg),\n",
        "          #PopupMenuItem(text=\"üì®  Email Skquark\", on_click=lambda _:page.launch_url(\"mailto:Alan@Skquark.com\")),\n",
        "          PopupMenuItem(text=\"ü§ë  Offer Donation\", on_click=open_donate_dlg),\n",
        "          #PopupMenuItem(text=\"‚ùé  Exit/Disconnect Runtime\", on_click=exit_disconnect) if is_Colab else PopupMenuItem(),\n",
        "      ])])\n",
        "    if is_Colab:\n",
        "        appbar.actions[1].items.append(PopupMenuItem())\n",
        "        appbar.actions[1].items.append(PopupMenuItem(text=\"‚ùé  Exit/Disconnect Runtime\", on_click=exit_disconnect))\n",
        "    else:\n",
        "        appbar.actions.append(IconButton(icon=icons.MINIMIZE, tooltip=\"Minimize Window\", on_click=minimize_window))\n",
        "        appbar.actions.append(IconButton(icon=icons.CHECK_BOX_OUTLINE_BLANK, tooltip=\"Maximize Window\", on_click=maximize_window))\n",
        "        appbar.actions.append(IconButton(icon=icons.CLOSE, tooltip=\"‚ùé  Exit Application\", on_click=exit_disconnect))\n",
        "        page.window_title_bar_hidden = True\n",
        "    page.appbar = appbar\n",
        "    def app_icon_save():\n",
        "        app_icon_color = colors.GREEN_800\n",
        "        appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Saving Settings File\")\n",
        "        appbar.update()\n",
        "        time.sleep(0.6)\n",
        "        app_icon_color = colors.AMBER_800\n",
        "        appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Save Settings File\", on_click=lambda _: app_icon_save())\n",
        "        appbar.update()\n",
        "    page.app_icon_save = app_icon_save\n",
        "    page.vertical_alignment = MainAxisAlignment.START\n",
        "    page.horizontal_alignment = CrossAxisAlignment.START\n",
        "    t = buildTabs(page)\n",
        "    t.on_change = tab_on_change\n",
        "    #(t,page)\n",
        "    def close_banner(e):\n",
        "        page.banner.open = False\n",
        "        page.update()\n",
        "    page.close_banner = close_banner\n",
        "    #leading=Icon(icons.DOWNLOADING, color=colors.AMBER, size=40),\n",
        "    page.status_msg = Text(\"\")\n",
        "    page.banner = Banner(bgcolor=colors.SECONDARY_CONTAINER, content=Column([]), actions=[Row([page.status_msg, TextButton(\"Close\", on_click=page.close_banner)])])\n",
        "    def show_banner_click(e):\n",
        "        page.banner.open = True\n",
        "        page.update()\n",
        "    def set_status(msg=\"\"):\n",
        "        page.status_msg.value = msg\n",
        "        page.status_msg.update()\n",
        "        page.banner.update()\n",
        "        page.update()\n",
        "    page.status = set_status\n",
        "    page.add(t)\n",
        "    initState(page)\n",
        "    if not status['initialized']:\n",
        "        status['initialized'] = True\n",
        "    #page.add(ElevatedButton(\"Show Banner\", on_click=show_banner_click))\n",
        "    #page.add (Text (\"Enhanced AEIONic Diffusion Deluxe by Skquark, Inc.\"))\n",
        "\n",
        "class Header(UserControl):\n",
        "    def __init__(self, title=\"\", subtitle=\"\", value=\"\", actions=[], divider=True):\n",
        "        super().__init__()\n",
        "        self.title = title\n",
        "        self.subtitle = subtitle\n",
        "        self.value = value\n",
        "        self.actions = actions\n",
        "        self.divider = divider\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)\n",
        "        self.column = Column([Row([Column([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Text(self.subtitle, style=\"titleSmall\", color=colors.TERTIARY) if bool(self.subtitle) else Container(content=None)], spacing=4, expand=True), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=1, vertical_alignment=CrossAxisAlignment.END), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT) if self.divider else Container(content=None), Container(content=None, height=3)], spacing=2)\n",
        "        return self.column\n",
        "\n",
        "class FileInput(UserControl):\n",
        "    def __init__(self, label=\"Initial Image\", ftype=\"image\", pref=\"\", key=\"\", expand=False, col=None, filled=False, max_size=None, output_dir=None, page=None):\n",
        "        super().__init__()\n",
        "        self.label = label\n",
        "        self.ftype = ftype\n",
        "        self.pref = pref\n",
        "        self.key = key\n",
        "        self.expand = expand\n",
        "        self.col = col\n",
        "        self.filled = filled\n",
        "        self.max_size = max_size\n",
        "        self.output_dir = output_dir\n",
        "        self.page = page\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        def upload_files(e):\n",
        "            uf = []\n",
        "            if self.file_picker.result != None and self.file_picker.result.files != None:\n",
        "                for f in self.file_picker.result.files:\n",
        "                  if self.page.web:\n",
        "                    uf.append(FilePickerUploadFile(f.name, upload_url=self.page.get_upload_url(f.name, 600)))\n",
        "                  else:\n",
        "                    on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                self.file_picker.upload(uf)\n",
        "        def file_picker_result(e: FilePickerResultEvent):\n",
        "            if e.files != None:\n",
        "              upload_files(e)\n",
        "        def on_upload_progress(e: FilePickerUploadEvent):\n",
        "            if e.progress == 1:\n",
        "              #TODO: Make save dir default to /root/uploads dir\n",
        "              if self.output_dir == None:\n",
        "                save_dir = root_dir\n",
        "              else:\n",
        "                save_dir = self.output_dir\n",
        "                if not os.path.exists(save_dir):\n",
        "                  os.mkdir(save_dir)\n",
        "              if not slash in e.file_name:\n",
        "                fname = os.path.join(save_dir, e.file_name)\n",
        "                fpath = os.path.join(save_dir, e.file_name)\n",
        "                self.pref[self.key] = e.file_name.rpartition('.')[0]\n",
        "              else:\n",
        "                fname = e.file_name\n",
        "                self.pref[self.key] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "              if self.max_size != None:\n",
        "                original_img = PILImage.open(fname)\n",
        "                width, height = original_img.size\n",
        "                width, height = scale_dimensions(width, height, self.max_size)\n",
        "                original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "                original_img.save(fpath)\n",
        "              self.textfield.value = fname\n",
        "              self.textfield.update()\n",
        "              self.pref[self.key] = fname\n",
        "              self.page.update()\n",
        "            else:\n",
        "              percent = int(e.progress * 100)\n",
        "              self.textfield.value = f\"  {percent}%\"\n",
        "              self.textfield.update()\n",
        "        def pick_file(e):\n",
        "            img_ext = [\"png\", \"PNG\", \"jpg\", \"jpeg\"]\n",
        "            vid_ext = [\"mp4\", \"avi\", \"MP4\", \"AVI\"]\n",
        "            gif_ext = [\"gif\", \"GIF\"]\n",
        "            aud_ext = [\"mp3\", \"wav\", \"MP3\", \"WAV\"]\n",
        "            font_ext = [\"ttf\", \"TTF\"]\n",
        "            ext = img_ext if self.ftype == \"image\" else vid_ext if self.ftype == \"video\" else font_ext if self.ftype == \"font\" else gif_ext if self.ftype == \"gif\" else aud_ext if self.ftype == \"audio\" else vid_ext+aud_ext if self.ftype == \"media\" else img_ext+vid_ext if self.ftype == \"picture\" else img_ext\n",
        "            name = self.key.replace(\"_\", \" \").title()\n",
        "            self.file_picker.pick_files(allow_multiple=False, allowed_extensions=ext, dialog_title=f\"Pick {name} File\")\n",
        "        def changed(e):\n",
        "            self.pref[self.key] = e.control.value\n",
        "            self.textfield.focus()\n",
        "        self.file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "        self.page.overlay.append(self.file_picker)\n",
        "        self.textfield = TextField(label=self.label, value=self.pref[self.key], expand=self.expand, filled=self.filled, autofocus=False, on_change=changed, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_file))\n",
        "        return self.textfield\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.textfield.value\n",
        "    @value.setter\n",
        "    def value(self, value):\n",
        "        self.pref[self.key] = value\n",
        "        self.textfield.value = value\n",
        "        self.textfield.update()\n",
        "\n",
        "class ImageButton(UserControl):\n",
        "    def __init__(self, src=\"\", subtitle=\"\", actions=[], center=True, width=None, height=None, data=None, src_base64=None, fit=ImageFit.SCALE_DOWN, show_subtitle=False, zoom=False, page=None):\n",
        "        super().__init__()\n",
        "        self.src = src\n",
        "        self.src_base64 = src_base64\n",
        "        self.subtitle = subtitle\n",
        "        self.actions = actions\n",
        "        self.center = center\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.data = data or src\n",
        "        self.fit = fit\n",
        "        self.show_subtitle = show_subtitle\n",
        "        self.zoom = zoom\n",
        "        self.page = page\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)\n",
        "        #Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True),\n",
        "        def download_image(e):\n",
        "            print(f\"{type(self.data)} {self.data}\")\n",
        "            if is_Colab:\n",
        "              self.page.launch_url(self.data)\n",
        "              '''from google.colab import files\n",
        "              if os.path.isfile(self.data):\n",
        "                  files.download(self.data)\n",
        "              else:\n",
        "                  time.sleep(4)\n",
        "                  files.download(self.data)'''\n",
        "            else:#, initial_directory=\n",
        "                self.file_saver.save_file(dialog_title=\"Save Image to...\", allowed_extensions=[\"png\", \"PNG\"], file_name=os.path.splitext(os.path.basename(self.data))[0], file_type=ft.FilePickerFileType.IMAGE)\n",
        "            self.page.snack_bar = SnackBar(content=Text(f\"üì≤  Downloading {self.data}... May have to Stop Script for Downloads to start in Colab.\"))\n",
        "            self.page.snack_bar.open = True\n",
        "            self.page.update()\n",
        "        def file_saver_result(e: FilePickerResultEvent):\n",
        "            if e.path != None:\n",
        "                shutil.copy(os.path.join(self.data), os.path.join(e.path))\n",
        "        \n",
        "        def copy_path(e):\n",
        "            self.page.set_clipboard(self.data)\n",
        "            self.page.snack_bar = SnackBar(content=Text(f\"üìã  {self.data} copied to clipboard... Paste as Init Image.\"))\n",
        "            self.page.snack_bar.open = True\n",
        "            self.page.update()\n",
        "        def image_details(e):\n",
        "          #TODO: Get size & meta\n",
        "            img = Img(src=self.data, gapless_playback=True)\n",
        "            #if self.zoom:\n",
        "            #img = PanZoom(img, self.width, self.height, width=self.width, height=self.height)\n",
        "            alert_msg(self.page, \"Image Details\", content=Column([Text(self.subtitle or self.data, selectable=True), img], horizontal_alignment=CrossAxisAlignment.CENTER), sound=False)\n",
        "        def delete_image(e):\n",
        "            #self.image = Container(content=None)\n",
        "            if os.path.exists(self.src):\n",
        "              os.remove(self.src)\n",
        "            if self.data != self.src:\n",
        "              if os.path.exists(self.data):\n",
        "                os.remove(self.data)\n",
        "            self.image.visible = False\n",
        "            self.image.update()\n",
        "            self.visible = False\n",
        "            self.update()\n",
        "            self.page.snack_bar = SnackBar(content=Text(f\"üóëÔ∏è  Deleted {self.data}...\"))\n",
        "            self.page.snack_bar.open = True\n",
        "            self.page.update()\n",
        "        def scale_width(width, height, max_width):\n",
        "            if width > max_width: return max_width, int(height / (width / max_width))\n",
        "            else: return width, height\n",
        "        #self.image = Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER)\n",
        "        if self.width == None:\n",
        "            if self.src_base64 != None:\n",
        "              self.image = Img(src_base64=self.src_base64, fit=self.fit, gapless_playback=True)\n",
        "            else:\n",
        "              self.image = Img(src=self.src, fit=self.fit, gapless_playback=True)\n",
        "        else:\n",
        "            self.width, self.height = scale_width(self.width, self.height, (self.page.width if self.page.web else self.page.window_width) - 28)\n",
        "            if self.src_base64 != None:\n",
        "              self.image = Img(src_base64=self.src_base64, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)\n",
        "            else:\n",
        "              self.image = Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)\n",
        "        if self.zoom:\n",
        "            self.image = PanZoom(self.image, self.width, self.height,\n",
        "                      width=self.width,\n",
        "                      height=self.height)#, on_click=lambda e: print(f'CLICK {e.local_x} {e.local_y}'))\n",
        "        self.column = Column([Row([PopupMenuButton(\n",
        "            items = [\n",
        "                PopupMenuItem(text=\"View Image\", icon=icons.FULLSCREEN, on_click=image_details),\n",
        "                PopupMenuItem(text=\"Copy Path\", icon=icons.CONTENT_PASTE, on_click=copy_path),\n",
        "                PopupMenuItem(text=\"Download Locally\" if is_Colab else \"Save As\", icon=icons.DOWNLOAD, on_click=download_image),\n",
        "                PopupMenuItem(text=\"Delete Image\", icon=icons.DELETE, on_click=delete_image),\n",
        "            ], expand=True, tooltip=\"üëÅÔ∏è\",\n",
        "            content=Row([self.image], expand=True),\n",
        "            #content=Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START),\n",
        "            data=self.src if self.data==None else self.data, width=self.width, height=self.height)],\n",
        "            alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START)], horizontal_alignment=CrossAxisAlignment.CENTER if self.center else CrossAxisAlignment.START)\n",
        "        if self.show_subtitle:\n",
        "            self.column.controls.append(Row([Text(self.subtitle)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START))\n",
        "        self.file_saver = FilePicker(on_result=file_saver_result)\n",
        "        self.page.overlay.append(self.file_saver)\n",
        "        return self.column\n",
        "\n",
        "\n",
        "class NumberPicker(UserControl):\n",
        "    def __init__(self, label=\"\", value=1, min=0, max=20, step=1, height=45, tooltip=None, on_change=None):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.step = step\n",
        "        self.label = label\n",
        "        self.height = height\n",
        "        self.tooltip = tooltip\n",
        "        self.on_change = on_change\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        def changed(e):\n",
        "            self.value = int(e.control.value)\n",
        "            if self.value < self.min:\n",
        "              self.value = self.min\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "            if self.value > self.max:\n",
        "              self.value = self.max\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "        def minus_click(e):\n",
        "            self.value = int(self.value)\n",
        "            if self.value > self.min:\n",
        "              self.value -= self.step\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "              e.control = self\n",
        "              if self.on_change is not None:\n",
        "                self.on_change(e)\n",
        "        def plus_click(e):\n",
        "            self.value = int(self.value)\n",
        "            if self.value < self.max:\n",
        "              self.value += self.step\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "              e.control = self\n",
        "              if self.on_change is not None:\n",
        "                self.on_change(e)\n",
        "        self.txt_number = TextField(value=str(self.value), text_align=TextAlign.CENTER, width=55, height=self.height, content_padding=padding.only(top=0), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "        label_text = Text(self.label) if not self.tooltip else Tooltip(message=self.tooltip, content=Text(self.label))\n",
        "        return Row([label_text, IconButton(icons.REMOVE, on_click=minus_click), self.txt_number, IconButton(icons.ADD, on_click=plus_click)], spacing=1)\n",
        "\n",
        "class SliderRow(UserControl):\n",
        "    def __init__(self, label=\"\", value=None, min=0, max=20, divisions=20, step=None, multiple=1, round=0, suffix=\"\", left_text=None, right_text=None, visible=True, tooltip=\"\", pref=None, key=None, expand=None, col=None, on_change=None, data=None):\n",
        "        super().__init__()\n",
        "        self.value = value or pref[key]\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        if step:\n",
        "            self.divisions = (max - min) / step\n",
        "        else:\n",
        "            self.divisions = divisions\n",
        "        self.multiple = multiple\n",
        "        self.label = label\n",
        "        self.round = round\n",
        "        self.suffix = suffix\n",
        "        self.left_text = left_text\n",
        "        self.right_text = right_text\n",
        "        self._visible = visible\n",
        "        self.slider_row = Container(content=None)\n",
        "        self.tooltip = tooltip\n",
        "        self.expand = expand\n",
        "        self.pref = pref\n",
        "        self.key = key\n",
        "        self.col = col\n",
        "        self.on_change = on_change\n",
        "        self.data = data\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        def change_slider(e):\n",
        "            self.value = round(e.control.value, self.round)\n",
        "            v = int(self.value) if self.round == 0 else float(self.value)\n",
        "            # TODO: Figure out how to update label text on web\n",
        "            #slider.label = f\"{v}{self.suffix}\"\n",
        "            #slider.update()\n",
        "            self.slider_value.value = f\"{v}{self.suffix}\"\n",
        "            self.slider_value.update()\n",
        "            self.slider_edit.value = f\"{v}\"\n",
        "            self.slider_edit.update()\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)\n",
        "        def changed(e):\n",
        "            try:\n",
        "              self.value = int(e.control.value) if self.round == 0 else round(float(e.control.value), self.round)\n",
        "            except ValueError:\n",
        "              e.control.value = str(self.value)\n",
        "              e.control.update()\n",
        "              return\n",
        "            if self.value < self.min:\n",
        "              self.value = self.min\n",
        "              self.slider_edit.value = self.value\n",
        "              self.slider_value.value = self.value\n",
        "              self.slider_value.update()\n",
        "            if self.value > self.max:\n",
        "              self.value = self.max\n",
        "              self.slider_edit.value = self.value\n",
        "              self.slider_value.value = self.value\n",
        "              self.slider_value.update()\n",
        "            if self.multiple != 1:\n",
        "              v = int(self.multiple * round(self.value / self.multiple))\n",
        "              if v != self.value:\n",
        "                self.slider_edit.value = v\n",
        "                self.slider_edit.update()\n",
        "                self.value = v\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "            self.slider.value = self.value\n",
        "            self.slider.label = f\"{self.value}{self.suffix}\"\n",
        "            self.slider.update()\n",
        "            self.slider_value.value = f\"{self.value}{self.suffix}\"\n",
        "            self.slider_value.update()\n",
        "            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)\n",
        "        def blur(e):\n",
        "            self.slider_edit.visible = False\n",
        "            slider_text.visible = True\n",
        "            self.slider_edit.update()\n",
        "            slider_text.update()\n",
        "            slider_label.value = f\"{self.label}: \"\n",
        "            slider_label.update()\n",
        "        def edit(e):\n",
        "            self.slider_edit.visible = True\n",
        "            slider_text.visible = False\n",
        "            slider_text.update()\n",
        "            self.slider_edit.update()\n",
        "            slider_label.value = f\"{self.label}\"\n",
        "            slider_label.update()\n",
        "            #self.slider_number = TextField(value=str(self.value), on_blur=blur, text_align=TextAlign.CENTER, width=55, height=50, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "            #self.update()\n",
        "            #e.control.update()\n",
        "            #e.page.update()\n",
        "        self.slider_edit = TextField(value=str(self.value), on_blur=blur, autofocus=True, visible=False, text_align=TextAlign.CENTER, width=51, height=45, content_padding=padding.only(top=6), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "        self.slider = Slider(min=float(self.min), max=float(self.max), divisions=int(self.divisions), round=self.round, label=\"{value}\" + self.suffix, value=float(self.pref[self.key]), tooltip=self.tooltip, expand=True, on_change=change_slider)\n",
        "        self.slider_value = Text(f\" {self.pref[self.key]}{self.suffix}\", weight=FontWeight.BOLD)\n",
        "        slider_text = GestureDetector(self.slider_value, on_tap=edit, mouse_cursor=ft.MouseCursor.PRECISE)\n",
        "        \n",
        "        slider_label = Text(f\"{self.label}: \")\n",
        "        left = Text(\"\", visible=False)\n",
        "        right = Text(\"\", visible=False)\n",
        "        if bool(self.left_text): left.value = self.left_text\n",
        "        if bool(self.right_text): right.value = self.right_text\n",
        "        self.slider_number = slider_text\n",
        "        self.slider_row = Container(Row([slider_label, slider_text, self.slider_edit, left, self.slider, right]), expand=self.expand, visible=self._visible, col=self.col)\n",
        "        return self.slider_row\n",
        "    def set_value(self, value):\n",
        "        self.value = value\n",
        "        self.slider.value = value\n",
        "        self.slider_edit.value = value\n",
        "        self.pref[self.key] = value\n",
        "        self.slider_value.value = f\" {self.pref[self.key]}{self.suffix}\"\n",
        "        self.slider_value.update()\n",
        "        self.slider.update()\n",
        "    def set_min(self, value):\n",
        "        self.min = value\n",
        "        self.slider.min = value\n",
        "    def set_max(self, value):\n",
        "        self.max = value\n",
        "        self.slider.max = value\n",
        "    def set_divisions(self, value):\n",
        "        self.divisions = value\n",
        "        self.slider.divisions = value\n",
        "    @property\n",
        "    def show(self):\n",
        "        return self._visible\n",
        "    @show.setter\n",
        "    def show(self, value):\n",
        "        self._visible = value\n",
        "        self.slider_row.visible = value\n",
        "        self.slider_row.update()\n",
        "    def update_slider(self):\n",
        "        self.slider.update()\n",
        "\n",
        "class Switcher(UserControl):\n",
        "    def __init__(self, label=\"\", value=False, tooltip=None, disabled=False, on_change=None, active_color=None, active_track_color=None, label_position=ft.LabelPosition.RIGHT):\n",
        "        super().__init__()\n",
        "        self.label = label\n",
        "        self.value = value\n",
        "        self.tooltip = tooltip\n",
        "        self.label_position=label_position\n",
        "        self.disabled = disabled\n",
        "        self.on_change = on_change\n",
        "        self.active_color = active_color if active_color != None else colors.PRIMARY_CONTAINER\n",
        "        self.active_track_color = active_track_color if active_track_color != None else colors.PRIMARY\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        self.switch = Switch(label=f\"  {self.label} \", value=self.value, disabled=self.disabled, active_color=self.active_color, active_track_color=self.active_track_color, label_position=self.label_position, on_change=self.on_change)\n",
        "        if self.tooltip != None:\n",
        "            return Tooltip(message=self.tooltip, content=self.switch)\n",
        "        else:\n",
        "            return self.switch\n",
        "\n",
        "class Installing(UserControl):\n",
        "    def __init__(self, message=\"\"):\n",
        "        super().__init__()\n",
        "        self.message = message\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        self.message_txt = Text(self.message, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)\n",
        "        self.details = Text(\"\")\n",
        "        self.progress = ProgressRing()\n",
        "        return Container(content=Row([self.progress, Container(content=None, width=1), self.message_txt, Container(content=None, expand=True), self.details]), padding=padding.only(left=9, bottom=4))\n",
        "    def set_message(self, msg):\n",
        "        self.message_txt.value = msg\n",
        "        self.message_txt.update()\n",
        "    def status(self, msg):\n",
        "        self.details.value = msg\n",
        "        self.details.update()\n",
        "    def show_progress(self, show):\n",
        "        self.progress.visible = show\n",
        "        self.progress.update()\n",
        "\n",
        "class Progress(UserControl):\n",
        "    def __init__(self, message=\"\"):\n",
        "        super().__init__()\n",
        "        self.message = message\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        self.message_txt = Text(self.message, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)\n",
        "        self.details = Text(\"\")\n",
        "        self.progress = ProgressBar(bar_height=8)\n",
        "        return Container(content=Column([Row([self.message_txt, Container(content=None, expand=True), self.details]), self.progress]), padding=padding.only(left=9, bottom=4))\n",
        "    def set_message(self, msg):\n",
        "        self.message_txt.value = msg\n",
        "        self.message_txt.update()\n",
        "    def status(self, msg):\n",
        "        self.details.value = msg\n",
        "        self.details.update()\n",
        "    def show_progress(self, show):\n",
        "        self.progress.visible = show\n",
        "        self.progress.update()\n",
        "\n",
        "def pip_install(packages, installer=None, print=False, prt=None, cwd=None, upgrade=False, q=False):\n",
        "    arg = \"\"\n",
        "    if upgrade: arg += \"--upgrade \"\n",
        "    if q: arg += \"-q \"\n",
        "    for package in packages.split():\n",
        "        try:\n",
        "            pkg = package\n",
        "            if '|' in pkg: pkg = pkg.rpartition('|')[2]\n",
        "            if '-' in pkg: pkg = pkg.replace('-', '_')\n",
        "            if '=' in pkg:\n",
        "                pkg = re.split('[=~]|>=|<=', pkg)[0]\n",
        "            exec(f\"import {pkg.lower()}\")\n",
        "        except ImportError:\n",
        "            if '|' in package: package = package.rpartition('|')[0]\n",
        "            if installer != None:\n",
        "                installer.status(f\"...installing {pkg}\")\n",
        "            try:\n",
        "                if prt == None:\n",
        "                    run_sp(f\"pip install {arg}{package}\", realtime=print, cwd=cwd)\n",
        "                else:\n",
        "                    console = RunConsole(show_progress=False)\n",
        "                    prt(console)\n",
        "                    console.run_process(f\"pip install {arg}{package}\", cwd=cwd)\n",
        "            except Exception as e:\n",
        "                print(f\"Error Installing {package}: {e}\")\n",
        "                if installer != None:\n",
        "                    installer.status(f\"...error installing {package}\")\n",
        "                pass\n",
        "            pass\n",
        "\n",
        "class RunConsole(UserControl):\n",
        "    def __init__(self, title=\"\", height=200, show_progress=True):\n",
        "        super().__init__()\n",
        "        self.title = title\n",
        "        self.height = height\n",
        "        self.show_progress = show_progress\n",
        "        self.build()\n",
        "    def add_text(self, text):\n",
        "        try:\n",
        "          self.column.controls.append(Text(text, style=\"titleSmall\", color=colors.ON_SURFACE_VARIANT))\n",
        "          self.column.update()\n",
        "        except Exception:\n",
        "          pass\n",
        "    def clear_last(self):\n",
        "        del self.column.controls[-1]\n",
        "        self.column.update()\n",
        "    def clear(self):\n",
        "        self.main_column.controls.clear()\n",
        "        self.main_column.update()\n",
        "    def run_process(self, cmd_str, cwd=None):\n",
        "        cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()\n",
        "        if cwd is None:\n",
        "          process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "        else:\n",
        "          process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )\n",
        "        while True:\n",
        "          realtime_output = process.stdout.readline()\n",
        "          if realtime_output == '' and process.poll() is not None:\n",
        "            break\n",
        "          if \"Downloading\" in realtime_output:\n",
        "              self.clear_last()\n",
        "          self.add_text(realtime_output.strip())\n",
        "          #self.column.controls.append(Text(realtime_output.strip(), style=\"titleSmall\", color=colors.TERTIARY))\n",
        "          #self.column.update()\n",
        "          sys.stdout.flush()\n",
        "        self.clear()\n",
        "    def build(self):\n",
        "        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)\n",
        "        self.column = Column([], spacing=1, scroll=ScrollMode.AUTO, auto_scroll=True) #, width=(page.width if page.web else page.window_width) - 20\n",
        "        self.container = Container(content=self.column, border_radius=ft.border_radius.all(16), height=self.height, bgcolor=colors.SURFACE_VARIANT, padding=ft.padding.symmetric(10, 12))\n",
        "        self.main_column = Column([])\n",
        "        if bool(self.title):\n",
        "            self.main_column.controls.append(Text(self.title, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3))\n",
        "        self.main_column.controls.append(self.container)\n",
        "        if self.show_progress:\n",
        "            self.main_column.controls.append(ProgressBar(bar_height=8))\n",
        "        return self.main_column\n",
        "\n",
        "def elapsed(start_time, end_time=None):\n",
        "    if end_time is None:\n",
        "        end_time = time.time()\n",
        "    return f\"{end_time-start_time:.0f}s\"\n",
        "\n",
        "def its(step_time):\n",
        "    if step_time < 1.0:\n",
        "        its = 1.0 / step_time\n",
        "        output = f\"{its:.1f} it/s\"\n",
        "    else:\n",
        "        sit = step_time\n",
        "        output = f\"{sit:.1f} s/it\"\n",
        "    return output\n",
        "\n",
        "def clear_line(column, lines=1, update=True):\n",
        "    for l in range(lines):\n",
        "        if len(column.controls) < 1: return\n",
        "        del column.controls[-1]\n",
        "    if update:\n",
        "        column.update()\n",
        "\n",
        "def prt_line(line, column, size=17, update=True, from_list=False, page=None):\n",
        "    if type(line) == str:\n",
        "      line = Text(line, size=size)\n",
        "    if from_list:\n",
        "      page.imageColumn.controls.append(line)\n",
        "      if update:\n",
        "        page.imageColumn.update()\n",
        "    else:\n",
        "      column.controls.append(line)\n",
        "      if update:\n",
        "        column.update()\n",
        "\n",
        "def nudge(column, page=None):\n",
        "    ''' Force an autoscroll column to go down. Mainly to show ProgressBar not scrolling to bottom.'''\n",
        "    column.controls.append(Container(content=Text(\" \")))\n",
        "    column.update()\n",
        "    time.sleep(0.1)\n",
        "    del column.controls[-1]\n",
        "    column.update()\n",
        "    if page is not None:\n",
        "        page.update()\n",
        "    \n",
        "def make_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "stop_thread = False\n",
        "def background_update(page):\n",
        "    global stop_thread\n",
        "    while not stop_thread:\n",
        "        update_stats(page)\n",
        "        time.sleep(prefs['stats_update'])\n",
        "\n",
        "def start_thread(page):\n",
        "    global stop_thread\n",
        "    stop_thread = False\n",
        "    polling_task = threading.Thread(target=background_update, args=(page,))\n",
        "    polling_task.daemon = True\n",
        "    polling_task.start()\n",
        "    \n",
        "def get_memory_stats(used=True):\n",
        "    global status\n",
        "    try:\n",
        "        #status['cpu_memory'] = psutil.virtual_memory().total / (1024 * 1024 * 1024)\n",
        "        #status['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)\n",
        "        status['cpu_used'] = psutil.virtual_memory().used / (1024 * 1024 * 1024)\n",
        "        status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device(\"cuda\")) / (1024 * 1024 * 1024)\n",
        "        gpu_mem = f\"{status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB\"\n",
        "    except Exception:\n",
        "        status['gpu_used'] = 0\n",
        "        gpu_mem = \"N/A\"\n",
        "        pass\n",
        "    if not used:\n",
        "        ram = float(status['cpu_memory']) - float(status['cpu_used'])\n",
        "        if gpu_mem == \"N/A\":\n",
        "            return f\"Free VRAM: None\", f\"Free RAM: {ram:.1f}GB\"\n",
        "        vram = float(status['gpu_memory']) - float(status['gpu_used'])\n",
        "        return f\"Free VRAM: {vram:.1f}GB\", f\"FreeRAM: {ram:.1f}GB\"\n",
        "    else:\n",
        "        return f\"VRAM: {gpu_mem}\", f\"RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB\"\n",
        "\n",
        "def update_stats(page):\n",
        "    try:\n",
        "        vram, ram = get_memory_stats(prefs['stats_used'])\n",
        "        page.stats.controls[0].value = vram\n",
        "        page.stats.controls[1].value = ram\n",
        "        page.stats.update()\n",
        "        page.appbar.update()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "def toggle_stats(page):\n",
        "    global prefs\n",
        "    prefs['stats_used'] = not prefs['stats_used']\n",
        "    page.stats_used.value = prefs['stats_used']\n",
        "    page.stats_used.update()\n",
        "    update_stats(page)\n",
        "    \n",
        "def interpolate_video(frames_dir, input_fps=None, output_fps=30, output_video=None, recursive_interpolation_passes=None, installer=None, denoise=False, sharpen=False, deflicker=False):\n",
        "    frame_interpolation_dir = os.path.join(root_dir, 'frame-interpolation')\n",
        "    saved_model_dir = os.path.join(frame_interpolation_dir, 'pretrained_models')\n",
        "    photos_dir = os.path.join(frame_interpolation_dir, 'photos')\n",
        "    interpolated = os.path.join(photos_dir, \"interpolated.mp4\")\n",
        "    if not os.path.exists(frame_interpolation_dir):\n",
        "        if installer != None:\n",
        "            installer.status(\"...cloning frame-interpolation\")\n",
        "        run_sp(\"git clone https://github.com/google-research/frame-interpolation\", cwd=root_dir, realtime=False) #pytti-tools\n",
        "    try:\n",
        "        import ffmpeg\n",
        "    except ImportError as e:\n",
        "        installer.status(\"...installing ffmpeg\")\n",
        "        run_sp(\"pip install -q ffmpeg-python\", realtime=False)\n",
        "        pass\n",
        "    try:\n",
        "        import frame_interpolation\n",
        "    except ModuleNotFoundError:\n",
        "        if installer != None:\n",
        "            installer.status(\"...installing frame-interpolation requirements\")\n",
        "        #run_sp(f\"pip install -r requirements.txt\", cwd=frame_interpolation_dir, realtime=True)\n",
        "        #run_sp(f\"pip install .\", cwd=frame_interpolation_dir, realtime=False)\n",
        "        pip_install(\"tensorflow tensorflow-datasets tensorflow-addons absl-py==0.12.0 gin-config==0.5.0 parameterized==0.8.1 mediapy scikit-image apache-beam==2.34.0 google-cloud-bigquery-storage==1.1.0 natsort==8.1.0 gdown tqdm\", upgrade=True, installer=installer)\n",
        "    pip_install(\"gdown\", upgrade=True, installer=installer)\n",
        "    if not os.path.exists(saved_model_dir):\n",
        "        run_sp(f\"gdown 1C1YwOo293_yrgSS8tAyFbbVcMeXxzftE -O pretrained_models-20220214T214839Z-001.zip\", cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6\n",
        "        run_sp('unzip -o \"pretrained_models-20220214T214839Z-001.zip\"', cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6\n",
        "        run_sp('rm -rf pretrained_models-20220214T214839Z-001.zip', cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6\n",
        "    saved_model = 'pretrained_models/film_net/Style/saved_model' #os.path.join(saved_model_dir, 'film_net','Style','saved_model') #\n",
        "    installer.status(\"...copying photo frames\")\n",
        "    for f in os.listdir(photos_dir):\n",
        "        os.remove(os.path.join(photos_dir, f))\n",
        "    if isinstance(frames_dir, list):\n",
        "        for n, f in enumerate(frames_dir):\n",
        "            if isinstance(f, str):\n",
        "                if f.endswith('png') or f.endswith('jpg'):\n",
        "                    shutil.copy(f, os.path.join(photos_dir, f))\n",
        "            else:\n",
        "                f.save(os.path.join(photos_dir, str(n).zfill(4) + '.png'))\n",
        "    else:\n",
        "        for f in os.listdir(frames_dir):\n",
        "            if f.endswith('png') or f.endswith('jpg'):\n",
        "                shutil.copy(os.path.join(frames_dir, f), os.path.join(photos_dir, f))\n",
        "    #run_sp(f\"mkdir -p frames\", cwd=frames_dir, realtime=False)\n",
        "    if bool(input_fps):\n",
        "        recursive_interpolation_passes = int((output_fps - input_fps) / input_fps)\n",
        "    else:\n",
        "        recursive_interpolation_passes = recursive_interpolation_passes or 1\n",
        "    if installer != None:\n",
        "        installer.status(\"...running frame-interpolation\")\n",
        "    run_sp(f\"python -m eval.interpolator_cli --model_path {saved_model} --pattern 'photos' --fps {output_fps} --times_to_interpolate {recursive_interpolation_passes} --output_video\", cwd=frame_interpolation_dir, realtime=False)\n",
        "    if os.path.exists(interpolated):\n",
        "        if denoise or sharpen or deflicker:\n",
        "            video = ffmpeg.input(interpolated)\n",
        "            if deflicker:\n",
        "                installer.status(\"...deflicker\")\n",
        "                video = video.filter(\"deflicker\")\n",
        "            if sharpen:\n",
        "                installer.status(\"...sharpen\")\n",
        "                video = video.filter(\"sharpen\")\n",
        "            if denoise:\n",
        "                installer.status(\"...denoise\")\n",
        "                video = video.filter(\"denoise\")\n",
        "            video.output(interpolated)\n",
        "        if output_video != None:\n",
        "            if not output_video.endswith('mp4'):\n",
        "                output_video = os.path.join(output_video, \"interpolated.mp4\")\n",
        "            shutil.move(interpolated, output_video)\n",
        "            return output_video\n",
        "        else:\n",
        "            return interpolated\n",
        "    else:\n",
        "        print(f\"Failed to save video {interpolated}\")\n",
        "        return \"\"\n",
        "\n",
        "def frames_to_video(frames_dir, pattern=\"%03d.png\", input_fps=None, output_fps=30, output_video=None, installer=None, denoise=False, sharpen=False, deflicker=False):\n",
        "    import ffmpeg\n",
        "    def stat(msg):\n",
        "        if installer is not None: installer.status(f\"...{msg}\")\n",
        "    stat(\"frames_to_video\")\n",
        "    video = ffmpeg.input(frames_dir + slash + pattern, pattern_type=\"glob\", framerate=input_fps or output_fps)\n",
        "    video = video.pix_fmt('yuv420p')\n",
        "    if input_fps is not None and input_fps != output_fps:\n",
        "        stat(\"changing fps\")\n",
        "        video = video.filter(\"setpts=pts/TIMEBASE*%f\" % (output_fps / input_fps))\n",
        "        video = video.framerate(output_fps)\n",
        "    if deflicker:\n",
        "        stat(\"deflicker\")\n",
        "        video = video.filter(\"deflicker\")\n",
        "    if sharpen:\n",
        "        stat(\"sharpen\")\n",
        "        video = video.filter(\"sharpen\")\n",
        "    if denoise:\n",
        "        stat(\"denoise\")\n",
        "        video = video.filter(\"denoise\")\n",
        "    if output_video != None:\n",
        "        if not output_video.endswith('mp4'):\n",
        "            output_video = os.path.join(output_video, \"interpolated.mp4\")\n",
        "    else:\n",
        "        output_video = os.path.join(os.path.dirname(frames_dir), \"interpolated.mp4\")\n",
        "    video.output(output_video)\n",
        "    return output_video\n",
        "\n",
        "\n",
        "def scale_video(video_path, to, max_size):\n",
        "    try:\n",
        "        from moviepy.editor import VideoFileClip\n",
        "    except Exception:\n",
        "        pip_install(\"moviepy\")\n",
        "        from moviepy.editor import VideoFileClip\n",
        "        pass\n",
        "    video = VideoFileClip(video_path)\n",
        "    original_width, original_height = video.size\n",
        "    width, height = scale_dimensions(original_width, original_height, max_size)\n",
        "    resized_video = video.resize(width=width, height=height)\n",
        "    resized_video.write_videofile(to)\n",
        "    return width, height\n",
        "\n",
        "class VideoPlayer(UserControl):\n",
        "    def __init__(self, video_file=\"\", width=500, height=500):\n",
        "        super().__init__()\n",
        "        self.video_file = video_file\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        import threading\n",
        "        import time\n",
        "        import base64\n",
        "        cap = cv2.VideoCapture(self.video_file)\n",
        "        b64_string = None\n",
        "        blank_img = np.zeros((self.width, self.height, 1), dtype = \"uint8\")#cv2.imencode('.jpg', frame)\n",
        "        b64_string = base64.b64encode(blank_img[1]).decode('utf-8')\n",
        "        image_box = ft.Image(src_base64=b64_string, width=self.width, height=self.height)\n",
        "        video_container = ft.Container(image_box, alignment=ft.alignment.center, expand=True)\n",
        "        def update_images():\n",
        "            while(cap.isOpened()):\n",
        "                # Capture frame-by-frame\n",
        "                ret, frame = cap.read()\n",
        "                if ret == True:\n",
        "                    # encode the resulting frame\n",
        "                    jpg_img = cv2.imencode('.jpg', frame)\n",
        "                    b64_string = base64.b64encode(jpg_img[1]).decode('utf-8')\n",
        "                    image_box.src_base64 = b64_string\n",
        "                    self.page.update()\n",
        "                else:\n",
        "                    break\n",
        "                time.sleep(1/115)\n",
        "            # when video is finished\n",
        "            #self.video_container.content.clean()\n",
        "            #self.video_container.content = ft.Text(\"Video Ended\", size=20)\n",
        "            self.page.update()\n",
        "        update_image_thread = threading.Thread(target=update_images)\n",
        "        update_image_thread.daemon = True\n",
        "        update_image_thread.start()\n",
        "        return video_container\n",
        "\n",
        "class AudioPlayer(UserControl):\n",
        "    def __init__(self, src=\"\", display=\"\", data=None, autoplay=False, page=None):\n",
        "        super().__init__()\n",
        "        self.src = src\n",
        "        self.display = display\n",
        "        self.data = data if data != None else src\n",
        "        self.page = page\n",
        "        self.state = \"stopped\"\n",
        "        self.audio = Audio(src=self.src, autoplay=autoplay, on_state_changed=self.state_changed)\n",
        "        self.page.overlay.append(self.audio)\n",
        "        self.build()\n",
        "    def play_audio(self, e):\n",
        "        if self.state == \"playing\":\n",
        "            self.audio.pause()\n",
        "        elif self.state == \"paused\":\n",
        "            self.audio.resume()\n",
        "        else:\n",
        "            self.audio.play()\n",
        "    def state_changed(self, e):\n",
        "        self.state = e.data\n",
        "        #print(self.state)\n",
        "        if e.data == \"completed\" or e.data == \"paused\":\n",
        "            self.icon = icons.PLAY_CIRCLE_FILLED\n",
        "        if e.data == \"playing\":\n",
        "            self.icon = icons.PAUSE\n",
        "        self.button.icon = self.icon\n",
        "        self.button.update()\n",
        "    def did_mount(self):\n",
        "        self.page.update()\n",
        "        try:\n",
        "            duration = self.audio.get_duration()\n",
        "        except Exception:\n",
        "            duration = 0\n",
        "            pass\n",
        "        #print(duration)\n",
        "        if duration > 0:\n",
        "            dt = datetime.datetime.fromtimestamp(duration / 1000)\n",
        "            dur = dt.strftime('%H:%M:%S.%f')[:-3]\n",
        "            self.button.tooltip = f\"Duration: {dur}\"\n",
        "            self.button.update()\n",
        "    def build(self):\n",
        "        self.icon = icons.PLAY_CIRCLE_FILLED\n",
        "        dur = \"\"#dt.strftime('%H:%M:%S.%f')[:-3]\n",
        "        self.button = IconButton(icon=self.icon, icon_size=48, tooltip=f\"Duration: {dur}\", on_click=self.play_audio, data=self.data)\n",
        "        self.row = Row([self.button, Text(self.display)])\n",
        "        return self.row\n",
        "\n",
        "port = 8000\n",
        "if tunnel_type == \"ngrok\":\n",
        "  #if bool(url):\n",
        "  #  public_url = url\n",
        "  #else:\n",
        "    from pyngrok import conf, ngrok\n",
        "    conf.get_default().ngrok_version = \"v3\"\n",
        "    #public_url = ngrok.connect(port = str(port)).public_url\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "elif tunnel_type == \"localtunnel\":\n",
        "  if False:\n",
        "  #if bool(url):\n",
        "    public_url = url\n",
        "  else:\n",
        "    import re\n",
        "    #print(run_sp('lt -p 80', realtime=False))\n",
        "    localtunnel = subprocess.Popen(['lt', '--port', port, 'http'], stdout=subprocess.PIPE)\n",
        "    url = str(localtunnel.stdout.readline())\n",
        "    public_url = (re.search(\"(?P<url>https?:\\/\\/[^\\s]+loca.lt)\", url).group(\"url\"))\n",
        "else: public_url=\"\"\n",
        "from IPython.display import Javascript\n",
        "if bool(public_url):\n",
        "    import urllib\n",
        "    if auto_launch_website:\n",
        "        display(Javascript('window.open(\"{url}\");'.format(url=public_url)))\n",
        "        time.sleep(0.7)\n",
        "        clear_output()\n",
        "    print(\"\\nOpen URL in browser to launch app in tab: \" + str(public_url))\n",
        "    if tunnel_type == \"localtunnel\":\n",
        "        print(\"Copy/Paste the Password/Enpoint IP for localtunnel:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "def close_tab():\n",
        "  display(Javascript(\"window.close('', '_parent', '');\"))\n",
        "#await google.colab.kernel.proxyPort(%s)\n",
        "# Still not working to display app in Colab console, but tried.\n",
        "def show_port(adr, height=500):\n",
        "  display(Javascript(\"\"\"\n",
        "  (async ()=>{\n",
        "    fm = document.createElement('iframe')\n",
        "    fm.src = '%s'\n",
        "    fm.width = '100%%'\n",
        "    fm.height = '%d'\n",
        "    fm.frameBorder = 0\n",
        "    document.body.append(fm)\n",
        "  })();\n",
        "  \"\"\" % (adr, height) ))\n",
        "#pip_install(\"PyDrive2|pydrive2\")\n",
        "#from pydrive2.auth import GoogleAuth\n",
        "#gauth = GoogleAuth()\n",
        "#gauth.LocalWebserverAuth()\n",
        "#r = requests.get('http://localhost:4040/api/tunnels')\n",
        "#url = r.json()['tunnels'][0]['public_url']\n",
        "#print(url)\n",
        "#await google.colab.kernel.proxyPort(%s)\n",
        "#get_ipython().system_raw('python3 -m http.server 8888 &')\n",
        "#get_ipython().system_raw('./ngrok http 8501 &')\n",
        "#show_port(public_url.public_url)\n",
        "#run_sp(f'python -m webbrowser -t \"{public_url.public_url}\"')\n",
        "#webbrowser.open(public_url.public_url, new=0, autoraise=True)\n",
        "#webbrowser.open_new_tab(public_url.public_url)\n",
        "#ft.app(target=main, view=flet.WEB_BROWSER, port=port, host=socket_host)\n",
        "#ft.app(target=main, view=flet.WEB_BROWSER, port=80, host=public_url.public_url)\n",
        "#ft.app(target=main, view=flet.WEB_BROWSER, port=port, host=\"0.0.0.0\")\n",
        "#ft.app(target=main, view=ft.WEB_BROWSER, port=80, assets_dir=root_dir, upload_dir=root_dir, web_renderer=\"html\")\n",
        "#import logging\n",
        "#logging.getLogger(\"flet_core\").setLevel(logging.DEBUG)\n",
        "#logging.basicConfig(level=logging.DEBUG)\n",
        "if tunnel_type == \"desktop\":\n",
        "  ft.app(target=main, assets_dir=root_dir, upload_dir=root_dir)\n",
        "else:\n",
        "  ft.app(target=main, view=ft.WEB_BROWSER, port=port, assets_dir=os.path.abspath(root_dir), upload_dir=os.path.abspath(root_dir), use_color_emoji=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfi-vDp6A8it"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "raise(RuntimeError(\"Stopping Execution for Run All to end...\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zE0dN7OiVBz0"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚ñ∂Ô∏è **Inpaint Dream Mask Maker** - GradIO WebUI (temporary until I make better Mask Painter in Flet UI)\n",
        "#@markdown Prepared your B&W ***mask_image*** and resizes ***init_image*** to add to your batch prompts list easily.\n",
        "max_image_resolution = 960 #@param {'type':'slider', min:256, max:1024, step:64}\n",
        "init_image_strength_override = False #param {type:'boolean'}\n",
        "import string\n",
        "#import PIL.ImageOps\n",
        "#@title ## üì• **Install GradIO Web Server** (_if_ you want to use Inpaint Helper interface)\n",
        "try:\n",
        "  import gradio as gr\n",
        "except ImportError:\n",
        "  run_sp(\"pip install gradio datasets tqdm\")\n",
        "  pass\n",
        "finally:\n",
        "  import gradio as gr\n",
        "\n",
        "#import numpy as np\n",
        "import torch\n",
        "from torch import autocast\n",
        "import requests\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "from io import BytesIO\n",
        "clear_output()\n",
        "#print(f'{Color.BLUE}Successfully Installed GradIO...{Color.END}')\n",
        "\n",
        "image = None\n",
        "def multiple_of_64(x):\n",
        "    return int(round(x/64)*64)\n",
        "def multiple_of_8(x):\n",
        "    return int(round(x/8)*8)\n",
        "def scale_dimensions(width, height):\n",
        "  max = int(max_image_resolution)\n",
        "  r_width = width\n",
        "  r_height = height\n",
        "  if width < max and height < max:\n",
        "    if width >= height:\n",
        "      ratio = max / width\n",
        "      r_width = max\n",
        "      r_height = int(height * ratio)\n",
        "    else:\n",
        "      ratio = max / height\n",
        "      r_height = max\n",
        "      r_width = int(width * ratio)\n",
        "    width = r_width\n",
        "    height = r_height\n",
        "  if width >= height:\n",
        "    if width > max:\n",
        "      r_width = max\n",
        "      r_height = int(height * (max/width))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  else:\n",
        "    if height > max:\n",
        "      r_height = max\n",
        "      r_width = int(width * (max/height))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  return multiple_of_64(r_width), multiple_of_64(r_height)\n",
        "\n",
        "def format_filename(s):\n",
        "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
        "    filename = ''.join(c for c in s if c in valid_chars)\n",
        "    filename = filename.replace(' ','_')\n",
        "    return filename[:24]\n",
        "\n",
        "def available_file(folder, name, idx, mask=False):\n",
        "  available = False\n",
        "  mask_str = '-mask' if mask else ''\n",
        "  while not available:\n",
        "    if os.path.isfile(os.path.join(folder, f'{name}-{idx}{mask_str}.png')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}{mask_str}.png')\n",
        "\n",
        "def available_folder(folder, name, idx):\n",
        "  available = False\n",
        "  while not available:\n",
        "    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}')\n",
        "  \n",
        "def create(prompt, img, option):\n",
        "    global prefs\n",
        "    mask_img = img[\"mask\"]\n",
        "    init_img = img[\"image\"]\n",
        "    w, h = init_img.size\n",
        "    w, h = scale_dimensions(w, h)\n",
        "    mask_img = mask_img.convert(\"1\")\n",
        "    mask_img = mask_img.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    if option == \"Replace everything else\":\n",
        "       mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "       #mask = mask.convert('1')\n",
        "    init_img = init_img.convert(\"RGB\")\n",
        "    init_img = init_img.resize((w, h))\n",
        "\n",
        "    fname = format_filename(prompt)\n",
        "    mask_name = f'{root_dir}{fname}-mask.png'\n",
        "    img_name = f'{root_dir}{fname}.png'\n",
        "    if os.path.isfile(img_name):\n",
        "      img_name = available_file(root_dir, fname, 1, False)\n",
        "    if os.path.isfile(mask_name):\n",
        "      mask_name = available_file(root_dir, fname, 1, True)\n",
        "    mask_img.save(mask_name)\n",
        "    init_img.save(img_name)\n",
        "    #print(str(img.name))\n",
        "    #print(str(img.filename))\n",
        "    image_strength = f'init_image_strength=0.4, ' if init_image_strength_override else ''\n",
        "    print(f'    Dream(\"{prompt}\", init_image=\"{img_name}\", mask_image=\"{mask_name}\", {image_strength}width={w}, height={h}),')\n",
        "    if prefs is not None:\n",
        "      prompt_pref = prefs['prompt_list']\n",
        "      new_dream = {'prompt': prompt, 'init_image': img_name, 'mask_image': mask_name, 'width': w, 'height': h}\n",
        "      if init_image_strength_override:\n",
        "        new_dream['init_image_strength':]\n",
        "      prompt_pref.append(new_dream)\n",
        "      prefs['prompt_list'] = prompt_pref\n",
        "      print(\"Added to your AEIONic Diffusion Deluxe Prompts List (in saved preferences)\")\n",
        "    #display(mask)\n",
        "    return f'Dream(\"{prompt}\", init_image=\"{img_name}\", mask_image=\"{mask_name}\", {image_strength}width={w}, height={h}),'\n",
        "\n",
        "block = gr.Blocks(css=\".container { max-width: 1200px; margin: auto; }\")\n",
        "with block as demo:\n",
        "    gr.Markdown(\"<h1><center>Stable Diffusion Inpaint Dream Mask Maker</center></h1>\")\n",
        "    with gr.Group():\n",
        "        with gr.Row().style(mobile_collapse=False, equal_height=False):\n",
        "                text = gr.Textbox(\n",
        "                    label=\"Enter your Inpaint prompt\", show_label=True, max_lines=1\n",
        "                ).style(\n",
        "                    border=(True, False, True, True),\n",
        "                    rounded=(True, False, False, True),\n",
        "                    container=False,\n",
        "                )\n",
        "        with gr.Row().style(mobile_collapse=False, equal_height=True):\n",
        "          option = gr.Radio(label=\"Masking\", choices=[\"Replace selection\", \"Replace everything else\"], value=\"Replace selection\")\n",
        "          btn = gr.Button(\"Prepare Dream\").style(\n",
        "                    margin=True,\n",
        "                    rounded=(True, True, True, True),\n",
        "                )\n",
        "        image = gr.Image(\n",
        "            tool=\"sketch\",\n",
        "            source=\"upload\",\n",
        "            #image_mode=\"L\", shape=(42, 42), invert_colors=True,\n",
        "            label=\"Input Init Image\",\n",
        "            type=\"pil\"\n",
        "            #type=\"numpy\"\n",
        "        )\n",
        "        gallery = gr.Markdown('')\n",
        "        #gallery = gr.Gallery(label=\"Generated mask_image\", show_label=False).style(grid=[2], height=ScrollMode.AUTO)\n",
        "        text.submit(create, inputs=[text,image,option], outputs=gallery)\n",
        "        btn.click(create, inputs=[text,image,option], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "za8wL7yUJJYI"
      ],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aecaad1160391d286dea8adfafabc0106aa931e7deb6a40f00e2f89f1c4ee412"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
